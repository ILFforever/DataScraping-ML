[
    {
        "id": "http://arxiv.org/abs/2412.09517v1",
        "title": "Geometric Deep Learning for Realized Covariance Matrix Forecasting",
        "abstract": "Traditional methods employed in matrix volatility forecasting often overlook\nthe inherent Riemannian manifold structure of symmetric positive definite\nmatrices, treating them as elements of Euclidean space, which can lead to\nsuboptimal predictive performance. Moreover, they often struggle to handle\nhigh-dimensional matrices. In this paper, we propose a novel approach for\nforecasting realized covariance matrices of asset returns using a\nRiemannian-geometry-aware deep learning framework. In this way, we account for\nthe geometric properties of the covariance matrices, including possible\nnon-linear dynamics and efficient handling of high-dimensionality. Moreover,\nbuilding upon a Fr\\'echet sample mean of realized covariance matrices, we are\nable to extend the HAR model to the matrix-variate. We demonstrate the efficacy\nof our approach using daily realized covariance matrices for the 50 most\ncapitalized companies in the S&P 500 index, showing that our method outperforms\ntraditional approaches in terms of predictive accuracy.",
        "authors": [
            "Andrea Bucci",
            "Michele Palma",
            "Chao Zhang"
        ],
        "categories": "q-fin.CP",
        "published": "2024-12-12T18:01:48Z",
        "updated": "2024-12-12T18:01:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.09430v1",
        "title": "A Kernel Score Perspective on Forecast Disagreement and the Linear Pool",
        "abstract": "The variance of a linearly combined forecast distribution (or linear pool)\nconsists of two components: The average variance of the component distributions\n(`average uncertainty'), and the average squared difference between the\ncomponents' means and the pool's mean (`disagreement'). This paper shows that\nsimilar decompositions hold for a class of uncertainty measures that can be\nconstructed as entropy functions of kernel scores. The latter are a rich family\nof scoring rules that covers point and distribution forecasts for univariate\nand multivariate, discrete and continuous settings. The results in this paper\nare useful for two reasons. First, they provide a generic description of the\nuncertainty implicit in the linear pool. Second, they suggest principled\nmeasures of forecast disagreement in a wide range of applied settings.",
        "authors": [
            "Fabian Kr\u00fcger"
        ],
        "categories": "econ.EM",
        "published": "2024-12-12T16:35:24Z",
        "updated": "2024-12-12T16:35:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.09226v1",
        "title": "The Global Carbon Budget as a cointegrated system",
        "abstract": "The Global Carbon Budget, maintained by the Global Carbon Project, summarizes\nEarth's global carbon cycle through four annual time series beginning in 1959:\natmospheric CO$_2$ concentrations, anthropogenic CO$_2$ emissions, and CO$_2$\nuptake by land and ocean. We analyze these four time series as a multivariate\n(cointegrated) system. Statistical tests show that the four time series are\ncointegrated with rank three and identify anthropogenic CO$_2$ emissions as the\nsingle stochastic trend driving the nonstationary dynamics of the system. The\nthree cointegrated relations correspond to the physical relations that the\nsinks are linearly related to atmospheric concentrations and that the change in\nconcentrations equals emissions minus the combined uptake by land and ocean.\nFurthermore, likelihood ratio tests show that a parametrically restricted\nerror-correction model that embodies these physical relations and accounts for\nthe El-Ni\\~no/Southern Oscillation cannot be rejected on the data. Finally,\nprojections based on this model, using Shared Socioeconomic Pathways scenarios,\nyield results consistent with established climate science.",
        "authors": [
            "Mikkel Bennedsen",
            "Eric Hillebrand",
            "Morten \u00d8rregaard Nielsen"
        ],
        "categories": "stat.AP",
        "published": "2024-12-12T12:28:38Z",
        "updated": "2024-12-12T12:28:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.08831v1",
        "title": "Panel Stochastic Frontier Models with Latent Group Structures",
        "abstract": "Stochastic frontier models have attracted significant interest over the years\ndue to their unique feature of including a distinct inefficiency term alongside\nthe usual error term. To effectively separate these two components, strong\ndistributional assumptions are often necessary. To overcome this limitation,\nnumerous studies have sought to relax or generalize these models for more\nrobust estimation. In line with these efforts, we introduce a latent group\nstructure that accommodates heterogeneity across firms, addressing not only the\nstochastic frontiers but also the distribution of the inefficiency term. This\nframework accounts for the distinctive features of stochastic frontier models,\nand we propose a practical estimation procedure to implement it. Simulation\nstudies demonstrate the strong performance of our proposed method, which is\nfurther illustrated through an application to study the cost efficiency of the\nU.S. commercial banking sector.",
        "authors": [
            "Kazuki Tomioka",
            "Thomas T. Yang",
            "Xibin Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-12-12T00:12:42Z",
        "updated": "2024-12-12T00:12:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.07649v1",
        "title": "Machine Learning the Macroeconomic Effects of Financial Shocks",
        "abstract": "We propose a method to learn the nonlinear impulse responses to structural\nshocks using neural networks, and apply it to uncover the effects of US\nfinancial shocks. The results reveal substantial asymmetries with respect to\nthe sign of the shock. Adverse financial shocks have powerful effects on the US\neconomy, while benign shocks trigger much smaller reactions. Instead, with\nrespect to the size of the shocks, we find no discernible asymmetries.",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Karin Klieber",
            "Massimiliano Marcellino"
        ],
        "categories": "econ.EM",
        "published": "2024-12-10T16:34:56Z",
        "updated": "2024-12-10T16:34:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.07352v1",
        "title": "Inference after discretizing unobserved heterogeneity",
        "abstract": "We consider a linear panel data model with nonseparable two-way unobserved\nheterogeneity corresponding to a linear version of the model studied in\nBonhomme et al. (2022). We show that inference is possible in this setting\nusing a straightforward two-step estimation procedure inspired by existing\ndiscretization approaches. In the first step, we construct a discrete\napproximation of the unobserved heterogeneity by (k-means) clustering\nobservations separately across the individual ($i$) and time ($t$) dimensions.\nIn the second step, we estimate a linear model with two-way group fixed effects\nspecific to each cluster. Our approach shares similarities with methods from\nthe double machine learning literature, as the underlying moment conditions\nexhibit the same type of bias-reducing properties. We provide a theoretical\nanalysis of a cross-fitted version of our estimator, establishing its\nasymptotic normality at parametric rate under the condition\n$\\max(N,T)=o(\\min(N,T)^3)$. Simulation studies demonstrate that our methodology\nachieves excellent finite-sample performance, even when $T$ is negligible with\nrespect to $N$.",
        "authors": [
            "Jad Beyhum",
            "Martin Mugnier"
        ],
        "categories": "econ.EM",
        "published": "2024-12-10T09:43:34Z",
        "updated": "2024-12-10T09:43:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.07184v1",
        "title": "Automatic Doubly Robust Forests",
        "abstract": "This paper proposes the automatic Doubly Robust Random Forest (DRRF)\nalgorithm for estimating the conditional expectation of a moment functional in\nthe presence of high-dimensional nuisance functions. DRRF combines the\nautomatic debiasing framework using the Riesz representer (Chernozhukov et al.,\n2022) with non-parametric, forest-based estimation methods for the conditional\nmoment (Athey et al., 2019; Oprescu et al., 2019). In contrast to existing\nmethods, DRRF does not require prior knowledge of the form of the debiasing\nterm nor impose restrictive parametric or semi-parametric assumptions on the\ntarget quantity. Additionally, it is computationally efficient for making\npredictions at multiple query points and significantly reduces runtime compared\nto methods such as Orthogonal Random Forest (Oprescu et al., 2019). We\nestablish the consistency and asymptotic normality results of DRRF estimator\nunder general assumptions, allowing for the construction of valid confidence\nintervals. Through extensive simulations in heterogeneous treatment effect\n(HTE) estimation, we demonstrate the superior performance of DRRF over\nbenchmark approaches in terms of estimation accuracy, robustness, and\ncomputational efficiency.",
        "authors": [
            "Zhaomeng Chen",
            "Junting Duan",
            "Victor Chernozhukov",
            "Vasilis Syrgkanis"
        ],
        "categories": "stat.ME",
        "published": "2024-12-10T04:45:50Z",
        "updated": "2024-12-10T04:45:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.07031v1",
        "title": "Large Language Models: An Applied Econometric Framework",
        "abstract": "Large language models (LLMs) are being used in economics research to form\npredictions, label text, simulate human responses, generate hypotheses, and\neven produce data for times and places where such data don't exist. While these\nuses are creative, are they valid? When can we abstract away from the inner\nworkings of an LLM and simply rely on their outputs? We develop an econometric\nframework to answer this question. Our framework distinguishes between two\ntypes of empirical tasks. Using LLM outputs for prediction problems (including\nhypothesis generation) is valid under one condition: no \"leakage\" between the\nLLM's training dataset and the researcher's sample. Using LLM outputs for\nestimation problems to automate the measurement of some economic concept\n(expressed by some text or from human subjects) requires an additional\nassumption: LLM outputs must be as good as the gold standard measurements they\nreplace. Otherwise estimates can be biased, even if LLM outputs are highly\naccurate but not perfectly so. We document the extent to which these conditions\nare violated and the implications for research findings in illustrative\napplications to finance and political economy. We also provide guidance to\nempirical researchers. The only way to ensure no training leakage is to use\nopen-source LLMs with documented training data and published weights. The only\nway to deal with LLM measurement error is to collect validation data and model\nthe error structure. A corollary is that if such conditions can't be met for a\ncandidate LLM application, our strong advice is: don't.",
        "authors": [
            "Jens Ludwig",
            "Sendhil Mullainathan",
            "Ashesh Rambachan"
        ],
        "categories": "econ.EM",
        "published": "2024-12-09T22:37:48Z",
        "updated": "2024-12-09T22:37:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.06688v1",
        "title": "Probabilistic Targeted Factor Analysis",
        "abstract": "We develop a probabilistic variant of Partial Least Squares (PLS) we call\nProbabilistic Targeted Factor Analysis (PTFA), which can be used to extract\ncommon factors in predictors that are useful to predict a set of predetermined\ntarget variables. Along with the technique, we provide an efficient\nexpectation-maximization (EM) algorithm to learn the parameters and forecast\nthe targets of interest. We develop a number of extensions to missing-at-random\ndata, stochastic volatility, and mixed-frequency data for real-time\nforecasting. In a simulation exercise, we show that PTFA outperforms PLS at\nrecovering the common underlying factors affecting both features and target\nvariables delivering better in-sample fit, and providing valid forecasts under\ncontamination such as measurement error or outliers. Finally, we provide two\napplications in Economics and Finance where PTFA performs competitively\ncompared with PLS and Principal Component Analysis (PCA) at out-of-sample\nforecasting.",
        "authors": [
            "Miguel C. Herculano",
            "Santiago Montoya-Bland\u00f3n"
        ],
        "categories": "econ.EM",
        "published": "2024-12-09T17:30:33Z",
        "updated": "2024-12-09T17:30:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.06092v1",
        "title": "Density forecast transformations",
        "abstract": "The popular choice of using a $direct$ forecasting scheme implies that the\nindividual predictions do not contain information on cross-horizon dependence.\nHowever, this dependence is needed if the forecaster has to construct, based on\n$direct$ density forecasts, predictive objects that are functions of several\nhorizons ($e.g.$ when constructing annual-average growth rates from\nquarter-on-quarter growth rates). To address this issue we propose to use\ncopulas to combine the individual $h$-step-ahead predictive distributions into\na joint predictive distribution. Our method is particularly appealing to\npractitioners for whom changing the $direct$ forecasting specification is too\ncostly. In a Monte Carlo study, we demonstrate that our approach leads to a\nbetter approximation of the true density than an approach that ignores the\npotential dependence. We show the superior performance of our method in several\nempirical examples, where we construct (i) quarterly forecasts using\nmonth-on-month $direct$ forecasts, (ii) annual-average forecasts using monthly\nyear-on-year $direct$ forecasts, and (iii) annual-average forecasts using\nquarter-on-quarter $direct$ forecasts.",
        "authors": [
            "Matteo Mogliani",
            "Florens Odendahl"
        ],
        "categories": "econ.EM",
        "published": "2024-12-08T22:48:29Z",
        "updated": "2024-12-08T22:48:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.05919v1",
        "title": "Estimating Spillover Effects in the Presence of Isolated Nodes",
        "abstract": "In estimating spillover effects under network interference, practitioners\noften use linear regression with either the number or fraction of treated\nneighbors as regressors. An often overlooked fact is that the latter is\nundefined for units without neighbors (``isolated nodes\"). The common practice\nis to impute this fraction as zero for isolated nodes. This paper shows that\nsuch practice introduces bias through theoretical derivations and simulations.\nCausal interpretations of the commonly used spillover regression coefficients\nare also provided.",
        "authors": [
            "Bora Kim"
        ],
        "categories": "econ.EM",
        "published": "2024-12-08T12:28:54Z",
        "updated": "2024-12-08T12:28:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.05794v1",
        "title": "Bundle Choice Model with Endogenous Regressors: An Application to Soda Tax",
        "abstract": "This paper proposes a Bayesian factor-augmented bundle choice model to\nestimate joint consumption as well as the substitutability and complementarity\nof multiple goods in the presence of endogenous regressors. The model extends\nthe two primary treatments of endogeneity in existing bundle choice models: (1)\nendogenous market-level prices and (2) time-invariant unobserved individual\nheterogeneity. A Bayesian sparse factor approach is employed to capture\nhigh-dimensional error correlations that induce taste correlation and\nendogeneity. Time-varying factor loadings allow for more general\nindividual-level and time-varying heterogeneity and endogeneity, while the\nsparsity induced by the shrinkage prior on loadings balances flexibility with\nparsimony. Applied to a soda tax in the context of complementarities, the new\napproach captures broader effects of the tax that were previously overlooked.\nResults suggest that a soda tax could yield additional health benefits by\nmarginally decreasing the consumption of salty snacks along with sugary drinks,\nextending the health benefits beyond the reduction in sugar consumption alone.",
        "authors": [
            "Tao Sun"
        ],
        "categories": "econ.EM",
        "published": "2024-12-08T03:16:31Z",
        "updated": "2024-12-08T03:16:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.05736v1",
        "title": "Convolution Mode Regression",
        "abstract": "For highly skewed or fat-tailed distributions, mean or median-based methods\noften fail to capture the central tendencies in the data. Despite being a\nviable alternative, estimating the conditional mode given certain covariates\n(or mode regression) presents significant challenges. Nonparametric approaches\nsuffer from the \"curse of dimensionality\", while semiparametric strategies\noften lead to non-convex optimization problems. In order to avoid these issues,\nwe propose a novel mode regression estimator that relies on an intermediate\nstep of inverting the conditional quantile density. In contrast to existing\napproaches, we employ a convolution-type smoothed variant of the quantile\nregression. Our estimator converges uniformly over the design points of the\ncovariates and, unlike previous quantile-based mode regressions, is uniform\nwith respect to the smoothing bandwidth. Additionally, the Convolution Mode\nRegression is dimension-free, carries no issues regarding optimization and\npreliminary simulations suggest the estimator is normally distributed in finite\nsamples.",
        "authors": [
            "Eduardo Schirmer Finn",
            "Eduardo Horta"
        ],
        "categories": "econ.EM",
        "published": "2024-12-07T20:18:17Z",
        "updated": "2024-12-07T20:18:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.05664v1",
        "title": "Property of Inverse Covariance Matrix-based Financial Adjacency Matrix for Detecting Local Groups",
        "abstract": "In financial applications, we often observe both global and local factors\nthat are modeled by a multi-level factor model. When detecting unknown local\ngroup memberships under such a model, employing a covariance matrix as an\nadjacency matrix for local group memberships is inadequate due to the\npredominant effect of global factors. Thus, to detect a local group structure\nmore effectively, this study introduces an inverse covariance matrix-based\nfinancial adjacency matrix (IFAM) that utilizes negative values of the inverse\ncovariance matrix. We show that IFAM ensures that the edge density between\ndifferent groups vanishes, while that within the same group remains\nnon-vanishing. This reduces falsely detected connections and helps identify\nlocal group membership accurately. To estimate IFAM under the multi-level\nfactor model, we introduce a factor-adjusted GLASSO estimator to address the\nprevalent global factor effect in the inverse covariance matrix. An empirical\nstudy using returns from international stocks across 20 financial markets\ndemonstrates that incorporating IFAM effectively detects latent local groups,\nwhich helps improve the minimum variance portfolio allocation performance.",
        "authors": [
            "Minseog Oh",
            "Donggyu Kim"
        ],
        "categories": "econ.EM",
        "published": "2024-12-07T14:24:32Z",
        "updated": "2024-12-07T14:24:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.05621v1",
        "title": "Minimum Sliced Distance Estimation in a Class of Nonregular Econometric Models",
        "abstract": "This paper proposes minimum sliced distance estimation in structural\neconometric models with possibly parameter-dependent supports. In contrast to\nlikelihood-based estimation, we show that under mild regularity conditions, the\nminimum sliced distance estimator is asymptotically normally distributed\nleading to simple inference regardless of the presence/absence of parameter\ndependent supports. We illustrate the performance of our estimator on an\nauction model.",
        "authors": [
            "Yanqin Fan",
            "Hyeonseok Park"
        ],
        "categories": "econ.EM",
        "published": "2024-12-07T11:29:38Z",
        "updated": "2024-12-07T11:29:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.05508v1",
        "title": "Optimizing Returns from Experimentation Programs",
        "abstract": "Experimentation in online digital platforms is used to inform decision\nmaking. Specifically, the goal of many experiments is to optimize a metric of\ninterest. Null hypothesis statistical testing can be ill-suited to this task,\nas it is indifferent to the magnitude of effect sizes and opportunity costs.\nGiven access to a pool of related past experiments, we discuss how\nexperimentation practice should change when the goal is optimization. We survey\nthe literature on empirical Bayes analyses of A/B test portfolios, and single\nout the A/B Testing Problem (Azevedo et al., 2020) as a starting point, which\ntreats experimentation as a constrained optimization problem. We show that the\nframework can be solved with dynamic programming and implemented by\nappropriately tuning $p$-value thresholds. Furthermore, we develop several\nextensions of the A/B Testing Problem and discuss the implications of these\nresults on experimentation programs in industry. For example, under no-cost\nassumptions, firms should be testing many more ideas, reducing test allocation\nsizes, and relaxing $p$-value thresholds away from $p = 0.05$.",
        "authors": [
            "Timothy Sudijono",
            "Simon Ejdemyr",
            "Apoorva Lal",
            "Martin Tingley"
        ],
        "categories": "stat.ME",
        "published": "2024-12-07T02:41:03Z",
        "updated": "2024-12-07T02:41:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.04816v1",
        "title": "Linear Regressions with Combined Data",
        "abstract": "We study best linear predictions in a context where the outcome of interest\nand some of the covariates are observed in two different datasets that cannot\nbe matched. Traditional approaches obtain point identification by relying,\noften implicitly, on exclusion restrictions. We show that without such\nrestrictions, coefficients of interest can still be partially identified and we\nderive a constructive characterization of the sharp identified set. We then\nbuild on this characterization to develop computationally simple and\nasymptotically normal estimators of the corresponding bounds. We show that\nthese estimators exhibit good finite sample performances.",
        "authors": [
            "Xavier D'Haultfoeuille",
            "Christophe Gaillac",
            "Arnaud Maurel"
        ],
        "categories": "econ.EM",
        "published": "2024-12-06T07:18:19Z",
        "updated": "2024-12-06T07:18:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.04605v1",
        "title": "Semiparametric Bayesian Difference-in-Differences",
        "abstract": "This paper studies semiparametric Bayesian inference for the average\ntreatment effect on the treated (ATT) within the difference-in-differences\nresearch design. We propose two new Bayesian methods with frequentist validity.\nThe first one places a standard Gaussian process prior on the conditional mean\nfunction of the control group. We obtain asymptotic equivalence of our Bayesian\nestimator and an efficient frequentist estimator by establishing a\nsemiparametric Bernstein-von Mises (BvM) theorem. The second method is a double\nrobust Bayesian procedure that adjusts the prior distribution of the\nconditional mean function and subsequently corrects the posterior distribution\nof the resulting ATT. We establish a semiparametric BvM result under double\nrobust smoothness conditions; i.e., the lack of smoothness of conditional mean\nfunctions can be compensated by high regularity of the propensity score, and\nvice versa. Monte Carlo simulations and an empirical application demonstrate\nthat the proposed Bayesian DiD methods exhibit strong finite-sample performance\ncompared to existing frequentist methods. Finally, we outline an extension to\ndifference-in-differences with multiple periods and staggered entry.",
        "authors": [
            "Christoph Breunig",
            "Ruixuan Liu",
            "Zhengfei Yu"
        ],
        "categories": "econ.EM",
        "published": "2024-12-05T20:41:36Z",
        "updated": "2024-12-05T20:41:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.04293v1",
        "title": "Cubic-based Prediction Approach for Large Volatility Matrix using High-Frequency Financial Data",
        "abstract": "In this paper, we develop a novel method for predicting future large\nvolatility matrices based on high-dimensional factor-based It\\^o processes.\nSeveral studies have proposed volatility matrix prediction methods using\nparametric models to account for volatility dynamics. However, these methods\noften impose restrictions, such as constant eigenvectors over time. To\ngeneralize the factor structure, we construct a cubic (order-3 tensor) form of\nan integrated volatility matrix process, which can be decomposed into low-rank\ntensor and idiosyncratic tensor components. To predict conditional expected\nlarge volatility matrices, we introduce the Projected Tensor Principal\nOrthogonal componEnt Thresholding (PT-POET) procedure and establish its\nasymptotic properties. Finally, the advantages of PT-POET are also verified by\na simulation study and illustrated by applying minimum variance portfolio\nallocation using high-frequency trading data.",
        "authors": [
            "Sung Hoon Choi",
            "Donggyu Kim"
        ],
        "categories": "econ.EM",
        "published": "2024-12-05T16:13:15Z",
        "updated": "2024-12-05T16:13:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.04265v1",
        "title": "On Extrapolation of Treatment Effects in Multiple-Cutoff Regression Discontinuity Designs",
        "abstract": "Regression discontinuity (RD) designs typically identify the treatment effect\nat a single cutoff point. But when and how can we learn about treatment effects\naway from the cutoff? This paper addresses this question within a\nmultiple-cutoff RD framework. We begin by examining the plausibility of the\nconstant bias assumption proposed by Cattaneo, Keele, Titiunik, and\nVazquez-Bare (2021) through the lens of rational decision-making behavior,\nwhich suggests that a kind of similarity between groups and whether individuals\ncan influence the running variable are important factors. We then introduce an\nalternative set of assumptions and propose a broadly applicable partial\nidentification strategy. The potential applicability and usefulness of the\nproposed bounds are illustrated through two empirical examples.",
        "authors": [
            "Yuta Okamoto",
            "Yuuki Ozaki"
        ],
        "categories": "econ.EM",
        "published": "2024-12-05T15:50:24Z",
        "updated": "2024-12-05T15:50:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.02767v1",
        "title": "Endogenous Heteroskedasticity in Linear Models",
        "abstract": "Linear regressions with endogeneity are widely used to estimate causal\neffects. This paper studies a statistical framework that has two common issues,\nendogeneity of the regressors, and heteroskedasticity that is allowed to depend\non endogenous regressors, i.e., endogenous heteroskedasticity. We show that the\npresence of such conditional heteroskedasticity in the structural regression\nrenders the two-stages least squares estimator inconsistent. To solve this\nissue, we propose sufficient conditions together with a control function\napproach to identify and estimate the causal parameters of interest. We\nestablish statistical properties of the estimator, say consistency and\nasymptotic normality, and propose valid inference procedures. Monte Carlo\nsimulations provide evidence of the finite sample performance of the proposed\nmethods, and evaluate different implementation procedures. We revisit an\nempirical application about job training to illustrate the methods.",
        "authors": [
            "Javier Alejo",
            "Antonio F. Galvao",
            "Julian Martinez-Iriarte",
            "Gabriel Montes-Rojas"
        ],
        "categories": "econ.EM",
        "published": "2024-12-03T19:09:48Z",
        "updated": "2024-12-03T19:09:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.02660v1",
        "title": "A Markowitz Approach to Managing a Dynamic Basket of Moving-Band Statistical Arbitrages",
        "abstract": "We consider the problem of managing a portfolio of moving-band statistical\narbitrages (MBSAs), inspired by the Markowitz optimization framework. We show\nhow to manage a dynamic basket of MBSAs, and illustrate the method on recent\nhistorical data, showing that it can perform very well in terms of\nrisk-adjusted return, essentially uncorrelated with the market.",
        "authors": [
            "Kasper Johansson",
            "Thomas Schmelzer",
            "Stephen Boyd"
        ],
        "categories": "econ.EM",
        "published": "2024-12-03T18:35:35Z",
        "updated": "2024-12-03T18:35:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.02654v1",
        "title": "Simple and Effective Portfolio Construction with Crypto Assets",
        "abstract": "We consider the problem of constructing a portfolio that combines traditional\nfinancial assets with crypto assets. We show that despite the documented\nattributes of crypto assets, such as high volatility, heavy tails, excess\nkurtosis, and skewness, a simple extension of traditional risk allocation\nprovides robust solutions for integrating these emerging assets into broader\ninvestment strategies. Examination of the risk allocation holdings suggests an\neven simpler method, analogous to the traditional 60/40 stocks/bonds\nallocation, involving a fixed allocation to crypto and traditional assets,\ndynamically diluted with cash to achieve a target risk level.",
        "authors": [
            "Kasper Johansson",
            "Stephen Boyd"
        ],
        "categories": "econ.EM",
        "published": "2024-12-03T18:29:04Z",
        "updated": "2024-12-03T18:29:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.02380v1",
        "title": "Use of surrogate endpoints in health technology assessment: a review of selected NICE technology appraisals in oncology",
        "abstract": "Objectives: Surrogate endpoints, used to substitute for and predict final\nclinical outcomes, are increasingly being used to support submissions to health\ntechnology assessment agencies. The increase in use of surrogate endpoints has\nbeen accompanied by literature describing frameworks and statistical methods to\nensure their robust validation. The aim of this review was to assess how\nsurrogate endpoints have recently been used in oncology technology appraisals\nby the National Institute for Health and Care Excellence (NICE) in England and\nWales.\n  Methods: This paper identified technology appraisals in oncology published by\nNICE between February 2022 and May 2023. Data are extracted on methods for the\nuse and validation of surrogate endpoints.\n  Results: Of the 47 technology appraisals in oncology available for review, 18\n(38 percent) utilised surrogate endpoints, with 37 separate surrogate endpoints\nbeing discussed. However, the evidence supporting the validity of the surrogate\nrelationship varied significantly across putative surrogate relationships with\n11 providing RCT evidence, 7 providing evidence from observational studies, 12\nbased on clinical opinion and 7 providing no evidence for the use of surrogate\nendpoints.\n  Conclusions: This review supports the assertion that surrogate endpoints are\nfrequently used in oncology technology appraisals in England and Wales. Despite\nincreasing availability of statistical methods and guidance on appropriate\nvalidation of surrogate endpoints, this review highlights that use and\nvalidation of surrogate endpoints can vary between technology appraisals which\ncan lead to uncertainty in decision-making.",
        "authors": [
            "Lorna Wheaton",
            "Sylwia Bujkiewicz"
        ],
        "categories": "stat.AP",
        "published": "2024-12-03T11:05:13Z",
        "updated": "2024-12-03T11:05:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.02251v1",
        "title": "Selective Reviews of Bandit Problems in AI via a Statistical View",
        "abstract": "Reinforcement Learning (RL) is a widely researched area in artificial\nintelligence that focuses on teaching agents decision-making through\ninteractions with their environment. A key subset includes stochastic\nmulti-armed bandit (MAB) and continuum-armed bandit (SCAB) problems, which\nmodel sequential decision-making under uncertainty. This review outlines the\nfoundational models and assumptions of bandit problems, explores non-asymptotic\ntheoretical tools like concentration inequalities and minimax regret bounds,\nand compares frequentist and Bayesian algorithms for managing\nexploration-exploitation trade-offs. We also extend the discussion to $K$-armed\ncontextual bandits and SCAB, examining their methodologies, regret analyses,\nand discussing the relation between the SCAB problems and the functional data\nanalysis. Finally, we highlight recent advances and ongoing challenges in the\nfield.",
        "authors": [
            "Pengjie Zhou",
            "Haoyu Wei",
            "Huiming Zhang"
        ],
        "categories": "stat.ML",
        "published": "2024-12-03T08:28:47Z",
        "updated": "2024-12-03T08:28:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.02183v1",
        "title": "Endogenous Interference in Randomized Experiments",
        "abstract": "This paper investigates the identification and inference of treatment effects\nin randomized controlled trials with social interactions. Two key network\nfeatures characterize the setting and introduce endogeneity: (1) latent\nvariables may affect both network formation and outcomes, and (2) the\nintervention may alter network structure, mediating treatment effects. I make\nthree contributions. First, I define parameters within a post-treatment network\nframework, distinguishing direct effects of treatment from indirect effects\nmediated through changes in network structure. I provide a causal\ninterpretation of the coefficients in a linear outcome model. For estimation\nand inference, I focus on a specific form of peer effects, represented by the\nfraction of treated friends. Second, in the absence of endogeneity, I establish\nthe consistency and asymptotic normality of ordinary least squares estimators.\nThird, if endogeneity is present, I propose addressing it through shift-share\ninstrumental variables, demonstrating the consistency and asymptotic normality\nof instrumental variable estimators in relatively sparse networks. For denser\nnetworks, I propose a denoised estimator based on eigendecomposition to restore\nconsistency. Finally, I revisit Prina (2015) as an empirical illustration,\ndemonstrating that treatment can influence outcomes both directly and through\nnetwork structure changes.",
        "authors": [
            "Mengsi Gao"
        ],
        "categories": "econ.EM",
        "published": "2024-12-03T05:44:50Z",
        "updated": "2024-12-03T05:44:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.01603v1",
        "title": "A Dimension-Agnostic Bootstrap Anderson-Rubin Test For Instrumental Variable Regressions",
        "abstract": "Weak-identification-robust Anderson-Rubin (AR) tests for instrumental\nvariable (IV) regressions are typically developed separately depending on\nwhether the number of IVs is treated as fixed or increasing with the sample\nsize. These tests rely on distinct test statistics and critical values. To\napply them, researchers are forced to take a stance on the asymptotic behavior\nof the number of IVs, which can be ambiguous when the number is moderate. In\nthis paper, we propose a bootstrap-based, dimension-agnostic AR test. By\nderiving strong approximations for the test statistic and its bootstrap\ncounterpart, we show that our new test has a correct asymptotic size regardless\nof whether the number of IVs is fixed or increasing -- allowing, but not\nrequiring, the number of IVs to exceed the sample size. We also analyze the\npower properties of the proposed uniformly valid test under both fixed and\nincreasing numbers of IVs.",
        "authors": [
            "Dennis Lim",
            "Wenjie Wang",
            "Yichong Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-12-02T15:21:34Z",
        "updated": "2024-12-02T15:21:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.01367v1",
        "title": "From rotational to scalar invariance: Enhancing identifiability in score-driven factor models",
        "abstract": "We show that, for a certain class of scaling matrices including the commonly\nused inverse square-root of the conditional Fisher Information, score-driven\nfactor models are identifiable up to a multiplicative scalar constant under\nvery mild restrictions. This result has no analogue in parameter-driven models,\nas it exploits the different structure of the score-driven factor dynamics.\nConsequently, score-driven models offer a clear advantage in terms of economic\ninterpretability compared to parameter-driven factor models, which are\nidentifiable only up to orthogonal transformations. Our restrictions are\norder-invariant and can be generalized to scoredriven factor models with\ndynamic loadings and nonlinear factor models. We test extensively the\nidentification strategy using simulated and real data. The empirical analysis\non financial and macroeconomic data reveals a substantial increase of\nlog-likelihood ratios and significantly improved out-of-sample forecast\nperformance when switching from the classical restrictions adopted in the\nliterature to our more flexible specifications.",
        "authors": [
            "Giuseppe Buccheri",
            "Fulvio Corsi",
            "Emilija Dzuverovic"
        ],
        "categories": "econ.EM",
        "published": "2024-12-02T10:53:14Z",
        "updated": "2024-12-02T10:53:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.01208v1",
        "title": "Locally robust semiparametric estimation of sample selection models without exclusion restrictions",
        "abstract": "Existing identification and estimation methods for semiparametric sample\nselection models rely heavily on exclusion restrictions. However, it is\ndifficult in practice to find a credible excluded variable that has a\ncorrelation with selection but no correlation with the outcome. In this paper,\nwe establish a new identification result for a semiparametric sample selection\nmodel without the exclusion restriction. The key identifying assumptions are\nnonlinearity on the selection equation and linearity on the outcome equation.\nThe difference in the functional form plays the role of an excluded variable\nand provides identification power. According to the identification result, we\npropose to estimate the model by a partially linear regression with a\nnonparametrically generated regressor. To accommodate modern machine learning\nmethods in generating the regressor, we construct an orthogonalized moment by\nadding the first-step influence function and develop a locally robust estimator\nby solving the cross-fitted orthogonalized moment condition. We prove\nroot-n-consistency and asymptotic normality of the proposed estimator under\nmild regularity conditions. A Monte Carlo simulation shows the satisfactory\nperformance of the estimator in finite samples, and an application to wage\nregression illustrates its usefulness in the absence of exclusion restrictions.",
        "authors": [
            "Zhewen Pan",
            "Yifan Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-12-02T07:25:23Z",
        "updated": "2024-12-02T07:25:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.01030v1",
        "title": "Iterative Distributed Multinomial Regression",
        "abstract": "This article introduces an iterative distributed computing estimator for the\nmultinomial logistic regression model with large choice sets. Compared to the\nmaximum likelihood estimator, the proposed iterative distributed estimator\nachieves significantly faster computation and, when initialized with a\nconsistent estimator, attains asymptotic efficiency under a weak dominance\ncondition. Additionally, we propose a parametric bootstrap inference procedure\nbased on the iterative distributed estimator and establish its consistency.\nExtensive simulation studies validate the effectiveness of the proposed methods\nand highlight the computational efficiency of the iterative distributed\nestimator.",
        "authors": [
            "Yanqin Fan",
            "Yigit Okar",
            "Xuetao Shi"
        ],
        "categories": "econ.EM",
        "published": "2024-12-02T01:25:39Z",
        "updated": "2024-12-02T01:25:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.00634v1",
        "title": "Optimization of Delivery Routes for Fresh E-commerce in Pre-warehouse Mode",
        "abstract": "With the development of the economy, fresh food e-commerce has experienced\nrapid growth. One of the core competitive advantages of fresh food e-commerce\nplatforms lies in selecting an appropriate logistics distribution model. This\nstudy focuses on the front warehouse model, aiming to minimize distribution\ncosts. Considering the perishable nature and short shelf life of fresh food, a\ndistribution route optimization model is constructed, and the saving mileage\nmethod is designed to determine the optimal distribution scheme. The results\nindicate that under certain conditions, different distribution schemes\nsignificantly impact the performance of fresh food e-commerce platforms. Based\non a review of domestic and international research, this paper takes Dingdong\nMaicai as an example to systematically introduce the basic concepts of\ndistribution route optimization in fresh food e-commerce platforms under the\nfront warehouse model, analyze the advantages of logistics distribution, and\nthoroughly examine the importance of distribution routes for fresh products.",
        "authors": [
            "Alice Harward",
            "Junjie Lin",
            "Yun Wang",
            "Xiaoke Xie"
        ],
        "categories": "econ.EM",
        "published": "2024-12-01T01:13:04Z",
        "updated": "2024-12-01T01:13:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.00233v1",
        "title": "Peer Effects and Herd Behavior: An Empirical Study Based on the \"Double 11\" Shopping Festival",
        "abstract": "This study employs a Bayesian Probit model to empirically analyze peer\neffects and herd behavior among consumers during the \"Double 11\" shopping\nfestival, using data collected through a questionnaire survey. The results\ndemonstrate that peer effects significantly influence consumer decision-making,\nwith the probability of participation in the shopping event increasing notably\nwhen roommates are involved. Additionally, factors such as gender, online\nshopping experience, and fashion consciousness significantly impact consumers'\nherd behavior. This research not only enhances the understanding of online\nshopping behavior among college students but also provides empirical evidence\nfor e-commerce platforms to formulate targeted marketing strategies. Finally,\nthe study discusses the fragility of online consumption activities, the need\nfor adjustments in corporate marketing strategies, and the importance of\npromoting a healthy online culture.",
        "authors": [
            "Hambur Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-11-29T20:03:59Z",
        "updated": "2024-11-29T20:03:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.19572v1",
        "title": "Canonical correlation analysis of stochastic trends via functional approximation",
        "abstract": "This paper proposes a novel canonical correlation analysis for semiparametric\ninference in $I(1)/I(0)$ systems via functional approximation. The approach can\nbe applied coherently to panels of $p$ variables with a generic number $s$ of\nstochastic trends, as well as to subsets or aggregations of variables. This\nstudy discusses inferential tools on $s$ and on the loading matrix $\\psi$ of\nthe stochastic trends (and on their duals $r$ and $\\beta$, the cointegration\nrank and the cointegrating matrix): asymptotically pivotal test sequences and\nconsistent estimators of $s$ and $r$, $T$-consistent, mixed Gaussian and\nefficient estimators of $\\psi$ and $\\beta$, Wald tests thereof, and\nmisspecification tests for checking model assumptions. Monte Carlo simulations\nshow that these tools have reliable performance uniformly in $s$ for small,\nmedium and large-dimensional systems, with $p$ ranging from 10 to 300. An\nempirical analysis of 20 exchange rates illustrates the methods.",
        "authors": [
            "Massimo Franchi",
            "Iliyan Georgiev",
            "Paolo Paruolo"
        ],
        "categories": "econ.EM",
        "published": "2024-11-29T09:36:10Z",
        "updated": "2024-11-29T09:36:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.18978v3",
        "title": "Warfare Ignited Price Contagion Dynamics in Early Modern Europe",
        "abstract": "Economic historians have long studied market integration and contagion\ndynamics during periods of warfare and global stress, but there is a lack of\nmodel-based evidence on these phenomena. This paper uses an econometric\ncontagion model, the Diebold-Yilmaz framework, to examine the dynamics of\neconomic shocks across European markets in the early modern period. Our\nfindings suggest that key periods of violent conflicts significantly increased\nfood price spillover across cities, causing widespread disruptions across\nEurope. We also demonstrate the ability of this framework to capture relevant\nhistorical dynamics between the main trade centers of the period.",
        "authors": [
            "Emile Esmaili",
            "Michael J. Puma",
            "Francis Ludlow",
            "Poul Holm",
            "Eva Jobbova"
        ],
        "categories": "econ.EM",
        "published": "2024-11-28T07:59:30Z",
        "updated": "2024-12-11T00:47:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.18838v1",
        "title": "Contrasting the optimal resource allocation to cybersecurity and cyber insurance using prospect theory versus expected utility theory",
        "abstract": "Protecting against cyber-threats is vital for every organization and can be\ndone by investing in cybersecurity controls and purchasing cyber insurance.\nHowever, these are interlinked since insurance premiums could be reduced by\ninvesting more in cybersecurity controls. The expected utility theory and the\nprospect theory are two alternative theories explaining decision-making under\nrisk and uncertainty, which can inform strategies for optimizing resource\nallocation. While the former is considered a rational approach, research has\nshown that most people make decisions consistent with the latter, including on\ninsurance uptakes. We compare and contrast these two approaches to provide\nimportant insights into how the two approaches could lead to different optimal\nallocations resulting in differing risk exposure as well as financial costs. We\nintroduce the concept of a risk curve and show that identifying the nature of\nthe risk curve is a key step in deriving the optimal resource allocation.",
        "authors": [
            "Chaitanya Joshi",
            "Jinming Yang",
            "Sergeja Slapnicar",
            "Ryan K L Ko"
        ],
        "categories": "econ.EM",
        "published": "2024-11-28T00:59:48Z",
        "updated": "2024-11-28T00:59:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.18772v1",
        "title": "Difference-in-differences Design with Outcomes Missing Not at Random",
        "abstract": "This paper addresses one of the most prevalent problems encountered by\npolitical scientists working with difference-in-differences (DID) design:\nmissingness in panel data. A common practice for handling missing data, known\nas complete case analysis, is to drop cases with any missing values over time.\nA more principled approach involves using nonparametric bounds on causal\neffects or applying inverse probability weighting based on baseline covariates.\nYet, these methods are general remedies that often under-utilize the\nassumptions already imposed on panel structure for causal identification. In\nthis paper, I outline the pitfalls of complete case analysis and propose an\nalternative identification strategy based on principal strata. To be specific,\nI impose parallel trends assumption within each latent group that shares the\nsame missingness pattern (e.g., always-respondents, if-treated-respondents) and\nleverage missingness rates over time to estimate the proportions of these\ngroups. Building on this, I tailor Lee bounds, a well-known nonparametric\nbounds under selection bias, to partially identify the causal effect within the\nDID design. Unlike complete case analysis, the proposed method does not require\nindependence between treatment selection and missingness patterns, nor does it\nassume homogeneous effects across these patterns.",
        "authors": [
            "Sooahn Shin"
        ],
        "categories": "stat.ME",
        "published": "2024-11-27T21:53:00Z",
        "updated": "2024-11-27T21:53:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.17136v1",
        "title": "Autoencoder Enhanced Realised GARCH on Volatility Forecasting",
        "abstract": "Realised volatility has become increasingly prominent in volatility\nforecasting due to its ability to capture intraday price fluctuations. With a\ngrowing variety of realised volatility estimators, each with unique advantages\nand limitations, selecting an optimal estimator may introduce challenges. In\nthis thesis, aiming to synthesise the impact of various realised volatility\nmeasures on volatility forecasting, we propose an extension of the Realised\nGARCH model that incorporates an autoencoder-generated synthetic realised\nmeasure, combining the information from multiple realised measures in a\nnonlinear manner. Our proposed model extends existing linear methods, such as\nPrincipal Component Analysis and Independent Component Analysis, to reduce the\ndimensionality of realised measures. The empirical evaluation, conducted across\nfour major stock markets from January 2000 to June 2022 and including the\nperiod of COVID-19, demonstrates both the feasibility of applying an\nautoencoder to synthesise volatility measures and the superior effectiveness of\nthe proposed model in one-step-ahead rolling volatility forecasting. The model\nexhibits enhanced flexibility in parameter estimations across each rolling\nwindow, outperforming traditional linear approaches. These findings indicate\nthat nonlinear dimension reduction offers further adaptability and flexibility\nin improving the synthetic realised measure, with promising implications for\nfuture volatility forecasting applications.",
        "authors": [
            "Qianli Zhao",
            "Chao Wang",
            "Richard Gerlach",
            "Giuseppe Storti",
            "Lingxiang Zhang"
        ],
        "categories": "q-fin.RM",
        "published": "2024-11-26T06:05:44Z",
        "updated": "2024-11-26T06:05:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.16978v1",
        "title": "Normal Approximation for U-Statistics with Cross-Sectional Dependence",
        "abstract": "We apply Stein's method to investigate the normal approximation for both\nnon-degenerate and degenerate U-statistics with cross-sectionally dependent\nunderlying processes in the Wasserstein metric. We show that the convergence\nrates depend on the mixing rates, the sparsity of the cross-sectional\ndependence, and the moments of the kernel functions. Conditions are derived for\ncentral limit theorems to hold as corollaries. We demonstrate one application\nof the theoretical results with nonparametric specification test for data with\ncross-sectional dependence.",
        "authors": [
            "Weiguang Liu"
        ],
        "categories": "econ.EM",
        "published": "2024-11-25T23:11:06Z",
        "updated": "2024-11-25T23:11:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2412.07787v1",
        "title": "Anomaly Detection in California Electricity Price Forecasting: Enhancing Accuracy and Reliability Using Principal Component Analysis",
        "abstract": "Accurate and reliable electricity price forecasting has significant practical\nimplications for grid management, renewable energy integration, power system\nplanning, and price volatility management. This study focuses on enhancing\nelectricity price forecasting in California's grid, addressing challenges from\ncomplex generation data and heteroskedasticity. Utilizing principal component\nanalysis (PCA), we analyze CAISO's hourly electricity prices and demand from\n2016-2021 to improve day-ahead forecasting accuracy. Initially, we apply\ntraditional outlier analysis with the interquartile range method, followed by\nrobust PCA (RPCA) for more effective outlier elimination. This approach\nimproves data symmetry and reduces skewness. We then construct multiple linear\nregression models using both raw and PCA-transformed features. The model with\ntransformed features, refined through traditional and SAS Sparse Matrix outlier\nremoval methods, shows superior forecasting performance. The SAS Sparse Matrix\nmethod, in particular, significantly enhances model accuracy. Our findings\ndemonstrate that PCA-based methods are key in advancing electricity price\nforecasting, supporting renewable integration and grid management in day-ahead\nmarkets.\n  Keywords: Electricity price forecasting, principal component analysis (PCA),\npower system planning, heteroskedasticity, renewable energy integration.",
        "authors": [
            "Joseph Nyangon",
            "Ruth Akintunde"
        ],
        "categories": "econ.EM",
        "published": "2024-11-25T20:55:25Z",
        "updated": "2024-11-25T20:55:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.16906v1",
        "title": "A Binary IV Model for Persuasion: Profiling Persuasion Types among Compliers",
        "abstract": "In an empirical study of persuasion, researchers often use a binary\ninstrument to encourage individuals to consume information and take some\naction. We show that, with a binary Imbens-Angrist instrumental variable model\nand the monotone treatment response assumption, it is possible to identify the\njoint distribution of potential outcomes among compliers. This is necessary to\nidentify the percentage of mobilised voters and their statistical\ncharacteristic defined by the moments of the joint distribution of treatment\nand covariates. Specifically, we develop a method that enables researchers to\nidentify the statistical characteristic of persuasion types: always-voters,\nnever-voters, and mobilised voters among compliers. These findings extend the\nkappa weighting results in Abadie (2003). We also provide a sharp test for the\ntwo sets of identification assumptions. The test boils down to testing whether\nthere exists a nonnegative solution to a possibly under-determined system of\nlinear equations with known coefficients. An application based on Green et al.\n(2003) is provided.",
        "authors": [
            "Zeyang Yu"
        ],
        "categories": "econ.EM",
        "published": "2024-11-25T20:16:59Z",
        "updated": "2024-11-25T20:16:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.16662v1",
        "title": "A Supervised Machine Learning Approach for Assessing Grant Peer Review Reports",
        "abstract": "Peer review in grant evaluation informs funding decisions, but the contents\nof peer review reports are rarely analyzed. In this work, we develop a\nthoroughly tested pipeline to analyze the texts of grant peer review reports\nusing methods from applied Natural Language Processing (NLP) and machine\nlearning. We start by developing twelve categories reflecting content of grant\npeer review reports that are of interest to research funders. This is followed\nby multiple human annotators' iterative annotation of these categories in a\nnovel text corpus of grant peer review reports submitted to the Swiss National\nScience Foundation. After validating the human annotation, we use the annotated\ntexts to fine-tune pre-trained transformer models to classify these categories\nat scale, while conducting several robustness and validation checks. Our\nresults show that many categories can be reliably identified by human\nannotators and machine learning approaches. However, the choice of text\nclassification approach considerably influences the classification performance.\nWe also find a high correspondence between out-of-sample classification\nperformance and human annotators' perceived difficulty in identifying\ncategories. Our results and publicly available fine-tuned transformer models\nwill allow researchers and research funders and anybody interested in peer\nreview to examine and report on the contents of these reports in a structured\nmanner. Ultimately, we hope our approach can contribute to ensuring the quality\nand trustworthiness of grant peer review.",
        "authors": [
            "Gabriel Okasa",
            "Alberto de Le\u00f3n",
            "Michaela Strinzel",
            "Anne Jorstad",
            "Katrin Milzow",
            "Matthias Egger",
            "Stefan M\u00fcller"
        ],
        "categories": "econ.EM",
        "published": "2024-11-25T18:46:34Z",
        "updated": "2024-11-25T18:46:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.16552v1",
        "title": "When Is Heterogeneity Actionable for Personalization?",
        "abstract": "Targeting and personalization policies can be used to improve outcomes beyond\nthe uniform policy that assigns the best performing treatment in an A/B test to\neveryone. Personalization relies on the presence of heterogeneity of treatment\neffects, yet, as we show in this paper, heterogeneity alone is not sufficient\nfor personalization to be successful. We develop a statistical model to\nquantify \"actionable heterogeneity,\" or the conditions when personalization is\nlikely to outperform the best uniform policy. We show that actionable\nheterogeneity can be visualized as crossover interactions in outcomes across\ntreatments and depends on three population-level parameters: within-treatment\nheterogeneity, cross-treatment correlation, and the variation in average\nresponses. Our model can be used to predict the expected gain from\npersonalization prior to running an experiment and also allows for sensitivity\nanalysis, providing guidance on how changing treatments can affect the\npersonalization gain. To validate our model, we apply five common\npersonalization approaches to two large-scale field experiments with many\ninterventions that encouraged flu vaccination. We find an 18% gain from\npersonalization in one and a more modest 4% gain in the other, which is\nconsistent with our model. Counterfactual analysis shows that this difference\nin the gains from personalization is driven by a drastic difference in\nwithin-treatment heterogeneity. However, reducing cross-treatment correlation\nholds a larger potential to further increase personalization gains. Our\nfindings provide a framework for assessing the potential from personalization\nand offer practical recommendations for improving gains from targeting in\nmulti-intervention settings.",
        "authors": [
            "Anya Shchetkina",
            "Ron Berman"
        ],
        "categories": "stat.AP",
        "published": "2024-11-25T16:37:17Z",
        "updated": "2024-11-25T16:37:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.16244v1",
        "title": "What events matter for exchange rate volatility ?",
        "abstract": "This paper expands on stochastic volatility models by proposing a data-driven\nmethod to select the macroeconomic events most likely to impact volatility. The\npaper identifies and quantifies the effects of macroeconomic events across\nmultiple countries on exchange rate volatility using high-frequency currency\nreturns, while accounting for persistent stochastic volatility effects and\nseasonal components capturing time-of-day patterns. Given the hundreds of\nmacroeconomic announcements and their lags, we rely on sparsity-based methods\nto select relevant events for the model. We contribute to the exchange rate\nliterature in four ways: First, we identify the macroeconomic events that drive\ncurrency volatility, estimate their effects and connect them to macroeconomic\nfundamentals. Second, we find a link between intraday seasonality, trading\nvolume, and the opening hours of major markets across the globe. We provide a\nsimple labor-based explanation for this observed pattern. Third, we show that\nincluding macroeconomic events and seasonal components is crucial for\nforecasting exchange rate volatility. Fourth, our proposed model yields the\nlowest volatility and highest Sharpe ratio in portfolio allocations when\ncompared to standard SV and GARCH models.",
        "authors": [
            "Igor Martins",
            "Hedibert Freitas Lopes"
        ],
        "categories": "q-fin.ST",
        "published": "2024-11-25T10:01:07Z",
        "updated": "2024-11-25T10:01:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.17743v1",
        "title": "Ranking probabilistic forecasting models with different loss functions",
        "abstract": "In this study, we introduced various statistical performance metrics, based\non the pinball loss and the empirical coverage, for the ranking of\nprobabilistic forecasting models. We tested the ability of the proposed metrics\nto determine the top performing forecasting model and investigated the use of\nwhich metric corresponds to the highest average per-trade profit in the\nout-of-sample period. Our findings show that for the considered trading\nstrategy, ranking the forecasting models according to the coverage of quantile\nforecasts used in the trading hours exhibits a superior economic performance.",
        "authors": [
            "Tomasz Serafin",
            "Bartosz Uniejewski"
        ],
        "categories": "econ.EM",
        "published": "2024-11-24T22:30:48Z",
        "updated": "2024-11-24T22:30:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.15996v1",
        "title": "Homeopathic Modernization and the Middle Science Trap: conceptual context of ergonomics, econometrics and logic of some national scientific case",
        "abstract": "This article analyses the structural and institutional barriers hindering the\ndevelopment of scientific systems in transition economies, such as Kazakhstan.\nThe main focus is on the concept of the \"middle science trap,\" which is\ncharacterized by steady growth in quantitative indicators (publications,\ngrants) but a lack of qualitative advancement. Excessive bureaucracy, weak\nintegration into the international scientific community, and ineffective\nscience management are key factors limiting development. This paper proposes an\napproach of \"homeopathic modernization,\" which focuses on minimal yet\nstrategically significant changes aimed at reducing bureaucratic barriers and\nenhancing the effectiveness of the scientific ecosystem. A comparative analysis\nof international experience (China, India, and the European Union) is provided,\ndemonstrating how targeted reforms in the scientific sector can lead to\nsignificant results. Social and cultural aspects, including the influence of\nmentality and institutional structure, are also examined, and practical\nrecommendations for reforming the scientific system in Kazakhstan and Central\nAsia are offered. The conclusions of the article could be useful for developing\nnational science modernization programs, particularly in countries with high\nlevels of bureaucracy and conservatism.",
        "authors": [
            "Eldar Knar"
        ],
        "categories": "econ.EM",
        "published": "2024-11-24T22:19:34Z",
        "updated": "2024-11-24T22:19:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.15797v1",
        "title": "Utilization and Profitability of Tractor Services for Maize Farming in Ejura-Sekyedumase Municipality, Ghana",
        "abstract": "Maize farming is a major livelihood activity for many farmers in Ghana.\nUnfortunately, farmers usually do not obtain the expected returns on their\ninvestment due to reliance on rudimentary, labor-intensive, and inefficient\nmethods of production. Using cross-sectional data from 359 maize farmers, this\nstudy investigates the profitability and determinants of the use of tractor\nservices for maize production in Ejura-Sekyedumase, Ashanti Region of Ghana.\nResults from descriptive and profitability analyses reveal that tractor\nservices such as ploughing and shelling are widely used, but their\nprofitability varies significantly among farmers. Key factors influencing\nprofitability include farm size, fertilizer quantity applied, and farmer\nexperience. Results from a multivariate probit analysis also showed that\nfarming experience, fertilizer quantity, and profit per acre have a positive\ninfluence on tractor service use for shelling, while household size, farm size,\nand FBO have a negative influence. Farming experience, fertilizer quantity, and\nprofit per acre positively influence tractor service use for ploughing, while\nfarm size has a negative influence. A t-test result reveals a statistically\nsignificant difference in profit between farmers who use tractor services and\nthose who do not. Specifically, farmers who utilize tractor services on their\nmaize farm had a return to cost of 9 percent more than those who do not\n(p-value < 0.05). The Kendall's result showed a moderate agreement among the\nmaize farmers (first ranked being financial issues) in their ability to\naccess/utilize tractor services on their farm.",
        "authors": [
            "Fred Nimoh",
            "Innocent Yao Yevu",
            "Attah-Nyame Essampong",
            "Asante Emmanuel Addo",
            "Addai Kevin"
        ],
        "categories": "econ.EM",
        "published": "2024-11-24T11:54:43Z",
        "updated": "2024-11-24T11:54:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.15625v1",
        "title": "Canonical Correlation Analysis: review",
        "abstract": "For over a century canonical correlations, variables, and related concepts\nhave been studied across various fields, with contributions dating back to\nJordan [1875] and Hotelling [1936]. This text surveys the evolution of\ncanonical correlation analysis, a fundamental statistical tool, beginning with\nits foundational theorems and progressing to recent developments and open\nresearch problems. Along the way we introduce and review methods, notions, and\nfundamental concepts from linear algebra, random matrix theory, and\nhigh-dimensional statistics, placing particular emphasis on rigorous\nmathematical treatment.\n  The survey is intended for technically proficient graduate students and other\nresearchers with an interest in this area. The content is organized into five\nchapters, supplemented by six sets of exercises found in Chapter 6. These\nexercises introduce additional material, reinforce key concepts, and serve to\nbridge ideas across chapters. We recommend the following sequence: first, solve\nProblem Set 0, then proceed with Chapter 1, solve Problem Set 1, and so on\nthrough the text.",
        "authors": [
            "Anna Bykhovskaya",
            "Vadim Gorin"
        ],
        "categories": "stat.ME",
        "published": "2024-11-23T18:34:26Z",
        "updated": "2024-11-23T18:34:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.14763v1",
        "title": "From Replications to Revelations: Heteroskedasticity-Robust Inference",
        "abstract": "We compare heteroskedasticity-robust inference methods with a large-scale\nMonte Carlo study based on regressions from 155 reproduction packages of\nleading economic journals. The results confirm established wisdom and uncover\nnew insights. Among well established methods HC2 standard errors with the\ndegree of freedom specification proposed by Bell and McCaffrey (2002) perform\nbest. To further improve the accuracy of t-tests, we propose a novel\ndegree-of-freedom specification based on partial leverages. We also show how\nHC2 to HC4 standard errors can be refined by more effectively addressing the\n15.6% of cases where at least one observation exhibits a leverage of one.",
        "authors": [
            "Sebastian Kranz"
        ],
        "categories": "econ.EM",
        "published": "2024-11-22T06:57:39Z",
        "updated": "2024-11-22T06:57:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.13810v1",
        "title": "Dynamic spatial interaction models for a leader's resource allocation and followers' multiple activities",
        "abstract": "This paper introduces a novel spatial interaction model to explore the\ndecision-making processes of two types of agents-a leader and followers-with\ncentral and local governments serving as empirical representations. The model\naccounts for three key features: (i) resource allocations from the leader to\nthe followers and the resulting strategic interactions, (ii) followers' choices\nacross multiple activities, and (iii) interactions among these activities. We\ndevelop a network game to examine the micro-foundations of these processes. In\nthis game, followers engage in multiple activities, while the leader allocates\nresources by monitoring the externalities arising from followers' interactions.\nThe game's unique NE is the foundation for our econometric framework, providing\nequilibrium measures to understand the short-term impacts of changes in\nfollowers' characteristics and their long-term consequences. To estimate the\nagent payoff parameters, we employ the QML estimation method and examine the\nasymptotic properties of the QML estimator to ensure robust statistical\ninferences. Empirically, we investigate interactions among U.S. states in\npublic welfare expenditures (PWE) and housing and community development\nexpenditures (HCDE), focusing on how federal grants influence these\nexpenditures and the interactions among state governments. Our findings reveal\npositive spillovers in states' PWEs, complementarity between the two\nexpenditures within states, and negative cross-variable spillovers between\nthem. Additionally, we observe positive effects of federal grants on both\nexpenditures. Counterfactual simulations indicate that federal interventions\nlead to a 6.46% increase in social welfare by increasing the states' efforts on\nPWE and HCDE. However, due to the limited flexibility in federal grants, their\nmagnitudes are smaller than the proportion of federal grants within the states'\ntotal revenues.",
        "authors": [
            "Hanbat Jeong"
        ],
        "categories": "econ.EM",
        "published": "2024-11-21T03:20:17Z",
        "updated": "2024-11-21T03:20:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.13372v1",
        "title": "Clustering with Potential Multidimensionality: Inference and Practice",
        "abstract": "We show how clustering standard errors in one or more dimensions can be\njustified in M-estimation when there is sampling or assignment uncertainty.\nSince existing procedures for variance estimation are either conservative or\ninvalid, we propose a variance estimator that refines a conservative procedure\nand remains valid. We then interpret environments where clustering is\nfrequently employed in empirical work from our design-based perspective and\nprovide insights on their estimands and inference procedures.",
        "authors": [
            "Ruonan Xu",
            "Luther Yap"
        ],
        "categories": "econ.EM",
        "published": "2024-11-20T14:52:09Z",
        "updated": "2024-11-20T14:52:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.13293v1",
        "title": "Revealed Information",
        "abstract": "An analyst observes the frequency with which a decision maker (DM) takes\nactions, but does not observe the frequency of actions conditional on the\npayoff-relevant state. We ask when can the analyst rationalize the DM's choices\nas if the DM first learns something about the state before taking action. We\nprovide a support function characterization of the triples of utility\nfunctions, prior beliefs, and (marginal) distributions over actions such that\nthe DM's action distribution is consistent with information given the agent's\nprior and utility function. Assumptions on the cardinality of the state space\nand the utility function allow us to refine this characterization, obtaining a\nsharp system of finitely many inequalities the utility function, prior, and\naction distribution must satisfy. We apply our characterization to study\ncomparative statics and ring-network games, and to identify conditions under\nwhich a data set is consistent with a public information structure in\nfirst-order Bayesian persuasion games. We characterize the set of distributions\nover posterior beliefs that are consistent with the DM's choices. Assuming the\nfirst-order approach applies, we extend our results to settings with a\ncontinuum of actions and/or states.%",
        "authors": [
            "Laura Doval",
            "Ran Eilat",
            "Tianhao Liu",
            "Yangfan Zhou"
        ],
        "categories": "econ.TH",
        "published": "2024-11-20T13:03:24Z",
        "updated": "2024-11-20T13:03:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.12036v2",
        "title": "Prediction-Guided Active Experiments",
        "abstract": "In this work, we introduce a new framework for active experimentation, the\nPrediction-Guided Active Experiment (PGAE), which leverages predictions from an\nexisting machine learning model to guide sampling and experimentation.\nSpecifically, at each time step, an experimental unit is sampled according to a\ndesignated sampling distribution, and the actual outcome is observed based on\nan experimental probability. Otherwise, only a prediction for the outcome is\navailable. We begin by analyzing the non-adaptive case, where full information\non the joint distribution of the predictor and the actual outcome is assumed.\nFor this scenario, we derive an optimal experimentation strategy by minimizing\nthe semi-parametric efficiency bound for the class of regular estimators. We\nthen introduce an estimator that meets this efficiency bound, achieving\nasymptotic optimality. Next, we move to the adaptive case, where the predictor\nis continuously updated with newly sampled data. We show that the adaptive\nversion of the estimator remains efficient and attains the same semi-parametric\nbound under certain regularity assumptions. Finally, we validate PGAE's\nperformance through simulations and a semi-synthetic experiment using data from\nthe US Census Bureau. The results underscore the PGAE framework's effectiveness\nand superiority compared to other existing methods.",
        "authors": [
            "Ruicheng Ao",
            "Hongyu Chen",
            "David Simchi-Levi"
        ],
        "categories": "stat.ML",
        "published": "2024-11-18T20:16:24Z",
        "updated": "2024-11-20T19:25:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.11748v3",
        "title": "Debiased Regression for Root-N-Consistent Conditional Mean Estimation",
        "abstract": "This study introduces a debiasing method for regression estimators, including\nhigh-dimensional and nonparametric regression estimators. For example,\nnonparametric regression methods allow for the estimation of regression\nfunctions in a data-driven manner with minimal assumptions; however, these\nmethods typically fail to achieve $\\sqrt{n}$-consistency in their convergence\nrates, and many, including those in machine learning, lack guarantees that\ntheir estimators asymptotically follow a normal distribution. To address these\nchallenges, we propose a debiasing technique for nonparametric estimators by\nadding a bias-correction term to the original estimators, extending the\nconventional one-step estimator used in semiparametric analysis. Specifically,\nfor each data point, we estimate the conditional expected residual of the\noriginal nonparametric estimator, which can, for instance, be computed using\nkernel (Nadaraya-Watson) regression, and incorporate it as a bias-reduction\nterm. Our theoretical analysis demonstrates that the proposed estimator\nachieves $\\sqrt{n}$-consistency and asymptotic normality under a mild\nconvergence rate condition for both the original nonparametric estimator and\nthe conditional expected residual estimator. Notably, this approach remains\nmodel-free as long as the original estimator and the conditional expected\nresidual estimator satisfy the convergence rate condition. The proposed method\noffers several advantages, including improved estimation accuracy and\nsimplified construction of confidence intervals.",
        "authors": [
            "Masahiro Kato"
        ],
        "categories": "stat.ML",
        "published": "2024-11-18T17:25:06Z",
        "updated": "2024-11-25T21:27:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.11559v2",
        "title": "Treatment Effect Estimators as Weighted Outcomes",
        "abstract": "Estimators that weight observed outcomes to form effect estimates have a long\ntradition. Their outcome weights are widely used in established procedures,\nsuch as checking covariate balance, characterizing target populations, or\ndetecting and managing extreme weights. This paper introduces a general\nframework for deriving such outcome weights. It establishes when and how\nnumerical equivalence between an original estimator representation as moment\ncondition and a unique weighted representation can be obtained. The framework\nis applied to derive novel outcome weights for the six seminal instances of\ndouble machine learning and generalized random forests, while recovering\nexisting results for other estimators as special cases. The analysis highlights\nthat implementation choices determine (i) the availability of outcome weights\nand (ii) their properties. Notably, standard implementations of partially\nlinear regression-based estimators, like causal forests, employ outcome weights\nthat do not sum to (minus) one in the (un)treated group, not fulfilling a\nproperty often considered desirable.",
        "authors": [
            "Michael C. Knaus"
        ],
        "categories": "econ.EM",
        "published": "2024-11-18T13:24:09Z",
        "updated": "2024-12-12T06:43:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.11058v1",
        "title": "Econometrics and Formalism of Psychological Archetypes of Scientific Workers with Introverted Thinking Type",
        "abstract": "The chronological hierarchy and classification of psychological types of\nindividuals are examined. The anomalous nature of psychological activity in\nindividuals involved in scientific work is highlighted. Certain aspects of the\nintroverted thinking type in scientific activities are analyzed. For the first\ntime, psychological archetypes of scientists with pronounced introversion are\npostulated in the context of twelve hypotheses about the specifics of\nprofessional attributes of introverted scientific activities.\n  A linear regression and Bayesian equation are proposed for quantitatively\nassessing the econometric degree of introversion in scientific employees,\nconsidering a wide range of characteristics inherent to introverts in\nscientific processing. Specifically, expressions for a comprehensive assessment\nof introversion in a linear model and the posterior probability of the\neconometric (scientometric) degree of introversion in a Bayesian model are\nformulated.\n  The models are based on several econometric (scientometric) hypotheses\nregarding various aspects of professional activities of introverted scientists,\nsuch as a preference for solo publications, low social activity, narrow\nspecialization, high research depth, and so forth. Empirical data and multiple\nlinear regression methods can be used to calibrate the equations. The model can\nbe applied to gain a deeper understanding of the psychological characteristics\nof scientific employees, which is particularly useful in ergonomics and the\nmanagement of scientific teams and projects. The proposed method also provides\nscientists with pronounced introversion the opportunity to develop their\ncareers, focusing on individual preferences and features.",
        "authors": [
            "Eldar Knar"
        ],
        "categories": "econ.EM",
        "published": "2024-11-17T12:45:25Z",
        "updated": "2024-11-17T12:45:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.10959v1",
        "title": "Program Evaluation with Remotely Sensed Outcomes",
        "abstract": "While traditional program evaluations typically rely on surveys to measure\noutcomes, certain economic outcomes such as living standards or environmental\nquality may be infeasible or costly to collect. As a result, recent empirical\nwork estimates treatment effects using remotely sensed variables (RSVs), such\nmobile phone activity or satellite images, instead of ground-truth outcome\nmeasurements. Common practice predicts the economic outcome from the RSV, using\nan auxiliary sample of labeled RSVs, and then uses such predictions as the\noutcome in the experiment. We prove that this approach leads to biased\nestimates of treatment effects when the RSV is a post-outcome variable. We\nnonparametrically identify the treatment effect, using an assumption that\nreflects the logic of recent empirical research: the conditional distribution\nof the RSV remains stable across both samples, given the outcome and treatment.\nOur results do not require researchers to know or consistently estimate the\nrelationship between the RSV, outcome, and treatment, which is typically\nmis-specified with unstructured data. We form a representation of the RSV for\ndownstream causal inference by predicting the outcome and predicting the\ntreatment, with better predictions leading to more precise causal estimates. We\nre-evaluate the efficacy of a large-scale public program in India, showing that\nthe program's measured effects on local consumption and poverty can be\nreplicated using satellite",
        "authors": [
            "Ashesh Rambachan",
            "Rahul Singh",
            "Davide Viviano"
        ],
        "categories": "econ.EM",
        "published": "2024-11-17T04:43:04Z",
        "updated": "2024-11-17T04:43:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.10768v1",
        "title": "Building Interpretable Climate Emulators for Economics",
        "abstract": "This paper presents a framework for developing efficient and interpretable\ncarbon-cycle emulators (CCEs) as part of climate emulators in Integrated\nAssessment Models, enabling economists to custom-build CCEs accurately\ncalibrated to advanced climate science. We propose a generalized\nmulti-reservoir linear box-model CCE that preserves key physical quantities and\ncan be use-case tailored for specific use cases. Three CCEs are presented for\nillustration: the 3SR model (replicating DICE-2016), the 4PR model (including\nthe land biosphere), and the 4PR-X model (accounting for dynamic land-use\nchanges like deforestation that impact the reservoir's storage capacity).\nEvaluation of these models within the DICE framework shows that land-use\nchanges in the 4PR-X model significantly impact atmospheric carbon and\ntemperatures -- emphasizing the importance of using tailored climate emulators.\nBy providing a transparent and flexible tool for policy analysis, our framework\nallows economists to assess the economic impacts of climate policies more\naccurately.",
        "authors": [
            "Aryan Eftekhari",
            "Doris Folini",
            "Aleksandra Friedl",
            "Felix K\u00fcbler",
            "Simon Scheidegger",
            "Olaf Schenk"
        ],
        "categories": "econ.EM",
        "published": "2024-11-16T10:22:23Z",
        "updated": "2024-11-16T10:22:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.10628v1",
        "title": "Feature Importance of Climate Vulnerability Indicators with Gradient Boosting across Five Global Cities",
        "abstract": "Efforts are needed to identify and measure both communities' exposure to\nclimate hazards and the social vulnerabilities that interact with these\nhazards, but the science of validating hazard vulnerability indicators is still\nin its infancy. Progress is needed to improve: 1) the selection of variables\nthat are used as proxies to represent hazard vulnerability; 2) the\napplicability and scale for which these indicators are intended, including\ntheir transnational applicability. We administered an international urban\nsurvey in Buenos Aires, Argentina; Johannesburg, South Africa; London, United\nKingdom; New York City, United States; and Seoul, South Korea in order to\ncollect data on exposure to various types of extreme weather events,\nsocioeconomic characteristics commonly used as proxies for vulnerability (i.e.,\nincome, education level, gender, and age), and additional characteristics not\noften included in existing composite indices (i.e., queer identity, disability\nidentity, non-dominant primary language, and self-perceptions of both\ndiscrimination and vulnerability to flood risk). We then use feature importance\nanalysis with gradient-boosted decision trees to measure the importance that\nthese variables have in predicting exposure to various types of extreme weather\nevents. Our results show that non-traditional variables were more relevant to\nself-reported exposure to extreme weather events than traditionally employed\nvariables such as income or age. Furthermore, differences in variable relevance\nacross different types of hazards and across urban contexts suggest that\nvulnerability indicators need to be fit to context and should not be used in a\none-size-fits-all fashion.",
        "authors": [
            "Lidia Cano Pecharroman",
            "Melissa O. Tier",
            "Elke U. Weber"
        ],
        "categories": "econ.EM",
        "published": "2024-11-15T23:22:42Z",
        "updated": "2024-11-15T23:22:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.10600v1",
        "title": "Monetary Incentives, Landowner Preferences: Estimating Cross-Elasticities in Farmland Conversion to Renewable Energy",
        "abstract": "This study examines the impact of monetary factors on the conversion of\nfarmland to renewable energy generation, specifically solar and wind, in the\ncontext of expanding U.S. energy production. We propose a new econometric\nmethod that accounts for the diverse circumstances of landowners, including\ntheir unordered alternative land use options, non-monetary benefits from\nfarming, and the influence of local regulations. We demonstrate that\nidentifying the cross elasticity of landowners' farming income in relation to\nthe conversion of farmland to renewable energy requires an understanding of\ntheir preferences. By utilizing county legislation that we assume to be shaped\nby land-use preferences, we estimate the cross-elasticities of farming income.\nOur findings indicate that monetary incentives may only influence landowners'\ndecisions in areas with potential for future residential development,\nunderscoring the importance of considering both preferences and regulatory\ncontexts.",
        "authors": [
            "Chad Fiechter",
            "Binayak Kunwar",
            "Guy Tchuente"
        ],
        "categories": "econ.EM",
        "published": "2024-11-15T21:51:02Z",
        "updated": "2024-11-15T21:51:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.10415v2",
        "title": "Dynamic Causal Effects in a Nonlinear World: the Good, the Bad, and the Ugly",
        "abstract": "Applied macroeconomists frequently use impulse response estimators motivated\nby linear models. We study whether the estimands of such procedures have a\ncausal interpretation when the true data generating process is in fact\nnonlinear. We show that vector autoregressions and linear local projections\nonto observed shocks or proxies identify weighted averages of causal effects\nregardless of the extent of nonlinearities. By contrast, identification\napproaches that exploit heteroskedasticity or non-Gaussianity of latent shocks\nare highly sensitive to departures from linearity. Our analysis is based on new\nresults on the identification of marginal treatment effects through weighted\nregressions, which may also be of interest to researchers outside\nmacroeconomics.",
        "authors": [
            "Michal Koles\u00e1r",
            "Mikkel Plagborg-M\u00f8ller"
        ],
        "categories": "econ.EM",
        "published": "2024-11-15T18:34:58Z",
        "updated": "2024-12-04T21:40:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.10009v1",
        "title": "Semiparametric inference for impulse response functions using double/debiased machine learning",
        "abstract": "We introduce a double/debiased machine learning (DML) estimator for the\nimpulse response function (IRF) in settings where a time series of interest is\nsubjected to multiple discrete treatments, assigned over time, which can have a\ncausal effect on future outcomes. The proposed estimator can rely on fully\nnonparametric relations between treatment and outcome variables, opening up the\npossibility to use flexible machine learning approaches to estimate IRFs. To\nthis end, we extend the theory of DML from an i.i.d. to a time series setting\nand show that the proposed DML estimator for the IRF is consistent and\nasymptotically normally distributed at the parametric rate, allowing for\nsemiparametric inference for dynamic effects in a time series setting. The\nproperties of the estimator are validated numerically in finite samples by\napplying it to learn the IRF in the presence of serial dependence in both the\nconfounder and observation innovation processes. We also illustrate the\nmethodology empirically by applying it to the estimation of the effects of\nmacroeconomic shocks.",
        "authors": [
            "Daniele Ballinari",
            "Alexander Wehrli"
        ],
        "categories": "econ.EM",
        "published": "2024-11-15T07:42:02Z",
        "updated": "2024-11-15T07:42:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.09808v1",
        "title": "Sharp Testable Implications of Encouragement Designs",
        "abstract": "This paper studies the sharp testable implications of an additive random\nutility model with a discrete multi-valued treatment and a discrete\nmulti-valued instrument, in which each value of the instrument only weakly\nincreases the utility of one choice. Borrowing the terminology used in\nrandomized experiments, we call such a setting an encouragement design. We\nderive inequalities in terms of the conditional choice probabilities that\ncharacterize when the distribution of the observed data is consistent with such\na model. Through a novel constructive argument, we further show these\ninequalities are sharp in the sense that any distribution of the observed data\nthat satisfies these inequalities is generated by this additive random utility\nmodel.",
        "authors": [
            "Yuehao Bai",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2024-11-14T20:52:10Z",
        "updated": "2024-11-14T20:52:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.09771v1",
        "title": "Bayesian estimation of finite mixtures of Tobit models",
        "abstract": "This paper outlines a Bayesian approach to estimate finite mixtures of Tobit\nmodels. The method consists of an MCMC approach that combines Gibbs sampling\nwith data augmentation and is simple to implement. I show through simulations\nthat the flexibility provided by this method is especially helpful when\ncensoring is not negligible. In addition, I demonstrate the broad utility of\nthis methodology with applications to a job training program, labor supply, and\ndemand for medical care. I find that this approach allows for non-trivial\nadditional flexibility that can alter results considerably and beyond improving\nmodel fit.",
        "authors": [
            "Caio Waisman"
        ],
        "categories": "econ.EM",
        "published": "2024-11-14T19:29:07Z",
        "updated": "2024-11-14T19:29:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.09452v1",
        "title": "Sparse Interval-valued Time Series Modeling with Machine Learning",
        "abstract": "By treating intervals as inseparable sets, this paper proposes sparse machine\nlearning regressions for high-dimensional interval-valued time series. With\nLASSO or adaptive LASSO techniques, we develop a penalized minimum distance\nestimation, which covers point-based estimators are special cases. We establish\nthe consistency and oracle properties of the proposed penalized estimator,\nregardless of whether the number of predictors is diverging with the sample\nsize. Monte Carlo simulations demonstrate the favorable finite sample\nproperties of the proposed estimation. Empirical applications to\ninterval-valued crude oil price forecasting and sparse index-tracking portfolio\nconstruction illustrate the robustness and effectiveness of our method against\ncompeting approaches, including random forest and multilayer perceptron for\ninterval-valued data. Our findings highlight the potential of machine learning\ntechniques in interval-valued time series analysis, offering new insights for\nfinancial forecasting and portfolio management.",
        "authors": [
            "Haowen Bao",
            "Yongmiao Hong",
            "Yuying Sun",
            "Shouyang Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-11-14T13:56:28Z",
        "updated": "2024-11-14T13:56:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.09258v1",
        "title": "On Asymptotic Optimality of Least Squares Model Averaging When True Model Is Included",
        "abstract": "Asymptotic optimality is a key theoretical property in model averaging. Due\nto technical difficulties, existing studies rely on restricted weight sets or\nthe assumption that there is no true model with fixed dimensions in the\ncandidate set. The focus of this paper is to overcome these difficulties.\nSurprisingly, we discover that when the penalty factor in the weight selection\ncriterion diverges with a certain order and the true model dimension is fixed,\nasymptotic loss optimality does not hold, but asymptotic risk optimality does.\nThis result differs from the corresponding result of Fang et al. (2023,\nEconometric Theory 39, 412-441) and reveals that using the discrete weight set\nof Hansen (2007, Econometrica 75, 1175-1189) can yield opposite asymptotic\nproperties compared to using the usual weight set. Simulation studies\nillustrate the theoretical findings in a variety of settings.",
        "authors": [
            "Wenchao Xu",
            "Xinyu Zhang"
        ],
        "categories": "math.ST",
        "published": "2024-11-14T07:49:30Z",
        "updated": "2024-11-14T07:49:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.09221v1",
        "title": "Difference-in-Differences with Sample Selection",
        "abstract": "Endogenous treatment and sample selection are two concomitant sources of\nendogeneity that challenge the validity of causal inference. In this paper, we\nfocus on the partial identification of treatment effects within a standard\ntwo-period difference-in-differences framework when the outcome is observed for\nan endogenously selected subpopulation. The identification strategy embeds\nLee's (2009) bounding approach based on principal stratification, which divides\nthe population into latent subgroups based on selection behaviour in\ncounterfactual treatment states in both periods. We establish identification\nresults for four latent types and illustrate the proposed approach by applying\nit to estimate 1) the effect of a job training program on earnings and 2) the\neffect of a working-from-home policy on employee performance.",
        "authors": [
            "Gayani Rathnayake",
            "Akanksha Negi",
            "Otavio Bartalotti",
            "Xueyan Zhao"
        ],
        "categories": "econ.EM",
        "published": "2024-11-14T06:37:43Z",
        "updated": "2024-11-14T06:37:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.09218v1",
        "title": "On the (Mis)Use of Machine Learning with Panel Data",
        "abstract": "Machine Learning (ML) is increasingly employed to inform and support\npolicymaking interventions. This methodological article cautions practitioners\nabout common but often overlooked pitfalls associated with the uncritical\napplication of supervised ML algorithms to panel data. Ignoring the\ncross-sectional and longitudinal structure of this data can lead to\nhard-to-detect data leakage, inflated out-of-sample performance, and an\ninadvertent overestimation of the real-world usefulness and applicability of ML\nmodels. After clarifying these issues, we provide practical guidelines and best\npractices for applied researchers to ensure the correct implementation of\nsupervised ML in panel data environments, emphasizing the need to define ex\nante the primary goal of the analysis and align the ML pipeline accordingly. An\nempirical application based on over 3,000 US counties from 2000 to 2019\nillustrates the practical relevance of these points across nearly 500 models\nfor both classification and regression tasks.",
        "authors": [
            "Augusto Cerqua",
            "Marco Letta",
            "Gabriele Pinto"
        ],
        "categories": "econ.EM",
        "published": "2024-11-14T06:29:59Z",
        "updated": "2024-11-14T06:29:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.08491v2",
        "title": "Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions",
        "abstract": "Higher-Order Influence Functions (HOIF), developed in a series of papers over\nthe past twenty years, is a fundamental theoretical device for constructing\nrate-optimal causal-effect estimators from observational studies. However, the\nvalue of HOIF for analyzing well-conducted randomized controlled trials (RCTs)\nhas not been explicitly explored. In the recent U.S. Food and Drug\nAdministration (FDA) and European Medicines Agency (EMA) guidelines on the\npractice of covariate adjustment in analyzing RCTs, in addition to the simple,\nunadjusted difference-in-mean estimator, it was also recommended to report the\nestimator adjusting for baseline covariates via a simple parametric working\nmodel, such as a linear model. In this paper, we show that a HOIF-motivated\nestimator for the treatment-specific mean has significantly improved\nstatistical properties compared to popular adjusted estimators in practice when\nthe number of baseline covariates $p$ is relatively large compared to the\nsample size $n$. We also characterize the conditions under which the\nHOIF-motivated estimator improves upon the unadjusted one. Furthermore, we\ndemonstrate that a novel debiased adjusted estimator proposed recently by Lu et\nal. is, in fact, another HOIF-motivated estimator in disguise. Numerical and\nempirical studies are conducted to corroborate our theoretical findings.",
        "authors": [
            "Sihui Zhao",
            "Xinbo Wang",
            "Lin Liu",
            "Xin Zhang"
        ],
        "categories": "stat.ME",
        "published": "2024-11-13T10:16:41Z",
        "updated": "2024-12-11T15:13:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.08188v1",
        "title": "MSTest: An R-Package for Testing Markov Switching Models",
        "abstract": "We present the R package MSTest, which implements hypothesis testing\nprocedures to identify the number of regimes in Markov switching models. These\nmodels have wide-ranging applications in economics, finance, and numerous other\nfields. The MSTest package includes the Monte Carlo likelihood ratio test\nprocedures proposed by Rodriguez-Rondon and Dufour (2024), the moment-based\ntests of Dufour and Luger (2017), the parameter stability tests of Carrasco,\nHu, and Ploberger (2014), and the likelihood ratio test of Hansen (1992).\nAdditionally, the package enables users to simulate and estimate univariate and\nmultivariate Markov switching and hidden Markov processes, using the\nexpectation-maximization (EM) algorithm or maximum likelihood estimation (MLE).\nWe demonstrate the functionality of the MSTest package through both simulation\nexperiments and an application to U.S. GNP growth data.",
        "authors": [
            "Gabriel Rodriguez-Rondon",
            "Jean-Marie Dufour"
        ],
        "categories": "stat.ME",
        "published": "2024-11-12T21:13:23Z",
        "updated": "2024-11-12T21:13:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.07978v3",
        "title": "A Note on Doubly Robust Estimator in Regression Continuity Designs",
        "abstract": "This note introduces a doubly robust (DR) estimator for regression\ndiscontinuity (RD) designs. RD designs provide a quasi-experimental framework\nfor estimating treatment effects, where treatment assignment depends on whether\na running variable surpasses a predefined cutoff. A common approach in RD\nestimation is the use of nonparametric regression methods, such as local linear\nregression. However, the validity of these methods still relies on the\nconsistency of the nonparametric estimators. In this study, we propose the\nDR-RD estimator, which combines two distinct estimators for the conditional\nexpected outcomes. The primary advantage of the DR-RD estimator lies in its\nability to ensure the consistency of the treatment effect estimation as long as\nat least one of the two estimators is consistent. Consequently, our DR-RD\nestimator enhances robustness of treatment effect estimators in RD designs.",
        "authors": [
            "Masahiro Kato"
        ],
        "categories": "econ.EM",
        "published": "2024-11-12T17:58:34Z",
        "updated": "2024-12-02T18:58:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.07952v1",
        "title": "Matching $\\leq$ Hybrid $\\leq$ Difference in Differences",
        "abstract": "Since LaLonde's (1986) seminal paper, there has been ongoing interest in\nestimating treatment effects using pre- and post-intervention data. Scholars\nhave traditionally used experimental benchmarks to evaluate the accuracy of\nalternative econometric methods, including Matching, Difference-in-Differences\n(DID), and their hybrid forms (e.g., Heckman et al., 1998b; Dehejia and Wahba,\n2002; Smith and Todd, 2005). We revisit these methodologies in the evaluation\nof job training and educational programs using four datasets (LaLonde, 1986;\nHeckman et al., 1998a; Smith and Todd, 2005; Chetty et al., 2014a; Athey et\nal., 2020), and show that the inequality relationship, Matching $\\leq$ Hybrid\n$\\leq$ DID, appears as a consistent norm, rather than a mere coincidence. We\nprovide a formal theoretical justification for this puzzling phenomenon under\nplausible conditions such as negative selection, by generalizing the classical\nbracketing (Angrist and Pischke, 2009, Section 5). Consequently, when\ntreatments are expected to be non-negative, DID tends to provide optimistic\nestimates, while Matching offers more conservative ones. Keywords: bias,\ndifference in differences, educational program, job training program, matching.",
        "authors": [
            "Yechan Park",
            "Yuya Sasaki"
        ],
        "categories": "econ.EM",
        "published": "2024-11-12T17:27:08Z",
        "updated": "2024-11-12T17:27:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.07817v1",
        "title": "Impact of R&D and AI Investments on Economic Growth and Credit Rating",
        "abstract": "The research and development (R&D) phase is essential for fostering\ninnovation and aligns with long-term strategies in both public and private\nsectors. This study addresses two primary research questions: (1) assessing the\nrelationship between R&D investments and GDP through regression analysis, and\n(2) estimating the economic value added (EVA) that Georgia must generate to\nprogress from a BB to a BBB credit rating. Using World Bank data from\n2014-2022, this analysis found that increasing R&D, with an emphasis on AI, by\n30-35% has a measurable impact on GDP. Regression results reveal a coefficient\nof 7.02%, indicating a 10% increase in R&D leads to a 0.70% GDP rise, with an\n81.1% determination coefficient and a strong 90.1% correlation.\n  Georgia's EVA model was calculated to determine the additional value needed\nfor a BBB rating, comparing indicators from Greece, Hungary, India, and\nKazakhstan as benchmarks. Key economic indicators considered were nominal GDP,\nGDP per capita, real GDP growth, and fiscal indicators (government balance/GDP,\ndebt/GDP). The EVA model projects that to achieve a BBB rating within nine\nyears, Georgia requires $61.7 billion in investments. Utilizing EVA and\ncomprehensive economic indicators will support informed decision-making and\nenhance the analysis of Georgia's economic trajectory.",
        "authors": [
            "Davit Gondauri",
            "Ekaterine Mikautadze"
        ],
        "categories": "econ.EM",
        "published": "2024-11-12T14:09:56Z",
        "updated": "2024-11-12T14:09:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.07808v2",
        "title": "Spatial Competition on Psychological Pricing Strategies: Preliminary Evidence from an Online Marketplace",
        "abstract": "According zu Kadir et al. (2023) online marketplaces are used to buy and sell\nproducts and services, as well as to exchange money and data between users or\nthe platform. Due to the large product selection, low costs and the ease of\nshopping without physical restrictions as well as the technical possibilities,\nonline marketplaces have grown rapidly Kadir et al. (2023). Online marketplaces\nare also used in the consumer-to-consumer (C2C) sector and thus offer a broad\nuser group a marketplace, for example for used products. This article focuses\non Willhaben.at (2024), a leading C2C marketplace in Austria, as stated by\nObersteiner, Schmied, and Pamperl (2023). The empirical analysis in this course\nessay centers around the offer ads of Woom Bikes, a standardised product which\nis sold on Willhaben. Through web scraping, a dataset of approximately 826\nobservations was created, focusing on mid-to-high price segment bicycles, which\nare characterized by price stability and uniformity as we claim. This analysis\naims to create analyse ad listing prices through predictive models using\nwillhaben product listing attributes and using the spatial distribution of one\nof the product attributes.",
        "authors": [
            "Magdalena Schindl",
            "Felix Reichel"
        ],
        "categories": "econ.EM",
        "published": "2024-11-12T14:02:24Z",
        "updated": "2024-11-14T12:06:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.07604v1",
        "title": "Dynamic Evolutionary Game Analysis of How Fintech in Banking Mitigates Risks in Agricultural Supply Chain Finance",
        "abstract": "This paper explores the impact of banking fintech on reducing financial risks\nin the agricultural supply chain, focusing on the secondary allocation of\ncommercial credit. The study constructs a three-player evolutionary game model\ninvolving banks, core enterprises, and SMEs to analyze how fintech innovations,\nsuch as big data credit assessment, blockchain, and AI-driven risk evaluation,\ninfluence financial risks and access to credit. The findings reveal that\nbanking fintech reduces financing costs and mitigates financial risks by\nimproving transaction reliability, enhancing risk identification, and\nminimizing information asymmetry. By optimizing cooperation between banks, core\nenterprises, and SMEs, fintech solutions enhance the stability of the\nagricultural supply chain, contributing to rural revitalization goals and\nsustainable agricultural development. The study provides new theoretical\ninsights and practical recommendations for improving agricultural finance\nsystems and reducing financial risks.\n  Keywords: banking fintech, agricultural supply chain, financial risk,\ncommercial credit, SMEs, evolutionary game model, big data, blockchain,\nAI-driven risk evaluation.",
        "authors": [
            "Qiang Wan",
            "Jun Cui"
        ],
        "categories": "econ.EM",
        "published": "2024-11-12T07:25:27Z",
        "updated": "2024-11-12T07:25:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.07031v1",
        "title": "Evaluating the Accuracy of Chatbots in Financial Literature",
        "abstract": "We evaluate the reliability of two chatbots, ChatGPT (4o and o1-preview\nversions), and Gemini Advanced, in providing references on financial literature\nand employing novel methodologies. Alongside the conventional binary approach\ncommonly used in the literature, we developed a nonbinary approach and a\nrecency measure to assess how hallucination rates vary with how recent a topic\nis. After analyzing 150 citations, ChatGPT-4o had a hallucination rate of 20.0%\n(95% CI, 13.6%-26.4%), while the o1-preview had a hallucination rate of 21.3%\n(95% CI, 14.8%-27.9%). In contrast, Gemini Advanced exhibited higher\nhallucination rates: 76.7% (95% CI, 69.9%-83.4%). While hallucination rates\nincreased for more recent topics, this trend was not statistically significant\nfor Gemini Advanced. These findings emphasize the importance of verifying\nchatbot-provided references, particularly in rapidly evolving fields.",
        "authors": [
            "Orhan Erdem",
            "Kristi Hassett",
            "Feyzullah Egriboyun"
        ],
        "categories": "cs.AI",
        "published": "2024-11-11T14:37:57Z",
        "updated": "2024-11-11T14:37:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.06327v1",
        "title": "Return-forecasting and Volatility-forecasting Power of On-chain Activities in the Cryptocurrency Market",
        "abstract": "We investigate the return-forecasting and volatility-forecasting power of\nintraday on-chain flow data for BTC, ETH, and USDT, and the associated option\nstrategies. First, we find that USDT net inflow into cryptocurrency exchanges\npositively forecasts future returns of both BTC and ETH, with the strongest\neffect at the 1-hour frequency. Second, we find that ETH net inflow into\ncryptocurrency exchanges negatively forecasts future returns of ETH. Third, we\nfind that BTC net inflow into cryptocurrency exchanges does not significantly\nforecast future returns of BTC. Finally, we confirm that selling 0DTE ETH call\noptions is a profitable trading strategy when the net inflow into\ncryptocurrency exchanges is high. Our study lends new insights into the\nemerging literature that studies the on-chain activities and their\nasset-pricing impact in the cryptocurrency market.",
        "authors": [
            "Yeguang Chi",
            "Qionghua",
            "Chu",
            "Wenyan Hao"
        ],
        "categories": "econ.EM",
        "published": "2024-11-10T01:27:49Z",
        "updated": "2024-11-10T01:27:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.05758v1",
        "title": "On the limiting variance of matching estimators",
        "abstract": "This paper examines the limiting variance of nearest neighbor matching\nestimators for average treatment effects with a fixed number of matches. We\npresent, for the first time, a closed-form expression for this limit. Here the\nkey is the establishment of the limiting second moment of the catchment area's\nvolume, which resolves a question of Abadie and Imbens. At the core of our\napproach is a new universality theorem on the measures of high-order Voronoi\ncells, extending a result by Devroye, Gy\\\"orfi, Lugosi, and Walk.",
        "authors": [
            "Songliang Chen",
            "Fang Han"
        ],
        "categories": "math.ST",
        "published": "2024-11-08T18:19:29Z",
        "updated": "2024-11-08T18:19:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.05695v1",
        "title": "Firm Heterogeneity and Macroeconomic Fluctuations: a Functional VAR model",
        "abstract": "We develop a Functional Augmented Vector Autoregression (FunVAR) model to\nexplicitly incorporate firm-level heterogeneity observed in more than one\ndimension and study its interaction with aggregate macroeconomic fluctuations.\nOur methodology employs dimensionality reduction techniques for tensor data\nobjects to approximate the joint distribution of firm-level characteristics.\nMore broadly, our framework can be used for assessing predictions from\nstructural models that account for micro-level heterogeneity observed on\nmultiple dimensions. Leveraging firm-level data from the Compustat database, we\nuse the FunVAR model to analyze the propagation of total factor productivity\n(TFP) shocks, examining their impact on both macroeconomic aggregates and the\ncross-sectional distribution of capital and labor across firms.",
        "authors": [
            "Massimiliano Marcellino",
            "Andrea Renzetti",
            "Tommaso Tornese"
        ],
        "categories": "econ.EM",
        "published": "2024-11-08T16:49:59Z",
        "updated": "2024-11-08T16:49:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.05629v1",
        "title": "Nowcasting distributions: a functional MIDAS model",
        "abstract": "We propose a functional MIDAS model to leverage high-frequency information\nfor forecasting and nowcasting distributions observed at a lower frequency. We\napproximate the low-frequency distribution using Functional Principal Component\nAnalysis and consider a group lasso spike-and-slab prior to identify the\nrelevant predictors in the finite-dimensional SUR-MIDAS approximation of the\nfunctional MIDAS model. In our application, we use the model to nowcast the\nU.S. households' income distribution. Our findings indicate that the model\nenhances forecast accuracy for the entire target distribution and for key\nfeatures of the distribution that signal changes in inequality.",
        "authors": [
            "Massimiliano Marcellino",
            "Andrea Renzetti",
            "Tommaso Tornese"
        ],
        "categories": "econ.EM",
        "published": "2024-11-08T15:20:21Z",
        "updated": "2024-11-08T15:20:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.05601v1",
        "title": "Detecting Cointegrating Relations in Non-stationary Matrix-Valued Time Series",
        "abstract": "This paper proposes a Matrix Error Correction Model to identify cointegration\nrelations in matrix-valued time series. We hereby allow separate cointegrating\nrelations along the rows and columns of the matrix-valued time series and use\ninformation criteria to select the cointegration ranks. Through Monte Carlo\nsimulations and a macroeconomic application, we demonstrate that our approach\nprovides a reliable estimation of the number of cointegrating relationships.",
        "authors": [
            "Alain Hecq",
            "Ivan Ricardo",
            "Ines Wilms"
        ],
        "categories": "econ.EM",
        "published": "2024-11-08T14:43:31Z",
        "updated": "2024-11-08T14:43:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.05220v1",
        "title": "Inference for Treatment Effects Conditional on Generalized Principal Strata using Instrumental Variables",
        "abstract": "In a setting with a multi-valued outcome, treatment and instrument, this\npaper considers the problem of inference for a general class of treatment\neffect parameters. The class of parameters considered are those that can be\nexpressed as the expectation of a function of the response type conditional on\na generalized principal stratum. Here, the response type simply refers to the\nvector of potential outcomes and potential treatments, and a generalized\nprincipal stratum is a set of possible values for the response type. In\naddition to instrument exogeneity, the main substantive restriction imposed\nrules out certain values for the response types in the sense that they are\nassumed to occur with probability zero. It is shown through a series of\nexamples that this framework includes a wide variety of parameters and\nassumptions that have been considered in the previous literature. A key result\nin our analysis is a characterization of the identified set for such parameters\nunder these assumptions in terms of existence of a non-negative solution to\nlinear systems of equations with a special structure. We propose methods for\ninference exploiting this special structure and recent results in Fang et al.\n(2023).",
        "authors": [
            "Yuehao Bai",
            "Shunzhuang Huang",
            "Sarah Moon",
            "Andres Santos",
            "Azeem M. Shaikh",
            "Edward J. Vytlacil"
        ],
        "categories": "econ.EM",
        "published": "2024-11-07T22:29:47Z",
        "updated": "2024-11-07T22:29:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.04640v1",
        "title": "The role of expansion strategies and operational attributes on hotel performance: a compositional approach",
        "abstract": "This study aims to explore the impact of expansion strategies and specific\nattributes of hotel establishments on the performance of international hotel\nchains, focusing on four key performance indicators: RevPAR, efficiency,\noccupancy, and asset turnover. Data were collected from 255 hotels across\nvarious international hotel chains, providing a comprehensive assessment of how\ndifferent expansion strategies and hotel attributes influence performance. The\nresearch employs compositional data analysis (CoDA) to address the\nmethodological limitations of traditional financial ratios in statistical\nanalysis. The findings indicate that ownership-based expansion strategies\nresult in higher operational performance, as measured by revenue per available\nroom, but yield lower economic performance due to the high capital investment\nrequired. Non-ownership strategies, such as management contracts and\nfranchising, show superior economic efficiency, offering more flexibility and\nreduced financial risk. This study contributes to the hospitality management\nliterature by applying CoDA, a novel methodological approach in this field, to\nexamine the performance of different hotel expansion strategies with a sound\nand more appropriate method. The insights provided can guide hotel managers and\ninvestors in making informed decisions to optimize both operational and\neconomic performance.",
        "authors": [
            "Carles Mulet-Forteza",
            "Berta Ferrer-Rosell",
            "Onofre Martorell Cunill",
            "Salvador Linares-Mustar\u00f3s"
        ],
        "categories": "econ.EM",
        "published": "2024-11-07T11:52:30Z",
        "updated": "2024-11-07T11:52:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.04450v1",
        "title": "Partial Identification of Distributional Treatment Effects in Panel Data using Copula Equality Assumptions",
        "abstract": "This paper aims to partially identify the distributional treatment effects\n(DTEs) that depend on the unknown joint distribution of treated and untreated\npotential outcomes. We construct the DTE bounds using panel data and allow\nindividuals to switch between the treated and untreated states more than once\nover time. Individuals are grouped based on their past treatment history, and\nDTEs are allowed to be heterogeneous across different groups. We provide two\nalternative group-wise copula equality assumptions to bound the unknown joint\nand the DTEs, both of which leverage information from the past observations.\nTestability of these two assumptions are also discussed, and test results are\npresented. We apply this method to study the treatment effect heterogeneity of\nexercising on the adults' body weight. These results demonstrate that our\nmethod improves the identification power of the DTE bounds compared to the\nexisting methods.",
        "authors": [
            "Heshani Madigasekara",
            "D. S. Poskitt",
            "Lina Zhang",
            "Xueyan Zhao"
        ],
        "categories": "econ.EM",
        "published": "2024-11-07T05:45:11Z",
        "updated": "2024-11-07T05:45:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.04380v1",
        "title": "Identification of Long-Term Treatment Effects via Temporal Links, Observational, and Experimental Data",
        "abstract": "Recent literature proposes combining short-term experimental and long-term\nobservational data to provide credible alternatives to conventional\nobservational studies for identification of long-term average treatment effects\n(LTEs). I show that experimental data have an auxiliary role in this context.\nThey bring no identifying power without additional modeling assumptions. When\nmodeling assumptions are imposed, experimental data serve to amplify their\nidentifying power. If the assumptions fail, adding experimental data may only\nyield results that are farther from the truth. Motivated by this, I introduce\ntwo assumptions on treatment response that may be defensible based on economic\ntheory or intuition. To utilize them, I develop a novel two-step identification\napproach that centers on bounding temporal link functions -- the relationship\nbetween short-term and mean long-term potential outcomes. The approach provides\nsharp bounds on LTEs for a general class of assumptions, and allows for\nimperfect experimental compliance -- extending existing results.",
        "authors": [
            "Filip Obradovi\u0107"
        ],
        "categories": "econ.EM",
        "published": "2024-11-07T02:47:13Z",
        "updated": "2024-11-07T02:47:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.04312v1",
        "title": "Lee Bounds with a Continuous Treatment in Sample Selection",
        "abstract": "Sample selection problems arise when treatment affects both the outcome and\nthe researcher's ability to observe it. This paper generalizes Lee (2009)\nbounds for the average treatment effect of a binary treatment to a\ncontinuous/multivalued treatment. We evaluate the Job Crops program to study\nthe causal effect of training hours on wages. To identify the average treatment\neffect of always-takers who are selected regardless of the treatment values, we\nassume that if a subject is selected at some sufficient treatment values, then\nit remains selected at all treatment values. For example, if program\nparticipants are employed with one month of training, then they remain employed\nwith any training hours. This sufficient treatment values assumption includes\nthe monotone assumption on the treatment effect on selection as a special case.\nWe further allow the conditional independence assumption and subjects with\ndifferent pretreatment covariates to have different sufficient treatment\nvalues. The estimation and inference theory utilize the orthogonal moment\nfunction and cross-fitting for double debiased machine learning.",
        "authors": [
            "Ying-Ying Lee",
            "Chu-An Liu"
        ],
        "categories": "econ.EM",
        "published": "2024-11-06T23:30:27Z",
        "updated": "2024-11-06T23:30:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.04286v1",
        "title": "Bounded Rationality in Central Bank Communication",
        "abstract": "This study explores the influence of FOMC sentiment on market expectations,\nfocusing on cognitive differences between experts and non-experts. Using\nsentiment analysis of FOMC minutes, we integrate these insights into a bounded\nrationality model to examine the impact on inflation expectations. Results show\nthat experts form more conservative expectations, anticipating FOMC\nstabilization actions, while non-experts react more directly to inflation\nconcerns. A lead-lag analysis indicates that institutions adjust faster, though\nthe gap with individual investors narrows in the short term. These findings\nhighlight the need for tailored communication strategies to better align public\nexpectations with policy goals.",
        "authors": [
            "Wonseong Kim",
            "Choong Lyol Lee"
        ],
        "categories": "econ.EM",
        "published": "2024-11-06T22:13:14Z",
        "updated": "2024-11-06T22:13:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.04239v1",
        "title": "An Adversarial Approach to Identification and Inference",
        "abstract": "We introduce a novel framework to characterize identified sets of structural\nand counterfactual parameters in econometric models. Our framework centers on a\ndiscrepancy function, which we construct using insights from convex analysis.\nThe zeros of the discrepancy function determine the identified set, which may\nbe a singleton. The discrepancy function has an adversarial game\ninterpretation: a critic maximizes the discrepancy between data and model\nfeatures, while a defender minimizes it by adjusting the probability measure of\nthe unobserved heterogeneity. Our approach enables fast computation via linear\nprogramming. We use the sample analog of the discrepancy function as a test\nstatistic, and show that it provides asymptotically valid inference for the\nidentified set. Applied to nonlinear panel models with fixed effects, it offers\na unified approach for identifying both structural and counterfactual\nparameters across exogeneity conditions, including strict and sequential,\nwithout imposing parametric restrictions on the distribution of error terms or\nfunctional form assumptions.",
        "authors": [
            "Irene Botosaru",
            "Isaac Loh",
            "Chris Muris"
        ],
        "categories": "econ.EM",
        "published": "2024-11-06T20:08:05Z",
        "updated": "2024-11-06T20:08:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.03625v2",
        "title": "Identification and Inference in General Bunching Designs",
        "abstract": "This paper develops a formal econometric framework and tools for the\nidentification and inference of a structural parameter in general bunching\ndesigns. We present both point and partial identification results, which\ngeneralize previous approaches in the literature. The key assumption for point\nidentification is the analyticity of the counterfactual density, which defines\na broader class of distributions than many well-known parametric families. In\nthe partial identification approach, the analyticity condition is relaxed and\nvarious shape restrictions can be incorporated, including those found in the\nliterature. Both of our identification results account for observable\nheterogeneity in the model, which has previously been permitted only in limited\nways. We provide a suite of counterfactual estimation and inference methods,\ntermed the generalized polynomial strategy. Our method restores the merits of\nthe original polynomial strategy proposed by Chetty et al. (2011) while\naddressing several weaknesses in the widespread practice. The efficacy of the\nproposed method is demonstrated compared to a version of the polynomial\nestimator in a series of Monte Carlo studies within the augmented isoelastic\nmodel. We revisit the data used in Saez (2010) and find substantially different\nresults relative to those from the polynomial strategy.",
        "authors": [
            "Myunghyun Song"
        ],
        "categories": "econ.EM",
        "published": "2024-11-06T02:46:02Z",
        "updated": "2024-11-19T15:48:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.03530v1",
        "title": "Improving precision of A/B experiments using trigger intensity",
        "abstract": "In industry, online randomized controlled experiment (a.k.a A/B experiment)\nis a standard approach to measure the impact of a causal change. These\nexperiments have small treatment effect to reduce the potential blast radius.\nAs a result, these experiments often lack statistical significance due to low\nsignal-to-noise ratio. To improve the precision (or reduce standard error), we\nintroduce the idea of trigger observations where the output of the treatment\nand the control model are different. We show that the evaluation with full\ninformation about trigger observations (full knowledge) improves the precision\nin comparison to a baseline method. However, detecting all such trigger\nobservations is a costly affair, hence we propose a sampling based evaluation\nmethod (partial knowledge) to reduce the cost. The randomness of sampling\nintroduces bias in the estimated outcome. We theoretically analyze this bias\nand show that the bias is inversely proportional to the number of observations\nused for sampling. We also compare the proposed evaluation methods using\nsimulation and empirical data. In simulation, evaluation with full knowledge\nreduces the standard error as much as 85%. In empirical setup, evaluation with\npartial knowledge reduces the standard error by 36.48%.",
        "authors": [
            "Tanmoy Das",
            "Dohyeon Lee",
            "Arnab Sinha"
        ],
        "categories": "econ.EM",
        "published": "2024-11-05T22:10:37Z",
        "updated": "2024-11-05T22:10:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.03208v3",
        "title": "Randomly Assigned First Differences?",
        "abstract": "I consider treatment-effect estimation with a two-periods panel, using a\nfirst-difference regression of the outcome evolution $\\Delta Y_g$ on the\ntreatment evolution $\\Delta D_g$. To justify this regression, one may assume\nthat $\\Delta D_g$ is as good as randomly assigned, namely uncorrelated to the\nresidual of the first-differenced model and to the treatment's effect. This\nnote shows that if one posits a causal model in levels between the treatment\nand the outcome, then the residual of the first-differenced model is a function\nof $D_{g,1}$, so $\\Delta D_g$ uncorrelated to that residual essentially implies\nthat $\\Delta D_g$ is uncorrelated to $D_{g,1}$. This is a strong, testable\ncondition. If $\\Delta D_g$ is correlated to $D_{g,1}$, a solution is simply to\ncontrol for $D_1$ in the regression. I use these results to revisit Acemoglu et\nal (2016).",
        "authors": [
            "Cl\u00e9ment de Chaisemartin"
        ],
        "categories": "econ.EM",
        "published": "2024-11-05T15:58:39Z",
        "updated": "2024-12-02T00:08:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.03026v1",
        "title": "Robust Market Interventions",
        "abstract": "A large differentiated oligopoly yields inefficient market equilibria. An\nauthority with imprecise information about the primitives of the market aims to\ndesign tax/subsidy interventions that increase efficiency robustly, i.e., with\nhigh probability. We identify a condition on demand that guarantees the\nexistence of such interventions, and we show how to construct them using noisy\nestimates of demand complementarities and substitutabilities across products.\nThe analysis works by deriving a novel description of the incidence of market\ninterventions in terms of spectral statistics of a Slutsky matrix. Our notion\nof recoverable structure ensures that parts of the spectrum that are useful for\nthe design of interventions are statistically recoverable from noisy demand\nestimates.",
        "authors": [
            "Andrea Galeotti",
            "Benjamin Golub",
            "Sanjeev Goyal",
            "Eduard Talam\u00e0s",
            "Omer Tamuz"
        ],
        "categories": "econ.TH",
        "published": "2024-11-05T11:49:11Z",
        "updated": "2024-11-05T11:49:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.02804v1",
        "title": "Beyond the Traditional VIX: A Novel Approach to Identifying Uncertainty Shocks in Financial Markets",
        "abstract": "We introduce a new identification strategy for uncertainty shocks to explain\nmacroeconomic volatility in financial markets. The Chicago Board Options\nExchange Volatility Index (VIX) measures market expectations of future\nvolatility, but traditional methods based on second-moment shocks and\ntime-varying volatility of the VIX often fail to capture the non-Gaussian,\nheavy-tailed nature of asset returns. To address this, we construct a revised\nVIX by fitting a double-subordinated Normal Inverse Gaussian Levy process to\nS&P 500 option prices, providing a more comprehensive measure of volatility\nthat reflects the extreme movements and heavy tails observed in financial data.\nUsing an axiomatic approach, we introduce a general family of risk-reward\nratios, computed with our revised VIX and fitted over a fractional time series\nto more accurately identify uncertainty shocks in financial markets.",
        "authors": [
            "Ayush Jha",
            "Abootaleb Shirvani",
            "Svetlozar T. Rachev",
            "Frank J. Fabozzi"
        ],
        "categories": "econ.EM",
        "published": "2024-11-05T04:34:27Z",
        "updated": "2024-11-05T04:34:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.02675v1",
        "title": "Does Regression Produce Representative Causal Rankings?",
        "abstract": "We examine the challenges in ranking multiple treatments based on their\nestimated effects when using linear regression or its popular\ndouble-machine-learning variant, the Partially Linear Model (PLM), in the\npresence of treatment effect heterogeneity. We demonstrate by example that\noverlap-weighting performed by linear models like PLM can produce Weighted\nAverage Treatment Effects (WATE) that have rankings that are inconsistent with\nthe rankings of the underlying Average Treatment Effects (ATE). We define this\nas ranking reversals and derive a necessary and sufficient condition for\nranking reversals under the PLM. We conclude with several simulation studies\nconditions under which ranking reversals occur.",
        "authors": [
            "Apoorva Lal"
        ],
        "categories": "econ.EM",
        "published": "2024-11-04T23:25:24Z",
        "updated": "2024-11-04T23:25:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.02531v3",
        "title": "Comment on 'Sparse Bayesian Factor Analysis when the Number of Factors is Unknown' by S. Fr\u00fchwirth-Schnatter, D. Hosszejni, and H. Freitas Lopes",
        "abstract": "The techniques suggested in Fr\\\"uhwirth-Schnatter et al. (2024) concern\nsparsity and factor selection and have enormous potential beyond standard\nfactor analysis applications. We show how these techniques can be applied to\nLatent Space (LS) models for network data. These models suffer from well-known\nidentification issues of the latent factors due to likelihood invariance to\nfactor translation, reflection, and rotation (see Hoff et al., 2002). A set of\nobservables can be instrumental in identifying the latent factors via auxiliary\nequations (see Liu et al., 2021). These, in turn, share many analogies with the\nequations used in factor modeling, and we argue that the factor loading\nrestrictions may be beneficial for achieving identification.",
        "authors": [
            "Roberto Casarin",
            "Antonio Peruzzi"
        ],
        "categories": "stat.ME",
        "published": "2024-11-04T19:07:38Z",
        "updated": "2024-11-14T13:37:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.02374v1",
        "title": "Identifying Economic Factors Affecting Unemployment Rates in the United States",
        "abstract": "In this study, we seek to understand how macroeconomic factors such as GDP,\ninflation, Unemployment Insurance, and S&P 500 index; as well as microeconomic\nfactors such as health, race, and educational attainment impacted the\nunemployment rate for about 20 years in the United States. Our research\nquestion is to identify which factor(s) contributed the most to the\nunemployment rate surge using linear regression. Results from our studies\nshowed that GDP (negative), inflation (positive), Unemployment Insurance\n(contrary to popular opinion; negative), and S&P 500 index (negative) were all\nsignificant factors, with inflation being the most important one. As for health\nissue factors, our model produced resultant correlation scores for occurrences\nof Cardiovascular Disease, Neurological Disease, and Interpersonal Violence\nwith unemployment. Race as a factor showed a huge discrepancies in the\nunemployment rate between Black Americans compared to their counterparts.\nAsians had the lowest unemployment rate throughout the years. As for education\nattainment, results showed that having a higher education attainment\nsignificantly reduced one chance of unemployment. People with higher degrees\nhad the lowest unemployment rate. Results of this study will be beneficial for\npolicymakers and researchers in understanding the unemployment rate during the\npandemic.",
        "authors": [
            "Alrick Green",
            "Ayesha Nasim",
            "Jaydeep Radadia",
            "Devi Manaswi Kallam",
            "Viswas Kalyanam",
            "Samfred Owenga",
            "Huthaifa I. Ashqar"
        ],
        "categories": "cs.CY",
        "published": "2024-11-04T18:43:29Z",
        "updated": "2024-11-04T18:43:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.01864v1",
        "title": "On the Asymptotic Properties of Debiased Machine Learning Estimators",
        "abstract": "This paper studies the properties of debiased machine learning (DML)\nestimators under a novel asymptotic framework, offering insights for improving\nthe performance of these estimators in applications. DML is an estimation\nmethod suited to economic models where the parameter of interest depends on\nunknown nuisance functions that must be estimated. It requires weaker\nconditions than previous methods while still ensuring standard asymptotic\nproperties. Existing theoretical results do not distinguish between two\nalternative versions of DML estimators, DML1 and DML2. Under a new asymptotic\nframework, this paper demonstrates that DML2 asymptotically dominates DML1 in\nterms of bias and mean squared error, formalizing a previous conjecture based\non simulation results regarding their relative performance. Additionally, this\npaper provides guidance for improving the performance of DML2 in applications.",
        "authors": [
            "Amilcar Velez"
        ],
        "categories": "econ.EM",
        "published": "2024-11-04T07:35:19Z",
        "updated": "2024-11-04T07:35:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.01799v1",
        "title": "Estimating Nonseparable Selection Models: A Functional Contraction Approach",
        "abstract": "We propose a new method for estimating nonseparable selection models. We show\nthat, given the selection rule and the observed selected outcome distribution,\nthe potential outcome distribution can be characterized as the fixed point of\nan operator, and we prove that this operator is a functional contraction. We\npropose a two-step semiparametric maximum likelihood estimator to estimate the\nselection model and the potential outcome distribution. The consistency and\nasymptotic normality of the estimator are established. Our approach performs\nwell in Monte Carlo simulations and is applicable in a variety of empirical\nsettings where only a selected sample of outcomes is observed. Examples include\nconsumer demand models with only transaction prices, auctions with incomplete\nbid data, and Roy models with data on accepted wages.",
        "authors": [
            "Fan Wu",
            "Yi Xin"
        ],
        "categories": "econ.EM",
        "published": "2024-11-04T04:53:50Z",
        "updated": "2024-11-04T04:53:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.01704v1",
        "title": "Understanding the decision-making process of choice modellers",
        "abstract": "Discrete Choice Modelling serves as a robust framework for modelling human\nchoice behaviour across various disciplines. Building a choice model is a semi\nstructured research process that involves a combination of a priori\nassumptions, behavioural theories, and statistical methods. This complex set of\ndecisions, coupled with diverse workflows, can lead to substantial variability\nin model outcomes. To better understand these dynamics, we developed the\nSerious Choice Modelling Game, which simulates the real world modelling process\nand tracks modellers' decisions in real time using a stated preference dataset.\nParticipants were asked to develop choice models to estimate Willingness to Pay\nvalues to inform policymakers about strategies for reducing noise pollution.\nThe game recorded actions across multiple phases, including descriptive\nanalysis, model specification, and outcome interpretation, allowing us to\nanalyse both individual decisions and differences in modelling approaches.\nWhile our findings reveal a strong preference for using data visualisation\ntools in descriptive analysis, it also identifies gaps in missing values\nhandling before model specification. We also found significant variation in the\nmodelling approach, even when modellers were working with the same choice\ndataset. Despite the availability of more complex models, simpler models such\nas Multinomial Logit were often preferred, suggesting that modellers tend to\navoid complexity when time and resources are limited. Participants who engaged\nin more comprehensive data exploration and iterative model comparison tended to\nachieve better model fit and parsimony, which demonstrate that the\nmethodological choices made throughout the workflow have significant\nimplications, particularly when modelling outcomes are used for policy\nformulation.",
        "authors": [
            "Gabriel Nova",
            "Sander van Cranenburgh",
            "Stephane Hess"
        ],
        "categories": "econ.EM",
        "published": "2024-11-03T22:23:47Z",
        "updated": "2024-11-03T22:23:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.01617v1",
        "title": "Changes-In-Changes For Discrete Treatment",
        "abstract": "This paper generalizes the changes-in-changes (CIC) model to handle discrete\ntreatments with more than two categories, extending the binary case of Athey\nand Imbens (2006). While the original CIC model is well-suited for binary\ntreatments, it cannot accommodate multi-category discrete treatments often\nfound in economic and policy settings. Although recent work has extended CIC to\ncontinuous treatments, there remains a gap for multi-category discrete\ntreatments. I introduce a generalized CIC model that adapts the rank invariance\nassumption to multiple treatment levels, allowing for robust modeling while\ncapturing the distinct effects of varying treatment intensities.",
        "authors": [
            "Onil Boussim"
        ],
        "categories": "econ.EM",
        "published": "2024-11-03T16:07:26Z",
        "updated": "2024-11-03T16:07:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.01498v1",
        "title": "Educational Effects in Mathematics: Conditional Average Treatment Effect depending on the Number of Treatments",
        "abstract": "This study examines the educational effect of the Academic Support Center at\nKogakuin University. Following the initial assessment, it was suggested that\ngroup bias had led to an underestimation of the Center's true impact. To\naddress this issue, the authors applied the theory of causal inference. By\nusing T-learner, the conditional average treatment effect (CATE) of the\nCenter's face-to-face (F2F) personal assistance program was evaluated.\nExtending T-learner, the authors produced a new CATE function that depends on\nthe number of treatments (F2F sessions) and used the estimated function to\npredict the CATE performance of F2F assistance.",
        "authors": [
            "Tomoko Nagai",
            "Takayuki Okuda",
            "Tomoya Nakamura",
            "Yuichiro Sato",
            "Yusuke Sato",
            "Kensaku Kinjo",
            "Kengo Kawamura",
            "Shin Kikuta",
            "Naoto Kumano-go"
        ],
        "categories": "stat.ME",
        "published": "2024-11-03T09:39:55Z",
        "updated": "2024-11-03T09:39:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.01064v1",
        "title": "Empirical Welfare Analysis with Hedonic Budget Constraints",
        "abstract": "We analyze demand settings where heterogeneous consumers maximize utility for\nproduct attributes subject to a nonlinear budget constraint. We develop\nnonparametric methods for welfare-analysis of interventions that change the\nconstraint. Two new findings are Roy's identity for smooth, nonlinear budgets,\nwhich yields a Partial Differential Equation system, and a Slutsky-like\nsymmetry condition for demand. Under scalar unobserved heterogeneity and\nsingle-crossing preferences, the coefficient functions in the PDEs are\nnonparametrically identified, and under symmetry, lead to path-independent,\nmoney-metric welfare. We illustrate our methods with welfare evaluation of a\nhypothetical change in relationship between property rent and neighborhood\nschool-quality using British microdata.",
        "authors": [
            "Debopam Bhattacharya",
            "Ekaterina Oparina",
            "Qianya Xu"
        ],
        "categories": "econ.EM",
        "published": "2024-11-01T22:38:00Z",
        "updated": "2024-11-01T22:38:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.00945v1",
        "title": "Higher-Order Causal Message Passing for Experimentation with Complex Interference",
        "abstract": "Accurate estimation of treatment effects is essential for decision-making\nacross various scientific fields. This task, however, becomes challenging in\nareas like social sciences and online marketplaces, where treating one\nexperimental unit can influence outcomes for others through direct or indirect\ninteractions. Such interference can lead to biased treatment effect estimates,\nparticularly when the structure of these interactions is unknown. We address\nthis challenge by introducing a new class of estimators based on causal\nmessage-passing, specifically designed for settings with pervasive, unknown\ninterference. Our estimator draws on information from the sample mean and\nvariance of unit outcomes and treatments over time, enabling efficient use of\nobserved data to estimate the evolution of the system state. Concretely, we\nconstruct non-linear features from the moments of unit outcomes and treatments\nand then learn a function that maps these features to future mean and variance\nof unit outcomes. This allows for the estimation of the treatment effect over\ntime. Extensive simulations across multiple domains, using synthetic and real\nnetwork data, demonstrate the efficacy of our approach in estimating total\ntreatment effect dynamics, even in cases where interference exhibits\nnon-monotonic behavior in the probability of treatment.",
        "authors": [
            "Mohsen Bayati",
            "Yuwei Luo",
            "William Overman",
            "Sadegh Shirani",
            "Ruoxuan Xiong"
        ],
        "categories": "cs.LG",
        "published": "2024-11-01T18:00:51Z",
        "updated": "2024-11-01T18:00:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.00520v1",
        "title": "Calibrated quantile prediction for Growth-at-Risk",
        "abstract": "Accurate computation of robust estimates for extremal quantiles of empirical\ndistributions is an essential task for a wide range of applicative fields,\nincluding economic policymaking and the financial industry. Such estimates are\nparticularly critical in calculating risk measures, such as Growth-at-Risk\n(GaR). % and Value-at-Risk (VaR). This work proposes a conformal framework to\nestimate calibrated quantiles, and presents an extensive simulation study and a\nreal-world analysis of GaR to examine its benefits with respect to the state of\nthe art. Our findings show that CP methods consistently improve the calibration\nand robustness of quantile estimates at all levels. The calibration gains are\nappreciated especially at extremal quantiles, which are critical for risk\nassessment and where traditional methods tend to fall short. In addition, we\nintroduce a novel property that guarantees coverage under the exchangeability\nassumption, providing a valuable tool for managing risks by quantifying and\ncontrolling the likelihood of future extreme observations.",
        "authors": [
            "Pietro Bogani",
            "Matteo Fontana",
            "Luca Neri",
            "Simone Vantini"
        ],
        "categories": "stat.ME",
        "published": "2024-11-01T11:38:37Z",
        "updated": "2024-11-01T11:38:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.00358v1",
        "title": "Inference in a Stationary/Nonstationary Autoregressive Time-Varying-Parameter Model",
        "abstract": "This paper considers nonparametric estimation and inference in first-order\nautoregressive (AR(1)) models with deterministically time-varying parameters. A\nkey feature of the proposed approach is to allow for time-varying stationarity\nin some time periods, time-varying nonstationarity (i.e., unit root or\nlocal-to-unit root behavior) in other periods, and smooth transitions between\nthe two. The estimation of the AR parameter at any time point is based on a\nlocal least squares regression method, where the relevant initial condition is\nendogenous. We obtain limit distributions for the AR parameter estimator and\nt-statistic at a given point $\\tau$ in time when the parameter exhibits unit\nroot, local-to-unity, or stationary/stationary-like behavior at time $\\tau$.\nThese results are used to construct confidence intervals and median-unbiased\ninterval estimators for the AR parameter at any specified point in time. The\nconfidence intervals have correct asymptotic coverage probabilities with the\ncoverage holding uniformly over stationary and nonstationary behavior of the\nobservations.",
        "authors": [
            "Donald W. K. Andrews",
            "Ming Li"
        ],
        "categories": "econ.EM",
        "published": "2024-11-01T04:46:12Z",
        "updated": "2024-11-01T04:46:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2411.00886v1",
        "title": "The ET Interview: Professor Joel L. Horowitz",
        "abstract": "Joel L. Horowitz has made profound contributions to many areas in\neconometrics and statistics. These include bootstrap methods, semiparametric\nand nonparametric estimation, specification testing, nonparametric instrumental\nvariables estimation, high-dimensional models, functional data analysis, and\nshape restrictions, among others. Originally trained as a physicist, Joel made\na pivotal transition to econometrics, greatly benefiting our profession.\nThroughout his career, he has collaborated extensively with a diverse range of\ncoauthors, including students, departmental colleagues, and scholars from\naround the globe. Joel was born in 1941 in Pasadena, California. He attended\nStanford for his undergraduate studies and obtained his Ph.D. in physics from\nCornell in 1967. He has been Charles E. and Emma H. Morrison Professor of\nEconomics at Northwestern University since 2001. Prior to that, he was a\nfaculty member at the University of Iowa (1982-2001). He has served as a\nco-editor of Econometric Theory (1992-2000) and Econometrica (2000-2004). He is\na Fellow of the Econometric Society and of the American Statistical\nAssociation, and an elected member of the International Statistical Institute.\nThe majority of this interview took place in London during June 2022.",
        "authors": [
            "Sokbae Lee"
        ],
        "categories": "econ.EM",
        "published": "2024-10-31T18:37:58Z",
        "updated": "2024-10-31T18:37:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.23852v1",
        "title": "Estimation and Inference in Dyadic Network Formation Models with Nontransferable Utilities",
        "abstract": "This paper studies estimation and inference in a dyadic network formation\nmodel with observed covariates, unobserved heterogeneity, and nontransferable\nutilities. With the presence of the high dimensional fixed effects, the maximum\nlikelihood estimator is numerically difficult to compute and suffers from the\nincidental parameter bias. We propose an easy-to-compute one-step estimator for\nthe homophily parameter of interest, which is further refined to achieve\n$\\sqrt{N}$-consistency via split-network jackknife and efficiency by the\nbootstrap aggregating (bagging) technique. We establish consistency for the\nestimator of the fixed effects and prove asymptotic normality for the\nunconditional average partial effects. Simulation studies show that our method\nworks well with finite samples, and an empirical application using the\nrisk-sharing data from Nyakatoke highlights the importance of employing proper\nstatistical inferential procedures.",
        "authors": [
            "Ming Li",
            "Zhentao Shi",
            "Yapeng Zheng"
        ],
        "categories": "econ.EM",
        "published": "2024-10-31T12:02:07Z",
        "updated": "2024-10-31T12:02:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.23785v1",
        "title": "Machine Learning Debiasing with Conditional Moment Restrictions: An Application to LATE",
        "abstract": "Models with Conditional Moment Restrictions (CMRs) are popular in economics.\nThese models involve finite and infinite dimensional parameters. The infinite\ndimensional components include conditional expectations, conditional choice\nprobabilities, or policy functions, which might be flexibly estimated using\nMachine Learning tools. This paper presents a characterization of locally\ndebiased moments for regular models defined by general semiparametric CMRs with\npossibly different conditioning variables. These moments are appealing as they\nare known to be less affected by first-step bias. Additionally, we study their\nexistence and relevance. Such results apply to a broad class of smooth\nfunctionals of finite and infinite dimensional parameters that do not\nnecessarily appear in the CMRs. As a leading application of our theory, we\ncharacterize debiased machine learning for settings of treatment effects with\nendogeneity, giving necessary and sufficient conditions. We present a large\nclass of relevant debiased moments in this context. We then propose the\nCompliance Machine Learning Estimator (CML), based on a practically convenient\northogonal relevant moment. We show that the resulting estimand can be written\nas a convex combination of conditional local average treatment effects (LATE).\nAltogether, CML enjoys three appealing properties in the LATE framework: (1)\nlocal robustness to first-stage estimation, (2) an estimand that can be\nidentified under a minimal relevance condition, and (3) a meaningful causal\ninterpretation. Our numerical experimentation shows satisfactory relative\nperformance of such an estimator. Finally, we revisit the Oregon Health\nInsurance Experiment, analyzed by Finkelstein et al. (2012). We find that the\nuse of machine learning and CML suggest larger positive effects on health care\nutilization than previously determined.",
        "authors": [
            "Facundo Arga\u00f1araz",
            "Juan Carlos Escanciano"
        ],
        "categories": "econ.EM",
        "published": "2024-10-31T10:00:23Z",
        "updated": "2024-10-31T10:00:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.23587v1",
        "title": "Fractional Moments by the Moment-Generating Function",
        "abstract": "We introduce a novel method for obtaining a wide variety of moments of a\nrandom variable with a well-defined moment-generating function (MGF). We derive\nnew expressions for fractional moments and fractional absolute moments, both\ncentral and non-central moments. The new moment expressions are relatively\nsimple integrals that involve the MGF, but do not require its derivatives. We\nlabel the new method CMGF because it uses a complex extension of the MGF and\ncan be used to obtain complex moments. We illustrate the new method with three\napplications where the MGF is available in closed-form, while the corresponding\ndensities and the derivatives of the MGF are either unavailable or very\ndifficult to obtain.",
        "authors": [
            "Peter Reinhard Hansen",
            "Chen Tong"
        ],
        "categories": "econ.EM",
        "published": "2024-10-31T02:58:56Z",
        "updated": "2024-10-31T02:58:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.23525v2",
        "title": "On the consistency of bootstrap for matching estimators",
        "abstract": "In a landmark paper, Abadie and Imbens (2008) showed that the naive bootstrap\nis inconsistent when applied to nearest neighbor matching estimators of the\naverage treatment effect with a fixed number of matches. Since then, this\nfinding has inspired numerous efforts to address the inconsistency issue,\ntypically by employing alternative bootstrap methods. In contrast, this paper\nshows that the naive bootstrap is provably consistent for the original matching\nestimator, provided that the number of matches, $M$, diverges. The bootstrap\ninconsistency identified by Abadie and Imbens (2008) thus arises solely from\nthe use of a fixed $M$.",
        "authors": [
            "Ziming Lin",
            "Fang Han"
        ],
        "categories": "math.ST",
        "published": "2024-10-31T00:14:18Z",
        "updated": "2024-11-19T18:28:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.22574v1",
        "title": "Inference in Partially Linear Models under Dependent Data with Deep Neural Networks",
        "abstract": "I consider inference in a partially linear regression model under stationary\n$\\beta$-mixing data after first stage deep neural network (DNN) estimation.\nUsing the DNN results of Brown (2024), I show that the estimator for the finite\ndimensional parameter, constructed using DNN-estimated nuisance components,\nachieves $\\sqrt{n}$-consistency and asymptotic normality. By avoiding sample\nsplitting, I address one of the key challenges in applying machine learning\ntechniques to econometric models with dependent data. In a future version of\nthis work, I plan to extend these results to obtain general conditions for\nsemiparametric inference after DNN estimation of nuisance components, which\nwill allow for considerations such as more efficient estimation procedures, and\ninstrumental variable settings.",
        "authors": [
            "Chad Brown"
        ],
        "categories": "econ.EM",
        "published": "2024-10-29T22:29:31Z",
        "updated": "2024-10-29T22:29:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.21516v2",
        "title": "Forecasting Political Stability in GCC Countries",
        "abstract": "Political stability is crucial for the socioeconomic development of nations,\nparticularly in geopolitically sensitive regions such as the Gulf Cooperation\nCouncil Countries, Saudi Arabia, UAE, Kuwait, Qatar, Oman, and Bahrain. This\nstudy focuses on predicting the political stability index for these six\ncountries using machine learning techniques. The study uses data from the World\nBanks comprehensive dataset, comprising 266 indicators covering economic,\npolitical, social, and environmental factors. Employing the Edit Distance on\nReal Sequence method for feature selection and XGBoost for model training, the\nstudy forecasts political stability trends for the next five years. The model\nachieves high accuracy, with mean absolute percentage error values under 10,\nindicating reliable predictions. The forecasts suggest that Oman, the UAE, and\nQatar will experience relatively stable political conditions, while Saudi\nArabia and Bahrain may continue to face negative political stability indices.\nThe findings underscore the significance of economic factors such as GDP and\nforeign investment, along with variables related to military expenditure and\ninternational tourism, as key predictors of political stability. These results\nprovide valuable insights for policymakers, enabling proactive measures to\nenhance governance and mitigate potential risks.",
        "authors": [
            "Mahdi Goldani"
        ],
        "categories": "econ.EM",
        "published": "2024-10-28T20:36:26Z",
        "updated": "2024-11-04T05:31:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.21505v2",
        "title": "Economic Diversification and Social Progress in the GCC Countries: A Study on the Transition from Oil-Dependency to Knowledge-Based Economies",
        "abstract": "The Gulf Cooperation Council countries -- Oman, Bahrain, Kuwait, UAE, Qatar,\nand Saudi Arabia -- holds strategic significance due to its large oil reserves.\nHowever, these nations face considerable challenges in shifting from\noil-dependent economies to more diversified, knowledge-based systems. This\nstudy examines the progress of Gulf Cooperation Council (GCC) countries in\nachieving economic diversification and social development, focusing on the\nSocial Progress Index (SPI), which provides a broader measure of societal\nwell-being beyond just economic growth. Using data from the World Bank,\ncovering 2010 to 2023, the study employs the XGBoost machine learning model to\nforecast SPI values for the period of 2024 to 2026. Key components of the\nmethodology include data preprocessing, feature selection, and the simulation\nof independent variables through ARIMA modeling. The results highlight\nsignificant improvements in education, healthcare, and women's rights,\ncontributing to enhanced SPI performance across the GCC countries. However,\nnotable challenges persist in areas like personal rights and inclusivity. The\nstudy further indicates that despite economic setbacks caused by global\ndisruptions, including the COVID-19 pandemic and oil price volatility, GCC\nnations are expected to see steady improvements in their SPI scores through\n2027. These findings underscore the critical importance of economic\ndiversification, investment in human capital, and ongoing social reforms to\nreduce dependence on hydrocarbons and build knowledge-driven economies. This\nresearch offers valuable insights for policymakers aiming to strengthen both\nsocial and economic resilience in the region while advancing long-term\nsustainable development goals.",
        "authors": [
            "Mahdi Goldani",
            "Soraya Asadi Tirvan"
        ],
        "categories": "econ.EM",
        "published": "2024-10-28T20:19:59Z",
        "updated": "2024-11-04T05:34:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.21105v1",
        "title": "Difference-in-Differences with Time-varying Continuous Treatments using Double/Debiased Machine Learning",
        "abstract": "We propose a difference-in-differences (DiD) method for a time-varying\ncontinuous treatment and multiple time periods. Our framework assesses the\naverage treatment effect on the treated (ATET) when comparing two non-zero\ntreatment doses. The identification is based on a conditional parallel trend\nassumption imposed on the mean potential outcome under the lower dose, given\nobserved covariates and past treatment histories. We employ kernel-based ATET\nestimators for repeated cross-sections and panel data adopting the\ndouble/debiased machine learning framework to control for covariates and past\ntreatment histories in a data-adaptive manner. We also demonstrate the\nasymptotic normality of our estimation approach under specific regularity\nconditions. In a simulation study, we find a compelling finite sample\nperformance of undersmoothed versions of our estimators in setups with several\nthousand observations.",
        "authors": [
            "Michel F. C. Haddad",
            "Martin Huber",
            "Lucas Z. Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-10-28T15:10:43Z",
        "updated": "2024-10-28T15:10:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.20915v1",
        "title": "On Spatio-Temporal Stochastic Frontier Models",
        "abstract": "In the literature on stochastic frontier models until the early 2000s, the\njoint consideration of spatial and temporal dimensions was often inadequately\naddressed, if not completely neglected. However, from an evolutionary economics\nperspective, the production process of the decision-making units constantly\nchanges over both dimensions: it is not stable over time due to managerial\nenhancements and/or internal or external shocks, and is influenced by the\nnearest territorial neighbours. This paper proposes an extension of the Fusco\nand Vidoli [2013] SEM-like approach, which globally accounts for spatial and\ntemporal effects in the term of inefficiency. In particular, coherently with\nthe stochastic panel frontier literature, two different versions of the model\nare proposed: the time-invariant and the time-varying spatial stochastic\nfrontier models. In order to evaluate the inferential properties of the\nproposed estimators, we first run Monte Carlo experiments and we then present\nthe results of an application to a set of commonly referenced data,\ndemonstrating robustness and stability of estimates across all scenarios.",
        "authors": [
            "Elisa Fusco",
            "Giuseppe Arbia",
            "Francesco Vidoli",
            "Vincenzo Nardelli"
        ],
        "categories": "stat.ME",
        "published": "2024-10-28T10:49:15Z",
        "updated": "2024-10-28T10:49:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.20885v1",
        "title": "A Distributed Lag Approach to the Generalised Dynamic Factor Model (GDFM)",
        "abstract": "We provide estimation and inference for the Generalised Dynamic Factor Model\n(GDFM) under the assumption that the dynamic common component can be expressed\nin terms of a finite number of lags of contemporaneously pervasive factors. The\nproposed estimator is simply an OLS regression of the observed variables on\nfactors extracted via static principal components and therefore avoids\nfrequency domain techniques entirely.",
        "authors": [
            "Philipp Gersing"
        ],
        "categories": "econ.EM",
        "published": "2024-10-28T10:07:06Z",
        "updated": "2024-10-28T10:07:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.20860v2",
        "title": "Robust Network Targeting with Multiple Nash Equilibria",
        "abstract": "Many policy problems involve designing individualized treatment allocation\nrules to maximize the equilibrium social welfare of interacting agents.\nFocusing on large-scale simultaneous decision games with strategic\ncomplementarities, we develop a method to estimate an optimal treatment\nallocation rule that is robust to the presence of multiple equilibria. Our\napproach remains agnostic about changes in the equilibrium selection mechanism\nunder counterfactual policies, and we provide a closed-form expression for the\nboundary of the set-identified equilibrium outcomes. To address the\nincompleteness that arises when an equilibrium selection mechanism is not\nspecified, we use the maximin welfare criterion to select a policy, and\nimplement this policy using a greedy algorithm. We establish a performance\nguarantee for our method by deriving a welfare regret bound, which accounts for\nsampling uncertainty and the use of the greedy algorithm. We demonstrate our\nmethod with an application to the microfinance dataset of Banerjee et al.\n(2013).",
        "authors": [
            "Guanyi Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-10-28T09:22:12Z",
        "updated": "2024-11-10T21:12:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.20628v2",
        "title": "International vulnerability of inflation",
        "abstract": "In a globalised world, inflation in a given country may be becoming less\nresponsive to domestic economic activity, while being increasingly determined\nby international conditions. Consequently, understanding the international\nsources of vulnerability of domestic inflation is turning fundamental for\npolicy makers. In this paper, we propose the construction of Inflation-at-risk\nand Deflation-at-risk measures of vulnerability obtained using factor-augmented\nquantile regressions estimated with international factors extracted from a\nmulti-level Dynamic Factor Model with overlapping blocks of inflations\ncorresponding to economies grouped either in a given geographical region or\naccording to their development level. The methodology is implemented to\ninflation observed monthly from 1999 to 2022 for over 115 countries. We\nconclude that, in a large number of developed countries, international factors\nare relevant to explain the right tail of the distribution of inflation, and,\nconsequently, they are more relevant for the vulnerability related to high\ninflation than for average or low inflation. However, while inflation of\ndeveloping low-income countries is hardly affected by international conditions,\nthe results for middle-income countries are mixed. Finally, based on a\nrolling-window out-of-sample forecasting exercise, we show that the predictive\npower of international factors has increased in the most recent years of high\ninflation.",
        "authors": [
            "Ignacio Garr\u00f3n",
            "C. Vladimir Rodr\u00edguez-Caballero",
            "Esther Ruiz"
        ],
        "categories": "econ.EM",
        "published": "2024-10-27T23:17:09Z",
        "updated": "2024-10-29T15:01:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.20029v1",
        "title": "Jacobian-free Efficient Pseudo-Likelihood (EPL) Algorithm",
        "abstract": "This study proposes a simple procedure to compute Efficient Pseudo Likelihood\n(EPL) estimator proposed by Dearing and Blevins (2024) for estimating dynamic\ndiscrete games, without computing Jacobians of equilibrium constraints. EPL\nestimator is efficient, convergent, and computationally fast. However, the\noriginal algorithm requires deriving and coding the Jacobians, which are\ncumbersome and prone to coding mistakes especially when considering complicated\nmodels. The current study proposes to avoid the computation of Jacobians by\ncombining the ideas of numerical derivatives (for computing Jacobian-vector\nproducts) and the Krylov method (for solving linear equations). It shows good\ncomputational performance of the proposed method by numerical experiments.",
        "authors": [
            "Takeshi Fukasawa"
        ],
        "categories": "econ.EM",
        "published": "2024-10-26T00:52:41Z",
        "updated": "2024-10-26T00:52:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.19947v1",
        "title": "Testing the effects of an unobservable factor: Do marriage prospects affect college major choice?",
        "abstract": "Motivated by studying the effects of marriage prospects on students' college\nmajor choices, this paper develops a new econometric test for analyzing the\neffects of an unobservable factor in a setting where this factor potentially\ninfluences both agents' decisions and a binary outcome variable. Our test is\nbuilt upon a flexible copula-based estimation procedure and leverages the\nordered nature of latent utilities of the polychotomous choice model. Using the\nproposed method, we demonstrate that marriage prospects significantly influence\nthe college major choices of college graduates participating in the National\nLongitudinal Study of Youth (97) Survey. Furthermore, we validate the\nrobustness of our findings with alternative tests that use stated marriage\nexpectation measures from our data, thereby demonstrating the applicability and\nvalidity of our testing procedure in real-life scenarios.",
        "authors": [
            "Hayri Alper Arslan",
            "Brantly Callaway",
            "Tong Li"
        ],
        "categories": "econ.EM",
        "published": "2024-10-25T19:50:08Z",
        "updated": "2024-10-25T19:50:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.19469v1",
        "title": "Unified Causality Analysis Based on the Degrees of Freedom",
        "abstract": "Temporally evolving systems are typically modeled by dynamic equations. A key\nchallenge in accurate modeling is understanding the causal relationships\nbetween subsystems, as well as identifying the presence and influence of\nunobserved hidden drivers on the observed dynamics. This paper presents a\nunified method capable of identifying fundamental causal relationships between\npairs of systems, whether deterministic or stochastic. Notably, the method also\nuncovers hidden common causes beyond the observed variables. By analyzing the\ndegrees of freedom in the system, our approach provides a more comprehensive\nunderstanding of both causal influence and hidden confounders. This unified\nframework is validated through theoretical models and simulations,\ndemonstrating its robustness and potential for broader application.",
        "authors": [
            "Andr\u00e1s Telcs",
            "Marcell T. Kurbucz",
            "Antal Jakov\u00e1c"
        ],
        "categories": "stat.ME",
        "published": "2024-10-25T10:57:35Z",
        "updated": "2024-10-25T10:57:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.19412v1",
        "title": "Robust Time Series Causal Discovery for Agent-Based Model Validation",
        "abstract": "Agent-Based Model (ABM) validation is crucial as it helps ensuring the\nreliability of simulations, and causal discovery has become a powerful tool in\nthis context. However, current causal discovery methods often face accuracy and\nrobustness challenges when applied to complex and noisy time series data, which\nis typical in ABM scenarios. This study addresses these issues by proposing a\nRobust Cross-Validation (RCV) approach to enhance causal structure learning for\nABM validation. We develop RCV-VarLiNGAM and RCV-PCMCI, novel extensions of two\nprominent causal discovery algorithms. These aim to reduce the impact of noise\nbetter and give more reliable causal relation results, even with\nhigh-dimensional, time-dependent data. The proposed approach is then integrated\ninto an enhanced ABM validation framework, which is designed to handle diverse\ndata and model structures.\n  The approach is evaluated using synthetic datasets and a complex simulated\nfMRI dataset. The results demonstrate greater reliability in causal structure\nidentification. The study examines how various characteristics of datasets\naffect the performance of established causal discovery methods. These\ncharacteristics include linearity, noise distribution, stationarity, and causal\nstructure density. This analysis is then extended to the RCV method to see how\nit compares in these different situations. This examination helps confirm\nwhether the results are consistent with existing literature and also reveals\nthe strengths and weaknesses of the novel approaches.\n  By tackling key methodological challenges, the study aims to enhance ABM\nvalidation with a more resilient valuation framework presented. These\nimprovements increase the reliability of model-driven decision making processes\nin complex systems analysis.",
        "authors": [
            "Gene Yu",
            "Ce Guo",
            "Wayne Luk"
        ],
        "categories": "cs.LG",
        "published": "2024-10-25T09:13:26Z",
        "updated": "2024-10-25T09:13:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.19212v1",
        "title": "Inference on Multiple Winners with Applications to Microcredit and Economic Mobility",
        "abstract": "While policymakers and researchers are often concerned with conducting\ninference based on a data-dependent selection, a strictly larger class of\ninference problems arises when considering multiple data-dependent selections,\nsuch as when selecting on statistical significance or quantiles. Given this, we\nstudy the problem of conducting inference on multiple selections, which we dub\nthe inference on multiple winners problem. In this setting, we encounter both\nselective and multiple testing problems, making existing approaches either not\napplicable or too conservative. Instead, we propose a novel, two-step approach\nto the inference on multiple winners problem, with the first step modeling the\nselection of winners, and the second step using this model to conduct inference\nonly on the set of likely winners. Our two-step approach reduces over-coverage\nerror by up to 96%. We apply our two-step approach to revisit the winner's\ncurse in the creating moves to opportunity (CMTO) program, and to study\nexternal validity issues in the microcredit literature. In the CMTO\napplication, we find that, after correcting for the inference on multiple\nwinners problem, we fail to reject the possibility of null effects in the\nmajority of census tracts selected by the CMTO program. In our microcredit\napplication, we find that heterogeneity in treatment effect estimates remains\nlargely unaffected even after our proposed inference corrections.",
        "authors": [
            "Andreas Petrou-Zeniou",
            "Azeem M. Shaikh"
        ],
        "categories": "econ.EM",
        "published": "2024-10-24T23:45:09Z",
        "updated": "2024-10-24T23:45:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.19060v1",
        "title": "Heterogeneous Intertemporal Treatment Effects via Dynamic Panel Data Models",
        "abstract": "We study the identification and estimation of heterogeneous, intertemporal\ntreatment effects (TE) when potential outcomes depend on past treatments.\nFirst, applying a dynamic panel data model to observed outcomes, we show that\ninstrument-based GMM estimators, such as Arellano and Bond (1991), converge to\na non-convex (negatively weighted) aggregate of TE plus non-vanishing trends.\nWe then provide restrictions on sequential exchangeability (SE) of treatment\nand TE heterogeneity that reduce the GMM estimand to a convex (positively\nweighted) aggregate of TE. Second, we introduce an adjusted\ninverse-propensity-weighted (IPW) estimator for a new notion of average\ntreatment effect (ATE) over past observed treatments. Third, we show that when\npotential outcomes are generated by dynamic panel data models with homogeneous\nTE, such GMM estimators converge to causal parameters (even when SE is\ngenerically violated without conditioning on individual fixed effects).\nFinally, we motivate SE and compare it with parallel trends (PT) in various\nsettings with observational data (when treatments are dynamic, rational choices\nunder learning) or experimental data (when treatments are sequentially\nrandomized).",
        "authors": [
            "Philip Marx",
            "Elie Tamer",
            "Xun Tang"
        ],
        "categories": "econ.EM",
        "published": "2024-10-24T18:05:59Z",
        "updated": "2024-10-24T18:05:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.18381v2",
        "title": "Inference on High Dimensional Selective Labeling Models",
        "abstract": "A class of simultaneous equation models arise in the many domains where\nobserved binary outcomes are themselves a consequence of the existing choices\nof of one of the agents in the model. These models are gaining increasing\ninterest in the computer science and machine learning literatures where they\nrefer the potentially endogenous sample selection as the {\\em selective labels}\nproblem. Empirical settings for such models arise in fields as diverse as\ncriminal justice, health care, and insurance. For important recent work in this\narea, see for example Lakkaruju et al. (2017), Kleinberg et al. (2018), and\nCoston et al.(2021) where the authors focus on judicial bail decisions, and\nwhere one observes the outcome of whether a defendant filed to return for their\ncourt appearance only if the judge in the case decides to release the defendant\non bail. Identifying and estimating such models can be computationally\nchallenging for two reasons. One is the nonconcavity of the bivariate\nlikelihood function, and the other is the large number of covariates in each\nequation. Despite these challenges, in this paper we propose a novel\ndistribution free estimation procedure that is computationally friendly in many\ncovariates settings. The new method combines the semiparametric batched\ngradient descent algorithm introduced in Khan et al.(2023) with a novel sorting\nalgorithms incorporated to control for selection bias. Asymptotic properties of\nthe new procedure are established under increasing dimension conditions in both\nequations, and its finite sample properties are explored through a simulation\nstudy and an application using judicial bail data.",
        "authors": [
            "Shakeeb Khan",
            "Elie Tamer",
            "Qingsong Yao"
        ],
        "categories": "econ.EM",
        "published": "2024-10-24T02:45:13Z",
        "updated": "2024-10-31T20:29:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.18272v1",
        "title": "Partially Identified Rankings from Pairwise Interactions",
        "abstract": "This paper considers the problem of ranking objects based on their latent\nmerits using data from pairwise interactions. Existing approaches rely on the\nrestrictive assumption that all the interactions are either observed or missed\nrandomly. We investigate what can be inferred about rankings when this\nassumption is relaxed. First, we demonstrate that in parametric models, such as\nthe popular Bradley-Terry-Luce model, rankings are point-identified if and only\nif the tournament graph is connected. Second, we show that in nonparametric\nmodels based on strong stochastic transitivity, rankings in a connected\ntournament are only partially identified. Finally, we propose two statistical\ntests to determine whether a ranking belongs to the identified set. One test is\nvalid in finite samples but computationally intensive, while the other is easy\nto implement and valid asymptotically. We illustrate our procedure using\nBrazilian employer-employee data to test whether male and female workers rank\nfirms differently when making job transitions.",
        "authors": [
            "Federico Crippa",
            "Danil Fedchenko"
        ],
        "categories": "econ.EM",
        "published": "2024-10-23T20:45:55Z",
        "updated": "2024-10-23T20:45:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.18261v1",
        "title": "Detecting Spatial Outliers: the Role of the Local Influence Function",
        "abstract": "In the analysis of large spatial datasets, identifying and treating spatial\noutliers is essential for accurately interpreting geographical phenomena. While\nspatial correlation measures, particularly Local Indicators of Spatial\nAssociation (LISA), are widely used to detect spatial patterns, the presence of\nabnormal observations frequently distorts the landscape and conceals critical\nspatial relationships. These outliers can significantly impact analysis due to\nthe inherent spatial dependencies present in the data. Traditional influence\nfunction (IF) methodologies, commonly used in statistical analysis to measure\nthe impact of individual observations, are not directly applicable in the\nspatial context because the influence of an observation is determined not only\nby its own value but also by its spatial location, its connections with\nneighboring regions, and the values of those neighboring observations. In this\npaper, we introduce a local version of the influence function (LIF) that\naccounts for these spatial dependencies. Through the analysis of both simulated\nand real-world datasets, we demonstrate how the LIF provides a more nuanced and\naccurate detection of spatial outliers compared to traditional LISA measures\nand local impact assessments, improving our understanding of spatial patterns.",
        "authors": [
            "Giuseppe Arbia",
            "Vincenzo Nardelli"
        ],
        "categories": "stat.ME",
        "published": "2024-10-23T20:21:55Z",
        "updated": "2024-10-23T20:21:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.18159v1",
        "title": "On the Existence of One-Sided Representations in the Generalised Dynamic Factor Model",
        "abstract": "We consider the generalised dynamic factor model (GDFM) and assume that the\ndynamic common component is purely non-deterministic. We show that then the\ncommon shocks (and therefore the dynamic common component) can always be\nrepresented in terms of current and past observed variables. Hence, we further\ngeneralise existing results on the so called One-Sidedness problem of the GDFM.\nWe may conclude that the existence of a one-sided representation that is\ncausally subordinated to the observed variables is in the very nature of the\nGDFM and the lack of one-sidedness is an artefact of the chosen representation.",
        "authors": [
            "Philipp Gersing"
        ],
        "categories": "econ.EM",
        "published": "2024-10-23T13:13:55Z",
        "updated": "2024-10-23T13:13:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.17153v1",
        "title": "A Bayesian Perspective on the Maximum Score Problem",
        "abstract": "This paper presents a Bayesian inference framework for a linear index\nthreshold-crossing binary choice model that satisfies a median independence\nrestriction. The key idea is that the model is observationally equivalent to a\nprobit model with nonparametric heteroskedasticity. Consequently, Gibbs\nsampling techniques from Albert and Chib (1993) and Chib and Greenberg (2013)\nlead to a computationally attractive Bayesian inference procedure in which a\nGaussian process forms a conditionally conjugate prior for the natural\nlogarithm of the skedastic function.",
        "authors": [
            "Christopher D. Walker"
        ],
        "categories": "econ.EM",
        "published": "2024-10-22T16:29:36Z",
        "updated": "2024-10-22T16:29:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.17105v1",
        "title": "General Seemingly Unrelated Local Projections",
        "abstract": "We provide a framework for efficiently estimating impulse response functions\nwith Local Projections (LPs). Our approach offers a Bayesian treatment for LPs\nwith Instrumental Variables, accommodating multiple shocks and instruments per\nshock, accounts for autocorrelation in multi-step forecasts by jointly modeling\nall LPs as a seemingly unrelated system of equations, defines a flexible yet\nparsimonious joint prior for impulse responses based on a Gaussian Process,\nallows for joint inference about the entire vector of impulse responses, and\nuses all available data across horizons by imputing missing values.",
        "authors": [
            "Florian Huber",
            "Christian Matthes",
            "Michael Pfarrhofer"
        ],
        "categories": "econ.EM",
        "published": "2024-10-22T15:30:01Z",
        "updated": "2024-10-22T15:30:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.16998v1",
        "title": "Identifying Conduct Parameters with Separable Demand: A Counterexample to Lau (1982)",
        "abstract": "We provide a counterexample to the conduct parameter identification result\nestablished in the foundational work of Lau (1982), which generalizes the\nidentification theorem of Bresnahan (1982) by relaxing the linearity\nassumptions. We identify a separable demand function that still permits\nidentification and validate this case both theoretically and through numerical\nsimulations.",
        "authors": [
            "Yuri Matsumura",
            "Suguru Otani"
        ],
        "categories": "econ.EM",
        "published": "2024-10-22T13:21:30Z",
        "updated": "2024-10-22T13:21:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.16526v1",
        "title": "A Dynamic Spatiotemporal and Network ARCH Model with Common Factors",
        "abstract": "We introduce a dynamic spatiotemporal volatility model that extends\ntraditional approaches by incorporating spatial, temporal, and spatiotemporal\nspillover effects, along with volatility-specific observed and latent factors.\nThe model offers a more general network interpretation, making it applicable\nfor studying various types of network spillovers. The primary innovation lies\nin incorporating volatility-specific latent factors into the dynamic\nspatiotemporal volatility model. Using Bayesian estimation via the Markov Chain\nMonte Carlo (MCMC) method, the model offers a robust framework for analyzing\nthe spatial, temporal, and spatiotemporal effects of a log-squared outcome\nvariable on its volatility. We recommend using the deviance information\ncriterion (DIC) and a regularized Bayesian MCMC method to select the number of\nrelevant factors in the model. The model's flexibility is demonstrated through\ntwo applications: a spatiotemporal model applied to the U.S. housing market and\nanother applied to financial stock market networks, both highlighting the\nmodel's ability to capture varying degrees of interconnectedness. In both\napplications, we find strong spatial/network interactions with relatively\nstronger spillover effects in the stock market.",
        "authors": [
            "Osman Do\u011fan",
            "Raffaele Mattera",
            "Philipp Otto",
            "S\u00fcleyman Ta\u015fp\u0131nar"
        ],
        "categories": "stat.ME",
        "published": "2024-10-21T21:35:32Z",
        "updated": "2024-10-21T21:35:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.16214v1",
        "title": "Asymmetries in Financial Spillovers",
        "abstract": "This paper analyzes nonlinearities in the international transmission of\nfinancial shocks originating in the US. To do so, we develop a flexible\nnonlinear multi-country model. Our framework is capable of producing\nasymmetries in the responses to financial shocks for shock size and sign, and\nover time. We show that international reactions to US-based financial shocks\nare asymmetric along these dimensions. Particularly, we find that adverse\nshocks trigger stronger declines in output, inflation, and stock markets than\nbenign shocks. Further, we investigate time variation in the estimated dynamic\neffects and characterize the responsiveness of three major central banks to\nfinancial shocks.",
        "authors": [
            "Florian Huber",
            "Karin Klieber",
            "Massimiliano Marcellino",
            "Luca Onorante",
            "Michael Pfarrhofer"
        ],
        "categories": "econ.EM",
        "published": "2024-10-21T17:14:58Z",
        "updated": "2024-10-21T17:14:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.16112v1",
        "title": "Dynamic Biases of Static Panel Data Estimators",
        "abstract": "This paper identifies an important bias - termed dynamic bias - in fixed\neffects panel estimators that arises when dynamic feedback is ignored in the\nestimating equation. Dynamic feedback occurs if past outcomes impact current\noutcomes, a feature of many settings ranging from economic growth to\nagricultural and labor markets. When estimating equations omit past outcomes,\ndynamic bias can lead to significantly inaccurate treatment effect estimates,\neven with randomly assigned treatments. This dynamic bias in simulations is\nlarger than Nickell bias. I show that dynamic bias stems from the estimation of\nfixed effects, as their estimation generates confounding in the data. To\nrecover consistent treatment effects, I develop a flexible estimator that\nprovides fixed-T bias correction. I apply this approach to study the impact of\ntemperature shocks on GDP, a canonical example where economic theory points to\nan important feedback from past to future outcomes. Accounting for dynamic bias\nlowers the estimated effects of higher yearly temperatures on GDP growth by 10%\nand GDP levels by 120%.",
        "authors": [
            "Sylvia Klosin"
        ],
        "categories": "econ.EM",
        "published": "2024-10-21T15:40:44Z",
        "updated": "2024-10-21T15:40:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.16017v1",
        "title": "Semiparametric Bayesian Inference for a Conditional Moment Equality Model",
        "abstract": "Conditional moment equality models are regularly encountered in empirical\neconomics, yet they are difficult to estimate. These models map a conditional\ndistribution of data to a structural parameter via the restriction that a\nconditional mean equals zero. Using this observation, I introduce a Bayesian\ninference framework in which an unknown conditional distribution is replaced\nwith a nonparametric posterior, and structural parameter inference is then\nperformed using an implied posterior. The method has the same flexibility as\nfrequentist semiparametric estimators and does not require converting\nconditional moments to unconditional moments. Importantly, I prove a\nsemiparametric Bernstein-von Mises theorem, providing conditions under which,\nin large samples, the posterior for the structural parameter is approximately\nnormal, centered at an efficient estimator, and has variance equal to the\nChamberlain (1987) semiparametric efficiency bound. As byproducts, I show that\nBayesian uncertainty quantification methods are asymptotically optimal\nfrequentist confidence sets and derive low-level sufficient conditions for\nGaussian process priors. The latter sheds light on a key prior stability\ncondition and relates to the numerical aspects of the paper in which these\npriors are used to predict the welfare effects of price changes.",
        "authors": [
            "Christopher D. Walker"
        ],
        "categories": "econ.EM",
        "published": "2024-10-21T13:49:38Z",
        "updated": "2024-10-21T13:49:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.15938v1",
        "title": "Quantifying world geography as seen through the lens of Soviet propaganda",
        "abstract": "Cultural data typically contains a variety of biases. In particular,\ngeographical locations are unequally portrayed in media, creating a distorted\nrepresentation of the world. Identifying and measuring such biases is crucial\nto understand both the data and the socio-cultural processes that have produced\nthem. Here we suggest to measure geographical biases in a large historical news\nmedia corpus by studying the representation of cities. Leveraging ideas of\nquantitative urban science, we develop a mixed quantitative-qualitative\nprocedure, which allows us to get robust quantitative estimates of the biases.\nThese biases can be further qualitatively interpreted resulting in a\nhermeneutic feedback loop. We apply this procedure to a corpus of the Soviet\nnewsreel series 'Novosti Dnya' (News of the Day) and show that city\nrepresentation grows super-linearly with city size, and is further biased by\ncity specialization and geographical location. This allows to systematically\nidentify geographical regions which are explicitly or sneakily emphasized by\nSoviet propaganda and quantify their importance.",
        "authors": [
            "M. V. Tamm",
            "M. Oiva",
            "K. D. Mukhina",
            "M. Mets",
            "M. Schich"
        ],
        "categories": "physics.soc-ph",
        "published": "2024-10-21T12:07:10Z",
        "updated": "2024-10-21T12:07:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.15734v1",
        "title": "A Kernelization-Based Approach to Nonparametric Binary Choice Models",
        "abstract": "We propose a new estimator for nonparametric binary choice models that does\nnot impose a parametric structure on either the systematic function of\ncovariates or the distribution of the error term. A key advantage of our\napproach is its computational efficiency. For instance, even when assuming a\nnormal error distribution as in probit models, commonly used sieves for\napproximating an unknown function of covariates can lead to a large-dimensional\noptimization problem when the number of covariates is moderate. Our approach,\nmotivated by kernel methods in machine learning, views certain reproducing\nkernel Hilbert spaces as special sieve spaces, coupled with spectral cut-off\nregularization for dimension reduction. We establish the consistency of the\nproposed estimator for both the systematic function of covariates and the\ndistribution function of the error term, and asymptotic normality of the\nplug-in estimator for weighted average partial derivatives. Simulation studies\nshow that, compared to parametric estimation methods, the proposed method\neffectively improves finite sample performance in cases of misspecification,\nand has a rather mild efficiency loss if the model is correctly specified.\nUsing administrative data on the grant decisions of US asylum applications to\nimmigration courts, along with nine case-day variables on weather and\npollution, we re-examine the effect of outdoor temperature on court judges'\n\"mood\", and thus, their grant decisions.",
        "authors": [
            "Guo Yan"
        ],
        "categories": "econ.EM",
        "published": "2024-10-21T07:53:14Z",
        "updated": "2024-10-21T07:53:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.15634v1",
        "title": "Distributionally Robust Instrumental Variables Estimation",
        "abstract": "Instrumental variables (IV) estimation is a fundamental method in\neconometrics and statistics for estimating causal effects in the presence of\nunobserved confounding. However, challenges such as untestable model\nassumptions and poor finite sample properties have undermined its reliability\nin practice. Viewing common issues in IV estimation as distributional\nuncertainties, we propose DRIVE, a distributionally robust framework of the\nclassical IV estimation method. When the ambiguity set is based on a\nWasserstein distance, DRIVE minimizes a square root ridge regularized variant\nof the two stage least squares (TSLS) objective. We develop a novel asymptotic\ntheory for this regularized regression estimator based on the square root\nridge, showing that it achieves consistency without requiring the\nregularization parameter to vanish. This result follows from a fundamental\nproperty of the square root ridge, which we call ``delayed shrinkage''. This\nnovel property, which also holds for a class of generalized method of moments\n(GMM) estimators, ensures that the estimator is robust to distributional\nuncertainties that persist in large samples. We further derive the asymptotic\ndistribution of Wasserstein DRIVE and propose data-driven procedures to select\nthe regularization parameter based on theoretical results. Simulation studies\nconfirm the superior finite sample performance of Wasserstein DRIVE. Thanks to\nits regularization and robustness properties, Wasserstein DRIVE could be\npreferable in practice, particularly when the practitioner is uncertain about\nmodel assumptions or distributional shifts in data.",
        "authors": [
            "Zhaonan Qu",
            "Yongchan Kwon"
        ],
        "categories": "econ.EM",
        "published": "2024-10-21T04:33:38Z",
        "updated": "2024-10-21T04:33:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.16333v1",
        "title": "Conformal Predictive Portfolio Selection",
        "abstract": "This study explores portfolio selection using predictive models for portfolio\nreturns. Portfolio selection is a fundamental task in finance, and various\nmethods have been developed to achieve this goal. For example, the\nmean-variance approach constructs portfolios by balancing the trade-off between\nthe mean and variance of asset returns, while the quantile-based approach\noptimizes portfolios by accounting for tail risk. These traditional methods\noften rely on distributional information estimated from historical data.\nHowever, a key concern is the uncertainty of future portfolio returns, which\nmay not be fully captured by simple reliance on historical data, such as using\nthe sample average. To address this, we propose a framework for predictive\nportfolio selection using conformal inference, called Conformal Predictive\nPortfolio Selection (CPPS). Our approach predicts future portfolio returns,\ncomputes corresponding prediction intervals, and selects the desirable\nportfolio based on these intervals. The framework is flexible and can\naccommodate a variety of predictive models, including autoregressive (AR)\nmodels, random forests, and neural networks. We demonstrate the effectiveness\nof our CPPS framework using an AR model and validate its performance through\nempirical studies, showing that it provides superior returns compared to\nsimpler strategies.",
        "authors": [
            "Masahiro Kato"
        ],
        "categories": "q-fin.PM",
        "published": "2024-10-19T15:42:49Z",
        "updated": "2024-10-19T15:42:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.15097v1",
        "title": "Predictive Quantile Regression with High-Dimensional Predictors: The Variable Screening Approach",
        "abstract": "This paper advances a variable screening approach to enhance conditional\nquantile forecasts using high-dimensional predictors. We have refined and\naugmented the quantile partial correlation (QPC)-based variable screening\nproposed by Ma et al. (2017) to accommodate $\\beta$-mixing time-series data.\nOur approach is inclusive of i.i.d scenarios but introduces new convergence\nbounds for time-series contexts, suggesting the performance of QPC-based\nscreening is influenced by the degree of time-series dependence. Through Monte\nCarlo simulations, we validate the effectiveness of QPC under weak dependence.\nOur empirical assessment of variable selection for growth-at-risk (GaR)\nforecasting underscores the method's advantages, revealing that specific labor\nmarket determinants play a pivotal role in forecasting GaR. While prior\nempirical research has predominantly considered a limited set of predictors, we\nemploy the comprehensive Fred-QD dataset, retaining a richer breadth of\ninformation for GaR forecasts.",
        "authors": [
            "Hongqi Chen",
            "Ji Hyung Lee"
        ],
        "categories": "econ.EM",
        "published": "2024-10-19T13:15:25Z",
        "updated": "2024-10-19T13:15:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.15090v1",
        "title": "Fast and Efficient Bayesian Analysis of Structural Vector Autoregressions Using the R Package bsvars",
        "abstract": "The R package bsvars provides a wide range of tools for empirical\nmacroeconomic and financial analyses using Bayesian Structural Vector\nAutoregressions. It uses frontier econometric techniques and C++ code to ensure\nfast and efficient estimation of these multivariate dynamic structural models,\npossibly with many variables, complex identification strategies, and non-linear\ncharacteristics. The models can be identified using adjustable exclusion\nrestrictions and heteroskedastic or non-normal shocks. They feature a flexible\nthree-level equation-specific local-global hierarchical prior distribution for\nthe estimated level of shrinkage for autoregressive and structural parameters.\nAdditionally, the package facilitates predictive and structural analyses such\nas impulse responses, forecast error variance and historical decompositions,\nforecasting, statistical verification of identification and hypotheses on\nautoregressive parameters, and analyses of structural shocks, volatilities, and\nfitted values. These features differentiate bsvars from existing R packages\nthat either focus on a specific structural model, do not consider\nheteroskedastic shocks, or lack the implementation using compiled code.",
        "authors": [
            "Tomasz Wo\u017aniak"
        ],
        "categories": "econ.EM",
        "published": "2024-10-19T12:56:44Z",
        "updated": "2024-10-19T12:56:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.14904v1",
        "title": "Switchback Price Experiments with Forward-Looking Demand",
        "abstract": "We consider a retailer running a switchback experiment for the price of a\nsingle product, with infinite supply. In each period, the seller chooses a\nprice $p$ from a set of predefined prices that consist of a reference price and\na few discounted price levels. The goal is to estimate the demand gradient at\nthe reference price point, with the goal of adjusting the reference price to\nimprove revenue after the experiment. In our model, in each period, a unit mass\nof buyers arrives on the market, with values distributed based on a\ntime-varying process. Crucially, buyers are forward looking with a discounted\nutility and will choose to not purchase now if they expect to face a discounted\nprice in the near future. We show that forward-looking demand introduces bias\nin naive estimators of the demand gradient, due to intertemporal interference.\nFurthermore, we prove that there is no estimator that uses data from price\nexperiments with only two price points that can recover the correct demand\ngradient, even in the limit of an infinitely long experiment with an\ninfinitesimal price discount. Moreover, we characterize the form of the bias of\nnaive estimators. Finally, we show that with a simple three price level\nexperiment, the seller can remove the bias due to strategic forward-looking\nbehavior and construct an estimator for the demand gradient that asymptotically\nrecovers the truth.",
        "authors": [
            "Yifan Wu",
            "Ramesh Johari",
            "Vasilis Syrgkanis",
            "Gabriel Y. Weintraub"
        ],
        "categories": "cs.GT",
        "published": "2024-10-18T23:11:31Z",
        "updated": "2024-10-18T23:11:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.14871v2",
        "title": "Learning the Effect of Persuasion via Difference-In-Differences",
        "abstract": "The persuasion rate is a key parameter for measuring the causal effect of a\ndirectional message on influencing the recipient's behavior. Its identification\nhas relied on exogenous treatment or the availability of credible instruments,\nbut the requirements are not always satisfied in observational studies.\nTherefore, we develop a novel econometric framework for the average persuasion\nrate on the treated and other related parameters by using the\ndifference-in-differences approach. The average treatment effect on the treated\nis a standard parameter in difference-in-differences, but we show that it is an\noverly conservative measure in the context of persuasion. For estimation and\ninference, we propose regression-based approaches as well as semiparametrically\nefficient estimators. Beginning with the two-period case, we extend the\nframework to staggered treatment settings, where we show how to conduct richer\nanalyses like the event-study design. We investigate the British election and\nthe Chinese curriculum reform as empirical examples.",
        "authors": [
            "Sung Jae Jun",
            "Sokbae Lee"
        ],
        "categories": "econ.EM",
        "published": "2024-10-18T21:34:10Z",
        "updated": "2024-12-06T03:30:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.14585v1",
        "title": "A GARCH model with two volatility components and two driving factors",
        "abstract": "We introduce a novel GARCH model that integrates two sources of uncertainty\nto better capture the rich, multi-component dynamics often observed in the\nvolatility of financial assets. This model provides a quasi closed-form\nrepresentation of the characteristic function for future log-returns, from\nwhich semi-analytical formulas for option pricing can be derived. A theoretical\nanalysis is conducted to establish sufficient conditions for strict\nstationarity and geometric ergodicity, while also obtaining the continuous-time\ndiffusion limit of the model. Empirical evaluations, conducted both in-sample\nand out-of-sample using S\\&P500 time series data, show that our model\noutperforms widely used single-factor models in predicting returns and option\nprices.",
        "authors": [
            "Luca Vincenzo Ballestra",
            "Enzo D'Innocenzo",
            "Christian Tezza"
        ],
        "categories": "econ.EM",
        "published": "2024-10-18T16:36:07Z",
        "updated": "2024-10-18T16:36:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.14513v1",
        "title": "GARCH option valuation with long-run and short-run volatility components: A novel framework ensuring positive variance",
        "abstract": "Christoffersen, Jacobs, Ornthanalai, and Wang (2008) (CJOW) proposed an\nimproved Generalized Autoregressive Conditional Heteroskedasticity (GARCH)\nmodel for valuing European options, where the return volatility is comprised of\ntwo distinct components. Empirical studies indicate that the model developed by\nCJOW outperforms widely-used single-component GARCH models and provides a\nsuperior fit to options data than models that combine conditional\nheteroskedasticity with Poisson-normal jumps. However, a significant limitation\nof this model is that it allows the variance process to become negative. Oh and\nPark [2023] partially addressed this issue by developing a related model, yet\nthe positivity of the volatility components is not guaranteed, both\ntheoretically and empirically. In this paper we introduce a new GARCH model\nthat improves upon the models by CJOW and Oh and Park [2023], ensuring the\npositivity of the return volatility. In comparison to the two earlier GARCH\napproaches, our novel methodology shows comparable in-sample performance on\nreturns data and superior performance on S&P500 options data.",
        "authors": [
            "Luca Vincenzo Ballestra",
            "Enzo D'Innocenzo",
            "Christian Tezza"
        ],
        "categories": "econ.EM",
        "published": "2024-10-18T14:47:48Z",
        "updated": "2024-10-18T14:47:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.14317v1",
        "title": "Identification of a Rank-dependent Peer Effect Model",
        "abstract": "This paper develops an econometric model to analyse heterogeneity in peer\neffects in network data with endogenous spillover across units. We introduce a\nrank-dependent peer effect model that captures how the relative ranking of a\npeer outcome shapes the influence units have on one another, by modeling the\npeer effect to be linear in ordered peer outcomes. In contrast to the\ntraditional linear-in-means model, our approach allows for greater flexibility\nin peer effect by accounting for the distribution of peer outcomes as well as\nthe size of peer groups. Under a minimal condition, the rank-dependent peer\neffect model admits a unique equilibrium and is therefore tractable. Our\nsimulations show that that estimation performs well in finite samples given\nsufficient covariate strength. We then apply our model to educational data from\nNorway, where we see that higher-performing students disproportionately drive\nGPA spillovers.",
        "authors": [
            "Eyo I. Herstad",
            "Myungkou Shin"
        ],
        "categories": "econ.EM",
        "published": "2024-10-18T09:25:20Z",
        "updated": "2024-10-18T09:25:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.13658v1",
        "title": "The Subtlety of Optimal Paternalism in a Population with Bounded Rationality",
        "abstract": "We consider a utilitarian planner with the power to design a discrete choice\nset for a heterogeneous population with bounded rationality. We find that\noptimal paternalism is subtle. The policy that most effectively constrains or\ninfluences choices depends on the preference distribution of the population and\non the choice probabilities conditional on preferences that measure the\nsuboptimality of behavior. We first consider the planning problem in\nabstraction. We next examine policy choice when individuals measure utility\nwith additive random error and maximize mismeasured rather than actual utility.\nWe then analyze a class of problems of binary treatment choice under\nuncertainty. Here we suppose that a planner can mandate a treatment conditional\non publicly observed personal covariates or can decentralize decision making,\nenabling persons to choose their own treatments. Bounded rationality may take\nthe form of deviations between subjective personal beliefs and objective\nprobabilities of uncertain outcomes. We apply our analysis to clinical decision\nmaking in medicine. Having documented that optimization of paternalism requires\nthe planner to possess extensive knowledge that is rarely available, we address\nthe difficult problem of paternalistic policy choice when the planner is\nboundedly rational.",
        "authors": [
            "Charles F. Manski",
            "Eytan Sheshinski"
        ],
        "categories": "econ.EM",
        "published": "2024-10-17T15:20:39Z",
        "updated": "2024-10-17T15:20:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.12731v1",
        "title": "Counterfactual Analysis in Empirical Games",
        "abstract": "We address counterfactual analysis in empirical models of games with\npartially identified parameters, and multiple equilibria and/or randomized\nstrategies, by constructing and analyzing the counterfactual predictive\ndistribution set (CPDS). This framework accommodates various outcomes of\ninterest, including behavioral and welfare outcomes. It allows a variety of\nchanges to the environment to generate the counterfactual, including\nmodifications of the utility functions, the distribution of utility\ndeterminants, the number of decision makers, and the solution concept. We use a\nBayesian approach to summarize statistical uncertainty. We establish conditions\nunder which the population CPDS is sharp from the point of view of\nidentification. We also establish conditions under which the posterior CPDS is\nconsistent if the posterior distribution for the underlying model parameter is\nconsistent. Consequently, our results can be employed to conduct counterfactual\nanalysis after a preliminary step of identifying and estimating the underlying\nmodel parameter based on the existing literature. Our consistency results\ninvolve the development of a new general theory for Bayesian consistency of\nposterior distributions for mappings of sets. Although we primarily focus on a\nmodel of a strategic game, our approach is applicable to other structural\nmodels with similar features.",
        "authors": [
            "Brendan Kline",
            "Elie Tamer"
        ],
        "categories": "econ.EM",
        "published": "2024-10-16T16:44:32Z",
        "updated": "2024-10-16T16:44:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.12709v1",
        "title": "A Simple Interactive Fixed Effects Estimator for Short Panels",
        "abstract": "We study the interactive effects (IE) model as an extension of the\nconventional additive effects (AE) model. For the AE model, the fixed effects\nestimator can be obtained by applying least squares to a regression that adds a\nlinear projection of the fixed effect on the explanatory variables (Mundlak,\n1978; Chamberlain, 1984). In this paper, we develop a novel estimator -- the\nprojection-based IE (PIE) estimator -- for the IE model that is based on a\nsimilar approach. We show that, for the IE model, fixed effects estimators that\nhave appeared in the literature are not equivalent to our PIE estimator, though\nboth can be expressed as a generalized within estimator. Unlike the fixed\neffects estimators for the IE model, the PIE estimator is consistent for a\nfixed number of time periods with no restrictions on serial correlation or\nconditional heteroskedasticity in the errors. We also derive a statistic for\ntesting the consistency of the two-way fixed effects estimator in the possible\npresence of iterative effects. Moreover, although the PIE estimator is the\nsolution to a high-dimensional nonlinear least squares problem, we show that it\ncan be computed by iterating between two steps, both of which have simple\nanalytical solutions. The computational simplicity is an important advantage\nrelative to other strategies that have been proposed for estimating the IE\nmodel for short panels. Finally, we compare the finite sample performance of IE\nestimators through simulations.",
        "authors": [
            "Robert F. Phillips",
            "Benjamin D. Williams"
        ],
        "categories": "econ.EM",
        "published": "2024-10-16T16:15:23Z",
        "updated": "2024-10-16T16:15:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.12098v1",
        "title": "Testing Identifying Assumptions in Parametric Separable Models: A Conditional Moment Inequality Approach",
        "abstract": "In this paper, we propose a simple method for testing identifying assumptions\nin parametric separable models, namely treatment exogeneity, instrument\nvalidity, and/or homoskedasticity. We show that the testable implications can\nbe written in the intersection bounds framework, which is easy to implement\nusing the inference method proposed in Chernozhukov, Lee, and Rosen (2013), and\nthe Stata package of Chernozhukov et al. (2015). Monte Carlo simulations\nconfirm that our test is consistent and controls size. We use our proposed\nmethod to test the validity of some commonly used instrumental variables, such\nas the average price in other markets in Nevo and Rosen (2012), the Bartik\ninstrument in Card (2009), and the test rejects both instrumental variable\nmodels. When the identifying assumptions are rejected, we discuss solutions\nthat allow researchers to identify some causal parameters of interest after\nrelaxing functional form assumptions. We show that the IV model is nontestable\nif no functional form assumption is made on the outcome equation, when there\nexists a one-to-one mapping between the continuous treatment variable, the\ninstrument, and the first-stage unobserved heterogeneity.",
        "authors": [
            "Leonard Goff",
            "D\u00e9sir\u00e9 K\u00e9dagni",
            "Huan Wu"
        ],
        "categories": "econ.EM",
        "published": "2024-10-15T22:46:51Z",
        "updated": "2024-10-15T22:46:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.11408v1",
        "title": "Aggregation Trees",
        "abstract": "Uncovering the heterogeneous effects of particular policies or \"treatments\"\nis a key concern for researchers and policymakers. A common approach is to\nreport average treatment effects across subgroups based on observable\ncovariates. However, the choice of subgroups is crucial as it poses the risk of\n$p$-hacking and requires balancing interpretability with granularity. This\npaper proposes a nonparametric approach to construct heterogeneous subgroups.\nThe approach enables a flexible exploration of the trade-off between\ninterpretability and the discovery of more granular heterogeneity by\nconstructing a sequence of nested groupings, each with an optimality property.\nBy integrating our approach with \"honesty\" and debiased machine learning, we\nprovide valid inference about the average treatment effect of each group. We\nvalidate the proposed methodology through an empirical Monte-Carlo study and\napply it to revisit the impact of maternal smoking on birth weight, revealing\nsystematic heterogeneity driven by parental and birth-related characteristics.",
        "authors": [
            "Riccardo Di Francesco"
        ],
        "categories": "econ.EM",
        "published": "2024-10-15T08:52:48Z",
        "updated": "2024-10-15T08:52:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.11263v1",
        "title": "Closed-form estimation and inference for panels with attrition and refreshment samples",
        "abstract": "It has long been established that, if a panel dataset suffers from attrition,\nauxiliary (refreshment) sampling restores full identification under additional\nassumptions that still allow for nontrivial attrition mechanisms. Such\nidentification results rely on implausible assumptions about the attrition\nprocess or lead to theoretically and computationally challenging estimation\nprocedures. We propose an alternative identifying assumption that, despite its\nnonparametric nature, suggests a simple estimation algorithm based on a\ntransformation of the empirical cumulative distribution function of the data.\nThis estimation procedure requires neither tuning parameters nor optimization\nin the first step, i.e. has a closed form. We prove that our estimator is\nconsistent and asymptotically normal and demonstrate its good performance in\nsimulations. We provide an empirical illustration with income data from the\nUnderstanding America Study.",
        "authors": [
            "Grigory Franguridi",
            "Lidia Kosenkova"
        ],
        "categories": "econ.EM",
        "published": "2024-10-15T04:37:12Z",
        "updated": "2024-10-15T04:37:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.11113v2",
        "title": "Statistical Properties of Deep Neural Networks with Dependent Data",
        "abstract": "This paper establishes statistical properties of deep neural network (DNN)\nestimators under dependent data. Two general results for nonparametric sieve\nestimators directly applicable to DNN estimators are given. The first\nestablishes rates for convergence in probability under nonstationary data. The\nsecond provides non-asymptotic probability bounds on $\\mathcal{L}^{2}$-errors\nunder stationary $\\beta$-mixing data. I apply these results to DNN estimators\nin both regression and classification contexts imposing only a standard\nH\\\"older smoothness assumption. The DNN architectures considered are common in\napplications, featuring fully connected feedforward networks with any\ncontinuous piecewise linear activation function, unbounded weights, and a width\nand depth that grows with sample size. The framework provided also offers\npotential for research into other DNN architectures and time-series\napplications.",
        "authors": [
            "Chad Brown"
        ],
        "categories": "stat.ML",
        "published": "2024-10-14T21:46:57Z",
        "updated": "2024-11-05T18:26:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.10749v1",
        "title": "Testing the order of fractional integration in the presence of smooth trends, with an application to UK Great Ratios",
        "abstract": "This note proposes semi-parametric tests for investigating whether a\nstochastic process is fractionally integrated of order $\\delta$, where\n$|\\delta| < 1/2$, when smooth trends are present in the model. We combine the\nsemi-parametric approach by Iacone, Nielsen & Taylor (2022) to model the short\nrange dependence with the use of Chebyshev polynomials by Cuestas & Gil-Alana\nto describe smooth trends. Our proposed statistics have standard limiting null\ndistributions and match the asymptotic local power of infeasible tests based on\nunobserved errors. We also establish the conditions under which an information\ncriterion can consistently estimate the order of the Chebyshev polynomial. The\nfinite sample performance is evaluated using simulations, and an empirical\napplication is given for the UK Great Ratios.",
        "authors": [
            "Mustafa R. K\u0131l\u0131n\u00e7",
            "Michael Massmann",
            "Maximilian Ambros"
        ],
        "categories": "econ.EM",
        "published": "2024-10-14T17:23:59Z",
        "updated": "2024-10-14T17:23:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.09952v1",
        "title": "Large Scale Longitudinal Experiments: Estimation and Inference",
        "abstract": "Large-scale randomized experiments are seldom analyzed using panel regression\nmethods because of computational challenges arising from the presence of\nmillions of nuisance parameters. We leverage Mundlak's insight that unit\nintercepts can be eliminated by using carefully chosen averages of the\nregressors to rewrite several common estimators in a form that is amenable to\nweighted-least squares estimation with frequency weights. This renders\nregressions involving arbitrary strata intercepts tractable with very large\ndatasets, optionally with the key compression step computed out-of-memory in\nSQL. We demonstrate that these methods yield more precise estimates than other\ncommonly used estimators, and also find that the compression strategy greatly\nincreases computational efficiency. We provide in-memory (pyfixest) and\nout-of-memory (duckreg) python libraries to implement these estimators.",
        "authors": [
            "Apoorva Lal",
            "Alexander Fischer",
            "Matthew Wardrop"
        ],
        "categories": "econ.EM",
        "published": "2024-10-13T18:20:00Z",
        "updated": "2024-10-13T18:20:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.09825v1",
        "title": "Nickell Meets Stambaugh: A Tale of Two Biases in Panel Predictive Regressions",
        "abstract": "In panel predictive regressions with persistent covariates, coexistence of\nthe Nickell bias and the Stambaugh bias imposes challenges for hypothesis\ntesting. This paper introduces a new estimator, the IVX-X-Jackknife (IVXJ),\nwhich effectively removes this composite bias and reinstates standard\ninferential procedures. The IVXJ estimator is inspired by the IVX technique in\ntime series. In panel data where the cross section is of the same order as the\ntime dimension, the bias of the baseline panel IVX estimator can be corrected\nvia an analytical formula by leveraging an innovative X-Jackknife scheme that\ndivides the time dimension into the odd and even indices. IVXJ is the first\nprocedure that achieves unified inference across a wide range of modes of\npersistence in panel predictive regressions, whereas such unified inference is\nunattainable for the popular within-group estimator. Extended to accommodate\nlong-horizon predictions with multiple regressions, IVXJ is used to examine the\nimpact of debt levels on financial crises by panel local projection. Our\nempirics provide comparable results across different categories of debt.",
        "authors": [
            "Chengwang Liao",
            "Ziwei Mei",
            "Zhentao Shi"
        ],
        "categories": "econ.EM",
        "published": "2024-10-13T12:57:10Z",
        "updated": "2024-10-13T12:57:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.09027v1",
        "title": "Variance reduction combining pre-experiment and in-experiment data",
        "abstract": "Online controlled experiments (A/B testing) are essential in data-driven\ndecision-making for many companies. Increasing the sensitivity of these\nexperiments, particularly with a fixed sample size, relies on reducing the\nvariance of the estimator for the average treatment effect (ATE). Existing\nmethods like CUPED and CUPAC use pre-experiment data to reduce variance, but\ntheir effectiveness depends on the correlation between the pre-experiment data\nand the outcome. In contrast, in-experiment data is often more strongly\ncorrelated with the outcome and thus more informative. In this paper, we\nintroduce a novel method that combines both pre-experiment and in-experiment\ndata to achieve greater variance reduction than CUPED and CUPAC, without\nintroducing bias or additional computation complexity. We also establish\nasymptotic theory and provide consistent variance estimators for our method.\nApplying this method to multiple online experiments at Etsy, we reach\nsubstantial variance reduction over CUPAC with the inclusion of only a few\nin-experiment covariates. These results highlight the potential of our approach\nto significantly improve experiment sensitivity and accelerate decision-making.",
        "authors": [
            "Zhexiao Lin",
            "Pablo Crespo"
        ],
        "categories": "stat.ME",
        "published": "2024-10-11T17:45:29Z",
        "updated": "2024-10-11T17:45:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.07443v2",
        "title": "On the Lower Confidence Band for the Optimal Welfare",
        "abstract": "This article addresses the question of reporting a lower confidence band\n(LCB) for optimal welfare in a policy learning problem. A straightforward\nprocedure inverts a one-sided t-test based on an efficient estimator of the\noptimal welfare. We show that under empirically relevant data-generating\nprocesses, this procedure can be dominated by an LCB corresponding to\nsuboptimal welfare, with the average difference of the order N-1/2. We relate\nthe first-order dominance result to a lack of uniformity in the margin\nassumption, a standard sufficient condition for debiased inference on the\noptimal welfare ensuring that the first-best policy is well-separated from the\nsuboptimal ones. Finally, we show that inverting the existing tests from the\nmoment inequality literature produces LCBs that are robust to the\nnon-uniqueness of the optimal policy and easy to compute. We find that this\napproach performs well empirically in the context of the National JTPA study.",
        "authors": [
            "Kirill Ponomarev",
            "Vira Semenova"
        ],
        "categories": "econ.EM",
        "published": "2024-10-09T21:20:04Z",
        "updated": "2024-10-25T21:09:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.07091v1",
        "title": "Collusion Detection with Graph Neural Networks",
        "abstract": "Collusion is a complex phenomenon in which companies secretly collaborate to\nengage in fraudulent practices. This paper presents an innovative methodology\nfor detecting and predicting collusion patterns in different national markets\nusing neural networks (NNs) and graph neural networks (GNNs). GNNs are\nparticularly well suited to this task because they can exploit the inherent\nnetwork structures present in collusion and many other economic problems. Our\napproach consists of two phases: In Phase I, we develop and train models on\nindividual market datasets from Japan, the United States, two regions in\nSwitzerland, Italy, and Brazil, focusing on predicting collusion in single\nmarkets. In Phase II, we extend the models' applicability through zero-shot\nlearning, employing a transfer learning approach that can detect collusion in\nmarkets in which training data is unavailable. This phase also incorporates\nout-of-distribution (OOD) generalization to evaluate the models' performance on\nunseen datasets from other countries and regions. In our empirical study, we\nshow that GNNs outperform NNs in detecting complex collusive patterns. This\nresearch contributes to the ongoing discourse on preventing collusion and\noptimizing detection methodologies, providing valuable guidance on the use of\nNNs and GNNs in economic applications to enhance market fairness and economic\nwelfare.",
        "authors": [
            "Lucas Gomes",
            "Jannis Kueck",
            "Mara Mattes",
            "Martin Spindler",
            "Alexey Zaytsev"
        ],
        "categories": "econ.EM",
        "published": "2024-10-09T17:31:41Z",
        "updated": "2024-10-09T17:31:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.06875v1",
        "title": "Group Shapley Value and Counterfactual Simulations in a Structural Model",
        "abstract": "We propose a variant of the Shapley value, the group Shapley value, to\ninterpret counterfactual simulations in structural economic models by\nquantifying the importance of different components. Our framework compares two\nsets of parameters, partitioned into multiple groups, and applying group\nShapley value decomposition yields unique additive contributions to the changes\nbetween these sets. The relative contributions sum to one, enabling us to\ngenerate an importance table that is as easily interpretable as a regression\ntable. The group Shapley value can be characterized as the solution to a\nconstrained weighted least squares problem. Using this property, we develop\nrobust decomposition methods to address scenarios where inputs for the group\nShapley value are missing. We first apply our methodology to a simple Roy model\nand then illustrate its usefulness by revisiting two published papers.",
        "authors": [
            "Yongchan Kwon",
            "Sokbae Lee",
            "Guillaume A. Pouliot"
        ],
        "categories": "econ.EM",
        "published": "2024-10-09T13:38:59Z",
        "updated": "2024-10-09T13:38:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.06564v1",
        "title": "Green bubbles: a four-stage paradigm for detection and propagation",
        "abstract": "Climate change has emerged as a significant global concern, attracting\nincreasing attention worldwide. While green bubbles may be examined through a\nsocial bubble hypothesis, it is essential not to neglect a Climate Minsky\nmoment triggered by sudden asset price changes. The significant increase in\ngreen investments highlights the urgent need for a comprehensive understanding\nof these market dynamics. Therefore, the current paper introduces a novel\nparadigm for studying such phenomena. Focusing on the renewable energy sector,\nStatistical Process Control (SPC) methodologies are employed to identify green\nbubbles within time series data. Furthermore, search volume indexes and social\nfactors are incorporated into established econometric models to reveal\npotential implications for the financial system. Inspired by Joseph\nSchumpeter's perspectives on business cycles, this study recognizes green\nbubbles as a necessary evil for facilitating a successful transition towards a\nmore sustainable future.",
        "authors": [
            "Gian Luca Vriz",
            "Luigi Grossi"
        ],
        "categories": "econ.EM",
        "published": "2024-10-09T05:58:29Z",
        "updated": "2024-10-09T05:58:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.05861v1",
        "title": "Persistence-Robust Break Detection in Predictive Quantile and CoVaR Regressions",
        "abstract": "Forecasting risk (as measured by quantiles) and systemic risk (as measured by\nAdrian and Brunnermeiers's (2016) CoVaR) is important in economics and finance.\nHowever, past research has shown that predictive relationships may be unstable\nover time. Therefore, this paper develops structural break tests in predictive\nquantile and CoVaR regressions. These tests can detect changes in the\nforecasting power of covariates, and are based on the principle of\nself-normalization. We show that our tests are valid irrespective of whether\nthe predictors are stationary or near-stationary, rendering the tests suitable\nfor a range of practical applications. Simulations illustrate the good\nfinite-sample properties of our tests. Two empirical applications concerning\nequity premium and systemic risk forecasting models show the usefulness of the\ntests.",
        "authors": [
            "Yannick Hoga"
        ],
        "categories": "stat.ME",
        "published": "2024-10-08T09:52:34Z",
        "updated": "2024-10-08T09:52:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.05741v3",
        "title": "The Transmission of Monetary Policy via Common Cycles in the Euro Area",
        "abstract": "We use a FAVAR model with proxy variables and sign restrictions to\ninvestigate the role of the euro area's common output and inflation cycles in\nthe transmission of monetary policy shocks. Our findings indicate that common\ncycles explain most of the variation in output and inflation across member\ncountries. However, Southern European economies exhibit a notable divergence\nfrom these cycles in the aftermath of the financial crisis. Building on this\nevidence, we demonstrate that monetary policy is homogeneously propagated to\nmember countries via the common cycles. In contrast, country-specific\ntransmission channels lead to heterogeneous country responses to monetary\npolicy shocks. Consequently, our empirical results suggest that the divergent\neffects of ECB monetary policy are attributable to heterogeneous\ncountry-specific exposures to financial markets, rather than to\ndis-synchronized economies within the euro area.",
        "authors": [
            "Lukas Berend",
            "Jan Pr\u00fcser"
        ],
        "categories": "econ.EM",
        "published": "2024-10-08T07:01:21Z",
        "updated": "2024-11-28T13:12:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.05634v1",
        "title": "Identification and estimation for matrix time series CP-factor models",
        "abstract": "We investigate the identification and the estimation for matrix time series\nCP-factor models. Unlike the generalized eigenanalysis-based method of Chang et\nal. (2023) which requires the two factor loading matrices to be full-ranked,\nthe newly proposed estimation can handle rank-deficient factor loading\nmatrices. The estimation procedure consists of the spectral decomposition of\nseveral matrices and a matrix joint diagonalization algorithm, resulting in low\ncomputational cost. The theoretical guarantee established without the\nstationarity assumption shows that the proposed estimation exhibits a faster\nconvergence rate than that of Chang et al. (2023). In fact the new estimator is\nfree from the adverse impact of any eigen-gaps, unlike most eigenanalysis-based\nmethods such as that of Chang et al. (2023). Furthermore, in terms of the error\nrates of the estimation, the proposed procedure is equivalent to handling a\nvector time series of dimension $\\max(p,q)$ instead of $p \\times q$, where $(p,\nq)$ are the dimensions of the matrix time series concerned. We have achieved\nthis without assuming the \"near orthogonality\" of the loadings under various\nincoherence conditions often imposed in the CP-decomposition literature, see\nHan and Zhang (2022), Han et al. (2024) and the references within. Illustration\nwith both simulated and real matrix time series data shows the usefulness of\nthe proposed approach.",
        "authors": [
            "Jinyuan Chang",
            "Yue Du",
            "Guanglin Huang",
            "Qiwei Yao"
        ],
        "categories": "stat.ME",
        "published": "2024-10-08T02:32:36Z",
        "updated": "2024-10-08T02:32:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.05630v1",
        "title": "Navigating Inflation in Ghana: How Can Machine Learning Enhance Economic Stability and Growth Strategies",
        "abstract": "Inflation remains a persistent challenge for many African countries. This\nresearch investigates the critical role of machine learning (ML) in\nunderstanding and managing inflation in Ghana, emphasizing its significance for\nthe country's economic stability and growth. Utilizing a comprehensive dataset\nspanning from 2010 to 2022, the study aims to employ advanced ML models,\nparticularly those adept in time series forecasting, to predict future\ninflation trends. The methodology is designed to provide accurate and reliable\ninflation forecasts, offering valuable insights for policymakers and advocating\nfor a shift towards data-driven approaches in economic decision-making. This\nstudy aims to significantly advance the academic field of economic analysis by\napplying machine learning (ML) and offering practical guidance for integrating\nadvanced technological tools into economic governance, ultimately demonstrating\nML's potential to enhance Ghana's economic resilience and support sustainable\ndevelopment through effective inflation management.",
        "authors": [
            "Theophilus G. Baidoo",
            "Ashley Obeng"
        ],
        "categories": "econ.EM",
        "published": "2024-10-08T02:26:50Z",
        "updated": "2024-10-08T02:26:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.05212v1",
        "title": "$\\texttt{rdid}$ and $\\texttt{rdidstag}$: Stata commands for robust difference-in-differences",
        "abstract": "This article provides a Stata package for the implementation of the robust\ndifference-in-differences (RDID) method developed in Ban and K\\'edagni (2023).\nIt contains three main commands: $\\texttt{rdid}$, $\\texttt{rdid_dy}$,\n$\\texttt{rdidstag}$, which we describe in the introduction and the main text.\nWe illustrate these commands through simulations and empirical examples.",
        "authors": [
            "Kyunghoon Ban",
            "D\u00e9sir\u00e9 K\u00e9dagni"
        ],
        "categories": "econ.EM",
        "published": "2024-10-07T17:18:52Z",
        "updated": "2024-10-07T17:18:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.05082v1",
        "title": "Large datasets for the Euro Area and its member countries and the dynamic effects of the common monetary policy",
        "abstract": "We present and describe a new publicly available large dataset which\nencompasses quarterly and monthly macroeconomic time series for both the Euro\nArea (EA) as a whole and its ten primary member countries. The dataset, which\nis called EA-MD-QD, includes more than 800 time series and spans the period\nfrom January 2000 to the latest available month. Since January 2024 EA-MD-QD is\nupdated on a monthly basis and constantly revised, making it an essential\nresource for conducting policy analysis related to economic outcomes in the EA.\nTo illustrate the usefulness of EA-MD-QD, we study the country specific Impulse\nResponses of the EA wide monetary policy shock by means of the Common Component\nVAR plus either Instrumental Variables or Sign Restrictions identification\nschemes. The results reveal asymmetries in the transmission of the monetary\npolicy shock across countries, particularly between core and peripheral\ncountries. Additionally, we find comovements across Euro Area countries'\nbusiness cycles to be driven mostly by real variables, compared to nominal\nones.",
        "authors": [
            "Matteo Barigozzi",
            "Claudio Lissona",
            "Lorenzo Tonni"
        ],
        "categories": "econ.EM",
        "published": "2024-10-07T14:36:32Z",
        "updated": "2024-10-07T14:36:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.04676v1",
        "title": "Democratizing Strategic Planning in Master-Planned Communities",
        "abstract": "This paper introduces a strategic planning tool for master-planned\ncommunities designed specifically to quantify residents' subjective preferences\nabout large investments in amenities and infrastructure projects. Drawing on\ndata obtained from brief online surveys, the tool ranks alternative plans by\nconsidering the aggregate anticipated utilization of each proposed amenity and\ncost sensitivity to it (or risk sensitivity for infrastructure plans). In\naddition, the tool estimates the percentage of households that favor the\npreferred plan and predicts whether residents would actually be willing to fund\nthe project. The mathematical underpinnings of the tool are borrowed from\nutility theory, incorporating exponential functions to model diminishing\nmarginal returns on quality, cost, and risk mitigation.",
        "authors": [
            "Christopher K. Allsup",
            "Irene S. Gabashvili"
        ],
        "categories": "econ.EM",
        "published": "2024-10-07T01:11:34Z",
        "updated": "2024-10-07T01:11:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.04431v1",
        "title": "A Structural Approach to Growth-at-Risk",
        "abstract": "We identify the structural impulse responses of quantiles of the outcome\nvariable to a shock. Our estimation strategy explicitly distinguishes treatment\nfrom control variables, allowing us to model responses of unconditional\nquantiles while using controls for identification. Disentangling the effect of\nadding control variables on identification versus interpretation brings our\nstructural quantile impulse responses conceptually closer to structural mean\nimpulse responses. Applying our methodology to study the impact of financial\nshocks on lower quantiles of output growth confirms that financial shocks have\nan outsized effect on growth-at-risk, but the magnitude of our estimates is\nmore extreme than in previous studies.",
        "authors": [
            "Robert Wojciechowski"
        ],
        "categories": "econ.EM",
        "published": "2024-10-06T09:56:49Z",
        "updated": "2024-10-06T09:56:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.04330v1",
        "title": "Inference in High-Dimensional Linear Projections: Multi-Horizon Granger Causality and Network Connectedness",
        "abstract": "This paper presents a Wald test for multi-horizon Granger causality within a\nhigh-dimensional sparse Vector Autoregression (VAR) framework. The null\nhypothesis focuses on the causal coefficients of interest in a local projection\n(LP) at a given horizon. Nevertheless, the post-double-selection method on LP\nmay not be applicable in this context, as a sparse VAR model does not\nnecessarily imply a sparse LP for horizon h>1. To validate the proposed test,\nwe develop two types of de-biased estimators for the causal coefficients of\ninterest, both relying on first-step machine learning estimators of the VAR\nslope parameters. The first estimator is derived from the Least Squares method,\nwhile the second is obtained through a two-stage approach that offers potential\nefficiency gains. We further derive heteroskedasticity- and\nautocorrelation-consistent (HAC) inference for each estimator. Additionally, we\npropose a robust inference method for the two-stage estimator, eliminating the\nneed to correct for serial correlation in the projection residuals. Monte Carlo\nsimulations show that the two-stage estimator with robust inference outperforms\nthe Least Squares method in terms of the Wald test size, particularly for\nlonger projection horizons. We apply our methodology to analyze the\ninterconnectedness of policy-related economic uncertainty among a large set of\ncountries in both the short and long run. Specifically, we construct a causal\nnetwork to visualize how economic uncertainty spreads across countries over\ntime. Our empirical findings reveal, among other insights, that in the short\nrun (1 and 3 months), the U.S. influences China, while in the long run (9 and\n12 months), China influences the U.S. Identifying these connections can help\nanticipate a country's potential vulnerabilities and propose proactive\nsolutions to mitigate the transmission of economic uncertainty.",
        "authors": [
            "Eugene Dettaa",
            "Endong Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-10-06T01:38:05Z",
        "updated": "2024-10-06T01:38:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.04165v1",
        "title": "How to Compare Copula Forecasts?",
        "abstract": "This paper lays out a principled approach to compare copula forecasts via\nstrictly consistent scores. We first establish the negative result that, in\ngeneral, copulas fail to be elicitable, implying that copula predictions cannot\nsensibly be compared on their own. A notable exception is on Fr\\'echet classes,\nthat is, when the marginal distribution structure is given and fixed, in which\ncase we give suitable scores for the copula forecast comparison. As a remedy\nfor the general non-elicitability of copulas, we establish novel\nmulti-objective scores for copula forecast along with marginal forecasts. They\ngive rise to two-step tests of equal or superior predictive ability which admit\nattribution of the forecast ranking to the accuracy of the copulas or the\nmarginals. Simulations show that our two-step tests work well in terms of size\nand power. We illustrate our new methodology via an empirical example using\ncopula forecasts for international stock market indices.",
        "authors": [
            "Tobias Fissler",
            "Yannick Hoga"
        ],
        "categories": "stat.ME",
        "published": "2024-10-05T14:02:17Z",
        "updated": "2024-10-05T14:02:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.07234v1",
        "title": "A Dynamic Approach to Stock Price Prediction: Comparing RNN and Mixture of Experts Models Across Different Volatility Profiles",
        "abstract": "This study evaluates the effectiveness of a Mixture of Experts (MoE) model\nfor stock price prediction by comparing it to a Recurrent Neural Network (RNN)\nand a linear regression model. The MoE framework combines an RNN for volatile\nstocks and a linear model for stable stocks, dynamically adjusting the weight\nof each model through a gating network. Results indicate that the MoE approach\nsignificantly improves predictive accuracy across different volatility\nprofiles. The RNN effectively captures non-linear patterns for volatile\ncompanies but tends to overfit stable data, whereas the linear model performs\nwell for predictable trends. The MoE model's adaptability allows it to\noutperform each individual model, reducing errors such as Mean Squared Error\n(MSE) and Mean Absolute Error (MAE). Future work should focus on enhancing the\ngating mechanism and validating the model with real-world datasets to optimize\nits practical applicability.",
        "authors": [
            "Diego Vallarino"
        ],
        "categories": "q-fin.CP",
        "published": "2024-10-04T14:36:21Z",
        "updated": "2024-10-04T14:36:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.03239v2",
        "title": "A new GARCH model with a deterministic time-varying intercept",
        "abstract": "It is common for long financial time series to exhibit gradual change in the\nunconditional volatility. We propose a new model that captures this type of\nnonstationarity in a parsimonious way. The model augments the volatility\nequation of a standard GARCH model by a deterministic time-varying intercept.\nIt captures structural change that slowly affects the amplitude of a time\nseries while keeping the short-run dynamics constant. We parameterize the\nintercept as a linear combination of logistic transition functions. We show\nthat the model can be derived from a multiplicative decomposition of volatility\nand preserves the financial motivation of variance decomposition. We use the\ntheory of locally stationary processes to show that the quasi maximum\nlikelihood estimator (QMLE) of the parameters of the model is consistent and\nasymptotically normally distributed. We examine the quality of the asymptotic\napproximation in a small simulation study. An empirical application to Oracle\nCorporation stock returns demonstrates the usefulness of the model. We find\nthat the persistence implied by the GARCH parameter estimates is reduced by\nincluding a time-varying intercept in the volatility equation.",
        "authors": [
            "Niklas Ahlgren",
            "Alexander Back",
            "Timo Ter\u00e4svirta"
        ],
        "categories": "econ.EM",
        "published": "2024-10-04T09:02:52Z",
        "updated": "2024-10-14T13:44:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.01658v1",
        "title": "Smaller Confidence Intervals From IPW Estimators via Data-Dependent Coarsening",
        "abstract": "Inverse propensity-score weighted (IPW) estimators are prevalent in causal\ninference for estimating average treatment effects in observational studies.\nUnder unconfoundedness, given accurate propensity scores and $n$ samples, the\nsize of confidence intervals of IPW estimators scales down with $n$, and,\nseveral of their variants improve the rate of scaling. However, neither IPW\nestimators nor their variants are robust to inaccuracies: even if a single\ncovariate has an $\\varepsilon>0$ additive error in the propensity score, the\nsize of confidence intervals of these estimators can increase arbitrarily.\nMoreover, even without errors, the rate with which the confidence intervals of\nthese estimators go to zero with $n$ can be arbitrarily slow in the presence of\nextreme propensity scores (those close to 0 or 1).\n  We introduce a family of Coarse IPW (CIPW) estimators that captures existing\nIPW estimators and their variants. Each CIPW estimator is an IPW estimator on a\ncoarsened covariate space, where certain covariates are merged. Under mild\nassumptions, e.g., Lipschitzness in expected outcomes and sparsity of extreme\npropensity scores, we give an efficient algorithm to find a robust estimator:\ngiven $\\varepsilon$-inaccurate propensity scores and $n$ samples, its\nconfidence interval size scales with $\\varepsilon+1/\\sqrt{n}$. In contrast,\nunder the same assumptions, existing estimators' confidence interval sizes are\n$\\Omega(1)$ irrespective of $\\varepsilon$ and $n$. Crucially, our estimator is\ndata-dependent and we show that no data-independent CIPW estimator can be\nrobust to inaccuracies.",
        "authors": [
            "Alkis Kalavasis",
            "Anay Mehrotra",
            "Manolis Zampetakis"
        ],
        "categories": "stat.ME",
        "published": "2024-10-02T15:25:26Z",
        "updated": "2024-10-02T15:25:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.01265v1",
        "title": "Transformers Handle Endogeneity in In-Context Linear Regression",
        "abstract": "We explore the capability of transformers to address endogeneity in\nin-context linear regression. Our main finding is that transformers inherently\npossess a mechanism to handle endogeneity effectively using instrumental\nvariables (IV). First, we demonstrate that the transformer architecture can\nemulate a gradient-based bi-level optimization procedure that converges to the\nwidely used two-stage least squares $(\\textsf{2SLS})$ solution at an\nexponential rate. Next, we propose an in-context pretraining scheme and provide\ntheoretical guarantees showing that the global minimizer of the pre-training\nloss achieves a small excess loss. Our extensive experiments validate these\ntheoretical findings, showing that the trained transformer provides more robust\nand reliable in-context predictions and coefficient estimates than the\n$\\textsf{2SLS}$ method, in the presence of endogeneity.",
        "authors": [
            "Haodong Liang",
            "Krishnakumar Balasubramanian",
            "Lifeng Lai"
        ],
        "categories": "stat.ML",
        "published": "2024-10-02T06:21:04Z",
        "updated": "2024-10-02T06:21:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.01175v1",
        "title": "Forecasting short-term inflation in Argentina with Random Forest Models",
        "abstract": "This paper examines the performance of Random Forest models in forecasting\nshort-term monthly inflation in Argentina, based on a database of monthly\nindicators since 1962. It is found that these models achieve forecast accuracy\nthat is statistically comparable to the consensus of market analysts'\nexpectations surveyed by the Central Bank of Argentina (BCRA) and to\ntraditional econometric models. One advantage of Random Forest models is that,\nas they are non-parametric, they allow for the exploration of nonlinear effects\nin the predictive power of certain macroeconomic variables on inflation. Among\nother findings, the relative importance of the exchange rate gap in forecasting\ninflation increases when the gap between the parallel and official exchange\nrates exceeds 60%. The predictive power of the exchange rate on inflation rises\nwhen the BCRA's net international reserves are negative or close to zero\n(specifically, below USD 2 billion). The relative importance of inflation\ninertia and the nominal interest rate in forecasting the following month's\ninflation increases when the nominal levels of inflation and/or interest rates\nrise.",
        "authors": [
            "Federico Daniel Forte"
        ],
        "categories": "econ.EM",
        "published": "2024-10-02T02:09:27Z",
        "updated": "2024-10-02T02:09:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.01159v2",
        "title": "Partially Identified Heterogeneous Treatment Effect with Selection: An Application to Gender Gaps",
        "abstract": "This paper addresses the sample selection model within the context of the\ngender gap problem, where even random treatment assignment is affected by\nselection bias. By offering a robust alternative free from distributional or\nspecification assumptions, we bound the treatment effect under the sample\nselection model with an exclusion restriction, an assumption whose validity is\ntested in the literature. This exclusion restriction allows for further\nsegmentation of the population into distinct types based on observed and\nunobserved characteristics. For each type, we derive the proportions and bound\nthe gender gap accordingly. Notably, trends in type proportions and gender gap\nbounds reveal an increasing proportion of always-working individuals over time,\nalongside variations in bounds, including a general decline across time and\nconsistently higher bounds for those in high-potential wage groups. Further\nanalysis, considering additional assumptions, highlights persistent gender gaps\nfor some types, while other types exhibit differing or inconclusive trends.\nThis underscores the necessity of separating individuals by type to understand\nthe heterogeneous nature of the gender gap.",
        "authors": [
            "Xiaolin Sun",
            "Xueyan Zhao",
            "D. S. Poskitt"
        ],
        "categories": "econ.EM",
        "published": "2024-10-02T01:29:13Z",
        "updated": "2024-10-03T06:01:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.00733v1",
        "title": "A Nonparametric Test of Heterogeneous Treatment Effects under Interference",
        "abstract": "Statistical inference of heterogeneous treatment effects (HTEs) across\npredefined subgroups is challenging when units interact because treatment\neffects may vary by pre-treatment variables, post-treatment exposure variables\n(that measure the exposure to other units' treatment statuses), or both. Thus,\nthe conventional HTEs testing procedures may be invalid under interference. In\nthis paper, I develop statistical methods to infer HTEs and disentangle the\ndrivers of treatment effects heterogeneity in populations where units interact.\nSpecifically, I incorporate clustered interference into the potential outcomes\nmodel and propose kernel-based test statistics for the null hypotheses of (i)\nno HTEs by treatment assignment (or post-treatment exposure variables) for all\npre-treatment variables values and (ii) no HTEs by pre-treatment variables for\nall treatment assignment vectors. I recommend a multiple-testing algorithm to\ndisentangle the source of heterogeneity in treatment effects. I prove the\nasymptotic properties of the proposed test statistics. Finally, I illustrate\nthe application of the test procedures in an empirical setting using an\nexperimental data set from a Chinese weather insurance program.",
        "authors": [
            "Julius Owusu"
        ],
        "categories": "econ.EM",
        "published": "2024-10-01T14:19:49Z",
        "updated": "2024-10-01T14:19:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.00217v1",
        "title": "Valid Inference on Functions of Causal Effects in the Absence of Microdata",
        "abstract": "Economists are often interested in functions of multiple causal effects, a\nleading example of which is evaluating the cost-effectiveness of a government\npolicy. In such settings, the benefits and costs might be captured by multiple\ncausal effects and aggregated into a scalar measure of cost-effectiveness.\nOftentimes, the microdata underlying these estimates is not accessible; only\nthe published estimates and their corresponding standard errors are available\nfor post-hoc analysis. We provide a method to conduct inference on functions of\ncausal effects when the only information available is the point estimates and\ntheir corresponding standard errors. We apply our method to conduct inference\non the Marginal Value of Public Funds (MVPF) for 8 different policies, and show\nthat even in the absence of any microdata, it is possible to conduct valid and\nmeaningful inference on the MVPF.",
        "authors": [
            "Vedant Vohra"
        ],
        "categories": "econ.EM",
        "published": "2024-09-30T20:29:13Z",
        "updated": "2024-09-30T20:29:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.20415v2",
        "title": "New Tests of Equal Forecast Accuracy for Factor-Augmented Regressions with Weaker Loadings",
        "abstract": "We provide the theoretical foundation for the recently proposed tests of\nequal forecast accuracy and encompassing by Pitarakis (2023a) and Pitarakis\n(2023b), when the competing forecast specification is that of a\nfactor-augmented regression model, whose loadings are allowed to be\nhomogeneously/heterogeneously weak. This should be of interest for\npractitioners, as at the moment there is no theory available to justify the use\nof these simple and powerful tests in such context.",
        "authors": [
            "Luca Margaritella",
            "Ovidijus Stauskas"
        ],
        "categories": "econ.EM",
        "published": "2024-09-30T15:41:55Z",
        "updated": "2024-10-02T09:26:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.20199v1",
        "title": "Synthetic Difference in Differences for Repeated Cross-Sectional Data",
        "abstract": "The synthetic difference-in-differences method provides an efficient method\nto estimate a causal effect with a latent factor model. However, it relies on\nthe use of panel data. This paper presents an adaptation of the synthetic\ndifference-in-differences method for repeated cross-sectional data. The\ntreatment is considered to be at the group level so that it is possible to\naggregate data by group to compute the two types of synthetic\ndifference-in-differences weights on these aggregated data. Then, I develop and\ncompute a third type of weight that accounts for the different number of\nobservations in each cross-section. Simulation results show that the\nperformance of the synthetic difference-in-differences estimator is improved\nwhen using the third type of weights on repeated cross-sectional data.",
        "authors": [
            "Yoann Morin"
        ],
        "categories": "econ.EM",
        "published": "2024-09-30T11:19:59Z",
        "updated": "2024-09-30T11:19:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.19287v1",
        "title": "Factors in Fashion: Factor Analysis towards the Mode",
        "abstract": "The modal factor model represents a new factor model for dimension reduction\nin high dimensional panel data. Unlike the approximate factor model that\ntargets for the mean factors, it captures factors that influence the\nconditional mode of the distribution of the observables. Statistical inference\nis developed with the aid of mode estimation, where the modal factors and the\nloadings are estimated through maximizing a kernel-type objective function. An\neasy-to-implement alternating maximization algorithm is designed to obtain the\nestimators numerically. Two model selection criteria are further proposed to\ndetermine the number of factors. The asymptotic properties of the proposed\nestimators are established under some regularity conditions. Simulations\ndemonstrate the nice finite sample performance of our proposed estimators, even\nin the presence of heavy-tailed and asymmetric idiosyncratic error\ndistributions. Finally, the application to inflation forecasting illustrates\nthe practical merits of modal factors.",
        "authors": [
            "Zhe Sun",
            "Yundong Tu"
        ],
        "categories": "econ.EM",
        "published": "2024-09-28T08:50:57Z",
        "updated": "2024-09-28T08:50:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.16132v1",
        "title": "Large Bayesian Tensor VARs with Stochastic Volatility",
        "abstract": "We consider Bayesian tensor vector autoregressions (TVARs) in which the VAR\ncoefficients are arranged as a three-dimensional array or tensor, and this\ncoefficient tensor is parameterized using a low-rank CP decomposition. We\ndevelop a family of TVARs using a general stochastic volatility specification,\nwhich includes a wide variety of commonly-used multivariate stochastic\nvolatility and COVID-19 outlier-augmented models. In a forecasting exercise\ninvolving 40 US quarterly variables, we show that these TVARs outperform the\nstandard Bayesian VAR with the Minnesota prior. The results also suggest that\nthe parsimonious common stochastic volatility model tends to forecast better\nthan the more flexible Cholesky stochastic volatility model.",
        "authors": [
            "Joshua C. C. Chan",
            "Yaling Qi"
        ],
        "categories": "econ.EM",
        "published": "2024-09-24T14:40:43Z",
        "updated": "2024-09-24T14:40:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.15530v1",
        "title": "Identifying Elasticities in Autocorrelated Time Series Using Causal Graphs",
        "abstract": "The price elasticity of demand can be estimated from observational data using\ninstrumental variables (IV). However, naive IV estimators may be inconsistent\nin settings with autocorrelated time series. We argue that causal time graphs\ncan simplify IV identification and help select consistent estimators. To do so,\nwe propose to first model the equilibrium condition by an unobserved\nconfounder, deriving a directed acyclic graph (DAG) while maintaining the\nassumption of a simultaneous determination of prices and quantities. We then\nexploit recent advances in graphical inference to derive valid IV estimators,\nincluding estimators that achieve consistency by simultaneously estimating\nnuisance effects. We further argue that observing significant differences\nbetween the estimates of presumably valid estimators can help to reject false\nmodel assumptions, thereby improving our understanding of underlying economic\ndynamics. We apply this approach to the German electricity market, estimating\nthe price elasticity of demand on simulated and real-world data. The findings\nunderscore the importance of accounting for structural autocorrelation in\nIV-based analysis.",
        "authors": [
            "Silvana Tiedemann",
            "Jorge Sanchez Canales",
            "Felix Schur",
            "Raffaele Sgarlato",
            "Lion Hirth",
            "Oliver Ruhnau",
            "Jonas Peters"
        ],
        "categories": "econ.EM",
        "published": "2024-09-23T20:28:08Z",
        "updated": "2024-09-23T20:28:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.15070v1",
        "title": "Non-linear dependence and Granger causality: A vine copula approach",
        "abstract": "Inspired by Jang et al. (2022), we propose a Granger causality-in-the-mean\ntest for bivariate $k-$Markov stationary processes based on a recently\nintroduced class of non-linear models, i.e., vine copula models. By means of a\nsimulation study, we show that the proposed test improves on the statistical\nproperties of the original test in Jang et al. (2022), constituting an\nexcellent tool for testing Granger causality in the presence of non-linear\ndependence structures. Finally, we apply our test to study the pairwise\nrelationships between energy consumption, GDP and investment in the U.S. and,\nnotably, we find that Granger-causality runs two ways between GDP and energy\nconsumption.",
        "authors": [
            "Roberto Fuentes M.",
            "Irene Crimaldi",
            "Armando Rungi"
        ],
        "categories": "econ.EM",
        "published": "2024-09-23T14:41:40Z",
        "updated": "2024-09-23T14:41:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.14776v1",
        "title": "Inequality Sensitive Optimal Treatment Assignment",
        "abstract": "The egalitarian equivalent, $ee$, of a societal distribution of outcomes with\nmean $m$ is the outcome level such that the evaluator is indifferent between\nthe distribution of outcomes and a society in which everyone obtains an outcome\nof $ee$. For an inequality averse evaluator, $ee < m$. In this paper, I extend\nthe optimal treatment choice framework in Manski (2024) to the case where the\nwelfare evaluation is made using egalitarian equivalent measures, and derive\noptimal treatment rules for the Bayesian, maximin and minimax regret inequality\naverse evaluators. I illustrate how the methodology operates in the context of\nthe JobCorps education and training program for disadvantaged youth (Schochet,\nBurghardt, and McConnell 2008) and in Meager (2022)'s Bayesian meta analysis of\nthe microcredit literature.",
        "authors": [
            "Eduardo Zambrano"
        ],
        "categories": "econ.EM",
        "published": "2024-09-23T07:47:09Z",
        "updated": "2024-09-23T07:47:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.14734v1",
        "title": "The continuous-time limit of quasi score-driven volatility models",
        "abstract": "This paper explores the continuous-time limit of a class of Quasi\nScore-Driven (QSD) models that characterize volatility. As the sampling\nfrequency increases and the time interval tends to zero, the model weakly\nconverges to a continuous-time stochastic volatility model where the two\nBrownian motions are correlated, thereby capturing the leverage effect in the\nmarket. Subsequently, we identify that a necessary condition for non-degenerate\ncorrelation is that the distribution of driving innovations differs from that\nof computing score, and at least one being asymmetric. We then illustrate this\nwith two typical examples. As an application, the QSD model is used as an\napproximation for correlated stochastic volatility diffusions and quasi maximum\nlikelihood estimation is performed. Simulation results confirm the method's\neffectiveness, particularly in estimating the correlation coefficient.",
        "authors": [
            "Yinhao Wu",
            "Ping He"
        ],
        "categories": "math.PR",
        "published": "2024-09-23T06:20:20Z",
        "updated": "2024-09-23T06:20:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.14202v2",
        "title": "Mining Causality: AI-Assisted Search for Instrumental Variables",
        "abstract": "The instrumental variables (IVs) method is a leading empirical strategy for\ncausal inference. Finding IVs is a heuristic and creative process, and\njustifying its validity--especially exclusion restrictions--is largely\nrhetorical. We propose using large language models (LLMs) to search for new IVs\nthrough narratives and counterfactual reasoning, similar to how a human\nresearcher would. The stark difference, however, is that LLMs can dramatically\naccelerate this process and explore an extremely large search space. We\ndemonstrate how to construct prompts to search for potentially valid IVs. We\ncontend that multi-step and role-playing prompting strategies are effective for\nsimulating the endogenous decision-making processes of economic agents and for\nnavigating language models through the realm of real-world scenarios. We apply\nour method to three well-known examples in economics: returns to schooling,\nsupply and demand, and peer effects. We then extend our strategy to finding (i)\ncontrol variables in regression and difference-in-differences and (ii) running\nvariables in regression discontinuity designs.",
        "authors": [
            "Sukjin Han"
        ],
        "categories": "econ.EM",
        "published": "2024-09-21T17:19:29Z",
        "updated": "2024-11-11T04:41:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.13531v1",
        "title": "A simple but powerful tail index regression",
        "abstract": "This paper introduces a flexible framework for the estimation of the\nconditional tail index of heavy tailed distributions. In this framework, the\ntail index is computed from an auxiliary linear regression model that\nfacilitates estimation and inference based on established econometric methods,\nsuch as ordinary least squares (OLS), least absolute deviations, or\nM-estimation. We show theoretically and via simulations that OLS provides\ninteresting results. Our Monte Carlo results highlight the adequate finite\nsample properties of the OLS tail index estimator computed from the proposed\nnew framework and contrast its behavior to that of tail index estimates\nobtained by maximum likelihood estimation of exponential regression models,\nwhich is one of the approaches currently in use in the literature. An empirical\nanalysis of the impact of determinants of the conditional left- and right-tail\nindexes of commodities' return distributions highlights the empirical relevance\nof our proposed approach. The novel framework's flexibility allows for\nextensions and generalizations in various directions, empowering researchers\nand practitioners to straightforwardly explore a wide range of research\nquestions.",
        "authors": [
            "Jo\u00e3o Nicolau",
            "Paulo M. M. Rodrigues"
        ],
        "categories": "econ.EM",
        "published": "2024-09-20T14:20:26Z",
        "updated": "2024-09-20T14:20:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.13516v1",
        "title": "Dynamic tail risk forecasting: what do realized skewness and kurtosis add?",
        "abstract": "This paper compares the accuracy of tail risk forecasts with a focus on\nincluding realized skewness and kurtosis in \"additive\" and \"multiplicative\"\nmodels. Utilizing a panel of 960 US stocks, we conduct diagnostic tests, employ\nscoring functions, and implement rolling window forecasting to evaluate the\nperformance of Value at Risk (VaR) and Expected Shortfall (ES) forecasts.\nAdditionally, we examine the impact of the window length on forecast accuracy.\nWe propose model specifications that incorporate realized skewness and kurtosis\nfor enhanced precision. Our findings provide insights into the importance of\nconsidering skewness and kurtosis in tail risk modeling, contributing to the\nexisting literature and offering practical implications for risk practitioners\nand researchers.",
        "authors": [
            "Giampiero Gallo",
            "Ostap Okhrin",
            "Giuseppe Storti"
        ],
        "categories": "econ.EM",
        "published": "2024-09-20T13:57:22Z",
        "updated": "2024-09-20T13:57:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.12662v1",
        "title": "Testing for equal predictive accuracy with strong dependence",
        "abstract": "We analyse the properties of the Diebold and Mariano (1995) test in the\npresence of autocorrelation in the loss differential. We show that the power of\nthe Diebold and Mariano (1995) test decreases as the dependence increases,\nmaking it more difficult to obtain statistically significant evidence of\nsuperior predictive ability against less accurate benchmarks. We also find\nthat, after a certain threshold, the test has no power and the correct null\nhypothesis is spuriously rejected. Taken together, these results caution to\nseriously consider the dependence properties of the loss differential before\nthe application of the Diebold and Mariano (1995) test.",
        "authors": [
            "Laura Coroneo",
            "Fabrizio Iacone"
        ],
        "categories": "econ.EM",
        "published": "2024-09-19T11:23:06Z",
        "updated": "2024-09-19T11:23:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.12611v1",
        "title": "Parameters on the boundary in predictive regression",
        "abstract": "We consider bootstrap inference in predictive (or Granger-causality)\nregressions when the parameter of interest may lie on the boundary of the\nparameter space, here defined by means of a smooth inequality constraint. For\ninstance, this situation occurs when the definition of the parameter space\nallows for the cases of either no predictability or sign-restricted\npredictability. We show that in this context constrained estimation gives rise\nto bootstrap statistics whose limit distribution is, in general, random, and\nthus distinct from the limit null distribution of the original statistics of\ninterest. This is due to both (i) the possible location of the true parameter\nvector on the boundary of the parameter space, and (ii) the possible\nnon-stationarity of the posited predicting (resp. Granger-causing) variable. We\ndiscuss a modification of the standard fixed-regressor wild bootstrap scheme\nwhere the bootstrap parameter space is shifted by a data-dependent function in\norder to eliminate the portion of limiting bootstrap randomness attributable to\nthe boundary, and prove validity of the associated bootstrap inference under\nnon-stationarity of the predicting variable as the only remaining source of\nlimiting bootstrap randomness. Our approach, which is initially presented in a\nsimple location model, has bearing on inference in parameter-on-the-boundary\nsituations beyond the predictive regression problem.",
        "authors": [
            "Giuseppe Cavaliere",
            "Iliyan Georgiev",
            "Edoardo Zanelli"
        ],
        "categories": "econ.EM",
        "published": "2024-09-19T09:33:19Z",
        "updated": "2024-09-19T09:33:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.03557v1",
        "title": "Robust Bond Risk Premia Predictability Test in the Quantiles",
        "abstract": "Different from existing literature on testing the macro-spanning hypothesis\nof bond risk premia, which only considers mean regressions, this paper\ninvestigates whether the yield curve represented by CP factor (Cochrane and\nPiazzesi, 2005) contains all available information about future bond returns in\na predictive quantile regression with many other macroeconomic variables. In\nthis study, we introduce the Trend in Debt Holding (TDH) as a novel predictor,\ntesting it alongside established macro indicators such as Trend Inflation (TI)\n(Cieslak and Povala, 2015), and macro factors from Ludvigson and Ng (2009). A\nsignificant challenge in this study is the invalidity of traditional quantile\nmodel inference approaches, given the high persistence of many macro variables\ninvolved. Furthermore, the existing methods addressing this issue do not\nperform well in the marginal test with many highly persistent predictors. Thus,\nwe suggest a robust inference approach, whose size and power performance are\nshown to be better than existing tests. Using data from 1980-2022, the\nmacro-spanning hypothesis is strongly supported at center quantiles by the\nempirical finding that the CP factor has predictive power while all other macro\nvariables have negligible predictive power in this case. On the other hand, the\nevidence against the macro-spanning hypothesis is found at tail quantiles, in\nwhich TDH has predictive power at right tail quantiles while TI has predictive\npower at both tails quantiles. Finally, we show the performance of in-sample\nand out-of-sample predictions implemented by the proposed method are better\nthan existing methods.",
        "authors": [
            "Xiaosai Liao",
            "Xinjue Li",
            "Qingliang Fan"
        ],
        "categories": "econ.EM",
        "published": "2024-09-19T05:35:09Z",
        "updated": "2024-09-19T05:35:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.12353v2",
        "title": "A Way to Synthetic Triple Difference",
        "abstract": "This paper discusses a practical approach that combines synthetic control\nwith triple difference to address violations of the parallel trends assumption.\nBy transforming triple difference into a DID structure, we can apply synthetic\ncontrol to a triple-difference framework, enabling more robust estimates when\nparallel trends are violated across multiple dimensions. The proposed procedure\nis applied to a real-world dataset to illustrate when and how we should apply\nthis practice, while cautions are presented afterwards. This method contributes\nto improving causal inference in policy evaluations and offers a valuable tool\nfor researchers dealing with heterogeneous treatment effects across subgroups.",
        "authors": [
            "Castiel Chen Zhuang"
        ],
        "categories": "econ.EM",
        "published": "2024-09-18T23:20:47Z",
        "updated": "2024-09-22T12:17:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.10820v1",
        "title": "Simple robust two-stage estimation and inference for generalized impulse responses and multi-horizon causality",
        "abstract": "This paper introduces a novel two-stage estimation and inference procedure\nfor generalized impulse responses (GIRs). GIRs encompass all coefficients in a\nmulti-horizon linear projection model of future outcomes of y on lagged values\n(Dufour and Renault, 1998), which include the Sims' impulse response. The\nconventional use of Least Squares (LS) with heteroskedasticity- and\nautocorrelation-consistent covariance estimation is less precise and often\nresults in unreliable finite sample tests, further complicated by the selection\nof bandwidth and kernel functions. Our two-stage method surpasses the LS\napproach in terms of estimation efficiency and inference robustness. The\nrobustness stems from our proposed covariance matrix estimates, which eliminate\nthe need to correct for serial correlation in the multi-horizon projection\nresiduals. Our method accommodates non-stationary data and allows the\nprojection horizon to grow with sample size. Monte Carlo simulations\ndemonstrate our two-stage method outperforms the LS method. We apply the\ntwo-stage method to investigate the GIRs, implement multi-horizon Granger\ncausality test, and find that economic uncertainty exerts both short-run (1-3\nmonths) and long-run (30 months) effects on economic activities.",
        "authors": [
            "Jean-Marie Dufour",
            "Endong Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-09-17T01:28:13Z",
        "updated": "2024-09-17T01:28:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.10750v1",
        "title": "GPT takes the SAT: Tracing changes in Test Difficulty and Math Performance of Students",
        "abstract": "Scholastic Aptitude Test (SAT) is crucial for college admissions but its\neffectiveness and relevance are increasingly questioned. This paper enhances\nSynthetic Control methods by introducing \"Transformed Control\", a novel method\nthat employs Large Language Models (LLMs) powered by Artificial Intelligence to\ngenerate control groups. We utilize OpenAI's API to generate a control group\nwhere GPT-4, or ChatGPT, takes multiple SATs annually from 2008 to 2023. This\ncontrol group helps analyze shifts in SAT math difficulty over time, starting\nfrom the baseline year of 2008. Using parallel trends, we calculate the Average\nDifference in Scores (ADS) to assess changes in high school students' math\nperformance. Our results indicate a significant decrease in the difficulty of\nthe SAT math section over time, alongside a decline in students' math\nperformance. The analysis shows a 71-point drop in the rigor of SAT math from\n2008 to 2023, with student performance decreasing by 36 points, resulting in a\n107-point total divergence in average student math performance. We investigate\npossible mechanisms for this decline in math proficiency, such as changing\nuniversity selection criteria, increased screen time, grade inflation, and\nworsening adolescent mental health. Disparities among demographic groups show a\n104-point drop for White students, 84 points for Black students, and 53 points\nfor Asian students. Male students saw a 117-point reduction, while female\nstudents had a 100-point decrease.",
        "authors": [
            "Vikram Krishnaveti",
            "Saannidhya Rawat"
        ],
        "categories": "econ.EM",
        "published": "2024-09-16T21:45:41Z",
        "updated": "2024-09-16T21:45:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.10448v2",
        "title": "Why you should also use OLS estimation of tail exponents",
        "abstract": "Even though practitioners often estimate Pareto exponents running OLS\nrank-size regressions, the usual recommendation is to use the Hill MLE with a\nsmall-sample correction instead, due to its unbiasedness and efficiency. In\nthis paper, we advocate that you should also apply OLS in empirical\napplications. On the one hand, we demonstrate that, with a small-sample\ncorrection, the OLS estimator is also unbiased. On the other hand, we show that\nthe MLE assigns significantly greater weight to smaller observations. This\nsuggests that the OLS estimator may outperform the MLE in cases where the\ndistribution is (i) strictly Pareto but only in the upper tail or (ii)\nregularly varying rather than strictly Pareto. We substantiate our theoretical\nfindings with Monte Carlo simulations and real-world applications,\ndemonstrating the practical relevance of the OLS method in estimating tail\nexponents.",
        "authors": [
            "Thiago Trafane Oliveira Santos",
            "Daniel Oliveira Cajueiro"
        ],
        "categories": "stat.ME",
        "published": "2024-09-16T16:36:43Z",
        "updated": "2024-09-17T11:10:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.10030v2",
        "title": "Econometric Inference for High Dimensional Predictive Regressions",
        "abstract": "LASSO introduces shrinkage bias into estimated coefficients, which can\nadversely affect the desirable asymptotic normality and invalidate the standard\ninferential procedure based on the $t$-statistic. The desparsified LASSO has\nemerged as a well-known remedy for this issue. In the context of high\ndimensional predictive regression, the desparsified LASSO faces an additional\nchallenge: the Stambaugh bias arising from nonstationary regressors. To restore\nthe standard inferential procedure, we propose a novel estimator called\nIVX-desparsified LASSO (XDlasso). XDlasso eliminates the shrinkage bias and the\nStambaugh bias simultaneously and does not require prior knowledge about the\nidentities of nonstationary and stationary regressors. We establish the\nasymptotic properties of XDlasso for hypothesis testing, and our theoretical\nfindings are supported by Monte Carlo simulations. Applying our method to\nreal-world applications from the FRED-MD database -- which includes a rich set\nof control variables -- we investigate two important empirical questions: (i)\nthe predictability of the U.S. stock returns based on the earnings-price ratio,\nand (ii) the predictability of the U.S. inflation using the unemployment rate.",
        "authors": [
            "Zhan Gao",
            "Ji Hyung Lee",
            "Ziwei Mei",
            "Zhentao Shi"
        ],
        "categories": "stat.ME",
        "published": "2024-09-16T06:41:58Z",
        "updated": "2024-11-09T15:23:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.09962v1",
        "title": "A Simple and Adaptive Confidence Interval when Nuisance Parameters Satisfy an Inequality",
        "abstract": "Inequalities may appear in many models. They can be as simple as assuming a\nparameter is nonnegative, possibly a regression coefficient or a treatment\neffect. This paper focuses on the case that there is only one inequality and\nproposes a confidence interval that is particularly attractive, called the\ninequality-imposed confidence interval (IICI). The IICI is simple. It does not\nrequire simulations or tuning parameters. The IICI is adaptive. It reduces to\nthe usual confidence interval (calculated by adding and subtracting the\nstandard error times the $1 - \\alpha/2$ standard normal quantile) when the\ninequality is sufficiently slack. When the inequality is sufficiently violated,\nthe IICI reduces to an equality-imposed confidence interval (the usual\nconfidence interval for the submodel where the inequality holds with equality).\nAlso, the IICI is uniformly valid and has (weakly) shorter length than the\nusual confidence interval; it is never longer. The first empirical application\nconsiders a linear regression when a coefficient is known to be nonpositive. A\nsecond empirical application considers an instrumental variables regression\nwhen the endogeneity of a regressor is known to be nonnegative.",
        "authors": [
            "Gregory Fletcher Cox"
        ],
        "categories": "econ.EM",
        "published": "2024-09-16T03:29:07Z",
        "updated": "2024-09-16T03:29:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.09894v1",
        "title": "Estimating Wage Disparities Using Foundation Models",
        "abstract": "One thread of empirical work in social science focuses on decomposing group\ndifferences in outcomes into unexplained components and components explained by\nobservable factors. In this paper, we study gender wage decompositions, which\nrequire estimating the portion of the gender wage gap explained by career\nhistories of workers. Classical methods for decomposing the wage gap employ\nsimple predictive models of wages which condition on a small set of simple\nsummaries of labor history. The problem is that these predictive models cannot\ntake advantage of the full complexity of a worker's history, and the resulting\ndecompositions thus suffer from omitted variable bias (OVB), where covariates\nthat are correlated with both gender and wages are not included in the model.\nHere we explore an alternative methodology for wage gap decomposition that\nemploys powerful foundation models, such as large language models, as the\npredictive engine. Foundation models excel at making accurate predictions from\ncomplex, high-dimensional inputs. We use a custom-built foundation model,\ndesigned to predict wages from full labor histories, to decompose the gender\nwage gap. We prove that the way such models are usually trained might still\nlead to OVB, but develop fine-tuning algorithms that empirically mitigate this\nissue. Our model captures a richer representation of career history than simple\nmodels and predicts wages more accurately. In detail, we first provide a novel\nset of conditions under which an estimator of the wage gap based on a\nfine-tuned foundation model is $\\sqrt{n}$-consistent. Building on the theory,\nwe then propose methods for fine-tuning foundation models that minimize OVB.\nUsing data from the Panel Study of Income Dynamics, we find that history\nexplains more of the gender wage gap than standard econometric models can\nmeasure, and we identify elements of history that are important for reducing\nOVB.",
        "authors": [
            "Keyon Vafa",
            "Susan Athey",
            "David M. Blei"
        ],
        "categories": "cs.LG",
        "published": "2024-09-15T23:22:21Z",
        "updated": "2024-09-15T23:22:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.09577v1",
        "title": "Structural counterfactual analysis in macroeconomics: theory and inference",
        "abstract": "We propose a structural model-free methodology to analyze two types of\nmacroeconomic counterfactuals related to policy path deviation: hypothetical\ntrajectory and policy intervention. Our model-free approach is built on a\nstructural vector moving-average (SVMA) model that relies solely on the\nidentification of policy shocks, thereby eliminating the need to specify an\nentire structural model. Analytical solutions are derived for the\ncounterfactual parameters, and statistical inference for these parameter\nestimates is provided using the Delta method. By utilizing external\ninstruments, we introduce a projection-based method for the identification,\nestimation, and inference of these parameters. This approach connects our\ncounterfactual analysis with the Local Projection literature. A\nsimulation-based approach with nonlinear model is provided to add in addressing\nLucas' critique. The innovative model-free methodology is applied in three\ncounterfactual studies on the U.S. monetary policy: (1) a historical scenario\nanalysis for a hypothetical interest rate path in the post-pandemic era, (2) a\nfuture scenario analysis under either hawkish or dovish interest rate policy,\nand (3) an evaluation of the policy intervention effect of an oil price shock\nby zeroing out the systematic responses of the interest rate.",
        "authors": [
            "Endong Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-09-15T01:36:22Z",
        "updated": "2024-09-15T01:36:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.09243v2",
        "title": "Unconditional Randomization Tests for Interference",
        "abstract": "When conducting causal inference or designing policy, researchers are often\nconcerned with the existence and extent of interference between units, which\nmay be influenced by factors such as distance, proximity, and connection\nstrength. However, complex correlations across units pose significant\nchallenges for inference. This paper introduces partial null randomization\ntests (PNRTs), a novel framework for testing interference in experimental\nsettings. PNRTs adopt a design-based approach, combining unconditional\nrandomization testing with pairwise comparisons to enable straightforward\nimplementation and ensure finite-sample validity under minimal assumptions\nabout network structure. To illustrate the method's broad applicability, this\npaper applies it to a large-scale experiment by Blattman et al. (2021) in\nBogota, Colombia, which evaluates the impact of hotspot policing on crime using\nstreet segments as units of analysis. The findings indicate that increasing\npolice patrolling time in hotspots has a significant displacement effect on\nviolent crime but not on property crime. A simulation study calibrated to this\ndataset further demonstrates the strong power properties of PNRTs and their\nsuitability for general interference scenarios.",
        "authors": [
            "Liang Zhong"
        ],
        "categories": "econ.EM",
        "published": "2024-09-14T00:51:45Z",
        "updated": "2024-10-28T17:02:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.08773v1",
        "title": "The Clustered Dose-Response Function Estimator for continuous treatment with heterogeneous treatment effects",
        "abstract": "Many treatments are non-randomly assigned, continuous in nature, and exhibit\nheterogeneous effects even at identical treatment intensities. Taken together,\nthese characteristics pose significant challenges for identifying causal\neffects, as no existing estimator can provide an unbiased estimate of the\naverage causal dose-response function. To address this gap, we introduce the\nClustered Dose-Response Function (Cl-DRF), a novel estimator designed to\ndiscern the continuous causal relationships between treatment intensity and the\ndependent variable across different subgroups. This approach leverages both\ntheoretical and data-driven sources of heterogeneity and operates under relaxed\nversions of the conditional independence and positivity assumptions, which are\nrequired to be met only within each identified subgroup. To demonstrate the\ncapabilities of the Cl-DRF estimator, we present both simulation evidence and\nan empirical application examining the impact of European Cohesion funds on\neconomic growth.",
        "authors": [
            "Cerqua Augusto",
            "Di Stefano Roberta",
            "Mattera Raffaele"
        ],
        "categories": "econ.EM",
        "published": "2024-09-13T12:31:07Z",
        "updated": "2024-09-13T12:31:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2410.00002v1",
        "title": "Machine Learning and Econometric Approaches to Fiscal Policies: Understanding Industrial Investment Dynamics in Uruguay (1974-2010)",
        "abstract": "This paper examines the impact of fiscal incentives on industrial investment\nin Uruguay from 1974 to 2010. Using a mixed-method approach that combines\neconometric models with machine learning techniques, the study investigates\nboth the short-term and long-term effects of fiscal benefits on industrial\ninvestment. The results confirm the significant role of fiscal incentives in\ndriving long-term industrial growth, while also highlighting the importance of\na stable macroeconomic environment, public investment, and access to credit.\nMachine learning models provide additional insights into nonlinear interactions\nbetween fiscal benefits and other macroeconomic factors, such as exchange\nrates, emphasizing the need for tailored fiscal policies. The findings have\nimportant policy implications, suggesting that fiscal incentives, when combined\nwith broader economic reforms, can effectively promote industrial development\nin emerging economies.",
        "authors": [
            "Diego Vallarino"
        ],
        "categories": "econ.GN",
        "published": "2024-09-12T19:01:16Z",
        "updated": "2024-09-12T19:01:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.08354v2",
        "title": "Bayesian Dynamic Factor Models for High-dimensional Matrix-valued Time Series",
        "abstract": "High-dimensional matrix-valued time series are of significant interest in\neconomics and finance, with prominent examples including cross region\nmacroeconomic panels and firms' financial data panels. We introduce a class of\nBayesian matrix dynamic factor models that utilize matrix structures to\nidentify more interpretable factor patterns and factor impacts. Our model\naccommodates time-varying volatility, adjusts for outliers, and allows\ncross-sectional correlations in the idiosyncratic components. To determine the\ndimension of the factor matrix, we employ an importance-sampling estimator\nbased on the cross-entropy method to estimate marginal likelihoods. Through a\nseries of Monte Carlo experiments, we show the properties of the factor\nestimators and the performance of the marginal likelihood estimator in\ncorrectly identifying the true dimensions of the factor matrices. Applying our\nmodel to a macroeconomic dataset and a financial dataset, we demonstrate its\nability in unveiling interesting features within matrix-valued time series.",
        "authors": [
            "Wei Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-09-12T18:37:45Z",
        "updated": "2024-11-20T18:15:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.08347v1",
        "title": "Substitution in the perturbed utility route choice model",
        "abstract": "This paper considers substitution patterns in the perturbed utility route\nchoice model. We provide a general result that determines the marginal change\nin link flows following a marginal change in link costs across the network. We\ngive a general condition on the network structure under which all paths are\nnecessarily substitutes and an example in which some paths are complements. The\npresence of complementarity contradicts a result in a previous paper in this\njournal; we point out and correct the error.",
        "authors": [
            "Mogens Fosgerau",
            "Nikolaj Nielsen",
            "Mads Paulsen",
            "Thomas Kj\u00e6r Rasmussen",
            "Rui Yao"
        ],
        "categories": "econ.EM",
        "published": "2024-09-12T18:19:02Z",
        "updated": "2024-09-12T18:19:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.08158v1",
        "title": "Trends and biases in the social cost of carbon",
        "abstract": "An updated and extended meta-analysis confirms that the central estimate of\nthe social cost of carbon is around $200/tC with a large, right-skewed\nuncertainty and trending up. The pure rate of time preference and the inverse\nof the elasticity of intertemporal substitution are key assumptions, the total\nimpact of 2.5K warming less so. The social cost of carbon is much higher if\nclimate change is assumed to affect economic growth rather than the level of\noutput and welfare. The literature is dominated by a relatively small network\nof authors, based in a few countries. Publication and citation bias have pushed\nthe social cost of carbon up.",
        "authors": [
            "Richard S. J. Tol"
        ],
        "categories": "econ.EM",
        "published": "2024-09-12T15:51:51Z",
        "updated": "2024-09-12T15:51:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.07859v1",
        "title": "Bootstrap Adaptive Lasso Solution Path Unit Root Tests",
        "abstract": "We propose sieve wild bootstrap analogues to the adaptive Lasso solution path\nunit root tests of Arnold and Reinschl\\\"ussel (2024) arXiv:2404.06205 to\nimprove finite sample properties and extend their applicability to a\ngeneralised framework, allowing for non-stationary volatility. Numerical\nevidence shows the bootstrap to improve the tests' precision for error\nprocesses that promote spurious rejections of the unit root null, depending on\nthe detrending procedure. The bootstrap mitigates finite-sample size\ndistortions and restores asymptotically valid inference when the data features\ntime-varying unconditional variance. We apply the bootstrap tests to real\nresidential property prices of the top six Eurozone economies and find evidence\nof stationarity to be period-specific, supporting the conjecture that\nexuberance in the housing market characterises the development of Euro-era\nresidential property prices in the recent past.",
        "authors": [
            "Martin C. Arnold",
            "Thilo Reinschl\u00fcssel"
        ],
        "categories": "stat.ME",
        "published": "2024-09-12T09:05:40Z",
        "updated": "2024-09-12T09:05:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.07087v1",
        "title": "Testing for a Forecast Accuracy Breakdown under Long Memory",
        "abstract": "We propose a test to detect a forecast accuracy breakdown in a long memory\ntime series and provide theoretical and simulation evidence on the memory\ntransfer from the time series to the forecast residuals. The proposed method\nuses a double sup-Wald test against the alternative of a structural break in\nthe mean of an out-of-sample loss series. To address the problem of estimating\nthe long-run variance under long memory, a robust estimator is applied. The\ncorresponding breakpoint results from a long memory robust CUSUM test. The\nfinite sample size and power properties of the test are derived in a Monte\nCarlo simulation. A monotonic power function is obtained for the fixed\nforecasting scheme. In our practical application, we find that the global\nenergy crisis that began in 2021 led to a forecast break in European\nelectricity prices, while the results for the U.S. are mixed.",
        "authors": [
            "Jannik Kreye",
            "Philipp Sibbertsen"
        ],
        "categories": "econ.EM",
        "published": "2024-09-11T08:15:09Z",
        "updated": "2024-09-11T08:15:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.06654v1",
        "title": "Estimation and Inference for Causal Functions with Multiway Clustered Data",
        "abstract": "This paper proposes methods of estimation and uniform inference for a general\nclass of causal functions, such as the conditional average treatment effects\nand the continuous treatment effects, under multiway clustering. The causal\nfunction is identified as a conditional expectation of an adjusted\n(Neyman-orthogonal) signal that depends on high-dimensional nuisance\nparameters. We propose a two-step procedure where the first step uses machine\nlearning to estimate the high-dimensional nuisance parameters. The second step\nprojects the estimated Neyman-orthogonal signal onto a dictionary of basis\nfunctions whose dimension grows with the sample size. For this two-step\nprocedure, we propose both the full-sample and the multiway cross-fitting\nestimation approaches. A functional limit theory is derived for these\nestimators. To construct the uniform confidence bands, we develop a novel\nresampling procedure, called the multiway cluster-robust sieve score bootstrap,\nthat extends the sieve score bootstrap (Chen and Christensen, 2018) to the\nnovel setting with multiway clustering. Extensive numerical simulations\nshowcase that our methods achieve desirable finite-sample behaviors. We apply\nthe proposed methods to analyze the causal relationship between mistrust levels\nin Africa and the historical slave trade. Our analysis rejects the null\nhypothesis of uniformly zero effects and reveals heterogeneous treatment\neffects, with significant impacts at higher levels of trade volumes.",
        "authors": [
            "Nan Liu",
            "Yanbo Liu",
            "Yuya Sasaki"
        ],
        "categories": "econ.EM",
        "published": "2024-09-10T17:17:53Z",
        "updated": "2024-09-10T17:17:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.05798v3",
        "title": "Enhancing Preference-based Linear Bandits via Human Response Time",
        "abstract": "Interactive preference learning systems present humans with queries as pairs\nof options; humans then select their preferred choice, allowing the system to\ninfer preferences from these binary choices. While binary choice feedback is\nsimple and widely used, it offers limited information about preference\nstrength. To address this, we leverage human response times, which inversely\ncorrelate with preference strength, as complementary information. We introduce\na computationally efficient method based on the EZ-diffusion model, combining\nchoices and response times to estimate the underlying human utility function.\nTheoretical and empirical comparisons with traditional choice-only estimators\nshow that for queries where humans have strong preferences (i.e., \"easy\"\nqueries), response times provide valuable complementary information and enhance\nutility estimates. We integrate this estimator into preference-based linear\nbandits for fixed-budget best-arm identification. Simulations on three\nreal-world datasets demonstrate that incorporating response times significantly\naccelerates preference learning.",
        "authors": [
            "Shen Li",
            "Yuyang Zhang",
            "Zhaolin Ren",
            "Claire Liang",
            "Na Li",
            "Julie A. Shah"
        ],
        "categories": "cs.LG",
        "published": "2024-09-09T17:02:47Z",
        "updated": "2024-10-30T10:28:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.05715v1",
        "title": "Uniform Estimation and Inference for Nonparametric Partitioning-Based M-Estimators",
        "abstract": "This paper presents uniform estimation and inference theory for a large class\nof nonparametric partitioning-based M-estimators. The main theoretical results\ninclude: (i) uniform consistency for convex and non-convex objective functions;\n(ii) optimal uniform Bahadur representations; (iii) optimal uniform (and mean\nsquare) convergence rates; (iv) valid strong approximations and feasible\nuniform inference methods; and (v) extensions to functional transformations of\nunderlying estimators. Uniformity is established over both the evaluation point\nof the nonparametric functional parameter and a Euclidean parameter indexing\nthe class of loss functions. The results also account explicitly for the\nsmoothness degree of the loss function (if any), and allow for a possibly\nnon-identity (inverse) link function. We illustrate the main theoretical and\nmethodological results with four substantive applications: quantile regression,\ndistribution regression, $L_p$ regression, and Logistic regression; many other\npossibly non-smooth, nonlinear, generalized, robust M-estimation settings are\ncovered by our theoretical results. We provide detailed comparisons with the\nexisting literature and demonstrate substantive improvements: we achieve the\nbest (in some cases optimal) known results under improved (in some cases\nminimal) requirements in terms of regularity conditions and side rate\nrestrictions. The supplemental appendix reports other technical results that\nmay be of independent interest.",
        "authors": [
            "Matias D. Cattaneo",
            "Yingjie Feng",
            "Boris Shigida"
        ],
        "categories": "math.ST",
        "published": "2024-09-09T15:25:41Z",
        "updated": "2024-09-09T15:25:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.05713v1",
        "title": "The Surprising Robustness of Partial Least Squares",
        "abstract": "Partial least squares (PLS) is a simple factorisation method that works well\nwith high dimensional problems in which the number of observations is limited\ngiven the number of independent variables. In this article, we show that PLS\ncan perform better than ordinary least squares (OLS), least absolute shrinkage\nand selection operator (LASSO) and ridge regression in forecasting quarterly\ngross domestic product (GDP) growth, covering the period from 2000 to 2023. In\nfact, through dimension reduction, PLS proved to be effective in lowering the\nout-of-sample forecasting error, specially since 2020. For the period\n2000-2019, the four methods produce similar results, suggesting that PLS is a\nvalid regularisation technique like LASSO or ridge.",
        "authors": [
            "Jo\u00e3o B. Assun\u00e7\u00e3o",
            "Pedro Afonso Fernandes"
        ],
        "categories": "econ.EM",
        "published": "2024-09-09T15:24:17Z",
        "updated": "2024-09-09T15:24:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.05192v1",
        "title": "Bellwether Trades: Characteristics of Trades influential in Predicting Future Price Movements in Markets",
        "abstract": "In this study, we leverage powerful non-linear machine learning methods to\nidentify the characteristics of trades that contain valuable information.\nFirst, we demonstrate the effectiveness of our optimized neural network\npredictor in accurately predicting future market movements. Then, we utilize\nthe information from this successful neural network predictor to pinpoint the\nindividual trades within each data point (trading window) that had the most\nimpact on the optimized neural network's prediction of future price movements.\nThis approach helps us uncover important insights about the heterogeneity in\ninformation content provided by trades of different sizes, venues, trading\ncontexts, and over time.",
        "authors": [
            "Tejas Ramdas",
            "Martin T. Wells"
        ],
        "categories": "q-fin.TR",
        "published": "2024-09-08T18:59:52Z",
        "updated": "2024-09-08T18:59:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.05184v1",
        "title": "Difference-in-Differences with Multiple Events",
        "abstract": "Confounding events with correlated timing violate the parallel trends\nassumption in Difference-in-Differences (DiD) designs. I show that the standard\nstaggered DiD estimator is biased in the presence of confounding events.\nIdentification can be achieved with units not yet treated by either event as\ncontrols and a double DiD design using variation in treatment timing. I apply\nthis method to examine the effect of states' staggered minimum wage raise on\nteen employment from 2010 to 2020. The Medicaid expansion under the ACA\nconfounded the raises, leading to a spurious negative estimate.",
        "authors": [
            "Lin-Tung Tsai"
        ],
        "categories": "econ.EM",
        "published": "2024-09-08T18:42:12Z",
        "updated": "2024-09-08T18:42:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.04876v1",
        "title": "DEPLOYERS: An agent based modeling tool for multi country real world data",
        "abstract": "We present recent progress in the design and development of DEPLOYERS, an\nagent-based macroeconomics modeling (ABM) framework, capable to deploy and\nsimulate a full economic system (individual workers, goods and services firms,\ngovernment, central and private banks, financial market, external sectors)\nwhose structure and activity analysis reproduce the desired calibration data,\nthat can be, for example a Social Accounting Matrix (SAM) or a Supply-Use Table\n(SUT) or an Input-Output Table (IOT).Here we extend our previous work to a\nmulti-country version and show an example using data from a 46-countries\n64-sectors FIGARO Inter-Country IOT. The simulation of each country runs on a\nseparate thread or CPU core to simulate the activity of one step (month, week,\nor day) and then interacts (updates imports, exports, transfer) with that\ncountry's foreign partners, and proceeds to the next step. This interaction can\nbe chosen to be aggregated (a single row and column IO account) or\ndisaggregated (64 rows and columns) with each partner. A typical run simulates\nthousands of individuals and firms engaged in their monthly activity and then\nrecords the results, much like a survey of the country's economic system. This\ndata can then be subjected to, for example, an Input-Output analysis to find\nout the sources of observed stylized effects as a function of time in the\ndetailed and realistic modeling environment that can be easily implemented in\nan ABM framework.",
        "authors": [
            "Martin Jaraiz",
            "Ruth Pinacho"
        ],
        "categories": "econ.EM",
        "published": "2024-09-07T17:53:20Z",
        "updated": "2024-09-07T17:53:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.04874v1",
        "title": "Improving the Finite Sample Performance of Double/Debiased Machine Learning with Propensity Score Calibration",
        "abstract": "Machine learning techniques are widely used for estimating causal effects.\nDouble/debiased machine learning (DML) (Chernozhukov et al., 2018) uses a\ndouble-robust score function that relies on the prediction of nuisance\nfunctions, such as the propensity score, which is the probability of treatment\nassignment conditional on covariates. Estimators relying on double-robust score\nfunctions are highly sensitive to errors in propensity score predictions.\nMachine learners increase the severity of this problem as they tend to over- or\nunderestimate these probabilities. Several calibration approaches have been\nproposed to improve probabilistic forecasts of machine learners. This paper\ninvestigates the use of probability calibration approaches within the DML\nframework. Simulation results demonstrate that calibrating propensity scores\nmay significantly reduces the root mean squared error of DML estimates of the\naverage treatment effect in finite samples. We showcase it in an empirical\nexample and provide conditions under which calibration does not alter the\nasymptotic properties of the DML estimator.",
        "authors": [
            "Daniele Ballinari",
            "Nora Bearth"
        ],
        "categories": "econ.EM",
        "published": "2024-09-07T17:44:01Z",
        "updated": "2024-09-07T17:44:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.04589v1",
        "title": "Lee Bounds with Multilayered Sample Selection",
        "abstract": "This paper investigates the causal effect of job training on wage rates in\nthe presence of firm heterogeneity. When training affects worker sorting to\nfirms, sample selection is no longer binary but is \"multilayered\". This paper\nextends the canonical Heckman (1979) sample selection model - which assumes\nselection is binary - to a setting where it is multilayered, and shows that in\nthis setting Lee bounds set identifies a total effect that combines a\nweighted-average of the causal effect of job training on wage rates across\nfirms with a weighted-average of the contrast in wages between different firms\nfor a fixed level of training. Thus, Lee bounds set identifies a\npolicy-relevant estimand only when firms pay homogeneous wages and/or when job\ntraining does not affect worker sorting across firms. We derive sharp\nclosed-form bounds for the causal effect of job training on wage rates at each\nfirm which leverage information on firm-specific wages. We illustrate our\npartial identification approach with an empirical application to the Job Corps\nStudy. Results show that while conventional Lee bounds are strictly positive,\nour within-firm bounds include 0 showing that canonical Lee bounds may be\ncapturing a pure sorting effect of job training.",
        "authors": [
            "Kory Kroft",
            "Ismael Mourifi\u00e9",
            "Atom Vayalinkal"
        ],
        "categories": "econ.EM",
        "published": "2024-09-06T19:59:23Z",
        "updated": "2024-09-06T19:59:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.04378v1",
        "title": "An MPEC Estimator for the Sequential Search Model",
        "abstract": "This paper proposes a constrained maximum likelihood estimator for sequential\nsearch models, using the MPEC (Mathematical Programming with Equilibrium\nConstraints) approach. This method enhances numerical accuracy while avoiding\nad hoc components and errors related to equilibrium conditions. Monte Carlo\nsimulations show that the estimator performs better in small samples, with\nlower bias and root-mean-squared error, though less effectively in large\nsamples. Despite these mixed results, the MPEC approach remains valuable for\nidentifying candidate parameters comparable to the benchmark, without relying\non ad hoc look-up tables, as it generates the table through solved equilibrium\nconstraints.",
        "authors": [
            "Shinji Koiso",
            "Suguru Otani"
        ],
        "categories": "econ.EM",
        "published": "2024-09-06T16:17:06Z",
        "updated": "2024-09-06T16:17:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.03979v1",
        "title": "Extreme Quantile Treatment Effects under Endogeneity: Evaluating Policy Effects for the Most Vulnerable Individuals",
        "abstract": "We introduce a novel method for estimating and conducting inference about\nextreme quantile treatment effects (QTEs) in the presence of endogeneity. Our\napproach is applicable to a broad range of empirical research designs,\nincluding instrumental variables design and regression discontinuity design,\namong others. By leveraging regular variation and subsampling, the method\nensures robust performance even in extreme tails, where data may be sparse or\nentirely absent. Simulation studies confirm the theoretical robustness of our\napproach. Applying our method to assess the impact of job training provided by\nthe Job Training Partnership Act (JTPA), we find significantly negative QTEs\nfor the lowest quantiles (i.e., the most disadvantaged individuals),\ncontrasting with previous literature that emphasizes positive QTEs for\nintermediate quantiles.",
        "authors": [
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-09-06T01:59:29Z",
        "updated": "2024-09-06T01:59:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.03606v2",
        "title": "Performance of Empirical Risk Minimization For Principal Component Regression",
        "abstract": "This paper establishes bounds on the predictive performance of empirical risk\nminimization for principal component regression. Our analysis is nonparametric,\nin the sense that the relation between the prediction target and the predictors\nis not specified. In particular, we do not rely on the assumption that the\nprediction target is generated by a factor model. In our analysis we consider\nthe cases in which the largest eigenvalues of the covariance matrix of the\npredictors grow linearly in the number of predictors (strong signal regime) or\nsublinearly (weak signal regime). The main result of this paper shows that\nempirical risk minimization for principal component regression is consistent\nfor prediction and, under appropriate conditions, it achieves near-optimal\nperformance in both the strong and weak signal regimes.",
        "authors": [
            "Christian Brownlees",
            "Gu\u00f0mundur Stef\u00e1n Gu\u00f0mundsson",
            "Yaping Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-09-05T15:12:35Z",
        "updated": "2024-09-17T14:04:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.09065v1",
        "title": "Automatic Pricing and Replenishment Strategies for Vegetable Products Based on Data Analysis and Nonlinear Programming",
        "abstract": "In the field of fresh produce retail, vegetables generally have a relatively\nlimited shelf life, and their quality deteriorates with time. Most vegetable\nvarieties, if not sold on the day of delivery, become difficult to sell the\nfollowing day. Therefore, retailers usually perform daily quantitative\nreplenishment based on historical sales data and demand conditions. Vegetable\npricing typically uses a \"cost-plus pricing\" method, with retailers often\ndiscounting products affected by transportation loss and quality decline. In\nthis context, reliable market demand analysis is crucial as it directly impacts\nreplenishment and pricing decisions. Given the limited retail space, a rational\nsales mix becomes essential. This paper first uses data analysis and\nvisualization techniques to examine the distribution patterns and\ninterrelationships of vegetable sales quantities by category and individual\nitem, based on provided data on vegetable types, sales records, wholesale\nprices, and recent loss rates. Next, it constructs a functional relationship\nbetween total sales volume and cost-plus pricing for vegetable categories,\nforecasts future wholesale prices using the ARIMA model, and establishes a\nsales profit function and constraints. A nonlinear programming model is then\ndeveloped and solved to provide daily replenishment quantities and pricing\nstrategies for each vegetable category for the upcoming week. Further, we\noptimize the profit function and constraints based on the actual sales\nconditions and requirements, providing replenishment quantities and pricing\nstrategies for individual items on July 1 to maximize retail profit. Finally,\nto better formulate replenishment and pricing decisions for vegetable products,\nwe discuss and forecast the data that retailers need to collect and analyses\nhow the collected data can be applied to the above issues.",
        "authors": [
            "Mingpu Ma"
        ],
        "categories": "econ.EM",
        "published": "2024-09-05T03:16:23Z",
        "updated": "2024-09-05T03:16:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.02872v1",
        "title": "Momentum Dynamics in Competitive Sports: A Multi-Model Analysis Using TOPSIS and Logistic Regression",
        "abstract": "This paper explores the concept of \"momentum\" in sports competitions through\nthe use of the TOPSIS model and 0-1 logistic regression model. First, the\nTOPSIS model is employed to evaluate the performance of two tennis players,\nwith visualizations used to analyze the situation's evolution at every moment\nin the match, explaining how \"momentum\" manifests in sports. Then, the 0-1\nlogistic regression model is utilized to verify the impact of \"momentum\" on\nmatch outcomes, demonstrating that fluctuations in player performance and the\nsuccessive occurrence of successes are not random. Additionally, this paper\nexamines the indicators that influence the reversal of game situations by\nanalyzing key match data and testing the accuracy of the models with match\ndata. The findings show that the model accurately explains the conditions\nduring matches and can be generalized to other sports competitions. Finally,\nthe strengths, weaknesses, and potential future improvements of the model are\ndiscussed.",
        "authors": [
            "Mingpu Ma"
        ],
        "categories": "econ.EM",
        "published": "2024-09-04T16:54:56Z",
        "updated": "2024-09-04T16:54:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.02662v1",
        "title": "The Impact of Data Elements on Narrowing the Urban-Rural Consumption Gap in China: Mechanisms and Policy Analysis",
        "abstract": "The urban-rural consumption gap, as one of the important indicators in social\ndevelopment, directly reflects the imbalance in urban and rural economic and\nsocial development. Data elements, as an important component of New Quality\nProductivity, are of significant importance in promoting economic development\nand improving people's living standards in the information age. This study,\nthrough the analysis of fixed-effects regression models, system GMM regression\nmodels, and the intermediate effect model, found that the development level of\ndata elements to some extent promotes the narrowing of the urban-rural\nconsumption gap. At the same time, the intermediate variable of urban-rural\nincome gap plays an important role between data elements and consumption gap,\nwith a significant intermediate effect. The results of the study indicate that\nthe advancement of data elements can promote the balance of urban and rural\nresidents' consumption levels by reducing the urban-rural income gap, providing\ntheoretical support and policy recommendations for achieving common prosperity\nand promoting coordinated urban-rural development. Building upon this, this\npaper emphasizes the complex correlation between the development of data\nelements and the urban-rural consumption gap, and puts forward policy\nsuggestions such as promoting the development of the data element market,\nstrengthening the construction of the digital economy and e-commerce, and\npromoting integrated urban-rural development. Overall, the development of data\nelements is not only an important path to reducing the urban-rural consumption\ngap but also one of the key drivers for promoting the balanced development of\nChina's economic and social development. This study has a certain theoretical\nand practical significance for understanding the mechanism of the urban-rural\nconsumption gap and improving policies for urban-rural economic development.",
        "authors": [
            "Mingpu Ma"
        ],
        "categories": "econ.EM",
        "published": "2024-09-04T12:42:41Z",
        "updated": "2024-09-04T12:42:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.02642v1",
        "title": "The Application of Green GDP and Its Impact on Global Economy and Environment: Analysis of GGDP based on SEEA model",
        "abstract": "This paper presents an analysis of Green Gross Domestic Product (GGDP) using\nthe System of Environmental-Economic Accounting (SEEA) model to evaluate its\nimpact on global climate mitigation and economic health. GGDP is proposed as a\nsuperior measure to tradi-tional GDP by incorporating natural resource\nconsumption, environmental pollution control, and degradation factors. The\nstudy develops a GGDP model and employs grey correlation analysis and grey\nprediction models to assess its relationship with these factors. Key findings\ndemonstrate that replacing GDP with GGDP can positively influence climate\nchange, partic-ularly in reducing CO2 emissions and stabilizing global\ntemperatures. The analysis further explores the implications of GGDP adoption\nacross developed and developing countries, with specific predictions for China\nand the United States. The results indicate a potential increase in economic\nlevels for developing countries, while developed nations may experi-ence a\ndecrease. Additionally, the shift to GGDP is shown to significantly reduce\nnatural re-source depletion and population growth rates in the United States,\nsuggesting broader envi-ronmental and economic benefits. This paper highlights\nthe universal applicability of the GGDP model and its potential to enhance\nenvironmental and economic policies globally.",
        "authors": [
            "Mingpu Ma"
        ],
        "categories": "econ.EM",
        "published": "2024-09-04T12:19:11Z",
        "updated": "2024-09-04T12:19:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.02573v1",
        "title": "Fitting an Equation to Data Impartially",
        "abstract": "We consider the problem of fitting a relationship (e.g. a potential\nscientific law) to data involving multiple variables. Ordinary (least squares)\nregression is not suitable for this because the estimated relationship will\ndiffer according to which variable is chosen as being dependent, and the\ndependent variable is unrealistically assumed to be the only variable which has\nany measurement error (noise). We present a very general method for estimating\na linear functional relationship between multiple noisy variables, which are\ntreated impartially, i.e. no distinction between dependent and independent\nvariables. The data are not assumed to follow any distribution, but all\nvariables are treated as being equally reliable. Our approach extends the\ngeometric mean functional relationship to multiple dimensions. This is\nespecially useful with variables measured in different units, as it is\nnaturally scale-invariant, whereas orthogonal regression is not. This is\nbecause our approach is not based on minimizing distances, but on the symmetric\nconcept of correlation. The estimated coefficients are easily obtained from the\ncovariances or correlations, and correspond to geometric means of associated\nleast squares coefficients. The ease of calculation will hopefully allow\nwidespread application of impartial fitting to estimate relationships in a\nneutral way.",
        "authors": [
            "Chris Tofallis"
        ],
        "categories": "stat.ME",
        "published": "2024-09-04T09:48:26Z",
        "updated": "2024-09-04T09:48:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.02332v1",
        "title": "Double Machine Learning at Scale to Predict Causal Impact of Customer Actions",
        "abstract": "Causal Impact (CI) of customer actions are broadly used across the industry\nto inform both short- and long-term investment decisions of various types. In\nthis paper, we apply the double machine learning (DML) methodology to estimate\nthe CI values across 100s of customer actions of business interest and 100s of\nmillions of customers. We operationalize DML through a causal ML library based\non Spark with a flexible, JSON-driven model configuration approach to estimate\nCI at scale (i.e., across hundred of actions and millions of customers). We\noutline the DML methodology and implementation, and associated benefits over\nthe traditional potential outcomes based CI model. We show population-level as\nwell as customer-level CI values along with confidence intervals. The\nvalidation metrics show a 2.2% gain over the baseline methods and a 2.5X gain\nin the computational time. Our contribution is to advance the scalable\napplication of CI, while also providing an interface that allows faster\nexperimentation, cross-platform support, ability to onboard new use cases, and\nimproves accessibility of underlying code for partner teams.",
        "authors": [
            "Sushant More",
            "Priya Kotwal",
            "Sujith Chappidi",
            "Dinesh Mandalapu",
            "Chris Khawand"
        ],
        "categories": "cs.LG",
        "published": "2024-09-03T23:13:04Z",
        "updated": "2024-09-03T23:13:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.02311v1",
        "title": "Distribution Regression Difference-In-Differences",
        "abstract": "We provide a simple distribution regression estimator for treatment effects\nin the difference-in-differences (DiD) design. Our procedure is particularly\nuseful when the treatment effect differs across the distribution of the outcome\nvariable. Our proposed estimator easily incorporates covariates and,\nimportantly, can be extended to settings where the treatment potentially\naffects the joint distribution of multiple outcomes. Our key identifying\nrestriction is that the counterfactual distribution of the treated in the\nuntreated state has no interaction effect between treatment and time. This\nassumption results in a parallel trend assumption on a transformation of the\ndistribution. We highlight the relationship between our procedure and\nassumptions with the changes-in-changes approach of Athey and Imbens (2006). We\nalso reexamine two existing empirical examples which highlight the utility of\nour approach.",
        "authors": [
            "Iv\u00e1n Fern\u00e1ndez-Val",
            "Jonas Meier",
            "Aico van Vuuren",
            "Francis Vella"
        ],
        "categories": "econ.EM",
        "published": "2024-09-03T21:47:48Z",
        "updated": "2024-09-03T21:47:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.01911v2",
        "title": "Variable selection in convex nonparametric least squares via structured Lasso: An application to the Swedish electricity distribution networks",
        "abstract": "We study the problem of variable selection in convex nonparametric least\nsquares (CNLS). Whereas the least absolute shrinkage and selection operator\n(Lasso) is a popular technique for least squares, its variable selection\nperformance is unknown in CNLS problems. In this work, we investigate the\nperformance of the Lasso estimator and find out it is usually unable to select\nvariables efficiently. Exploiting the unique structure of the subgradients in\nCNLS, we develop a structured Lasso method by combining $\\ell_1$-norm and\n$\\ell_{\\infty}$-norm. The relaxed version of the structured Lasso is proposed\nfor achieving model sparsity and predictive performance simultaneously, where\nwe can control the two effects--variable selection and model shrinkage--using\nseparate tuning parameters. A Monte Carlo study is implemented to verify the\nfinite sample performance of the proposed approaches. We also use real data\nfrom Swedish electricity distribution networks to illustrate the effects of the\nproposed variable selection techniques. The results from the simulation and\napplication confirm that the proposed structured Lasso performs favorably,\ngenerally leading to sparser and more accurate predictive models, relative to\nthe conventional Lasso methods in the literature.",
        "authors": [
            "Zhiqiang Liao"
        ],
        "categories": "stat.ME",
        "published": "2024-09-03T14:01:32Z",
        "updated": "2024-11-13T15:25:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.01266v1",
        "title": "Double Machine Learning meets Panel Data -- Promises, Pitfalls, and Potential Solutions",
        "abstract": "Estimating causal effect using machine learning (ML) algorithms can help to\nrelax functional form assumptions if used within appropriate frameworks.\nHowever, most of these frameworks assume settings with cross-sectional data,\nwhereas researchers often have access to panel data, which in traditional\nmethods helps to deal with unobserved heterogeneity between units. In this\npaper, we explore how we can adapt double/debiased machine learning (DML)\n(Chernozhukov et al., 2018) for panel data in the presence of unobserved\nheterogeneity. This adaptation is challenging because DML's cross-fitting\nprocedure assumes independent data and the unobserved heterogeneity is not\nnecessarily additively separable in settings with nonlinear observed\nconfounding. We assess the performance of several intuitively appealing\nestimators in a variety of simulations. While we find violations of the\ncross-fitting assumptions to be largely inconsequential for the accuracy of the\neffect estimates, many of the considered methods fail to adequately account for\nthe presence of unobserved heterogeneity. However, we find that using\npredictive models based on the correlated random effects approach (Mundlak,\n1978) within DML leads to accurate coefficient estimates across settings, given\na sample size that is large relative to the number of observed confounders. We\nalso show that the influence of the unobserved heterogeneity on the observed\nconfounders plays a significant role for the performance of most alternative\nmethods.",
        "authors": [
            "Jonathan Fuhr",
            "Dominik Papies"
        ],
        "categories": "econ.EM",
        "published": "2024-09-02T13:59:54Z",
        "updated": "2024-09-02T13:59:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2409.00379v1",
        "title": "Bandit Algorithms for Policy Learning: Methods, Implementation, and Welfare-performance",
        "abstract": "Static supervised learning-in which experimental data serves as a training\nsample for the estimation of an optimal treatment assignment policy-is a\ncommonly assumed framework of policy learning. An arguably more realistic but\nchallenging scenario is a dynamic setting in which the planner performs\nexperimentation and exploitation simultaneously with subjects that arrive\nsequentially. This paper studies bandit algorithms for learning an optimal\nindividualised treatment assignment policy. Specifically, we study\napplicability of the EXP4.P (Exponential weighting for Exploration and\nExploitation with Experts) algorithm developed by Beygelzimer et al. (2011) to\npolicy learning. Assuming that the class of policies has a finite\nVapnik-Chervonenkis dimension and that the number of subjects to be allocated\nis known, we present a high probability welfare-regret bound of the algorithm.\nTo implement the algorithm, we use an incremental enumeration algorithm for\nhyperplane arrangements. We perform extensive numerical analysis to assess the\nalgorithm's sensitivity to its tuning parameters and its welfare-regret\nperformance. Further simulation exercises are calibrated to the National Job\nTraining Partnership Act (JTPA) Study sample to determine how the algorithm\nperforms when applied to economic data. Our findings highlight various\ncomputational challenges and suggest that the limited welfare gain from the\nalgorithm is due to substantial heterogeneity in causal effects in the JTPA\ndata.",
        "authors": [
            "Toru Kitagawa",
            "Jeff Rowley"
        ],
        "categories": "econ.EM",
        "published": "2024-08-31T08:11:53Z",
        "updated": "2024-08-31T08:11:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.17426v3",
        "title": "Weighted Regression with Sybil Networks",
        "abstract": "In many online domains, Sybil networks -- or cases where a single user\nassumes multiple identities -- is a pervasive feature. This complicates\nexperiments, as off-the-shelf regression estimators at least assume known\nnetwork topologies (if not fully independent observations) when Sybil network\ntopologies in practice are often unknown. The literature has exclusively\nfocused on techniques to detect Sybil networks, leading many experimenters to\nsubsequently exclude suspected networks entirely before estimating treatment\neffects. I present a more efficient solution in the presence of these suspected\nSybil networks: a weighted regression framework that applies weights based on\nthe probabilities that sets of observations are controlled by single actors. I\nshow in the paper that the MSE-minimizing solution is to set the weight matrix\nequal to the inverse of the expected network topology. I demonstrate the\nmethodology on simulated data, and then I apply the technique to a competition\nwith suspected Sybil networks run on the Sui blockchain and show reductions in\nthe standard error of the estimate by 6 - 24%.",
        "authors": [
            "Nihar Shah"
        ],
        "categories": "stat.ME",
        "published": "2024-08-30T17:16:48Z",
        "updated": "2024-09-06T17:51:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.17187v1",
        "title": "State Space Model of Realized Volatility under the Existence of Dependent Market Microstructure Noise",
        "abstract": "Volatility means the degree of variation of a stock price which is important\nin finance. Realized Volatility (RV) is an estimator of the volatility\ncalculated using high-frequency observed prices. RV has lately attracted\nconsiderable attention of econometrics and mathematical finance. However, it is\nknown that high-frequency data includes observation errors called market\nmicrostructure noise (MN). Nagakura and Watanabe[2015] proposed a state space\nmodel that resolves RV into true volatility and influence of MN. In this paper,\nwe assume a dependent MN that autocorrelates and correlates with return as\nreported by Hansen and Lunde[2006] and extends the results of Nagakura and\nWatanabe[2015] and compare models by simulation and actual data.",
        "authors": [
            "Toru Yano"
        ],
        "categories": "econ.EM",
        "published": "2024-08-30T10:39:05Z",
        "updated": "2024-08-30T10:39:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.16330v1",
        "title": "Sensitivity Analysis for Dynamic Discrete Choice Models",
        "abstract": "In dynamic discrete choice models, some parameters, such as the discount\nfactor, are being fixed instead of being estimated. This paper proposes two\nsensitivity analysis procedures for dynamic discrete choice models with respect\nto the fixed parameters. First, I develop a local sensitivity measure that\nestimates the change in the target parameter for a unit change in the fixed\nparameter. This measure is fast to compute as it does not require model\nre-estimation. Second, I propose a global sensitivity analysis procedure that\nuses model primitives to study the relationship between target parameters and\nfixed parameters. I show how to apply the sensitivity analysis procedures of\nthis paper through two empirical applications.",
        "authors": [
            "Chun Pong Lau"
        ],
        "categories": "econ.EM",
        "published": "2024-08-29T08:08:31Z",
        "updated": "2024-08-29T08:08:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.15862v1",
        "title": "Marginal homogeneity tests with panel data",
        "abstract": "A panel dataset satisfies marginal homogeneity if the time-specific marginal\ndistributions are homogeneous or time-invariant. Marginal homogeneity is\nrelevant in economic settings such as dynamic discrete games. In this paper, we\npropose several tests for the hypothesis of marginal homogeneity and\ninvestigate their properties. We consider an asymptotic framework in which the\nnumber of individuals n in the panel diverges, and the number of periods T is\nfixed. We implement our tests by comparing a studentized or non-studentized\nT-sample version of the Cramer-von Mises statistic with a suitable critical\nvalue. We propose three methods to construct the critical value: asymptotic\napproximations, the bootstrap, and time permutations. We show that the first\ntwo methods result in asymptotically exact hypothesis tests. The permutation\ntest based on a non-studentized statistic is asymptotically exact when T=2, but\nis asymptotically invalid when T>2. In contrast, the permutation test based on\na studentized statistic is always asymptotically exact. Finally, under a\ntime-exchangeability assumption, the permutation test is exact in finite\nsamples, both with and without studentization.",
        "authors": [
            "Federico Bugni",
            "Jackson Bunting",
            "Muyang Ren"
        ],
        "categories": "econ.EM",
        "published": "2024-08-28T15:21:25Z",
        "updated": "2024-08-28T15:21:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.15454v1",
        "title": "BayesSRW: Bayesian Sampling and Re-weighting approach for variance reduction",
        "abstract": "In this paper, we address the challenge of sampling in scenarios where\nlimited resources prevent exhaustive measurement across all subjects. We\nconsider a setting where samples are drawn from multiple groups, each following\na distribution with unknown mean and variance parameters. We introduce a novel\nsampling strategy, motivated simply by Cauchy-Schwarz inequality, which\nminimizes the variance of the population mean estimator by allocating samples\nproportionally to both the group size and the standard deviation. This approach\nimproves the efficiency of sampling by focusing resources on groups with\ngreater variability, thereby enhancing the precision of the overall estimate.\nAdditionally, we extend our method to a two-stage sampling procedure in a Bayes\napproach, named BayesSRW, where a preliminary stage is used to estimate the\nvariance, which then informs the optimal allocation of the remaining sampling\nbudget. Through simulation examples, we demonstrate the effectiveness of our\napproach in reducing estimation uncertainty and providing more reliable\ninsights in applications ranging from user experience surveys to\nhigh-dimensional peptide array studies.",
        "authors": [
            "Carol Liu"
        ],
        "categories": "econ.EM",
        "published": "2024-08-28T00:28:31Z",
        "updated": "2024-08-28T00:28:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.15452v1",
        "title": "The effects of data preprocessing on probability of default model fairness",
        "abstract": "In the context of financial credit risk evaluation, the fairness of machine\nlearning models has become a critical concern, especially given the potential\nfor biased predictions that disproportionately affect certain demographic\ngroups. This study investigates the impact of data preprocessing, with a\nspecific focus on Truncated Singular Value Decomposition (SVD), on the fairness\nand performance of probability of default models. Using a comprehensive dataset\nsourced from Kaggle, various preprocessing techniques, including SVD, were\napplied to assess their effect on model accuracy, discriminatory power, and\nfairness.",
        "authors": [
            "Di Wu"
        ],
        "categories": "econ.EM",
        "published": "2024-08-28T00:20:49Z",
        "updated": "2024-08-28T00:20:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.14671v1",
        "title": "Double/Debiased CoCoLASSO of Treatment Effects with Mismeasured High-Dimensional Control Variables",
        "abstract": "We develop an estimator for treatment effects in high-dimensional settings\nwith additive measurement error, a prevalent challenge in modern econometrics.\nWe introduce the Double/Debiased Convex Conditioned LASSO (Double/Debiased\nCoCoLASSO), which extends the double/debiased machine learning framework to\naccommodate mismeasured covariates. Our principal contributions are threefold.\n(1) We construct a Neyman-orthogonal score function that remains valid under\nmeasurement error, incorporating a bias correction term to account for\nerror-induced correlations. (2) We propose a method of moments estimator for\nthe measurement error variance, enabling implementation without prior knowledge\nof the error covariance structure. (3) We establish the $\\sqrt{N}$-consistency\nand asymptotic normality of our estimator under general conditions, allowing\nfor both the number of covariates and the magnitude of measurement error to\nincrease with the sample size. Our theoretical results demonstrate the\nestimator's efficiency within the class of regularized high-dimensional\nestimators accounting for measurement error. Monte Carlo simulations\ncorroborate our asymptotic theory and illustrate the estimator's robust\nperformance across various levels of measurement error. Notably, our\ncovariance-oblivious approach nearly matches the efficiency of methods that\nassume known error variance.",
        "authors": [
            "Geonwoo Kim",
            "Suyong Song"
        ],
        "categories": "econ.EM",
        "published": "2024-08-26T22:28:23Z",
        "updated": "2024-08-26T22:28:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.14214v2",
        "title": "Modeling the Dynamics of Growth in Master-Planned Communities",
        "abstract": "This paper describes how a time-varying Markov model was used to forecast\nhousing development at a master-planned community during a transition from high\nto low growth. Our approach draws on detailed historical data to model the\ndynamics of the market participants, producing results that are entirely\ndata-driven and free of bias. While traditional time series forecasting methods\noften struggle to account for nonlinear regime changes in growth, our approach\nsuccessfully captures the onset of buildout as well as external economic\nshocks, such as the 1990 and 2008-2011 recessions and the 2021 post-pandemic\nboom.\n  This research serves as a valuable tool for urban planners, homeowner\nassociations, and property stakeholders aiming to navigate the complexities of\ngrowth at master-planned communities during periods of both system stability\nand instability.",
        "authors": [
            "Christopher K. Allsup",
            "Irene S. Gabashvili"
        ],
        "categories": "econ.EM",
        "published": "2024-08-26T12:14:26Z",
        "updated": "2024-08-29T15:30:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.13971v1",
        "title": "Endogenous Treatment Models with Social Interactions: An Application to the Impact of Exercise on Self-Esteem",
        "abstract": "We address the estimation of endogenous treatment models with social\ninteractions in both the treatment and outcome equations. We model the\ninteractions between individuals in an internally consistent manner via a game\ntheoretic approach based on discrete Bayesian games. This introduces a\nsubstantial computational burden in estimation which we address through a\nsequential version of the nested fixed point algorithm. We also provide some\nrelevant treatment effects, and procedures for their estimation, which capture\nthe impact on both the individual and the total sample. Our empirical\napplication examines the impact of an individual's exercise frequency on her\nlevel of self-esteem. We find that an individual's exercise frequency is\ninfluenced by her expectation of her friends'. We also find that an\nindividual's level of self-esteem is affected by her level of exercise and, at\nrelatively lower levels of self-esteem, by the expectation of her friends'\nself-esteem.",
        "authors": [
            "Zhongjian Lin",
            "Francis Vella"
        ],
        "categories": "econ.EM",
        "published": "2024-08-26T01:36:42Z",
        "updated": "2024-08-26T01:36:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.13949v1",
        "title": "Inference on Consensus Ranking of Distributions",
        "abstract": "Instead of testing for unanimous agreement, I propose learning how broad of a\nconsensus favors one distribution over another (of earnings, productivity,\nasset returns, test scores, etc.). Specifically, given a sample from each of\ntwo distributions, I propose statistical inference methods to learn about the\nset of utility functions for which the first distribution has higher expected\nutility than the second distribution. With high probability, an \"inner\"\nconfidence set is contained within this true set, while an \"outer\" confidence\nset contains the true set. Such confidence sets can be formed by inverting a\nproposed multiple testing procedure that controls the familywise error rate.\nTheoretical justification comes from empirical process results, given that very\nlarge classes of utility functions are generally Donsker (subject to finite\nmoments). The theory additionally justifies a uniform (over utility functions)\nconfidence band of expected utility differences, as well as tests with a\nutility-based \"restricted stochastic dominance\" as either the null or\nalternative hypothesis. Simulated and empirical examples illustrate the\nmethodology.",
        "authors": [
            "David M. Kaplan"
        ],
        "categories": "econ.EM",
        "published": "2024-08-25T22:07:06Z",
        "updated": "2024-08-25T22:07:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.13437v1",
        "title": "Cross-sectional Dependence in Idiosyncratic Volatility",
        "abstract": "This paper introduces an econometric framework for analyzing cross-sectional\ndependence in the idiosyncratic volatilities of assets using high frequency\ndata. We first consider the estimation of standard measures of dependence in\nthe idiosyncratic volatilities such as covariances and correlations. Naive\nestimators of these measures are biased due to the use of the error-laden\nestimates of idiosyncratic volatilities. We provide bias-corrected estimators\nand the relevant asymptotic theory. Next, we introduce an idiosyncratic\nvolatility factor model, in which we decompose the variation in idiosyncratic\nvolatilities into two parts: the variation related to the systematic factors\nsuch as the market volatility, and the residual variation. Again, naive\nestimators of the decomposition are biased, and we provide bias-corrected\nestimators. We also provide the asymptotic theory that allows us to test\nwhether the residual (non-systematic) components of the idiosyncratic\nvolatilities exhibit cross-sectional dependence. We apply our methodology to\nthe S&P 100 index constituents, and document strong cross-sectional dependence\nin their idiosyncratic volatilities. We consider two different sets of\nidiosyncratic volatility factors, and find that neither can fully account for\nthe cross-sectional dependence in idiosyncratic volatilities. For each model,\nwe map out the network of dependencies in residual (non-systematic)\nidiosyncratic volatilities across all stocks.",
        "authors": [
            "Ilze Kalnina",
            "Kokouvi Tewou"
        ],
        "categories": "econ.EM",
        "published": "2024-08-24T02:22:56Z",
        "updated": "2024-08-24T02:22:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.13047v2",
        "title": "Difference-in-differences with as few as two cross-sectional units -- A new perspective to the democracy-growth debate",
        "abstract": "Pooled panel analyses often mask heterogeneity in unit-specific treatment\neffects. This challenge, for example, crops up in studies of the impact of\ndemocracy on economic growth, where findings vary substantially due to\ndifferences in country composition. To address this challenge, this paper\nintroduces a Difference-in-Differences (DiD) estimator that leverages the\ntemporal dimension of the data to estimate unit-specific average treatment\neffects on the treated (ATT) with as few as two cross-sectional units. Under\nweak identification and temporal dependence conditions, the proposed DiD\nestimator is shown to be asymptotically normal. The method is further\ncomplemented with an identification test that, unlike pre-trends tests, is more\npowerful and can detect violations of parallel trends in the post-treatment\nperiod. Empirical results using the DiD estimator suggest Benin's economy would\nhave been 6.3% smaller on average over the 1993-2018 period had she not\ndemocratised.",
        "authors": [
            "Gilles Koumou",
            "Emmanuel Selorm Tsyawo"
        ],
        "categories": "econ.EM",
        "published": "2024-08-23T13:10:43Z",
        "updated": "2024-09-04T20:37:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.12863v1",
        "title": "Machine Learning and the Yield Curve: Tree-Based Macroeconomic Regime Switching",
        "abstract": "We explore tree-based macroeconomic regime-switching in the context of the\ndynamic Nelson-Siegel (DNS) yield-curve model. In particular, we customize the\ntree-growing algorithm to partition macroeconomic variables based on the DNS\nmodel's marginal likelihood, thereby identifying regime-shifting patterns in\nthe yield curve. Compared to traditional Markov-switching models, our model\noffers clear economic interpretation via macroeconomic linkages and ensures\ncomputational simplicity. In an empirical application to U.S. Treasury bond\nyields, we find (1) important yield curve regime switching, and (2) evidence\nthat macroeconomic variables have predictive power for the yield curve when the\nshort rate is high, but not in other regimes, thereby refining the notion of\nyield curve ``macro-spanning\".",
        "authors": [
            "Siyu Bie",
            "Francis X. Diebold",
            "Jingyu He",
            "Junye Li"
        ],
        "categories": "econ.EM",
        "published": "2024-08-23T06:25:04Z",
        "updated": "2024-08-23T06:25:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.12577v1",
        "title": "Integrating an agent-based behavioral model in microtransit forecasting and revenue management",
        "abstract": "As an IT-enabled multi-passenger mobility service, microtransit has the\npotential to improve accessibility, reduce congestion, and enhance flexibility\nin transportation options. However, due to its heterogeneous impacts on\ndifferent communities and population segments, there is a need for better tools\nin microtransit forecast and revenue management, especially when actual usage\ndata are limited. We propose a novel framework based on an agent-based mixed\nlogit model estimated with microtransit usage data and synthetic trip data. The\nframework involves estimating a lower-branch mode choice model with synthetic\ntrip data, combining lower-branch parameters with microtransit data to estimate\nan upper-branch ride pass subscription model, and applying the nested model to\nevaluate microtransit pricing and subsidy policies. The framework enables\nfurther decision-support analysis to consider diverse travel patterns and\nheterogeneous tastes of the total population. We test the framework in a case\nstudy with synthetic trip data from Replica Inc. and microtransit data from\nArlington Via. The lower-branch model result in a rho-square value of 0.603 on\nweekdays and 0.576 on weekends. Predictions made by the upper-branch model\nclosely match the marginal subscription data. In a ride pass pricing policy\nscenario, we show that a discount in weekly pass (from $25 to $18.9) and\nmonthly pass (from $80 to $71.5) would surprisingly increase total revenue by\n$102/day. In an event- or place-based subsidy policy scenario, we show that a\n100% fare discount would reduce 80 car trips during peak hours at AT&T Stadium,\nrequiring a subsidy of $32,068/year.",
        "authors": [
            "Xiyuan Ren",
            "Joseph Y. J. Chow",
            "Venktesh Pandey",
            "Linfei Yuan"
        ],
        "categories": "econ.EM",
        "published": "2024-08-22T17:43:04Z",
        "updated": "2024-08-22T17:43:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.12286v1",
        "title": "Momentum Informed Inflation-at-Risk",
        "abstract": "Growth-at-Risk has recently become a key measure of macroeconomic tail-risk,\nwhich has seen it be researched extensively. Surprisingly, the same cannot be\nsaid for Inflation-at-Risk where both tails, deflation and high inflation, are\nof key concern to policymakers, which has seen comparatively much less\nresearch. This paper will tackle this gap and provide estimates for\nInflation-at-Risk. The key insight of the paper is that inflation is best\ncharacterised by a combination of two types of nonlinearities: quantile\nvariation, and conditioning on the momentum of inflation.",
        "authors": [
            "Tibor Szendrei",
            "Arnab Bhattacharjee"
        ],
        "categories": "econ.EM",
        "published": "2024-08-22T10:42:32Z",
        "updated": "2024-08-22T10:42:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.12210v1",
        "title": "Enhancing Causal Discovery in Financial Networks with Piecewise Quantile Regression",
        "abstract": "Financial networks can be constructed using statistical dependencies found\nwithin the price series of speculative assets. Across the various methods used\nto infer these networks, there is a general reliance on predictive modelling to\ncapture cross-correlation effects. These methods usually model the flow of\nmean-response information, or the propagation of volatility and risk within the\nmarket. Such techniques, though insightful, don't fully capture the broader\ndistribution-level causality that is possible within speculative markets. This\npaper introduces a novel approach, combining quantile regression with a\npiecewise linear embedding scheme - allowing us to construct causality networks\nthat identify the complex tail interactions inherent to financial markets.\nApplying this method to 260 cryptocurrency return series, we uncover\nsignificant tail-tail causal effects and substantial causal asymmetry. We\nidentify a propensity for coins to be self-influencing, with comparatively\nsparse cross variable effects. Assessing all link types in conjunction, Bitcoin\nstands out as the primary influencer - a nuance that is missed in conventional\nlinear mean-response analyses. Our findings introduce a comprehensive framework\nfor modelling distributional causality, paving the way towards more holistic\nrepresentations of causality in financial markets.",
        "authors": [
            "Cameron Cornell",
            "Lewis Mitchell",
            "Matthew Roughan"
        ],
        "categories": "q-fin.ST",
        "published": "2024-08-22T08:39:09Z",
        "updated": "2024-08-22T08:39:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.12014v1",
        "title": "An Econometric Analysis of Large Flexible Cryptocurrency-mining Consumers in Electricity Markets",
        "abstract": "In recent years, power grids have seen a surge in large cryptocurrency mining\nfirms, with individual consumption levels reaching 700MW. This study examines\nthe behavior of these firms in Texas, focusing on how their consumption is\ninfluenced by cryptocurrency conversion rates, electricity prices, local\nweather, and other factors. We transform the skewed electricity consumption\ndata of these firms, perform correlation analysis, and apply a seasonal\nautoregressive moving average model for analysis. Our findings reveal that,\nsurprisingly, short-term mining electricity consumption is not correlated with\ncryptocurrency conversion rates. Instead, the primary influencers are the\ntemperature and electricity prices. These firms also respond to avoid\ntransmission and distribution network (T\\&D) charges -- famously known as four\nCoincident peak (4CP) charges -- during summer times. As the scale of these\nfirms is likely to surge in future years, the developed electricity consumption\nmodel can be used to generate public, synthetic datasets to understand the\noverall impact on power grid. The developed model could also lead to better\npricing mechanisms to effectively use the flexibility of these resources\ntowards improving power grid reliability.",
        "authors": [
            "Subir Majumder",
            "Ignacio Aravena",
            "Le Xie"
        ],
        "categories": "eess.SY",
        "published": "2024-08-21T21:52:49Z",
        "updated": "2024-08-21T21:52:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.11967v1",
        "title": "Valuing an Engagement Surface using a Large Scale Dynamic Causal Model",
        "abstract": "With recent rapid growth in online shopping, AI-powered Engagement Surfaces\n(ES) have become ubiquitous across retail services. These engagement surfaces\nperform an increasing range of functions, including recommending new products\nfor purchase, reminding customers of their orders and providing delivery\nnotifications. Understanding the causal effect of engagement surfaces on value\ndriven for customers and businesses remains an open scientific question. In\nthis paper, we develop a dynamic causal model at scale to disentangle value\nattributable to an ES, and to assess its effectiveness. We demonstrate the\napplication of this model to inform business decision-making by understanding\nreturns on investment in the ES, and identifying product lines and features\nwhere the ES adds the most value.",
        "authors": [
            "Abhimanyu Mukerji",
            "Sushant More",
            "Ashwin Viswanathan Kannan",
            "Lakshmi Ravi",
            "Hua Chen",
            "Naman Kohli",
            "Chris Khawand",
            "Dinesh Mandalapu"
        ],
        "categories": "cs.LG",
        "published": "2024-08-21T19:42:45Z",
        "updated": "2024-08-21T19:42:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.11951v1",
        "title": "SPORTSCausal: Spill-Over Time Series Causal Inference",
        "abstract": "Randomized controlled trials (RCTs) have long been the gold standard for\ncausal inference across various fields, including business analysis, economic\nstudies, sociology, clinical research, and network learning. The primary\nadvantage of RCTs over observational studies lies in their ability to\nsignificantly reduce noise from individual variance. However, RCTs depend on\nstrong assumptions, such as group independence, time independence, and group\nrandomness, which are not always feasible in real-world applications.\nTraditional inferential methods, including analysis of covariance (ANCOVA),\noften fail when these assumptions do not hold. In this paper, we propose a\nnovel approach named \\textbf{Sp}ill\\textbf{o}ve\\textbf{r} \\textbf{T}ime\n\\textbf{S}eries \\textbf{Causal} (\\verb+SPORTSCausal+), which enables the\nestimation of treatment effects without relying on these stringent assumptions.\nWe demonstrate the practical applicability of \\verb+SPORTSCausal+ through a\nreal-world budget-control experiment. In this experiment, data was collected\nfrom both a 5\\% live experiment and a 50\\% live experiment using the same\ntreatment. Due to the spillover effect, the vanilla estimation of the treatment\neffect was not robust across different treatment sizes, whereas\n\\verb+SPORTSCausal+ provided a robust estimation.",
        "authors": [
            "Carol Liu"
        ],
        "categories": "econ.EM",
        "published": "2024-08-21T19:09:29Z",
        "updated": "2024-08-21T19:09:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.11676v2",
        "title": "Actually, There is No Rotational Indeterminacy in the Approximate Factor Model",
        "abstract": "We show that in the approximate factor model the population normalised\nprincipal components converge in mean square (up to sign) under the standard\nassumptions for $n\\to \\infty$. Consequently, we have a generic interpretation\nof what the principal components estimator is actually identifying and existing\nresults on factor identification are reinforced and refined. Based on this\nresult, we provide a new asymptotic theory for the approximate factor model\nentirely without rotation matrices. We show that the factors space is\nconsistently estimated with finite $T$ for $n\\to \\infty$ while consistency of\nthe factors a.k.a the $L^2$ limit of the normalised principal components\nrequires that both $(n, T)\\to \\infty$.",
        "authors": [
            "Philipp Gersing"
        ],
        "categories": "econ.EM",
        "published": "2024-08-21T14:56:23Z",
        "updated": "2024-10-25T11:40:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.11621v1",
        "title": "Robust Bayes Treatment Choice with Partial Identification",
        "abstract": "We study a class of binary treatment choice problems with partial\nidentification, through the lens of robust (multiple prior) Bayesian analysis.\nWe use a convenient set of prior distributions to derive ex-ante and ex-post\nrobust Bayes decision rules, both for decision makers who can randomize and for\ndecision makers who cannot.\n  Our main messages are as follows: First, ex-ante and ex-post robust Bayes\ndecision rules do not tend to agree in general, whether or not randomized rules\nare allowed. Second, randomized treatment assignment for some data realizations\ncan be optimal in both ex-ante and, perhaps more surprisingly, ex-post\nproblems. Therefore, it is usually with loss of generality to exclude\nrandomized rules from consideration, even when regret is evaluated ex-post.\n  We apply our results to a stylized problem where a policy maker uses\nexperimental data to choose whether to implement a new policy in a population\nof interest, but is concerned about the external validity of the experiment at\nhand (Stoye, 2012); and to the aggregation of data generated by multiple\nrandomized control trials in different sites to make a policy choice in a\npopulation for which no experimental data are available (Manski, 2020; Ishihara\nand Kitagawa, 2021).",
        "authors": [
            "Andr\u00e9s Aradillas Fern\u00e1ndez",
            "Jos\u00e9 Luis Montiel Olea",
            "Chen Qiu",
            "J\u00f6rg Stoye",
            "Serdil Tinda"
        ],
        "categories": "econ.EM",
        "published": "2024-08-21T13:47:45Z",
        "updated": "2024-08-21T13:47:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.11519v1",
        "title": "Towards an Inclusive Approach to Corporate Social Responsibility (CSR) in Morocco: CGEM's Commitment",
        "abstract": "Corporate social responsibility encourages companies to integrate social and\nenvironmental concerns into their activities and their relations with\nstakeholders. It encompasses all actions aimed at the social good, above and\nbeyond corporate interests and legal requirements. Various international\norganizations, authors and researchers have explored the notion of CSR and\nproposed a range of definitions reflecting their perspectives on the concept.\nIn Morocco, although Moroccan companies are not overwhelmingly embracing CSR,\nseveral factors are encouraging them to integrate the CSR approach not only\ninto their discourse, but also into their strategies. The CGEM is actively\ninvolved in promoting CSR within Moroccan companies, awarding the \"CGEM Label\nfor CSR\" to companies that meet the criteria set out in the CSR Charter. The\nprocess of labeling Moroccan companies is in full expansion. The graphs\npresented in this article are broken down according to several criteria, such\nas company size, sector of activity and listing on the Casablanca Stock\nExchange, in order to provide an overview of CSR-labeled companies in Morocco.\nThe approach adopted for this article is a qualitative one aimed at presenting,\nfirstly, the different definitions of the CSR concept and its evolution over\ntime. In this way, the study focuses on the Moroccan context to dissect and\nanalyze the state of progress of CSR integration in Morocco and the various\nefforts made by the CGEM to implement it. According to the data, 124 Moroccan\ncompanies have been awarded the CSR label. For a label in existence since 2006,\nthis figure reflects a certain reluctance on the part of Moroccan companies to\nfully implement the CSR approach in their strategies. Nevertheless, Morocco is\nin a transitional phase, marked by the gradual adoption of various socially\nresponsible practices.",
        "authors": [
            "Gnaoui Imane",
            "Moutahaddib Aziz"
        ],
        "categories": "econ.EM",
        "published": "2024-08-21T10:51:27Z",
        "updated": "2024-08-21T10:51:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.11193v2",
        "title": "Inference with Many Weak Instruments and Heterogeneity",
        "abstract": "This paper considers inference in a linear instrumental variable regression\nmodel with many potentially weak instruments and heterogeneous treatment\neffects. I first show that existing test procedures, including those that are\nrobust to only either weak instruments or heterogeneous treatment effects, can\nbe arbitrarily oversized in this setup. Then, I propose a valid inference\nprocedure based on a score statistic and a leave-three-out variance estimator.\nTo establish this procedure's validity, this paper proves that the score\nstatistic is asymptotically normal and the variance estimator is consistent.\nThe power of the score test is also close to a power envelope in an empirical\napplication.",
        "authors": [
            "Luther Yap"
        ],
        "categories": "econ.EM",
        "published": "2024-08-20T20:59:38Z",
        "updated": "2024-09-26T14:48:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.10825v1",
        "title": "Conditional nonparametric variable screening by neural factor regression",
        "abstract": "High-dimensional covariates often admit linear factor structure. To\neffectively screen correlated covariates in high-dimension, we propose a\nconditional variable screening test based on non-parametric regression using\nneural networks due to their representation power. We ask the question whether\nindividual covariates have additional contributions given the latent factors or\nmore generally a set of variables. Our test statistics are based on the\nestimated partial derivative of the regression function of the candidate\nvariable for screening and a observable proxy for the latent factors. Hence,\nour test reveals how much predictors contribute additionally to the\nnon-parametric regression after accounting for the latent factors. Our\nderivative estimator is the convolution of a deep neural network regression\nestimator and a smoothing kernel. We demonstrate that when the neural network\nsize diverges with the sample size, unlike estimating the regression function\nitself, it is necessary to smooth the partial derivative of the neural network\nestimator to recover the desired convergence rate for the derivative. Moreover,\nour screening test achieves asymptotic normality under the null after finely\ncentering our test statistics that makes the biases negligible, as well as\nconsistency for local alternatives under mild conditions. We demonstrate the\nperformance of our test in a simulation study and two real world applications.",
        "authors": [
            "Jianqing Fan",
            "Weining Wang",
            "Yue Zhao"
        ],
        "categories": "econ.EM",
        "published": "2024-08-20T13:21:35Z",
        "updated": "2024-08-20T13:21:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.10686v1",
        "title": "Gradient Wild Bootstrap for Instrumental Variable Quantile Regressions with Weak and Few Clusters",
        "abstract": "We study the gradient wild bootstrap-based inference for instrumental\nvariable quantile regressions in the framework of a small number of large\nclusters in which the number of clusters is viewed as fixed, and the number of\nobservations for each cluster diverges to infinity. For the Wald inference, we\nshow that our wild bootstrap Wald test, with or without studentization using\nthe cluster-robust covariance estimator (CRVE), controls size asymptotically up\nto a small error as long as the parameter of endogenous variable is strongly\nidentified in at least one of the clusters. We further show that the wild\nbootstrap Wald test with CRVE studentization is more powerful for distant local\nalternatives than that without. Last, we develop a wild bootstrap\nAnderson-Rubin (AR) test for the weak-identification-robust inference. We show\nit controls size asymptotically up to a small error, even under weak or partial\nidentification for all clusters. We illustrate the good finite-sample\nperformance of the new inference methods using simulations and provide an\nempirical application to a well-known dataset about US local labor markets.",
        "authors": [
            "Wenjie Wang",
            "Yichong Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-08-20T09:37:36Z",
        "updated": "2024-08-20T09:37:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.10509v1",
        "title": "Continuous difference-in-differences with double/debiased machine learning",
        "abstract": "This paper extends difference-in-differences to settings involving continuous\ntreatments. Specifically, the average treatment effect on the treated (ATT) at\nany level of continuous treatment intensity is identified using a conditional\nparallel trends assumption. In this framework, estimating the ATTs requires\nfirst estimating infinite-dimensional nuisance parameters, especially the\nconditional density of the continuous treatment, which can introduce\nsignificant biases. To address this challenge, estimators for the causal\nparameters are proposed under the double/debiased machine learning framework.\nWe show that these estimators are asymptotically normal and provide consistent\nvariance estimators. To illustrate the effectiveness of our methods, we\nre-examine the study by Acemoglu and Finkelstein (2008), which assessed the\neffects of the 1983 Medicare Prospective Payment System (PPS) reform. By\nreinterpreting their research design using a difference-in-differences approach\nwith continuous treatment, we nonparametrically estimate the treatment effects\nof the 1983 PPS reform, thereby providing a more detailed understanding of its\nimpact.",
        "authors": [
            "Lucas Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-08-20T03:13:06Z",
        "updated": "2024-08-20T03:13:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.09618v5",
        "title": "kendallknight: An R Package for Efficient Implementation of Kendall's Correlation Coefficient Computation",
        "abstract": "The kendallknight package introduces an efficient implementation of Kendall's\ncorrelation coefficient computation, significantly improving the processing\ntime for large datasets without sacrificing accuracy. The kendallknight\npackage, following Knight (1966) and posterior literature, reduces the\ncomputational complexity resulting in drastic reductions in computation time,\ntransforming operations that would take minutes or hours into milliseconds or\nminutes, while maintaining precision and correctly handling edge cases and\nerrors. The package is particularly advantageous in econometric and statistical\ncontexts where rapid and accurate calculation of Kendall's correlation\ncoefficient is desirable. Benchmarks demonstrate substantial performance gains\nover the base R implementation, especially for large datasets.",
        "authors": [
            "Mauricio Vargas Sep\u00falveda"
        ],
        "categories": "stat.CO",
        "published": "2024-08-19T00:05:42Z",
        "updated": "2024-12-08T19:09:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.09607v2",
        "title": "Experimental Design For Causal Inference Through An Optimization Lens",
        "abstract": "The study of experimental design offers tremendous benefits for answering\ncausal questions across a wide range of applications, including agricultural\nexperiments, clinical trials, industrial experiments, social experiments, and\ndigital experiments. Although valuable in such applications, the costs of\nexperiments often drive experimenters to seek more efficient designs. Recently,\nexperimenters have started to examine such efficiency questions from an\noptimization perspective, as experimental design problems are fundamentally\ndecision-making problems. This perspective offers a lot of flexibility in\nleveraging various existing optimization tools to study experimental design\nproblems. This manuscript thus aims to examine the foundations of experimental\ndesign problems in the context of causal inference as viewed through an\noptimization lens.",
        "authors": [
            "Jinglong Zhao"
        ],
        "categories": "stat.ME",
        "published": "2024-08-18T22:25:08Z",
        "updated": "2024-08-24T19:56:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.09598v2",
        "title": "Anytime-Valid Inference for Double/Debiased Machine Learning of Causal Parameters",
        "abstract": "Double (debiased) machine learning (DML) has seen widespread use in recent\nyears for learning causal/structural parameters, in part due to its flexibility\nand adaptability to high-dimensional nuisance functions as well as its ability\nto avoid bias from regularization or overfitting. However, the classic\ndouble-debiased framework is only valid asymptotically for a predetermined\nsample size, thus lacking the flexibility of collecting more data if sharper\ninference is needed, or stopping data collection early if useful inferences can\nbe made earlier than expected. This can be of particular concern in large scale\nexperimental studies with huge financial costs or human lives at stake, as well\nas in observational studies where the length of confidence of intervals do not\nshrink to zero even with increasing sample size due to partial identifiability\nof a structural parameter. In this paper, we present time-uniform counterparts\nto the asymptotic DML results, enabling valid inference and confidence\nintervals for structural parameters to be constructed at any arbitrary\n(possibly data-dependent) stopping time. We provide conditions which are only\nslightly stronger than the standard DML conditions, but offer the stronger\nguarantee for anytime-valid inference. This facilitates the transformation of\nany existing DML method to provide anytime-valid guarantees with minimal\nmodifications, making it highly adaptable and easy to use. We illustrate our\nprocedure using two instances: a) local average treatment effect in online\nexperiments with non-compliance, and b) partial identification of average\ntreatment effect in observational studies with potential unmeasured\nconfounding.",
        "authors": [
            "Abhinandan Dalal",
            "Patrick Bl\u00f6baum",
            "Shiva Kasiviswanathan",
            "Aaditya Ramdas"
        ],
        "categories": "stat.ME",
        "published": "2024-08-18T21:19:56Z",
        "updated": "2024-09-10T21:10:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.09560v1",
        "title": "Deep Learning for the Estimation of Heterogeneous Parameters in Discrete Choice Models",
        "abstract": "This paper studies the finite sample performance of the flexible estimation\napproach of Farrell, Liang, and Misra (2021a), who propose to use deep learning\nfor the estimation of heterogeneous parameters in economic models, in the\ncontext of discrete choice models. The approach combines the structure imposed\nby economic models with the flexibility of deep learning, which assures the\ninterpretebility of results on the one hand, and allows estimating flexible\nfunctional forms of observed heterogeneity on the other hand. For inference\nafter the estimation with deep learning, Farrell et al. (2021a) derive an\ninfluence function that can be applied to many quantities of interest. We\nconduct a series of Monte Carlo experiments that investigate the impact of\nregularization on the proposed estimation and inference procedure in the\ncontext of discrete choice models. The results show that the deep learning\napproach generally leads to precise estimates of the true average parameters\nand that regular robust standard errors lead to invalid inference results,\nshowing the need for the influence function approach for inference. Without\nregularization, the influence function approach can lead to substantial bias\nand large estimated standard errors caused by extreme outliers. Regularization\nreduces this property and stabilizes the estimation procedure, but at the\nexpense of inducing an additional bias. The bias in combination with decreasing\nvariance associated with increasing regularization leads to the construction of\ninvalid inferential statements in our experiments. Repeated sample splitting,\nunlike regularization, stabilizes the estimation approach without introducing\nan additional bias, thereby allowing for the construction of valid inferential\nstatements.",
        "authors": [
            "Stephan Hetzenecker",
            "Maximilian Osterhaus"
        ],
        "categories": "econ.EM",
        "published": "2024-08-18T18:11:33Z",
        "updated": "2024-08-18T18:11:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.09271v2",
        "title": "Counterfactual and Synthetic Control Method: Causal Inference with Instrumented Principal Component Analysis",
        "abstract": "In this paper, we propose a novel method for causal inference within the\nframework of counterfactual and synthetic control. Matching forward the\ngeneralized synthetic control method, our instrumented principal component\nanalysis method instruments factor loadings with predictive covariates rather\nthan including them as regressors. These instrumented factor loadings exhibit\ntime-varying dynamics, offering a better economic interpretation. Covariates\nare instrumented through a transformation matrix, $\\Gamma$, when we have a\nlarge number of covariates it can be easily reduced in accordance with a small\nnumber of latent factors helping us to effectively handle high-dimensional\ndatasets and making the model parsimonious. Moreover, the novel way of handling\ncovariates is less exposed to model misspecification and achieved better\nprediction accuracy. Our simulations show that this method is less biased in\nthe presence of unobserved covariates compared to other mainstream approaches.\nIn the empirical application, we use the proposed method to evaluate the effect\nof Brexit on foreign direct investment to the UK.",
        "authors": [
            "Cong Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-08-17T18:57:51Z",
        "updated": "2024-09-14T17:50:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.09187v1",
        "title": "Externally Valid Selection of Experimental Sites via the k-Median Problem",
        "abstract": "We present a decision-theoretic justification for viewing the question of how\nto best choose where to experiment in order to optimize external validity as a\nk-median (clustering) problem, a popular problem in computer science and\noperations research. We present conditions under which minimizing the\nworst-case, welfare-based regret among all nonrandom schemes that select k\nsites to experiment is approximately equal - and sometimes exactly equal - to\nfinding the k most central vectors of baseline site-level covariates. The\nk-median problem can be formulated as a linear integer program. Two empirical\napplications illustrate the theoretical and computational benefits of the\nsuggested procedure.",
        "authors": [
            "Jos\u00e9 Luis Montiel Olea",
            "Brenda Prallon",
            "Chen Qiu",
            "J\u00f6rg Stoye",
            "Yiwei Sun"
        ],
        "categories": "econ.EM",
        "published": "2024-08-17T12:56:12Z",
        "updated": "2024-08-17T12:56:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.09185v1",
        "title": "Method of Moments Estimation for Affine Stochastic Volatility Models",
        "abstract": "We develop moment estimators for the parameters of affine stochastic\nvolatility models. We first address the challenge of calculating moments for\nthe models by introducing a recursive equation for deriving closed-form\nexpressions for moments of any order. Consequently, we propose our moment\nestimators. We then establish a central limit theorem for our estimators and\nderive the explicit formulas for the asymptotic covariance matrix. Finally, we\nprovide numerical results to validate our method.",
        "authors": [
            "Yan-Feng Wu",
            "Xiangyu Yang",
            "Jian-Qiang Hu"
        ],
        "categories": "q-fin.ST",
        "published": "2024-08-17T12:29:55Z",
        "updated": "2024-08-17T12:29:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.08580v1",
        "title": "Revisiting the Many Instruments Problem using Random Matrix Theory",
        "abstract": "We use recent results from the theory of random matrices to improve\ninstrumental variables estimation with many instruments. In settings where the\nfirst-stage parameters are dense, we show that Ridge lowers the implicit price\nof a bias adjustment. This comes along with improved (finite-sample) properties\nin the second stage regression. Our theoretical results nest existing results\non bias approximation and bias adjustment. Moreover, it extends them to\nsettings with more instruments than observations.",
        "authors": [
            "Helmut Farbmacher",
            "Rebecca Groh",
            "Michael M\u00fchlegger",
            "Gabriel Vollert"
        ],
        "categories": "econ.EM",
        "published": "2024-08-16T07:26:01Z",
        "updated": "2024-08-16T07:26:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.07842v1",
        "title": "Quantile and Distribution Treatment Effects on the Treated with Possibly Non-Continuous Outcomes",
        "abstract": "Quantile and Distribution Treatment effects on the Treated (QTT/DTT) for\nnon-continuous outcomes are either not identified or inference thereon is\ninfeasible using existing methods. By introducing functional index parallel\ntrends and no anticipation assumptions, this paper identifies and provides\nuniform inference procedures for QTT/DTT. The inference procedure applies under\nboth the canonical two-group and staggered treatment designs with balanced\npanels, unbalanced panels, or repeated cross-sections. Monte Carlo experiments\ndemonstrate the proposed method's robust and competitive performance, while an\nempirical application illustrates its practical utility.",
        "authors": [
            "Nelly K. Djuazon",
            "Emmanuel Selorm Tsyawo"
        ],
        "categories": "econ.EM",
        "published": "2024-08-14T22:44:27Z",
        "updated": "2024-08-14T22:44:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.07678v1",
        "title": "Your MMM is Broken: Identification of Nonlinear and Time-varying Effects in Marketing Mix Models",
        "abstract": "Recent years have seen a resurgence in interest in marketing mix models\n(MMMs), which are aggregate-level models of marketing effectiveness. Often\nthese models incorporate nonlinear effects, and either implicitly or explicitly\nassume that marketing effectiveness varies over time. In this paper, we show\nthat nonlinear and time-varying effects are often not identifiable from\nstandard marketing mix data: while certain data patterns may be suggestive of\nnonlinear effects, such patterns may also emerge under simpler models that\nincorporate dynamics in marketing effectiveness. This lack of identification is\nproblematic because nonlinearities and dynamics suggest fundamentally different\noptimal marketing allocations. We examine this identification issue through\ntheory and simulations, wherein we explore the exact conditions under which\nconflation between the two types of models is likely to occur. In doing so, we\nintroduce a flexible Bayesian nonparametric model that allows us to both\nflexibly simulate and estimate different data-generating processes. We show\nthat conflating the two types of effects is especially likely in the presence\nof autocorrelated marketing variables, which are common in practice, especially\ngiven the widespread use of stock variables to capture long-run effects of\nadvertising. We illustrate these ideas through numerous empirical applications\nto real-world marketing mix data, showing the prevalence of the conflation\nissue in practice. Finally, we show how marketers can avoid this conflation, by\ndesigning experiments that strategically manipulate spending in ways that pin\ndown model form.",
        "authors": [
            "Ryan Dew",
            "Nicolas Padilla",
            "Anya Shchetkina"
        ],
        "categories": "econ.EM",
        "published": "2024-08-14T17:24:10Z",
        "updated": "2024-08-14T17:24:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.07185v1",
        "title": "A Sparse Grid Approach for the Nonparametric Estimation of High-Dimensional Random Coefficient Models",
        "abstract": "A severe limitation of many nonparametric estimators for random coefficient\nmodels is the exponential increase of the number of parameters in the number of\nrandom coefficients included into the model. This property, known as the curse\nof dimensionality, restricts the application of such estimators to models with\nmoderately few random coefficients. This paper proposes a scalable\nnonparametric estimator for high-dimensional random coefficient models. The\nestimator uses a truncated tensor product of one-dimensional hierarchical basis\nfunctions to approximate the underlying random coefficients' distribution. Due\nto the truncation, the number of parameters increases at a much slower rate\nthan in the regular tensor product basis, rendering the nonparametric\nestimation of high-dimensional random coefficient models feasible. The derived\nestimator allows estimating the underlying distribution with constrained least\nsquares, making the approach computationally simple and fast. Monte Carlo\nexperiments and an application to data on the regulation of air pollution\nillustrate the good performance of the estimator.",
        "authors": [
            "Maximilian Osterhaus"
        ],
        "categories": "econ.EM",
        "published": "2024-08-13T19:55:31Z",
        "updated": "2024-08-13T19:55:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.06977v3",
        "title": "Endogeneity Corrections in Binary Outcome Models with Nonlinear Transformations: Identification and Inference",
        "abstract": "For binary outcome models, an endogeneity correction based on nonlinear\nrank-based transformations is proposed. Identification without external\ninstruments is achieved under one of two assumptions: either the endogenous\nregressor is a nonlinear function of one component of the error term,\nconditional on the exogenous regressors, or the dependence between the\nendogenous and exogenous regressors is nonlinear. Under these conditions, we\nprove consistency and asymptotic normality. Monte Carlo simulations and an\napplication on German insolvency data illustrate the usefulness of the method.",
        "authors": [
            "Alexander Mayer",
            "Dominik Wied"
        ],
        "categories": "econ.EM",
        "published": "2024-08-13T15:33:27Z",
        "updated": "2024-11-10T15:49:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.08908v1",
        "title": "Panel Data Unit Root testing: Overview",
        "abstract": "This review discusses methods of testing for a panel unit root. Modern\napproaches to testing in cross-sectionally correlated panels are discussed,\npreceding the analysis with an analysis of independent panels. In addition,\nmethods for testing in the case of non-linearity in the data (for example, in\nthe case of structural breaks) are presented, as well as methods for testing in\nshort panels, when the time dimension is small and finite. In conclusion, links\nto existing packages that allow implementing some of the described methods are\nprovided.",
        "authors": [
            "Anton Skrobotov"
        ],
        "categories": "econ.EM",
        "published": "2024-08-13T12:01:24Z",
        "updated": "2024-08-13T12:01:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.06624v1",
        "title": "Estimation and Inference of Average Treatment Effect in Percentage Points under Heterogeneity",
        "abstract": "In semi-log regression models with heterogeneous treatment effects, the\naverage treatment effect (ATE) in log points and its exponential transformation\nminus one underestimate the ATE in percentage points. I propose new estimation\nand inference methods for the ATE in percentage points, with inference\nutilizing the Fenton-Wilkinson approximation. These methods are particularly\nrelevant for staggered difference-in-differences designs, where treatment\neffects often vary across groups and periods. I prove the methods' large-sample\nproperties and demonstrate their finite-sample performance through simulations,\nrevealing substantial discrepancies between conventional and proposed measures.\nTwo empirical applications further underscore the practical importance of these\nmethods.",
        "authors": [
            "Ying Zeng"
        ],
        "categories": "econ.EM",
        "published": "2024-08-13T04:23:55Z",
        "updated": "2024-08-13T04:23:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.06519v1",
        "title": "An unbounded intensity model for point processes",
        "abstract": "We develop a model for point processes on the real line, where the intensity\ncan be locally unbounded without inducing an explosion. In contrast to an\norderly point process, for which the probability of observing more than one\nevent over a short time interval is negligible, the bursting intensity causes\nan extreme clustering of events around the singularity. We propose a\nnonparametric approach to detect such bursts in the intensity. It relies on a\nheavy traffic condition, which admits inference for point processes over a\nfinite time interval. With Monte Carlo evidence, we show that our testing\nprocedure exhibits size control under the null, whereas it has high rejection\nrates under the alternative. We implement our approach on high-frequency data\nfor the EUR/USD spot exchange rate, where the test statistic captures abnormal\nsurges in trading activity. We detect a nontrivial amount of intensity bursts\nin these data and describe their basic properties. Trading activity during an\nintensity burst is positively related to volatility, illiquidity, and the\nprobability of observing a drift burst. The latter effect is reinforced if the\norder flow is imbalanced or the price elasticity of the limit order book is\nlarge.",
        "authors": [
            "Kim Christensen",
            "Alexei Kolokolov"
        ],
        "categories": "econ.EM",
        "published": "2024-08-12T22:37:42Z",
        "updated": "2024-08-12T22:37:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.06103v1",
        "title": "Method-of-Moments Inference for GLMs and Doubly Robust Functionals under Proportional Asymptotics",
        "abstract": "In this paper, we consider the estimation of regression coefficients and\nsignal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models\n(GLMs), and explore their implications in inferring popular estimands such as\naverage treatment effects in high-dimensional observational studies. Under the\n``proportional asymptotic'' regime and Gaussian covariates with known\n(population) covariance $\\Sigma$, we derive Consistent and Asymptotically\nNormal (CAN) estimators of our targets of inference through a Method-of-Moments\ntype of estimators that bypasses estimation of high dimensional nuisance\nfunctions and hyperparameter tuning altogether. Additionally, under\nnon-Gaussian covariates, we demonstrate universality of our results under\ncertain additional assumptions on the regression coefficients and $\\Sigma$. We\nalso demonstrate that knowing $\\Sigma$ is not essential to our proposed\nmethodology when the sample covariance matrix estimator is invertible. Finally,\nwe complement our theoretical results with numerical experiments and\ncomparisons with existing literature.",
        "authors": [
            "Xingyu Chen",
            "Lin Liu",
            "Rajarshi Mukherjee"
        ],
        "categories": "math.ST",
        "published": "2024-08-12T12:43:30Z",
        "updated": "2024-08-12T12:43:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.05847v1",
        "title": "Correcting invalid regression discontinuity designs with multiple time period data",
        "abstract": "A common approach to Regression Discontinuity (RD) designs relies on a\ncontinuity assumption of the mean potential outcomes at the cutoff defining the\nRD design. In practice, this assumption is often implausible when changes other\nthan the intervention of interest occur at the cutoff (e.g., other policies are\nimplemented at the same cutoff). When the continuity assumption is implausible,\nresearchers often retreat to ad-hoc analyses that are not supported by any\ntheory and yield results with unclear causal interpretation. These analyses\nseek to exploit additional data where either all units are treated or all units\nare untreated (regardless of their running variable value). For example, when\ndata from multiple time periods are available. We first derive the bias of RD\ndesigns when the continuity assumption does not hold. We then present a\ntheoretical foundation for analyses using multiple time periods by the means of\na general identification framework incorporating data from additional time\nperiods to overcome the bias. We discuss this framework under various RD\ndesigns, and also extend our work to carry-over effects and time-varying\nrunning variables. We develop local linear regression estimators, bias\ncorrection procedures, and standard errors that are robust to bias-correction\nfor the multiple period setup. The approach is illustrated using an application\nthat studied the effect of new fiscal laws on debt of Italian municipalities.",
        "authors": [
            "Dor Leventer",
            "Daniel Nevo"
        ],
        "categories": "econ.EM",
        "published": "2024-08-11T19:06:26Z",
        "updated": "2024-08-11T19:06:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.05688v1",
        "title": "Bank Cost Efficiency and Credit Market Structure Under a Volatile Exchange Rate",
        "abstract": "We study the impact of exchange rate volatility on cost efficiency and market\nstructure in a cross-section of banks that have non-trivial exposures to\nforeign currency (FX) operations. We use unique data on quarterly revaluations\nof FX assets and liabilities (Revals) that Russian banks were reporting between\n2004 Q1 and 2020 Q2. {\\it First}, we document that Revals constitute the\nlargest part of the banks' total costs, 26.5\\% on average, with considerable\nvariation across banks. {\\it Second}, we find that stochastic estimates of cost\nefficiency are both severely downward biased -- by 30\\% on average -- and\ngenerally not rank preserving when Revals are ignored, except for the tails, as\nour nonparametric copulas reveal. To ensure generalizability to other emerging\nmarket economies, we suggest a two-stage approach that does not rely on Revals\nbut is able to shrink the downward bias in cost efficiency estimates by\ntwo-thirds. {\\it Third}, we show that Revals are triggered by the mismatch in\nthe banks' FX operations, which, in turn, is driven by household FX deposits\nand the instability of Ruble's exchange rate. {\\it Fourth}, we find that the\nfailure to account for Revals leads to the erroneous conclusion that the credit\nmarket is inefficient, which is driven by the upper quartile of the banks'\ndistribution by total assets. Revals have considerable negative implications\nfor financial stability which can be attenuated by the cross-border\ndiversification of bank assets.",
        "authors": [
            "Mikhail Mamonov",
            "Christopher Parmeter",
            "Artem Prokhorov"
        ],
        "categories": "econ.EM",
        "published": "2024-08-11T03:49:26Z",
        "updated": "2024-08-11T03:49:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.05665v1",
        "title": "Change-Point Detection in Time Series Using Mixed Integer Programming",
        "abstract": "We use cutting-edge mixed integer optimization (MIO) methods to develop a\nframework for detection and estimation of structural breaks in time series\nregression models. The framework is constructed based on the least squares\nproblem subject to a penalty on the number of breakpoints. We restate the\n$l_0$-penalized regression problem as a quadratic programming problem with\ninteger- and real-valued arguments and show that MIO is capable of finding\nprovably optimal solutions using a well-known optimization solver. Compared to\nthe popular $l_1$-penalized regression (LASSO) and other classical methods, the\nMIO framework permits simultaneous estimation of the number and location of\nstructural breaks as well as regression coefficients, while accommodating the\noption of specifying a given or minimal number of breaks. We derive the\nasymptotic properties of the estimator and demonstrate its effectiveness\nthrough extensive numerical experiments, confirming a more accurate estimation\nof multiple breaks as compared to popular non-MIO alternatives. Two empirical\nexamples demonstrate usefulness of the framework in applications from business\nand economic statistics.",
        "authors": [
            "Artem Prokhorov",
            "Peter Radchenko",
            "Alexander Semenov",
            "Anton Skrobotov"
        ],
        "categories": "econ.EM",
        "published": "2024-08-11T01:02:58Z",
        "updated": "2024-08-11T01:02:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.05342v2",
        "title": "Optimal Treatment Allocation Strategies for A/B Testing in Partially Observable Time Series Experiments",
        "abstract": "Time series experiments, in which experimental units receive a sequence of\ntreatments over time, are frequently employed in many technological companies\nto evaluate the performance of a newly developed policy, product, or treatment\nrelative to a baseline control. Many existing A/B testing solutions assume a\nfully observable experimental environment that satisfies the Markov condition,\nwhich often does not hold in practice. This paper studies the optimal design\nfor A/B testing in partially observable environments. We introduce a controlled\n(vector) autoregressive moving average model to capture partial observability.\nWe introduce a small signal asymptotic framework to simplify the analysis of\nasymptotic mean squared errors of average treatment effect estimators under\nvarious designs. We develop two algorithms to estimate the optimal design: one\nutilizing constrained optimization and the other employing reinforcement\nlearning. We demonstrate the superior performance of our designs using a\ndispatch simulator and two real datasets from a ride-sharing company. A Python\nimplementation of our proposal is available at\nhttps://github.com/datake/ARMADesign.",
        "authors": [
            "Ke Sun",
            "Linglong Kong",
            "Hongtu Zhu",
            "Chengchun Shi"
        ],
        "categories": "econ.EM",
        "published": "2024-08-09T21:20:55Z",
        "updated": "2024-10-08T08:00:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.05209v1",
        "title": "What are the real implications for $CO_2$ as generation from renewables increases?",
        "abstract": "Wind and solar electricity generation account for 14% of total electricity\ngeneration in the United States and are expected to continue to grow in the\nnext decades. In low carbon systems, generation from renewable energy sources\ndisplaces conventional fossil fuel power plants resulting in lower system-level\nemissions and emissions intensity. However, we find that intermittent\ngeneration from renewables changes the way conventional thermal power plants\noperate, and that the displacement of generation is not 1 to 1 as expected. Our\nwork provides a method that allows policy and decision makers to continue to\ntrack the effect of additional renewable capacity and the resulting thermal\npower plant operational responses.",
        "authors": [
            "Dhruv Suri",
            "Jacques de Chalendar",
            "Ines Azevedo"
        ],
        "categories": "econ.EM",
        "published": "2024-08-09T17:58:47Z",
        "updated": "2024-08-09T17:58:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.04730v1",
        "title": "Vela: A Data-Driven Proposal for Joint Collaboration in Space Exploration",
        "abstract": "The UN Office of Outer Space Affairs identifies synergy of space development\nactivities and international cooperation through data and infrastructure\nsharing in their Sustainable Development Goal 17 (SDG17). Current multilateral\nspace exploration paradigms, however, are divided between the Artemis and the\nRoscosmos-CNSA programs to return to the moon and establish permanent human\nsettlements. As space agencies work to expand human presence in space, economic\nresource consolidation in pursuit of technologically ambitious space\nexpeditions is the most sensible path to accomplish SDG17. This paper compiles\na budget dataset for the top five federally-funded space agencies: CNSA, ESA,\nJAXA, NASA, and Roscosmos. Using time-series econometric anslysis methods in\nSTATA, this work analyzes each agency's economic contributions toward space\nexploration. The dataset results are used to propose a multinational space\nmission, Vela, for the development of an orbiting space station around Mars in\nthe late 2030s. Distribution of economic resources and technological\ncapabilities by the respective space programs are proposed to ensure\nprogrammatic redundancy and increase the odds of success on the given timeline.",
        "authors": [
            "Holly M. Dinkel",
            "Jason K. Cornelius"
        ],
        "categories": "econ.EM",
        "published": "2024-08-08T19:06:03Z",
        "updated": "2024-08-08T19:06:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.04617v1",
        "title": "Difference-in-Differences for Health Policy and Practice: A Review of Modern Methods",
        "abstract": "Difference-in-differences (DiD) is the most popular observational causal\ninference method in health policy, employed to evaluate the real-world impact\nof policies and programs. To estimate treatment effects, DiD relies on the\n\"parallel trends assumption\", that on average treatment and comparison groups\nwould have had parallel trajectories in the absence of an intervention.\nHistorically, DiD has been considered broadly applicable and straightforward to\nimplement, but recent years have seen rapid advancements in DiD methods. This\npaper reviews and synthesizes these innovations for medical and health policy\nresearchers. We focus on four topics: (1) assessing the parallel trends\nassumption in health policy contexts; (2) relaxing the parallel trends\nassumption when appropriate; (3) employing estimators to account for staggered\ntreatment timing; and (4) conducting robust inference for analyses in which\nnormal-based clustered standard errors are inappropriate. For each, we explain\nchallenges and common pitfalls in traditional DiD and modern methods available\nto address these issues.",
        "authors": [
            "Shuo Feng",
            "Ishani Ganguli",
            "Youjin Lee",
            "John Poe",
            "Andrew Ryan",
            "Alyssa Bilinski"
        ],
        "categories": "stat.AP",
        "published": "2024-08-08T17:47:04Z",
        "updated": "2024-08-08T17:47:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.04552v1",
        "title": "Semiparametric Estimation of Individual Coefficients in a Dyadic Link Formation Model Lacking Observable Characteristics",
        "abstract": "Dyadic network formation models have wide applicability in economic research,\nyet are difficult to estimate in the presence of individual specific effects\nand in the absence of distributional assumptions regarding the model noise\ncomponent. The availability of (continuously distributed) individual or link\ncharacteristics generally facilitates estimation. Yet, while data on social\nnetworks has recently become more abundant, the characteristics of the entities\ninvolved in the link may not be measured. Adapting the procedure of \\citet{KS},\nI propose to use network data alone in a semiparametric estimation of the\nindividual fixed effect coefficients, which carry the interpretation of the\nindividual relative popularity. This entails the possibility to anticipate how\na new-coming individual will connect in a pre-existing group. The estimator,\nneeded for its fast convergence, fails to implement the monotonicity assumption\nregarding the model noise component, thereby potentially reversing the order if\nthe fixed effect coefficients. This and other numerical issues can be\nconveniently tackled by my novel, data-driven way of normalising the fixed\neffects, which proves to outperform a conventional standardisation in many\ncases. I demonstrate that the normalised coefficients converge both at the same\nrate and to the same limiting distribution as if the true error distribution\nwas known. The cost of semiparametric estimation is thus purely computational,\nwhile the potential benefits are large whenever the errors have a strongly\nconvex or strongly concave distribution.",
        "authors": [
            "L. Sanna Stephan"
        ],
        "categories": "econ.EM",
        "published": "2024-08-08T16:05:45Z",
        "updated": "2024-08-08T16:05:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.03930v1",
        "title": "Robust Estimation of Regression Models with Potentially Endogenous Outliers via a Modern Optimization Lens",
        "abstract": "This paper addresses the robust estimation of linear regression models in the\npresence of potentially endogenous outliers. Through Monte Carlo simulations,\nwe demonstrate that existing $L_1$-regularized estimation methods, including\nthe Huber estimator and the least absolute deviation (LAD) estimator, exhibit\nsignificant bias when outliers are endogenous. Motivated by this finding, we\ninvestigate $L_0$-regularized estimation methods. We propose systematic\nheuristic algorithms, notably an iterative hard-thresholding algorithm and a\nlocal combinatorial search refinement, to solve the combinatorial optimization\nproblem of the \\(L_0\\)-regularized estimation efficiently. Our Monte Carlo\nsimulations yield two key results: (i) The local combinatorial search algorithm\nsubstantially improves solution quality compared to the initial\nprojection-based hard-thresholding algorithm while offering greater\ncomputational efficiency than directly solving the mixed integer optimization\nproblem. (ii) The $L_0$-regularized estimator demonstrates superior performance\nin terms of bias reduction, estimation accuracy, and out-of-sample prediction\nerrors compared to $L_1$-regularized alternatives. We illustrate the practical\nvalue of our method through an empirical application to stock return\nforecasting.",
        "authors": [
            "Zhan Gao",
            "Hyungsik Roger Moon"
        ],
        "categories": "econ.EM",
        "published": "2024-08-07T17:46:08Z",
        "updated": "2024-08-07T17:46:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.03530v3",
        "title": "Robust Identification in Randomized Experiments with Noncompliance",
        "abstract": "This paper considers a robust identification of causal parameters in a\nrandomized experiment setting with noncompliance where the standard local\naverage treatment effect assumptions could be violated. Following Li,\nK\\'edagni, and Mourifi\\'e (2024), we propose a misspecification robust bound\nfor a real-valued vector of various causal parameters. We discuss\nidentification under two sets of weaker assumptions: random assignment and\nexclusion restriction (without monotonicity), and random assignment and\nmonotonicity (without exclusion restriction). We introduce two causal\nparameters: the local average treatment-controlled direct effect (LATCDE), and\nthe local average instrument-controlled direct effect (LAICDE). Under the\nrandom assignment and monotonicity assumptions, we derive sharp bounds on the\nlocal average treatment-controlled direct effects for the always-takers and\nnever-takers, respectively, and the total average controlled direct effect for\nthe compliers. Additionally, we show that the intent-to-treat effect can be\nexpressed as a convex weighted average of these three effects. Finally, we\napply our method on the proximity to college instrument and find that growing\nup near a four-year college increases the wage of never-takers (who represent\nmore than 70% of the population) by a range of 4.15% to 27.07%.",
        "authors": [
            "Yi Cui",
            "D\u00e9sir\u00e9 K\u00e9dagni",
            "Huan Wu"
        ],
        "categories": "econ.EM",
        "published": "2024-08-07T04:00:48Z",
        "updated": "2024-09-01T23:41:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.03137v4",
        "title": "Efficient Asymmetric Causality Tests",
        "abstract": "Asymmetric causality tests are increasingly gaining popularity in different\nscientific fields. This approach corresponds better to reality since logical\nreasons behind asymmetric behavior exist and need to be considered in empirical\ninvestigations. Hatemi-J (2012) introduced the asymmetric causality tests via\npartial cumulative sums for positive and negative components of the variables\noperating within the vector autoregressive (VAR) model. However, since the\nresiduals across the equations in the VAR model are not independent, the\nordinary least squares method for estimating the parameters is not efficient.\nAdditionally, asymmetric causality tests mean having different causal\nparameters (i.e., for positive or negative components), thus, it is crucial to\nassess not only if these causal parameters are individually statistically\nsignificant, but also if their difference is statistically significant.\nConsequently, tests of difference between estimated causal parameters should\nexplicitly be conducted, which are neglected in the existing literature. The\npurpose of the current paper is to deal with these issues explicitly. An\napplication is provided, and ten different hypotheses pertinent to the\nasymmetric causal interaction between two largest financial markets worldwide\nare efficiently tested within a multivariate setting.",
        "authors": [
            "Abdulnasser Hatemi-J"
        ],
        "categories": "econ.EM",
        "published": "2024-08-06T12:24:11Z",
        "updated": "2024-10-08T08:11:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.02757v1",
        "title": "A nonparametric test for diurnal variation in spot correlation processes",
        "abstract": "The association between log-price increments of exchange-traded equities, as\nmeasured by their spot correlation estimated from high-frequency data, exhibits\na pronounced upward-sloping and almost piecewise linear relationship at the\nintraday horizon. There is notably lower-on average less positive-correlation\nin the morning than in the afternoon. We develop a nonparametric testing\nprocedure to detect such deterministic variation in a correlation process. The\ntest statistic has a known distribution under the null hypothesis, whereas it\ndiverges under the alternative. It is robust against stochastic correlation. We\nrun a Monte Carlo simulation to discover the finite sample properties of the\ntest statistic, which are close to the large sample predictions, even for small\nsample sizes and realistic levels of diurnal variation. In an application, we\nimplement the test on a monthly basis for a high-frequency dataset covering the\nstock market over an extended period. The test leads to rejection of the null\nmost of the time. This suggests diurnal variation in the correlation process is\na nontrivial effect in practice.",
        "authors": [
            "Kim Christensen",
            "Ulrich Hounyo",
            "Zhi Liu"
        ],
        "categories": "econ.EM",
        "published": "2024-08-05T18:19:09Z",
        "updated": "2024-08-05T18:19:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.02573v1",
        "title": "Testing identifying assumptions in Tobit Models",
        "abstract": "This paper develops sharp testable implications for Tobit and IV-Tobit\nmodels' identifying assumptions: linear index specification, (joint) normality\nof latent errors, and treatment (instrument) exogeneity and relevance. The new\nsharp testable equalities can detect all possible observable violations of the\nidentifying conditions. We propose a testing procedure for the model's validity\nusing existing inference methods for intersection bounds. Simulation results\nsuggests proper size for large samples and that the test is powerful to detect\nlarge violation of the exogeneity assumption and violations in the error\nstructure. Finally, we review and propose new alternative paths to partially\nidentify the parameters of interest under less restrictive assumptions.",
        "authors": [
            "Santiago Acerenza",
            "Ot\u00e1vio Bartalotti",
            "Federico Veneri"
        ],
        "categories": "econ.EM",
        "published": "2024-08-05T15:48:14Z",
        "updated": "2024-08-05T15:48:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.02391v2",
        "title": "Kullback-Leibler-based characterizations of score-driven updates",
        "abstract": "Score-driven models have been applied in some 400 published articles over the\nlast decade. Much of this literature cites the optimality result in Blasques et\nal. (2015), which, roughly, states that sufficiently small score-driven updates\nare unique in locally reducing the Kullback-Leibler divergence relative to the\ntrue density for every observation. This is at odds with other well-known\noptimality results; the Kalman filter, for example, is optimal in a\nmean-squared-error sense, but occasionally moves away from the true state. We\nshow that score-driven updates are, similarly, not guaranteed to improve the\nlocalized Kullback-Leibler divergence at every observation. The seemingly\nstronger result in Blasques et al. (2015) is due to their use of an improper\n(localized) scoring rule. Even as a guaranteed improvement for every\nobservation is unattainable, we prove that sufficiently small score-driven\nupdates are unique in reducing the Kullback-Leibler divergence relative to the\ntrue density in expectation. This positive, albeit weaker, result justifies the\ncontinued use of score-driven models and places their information-theoretic\nproperties on solid footing.",
        "authors": [
            "Ramon de Punder",
            "Timo Dimitriadis",
            "Rutger-Jan Lange"
        ],
        "categories": "math.ST",
        "published": "2024-08-05T11:35:11Z",
        "updated": "2024-09-13T09:37:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.01985v1",
        "title": "Analysis of Factors Affecting the Entry of Foreign Direct Investment into Indonesia (Case Study of Three Industrial Sectors in Indonesia)",
        "abstract": "The realization of FDI and DDI from January to December 2022 reached\nRp1,207.2 trillion. The largest FDI investment realization by sector was led by\nthe Basic Metal, Metal Goods, Non-Machinery, and Equipment Industry sector,\nfollowed by the Mining sector and the Electricity, Gas, and Water sector. The\nuneven amount of FDI investment realization in each industry and the impact of\nthe COVID-19 pandemic in Indonesia are the main issues addressed in this study.\nThis study aims to identify the factors that influence the entry of FDI into\nindustries in Indonesia and measure the extent of these factors' influence on\nthe entry of FDI. In this study, classical assumption tests and hypothesis\ntests are conducted to investigate whether the research model is robust enough\nto provide strategic options nationally. Moreover, this study uses the ordinary\nleast squares (OLS) method. The results show that the electricity factor does\nnot influence FDI inflows in the three industries. The Human Development Index\n(HDI) factor has a significant negative effect on FDI in the Mining Industry\nand a significant positive effect on FDI in the Basic Metal, Metal Goods,\nNon-Machinery, and Equipment Industries. However, HDI does not influence FDI in\nthe Electricity, Gas, and Water Industries in Indonesia.",
        "authors": [
            "Tracy Patricia Nindry Abigail Rolnmuch",
            "Yuhana Astuti"
        ],
        "categories": "econ.EM",
        "published": "2024-08-04T10:52:55Z",
        "updated": "2024-08-04T10:52:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.01208v1",
        "title": "Distributional Difference-in-Differences Models with Multiple Time Periods: A Monte Carlo Analysis",
        "abstract": "Researchers are often interested in evaluating the impact of a policy on the\nentire (or specific parts of the) distribution of the outcome of interest. In\nthis paper, I provide a practical toolkit to recover the whole counterfactual\ndistribution of the untreated potential outcome for the treated group in\nnon-experimental settings with staggered treatment adoption by generalizing the\nexisting quantile treatment effects on the treated (QTT) estimator proposed by\nCallaway and Li (2019). Besides the QTT, I consider different approaches that\nanonymously summarize the quantiles of the distribution of the outcome of\ninterest (such as tests for stochastic dominance rankings) without relying on\nrank invariance assumptions. The finite-sample properties of the estimator\nproposed are analyzed via different Monte Carlo simulations. Despite being\nslightly biased for relatively small sample sizes, the proposed method's\nperformance increases substantially when the sample size increases.",
        "authors": [
            "Andrea Ciaccio"
        ],
        "categories": "econ.EM",
        "published": "2024-08-02T11:43:08Z",
        "updated": "2024-08-02T11:43:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.01023v1",
        "title": "Distilling interpretable causal trees from causal forests",
        "abstract": "Machine learning methods for estimating treatment effect heterogeneity\npromise greater flexibility than existing methods that test a few pre-specified\nhypotheses. However, one problem these methods can have is that it can be\nchallenging to extract insights from complicated machine learning models. A\nhigh-dimensional distribution of conditional average treatment effects may give\naccurate, individual-level estimates, but it can be hard to understand the\nunderlying patterns; hard to know what the implications of the analysis are.\nThis paper proposes the Distilled Causal Tree, a method for distilling a\nsingle, interpretable causal tree from a causal forest. This compares well to\nexisting methods of extracting a single tree, particularly in noisy data or\nhigh-dimensional data where there are many correlated features. Here it even\noutperforms the base causal forest in most simulations. Its estimates are\ndoubly robust and asymptotically normal just as those of the causal forest are.",
        "authors": [
            "Patrick Rehill"
        ],
        "categories": "econ.EM",
        "published": "2024-08-02T05:48:15Z",
        "updated": "2024-08-02T05:48:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.01017v1",
        "title": "Application of Superconducting Technology in the Electricity Industry: A Game-Theoretic Analysis of Government Subsidy Policies and Power Company Equipment Upgrade Decisions",
        "abstract": "This study investigates the potential impact of \"LK-99,\" a novel material\ndeveloped by a Korean research team, on the power equipment industry. Using\nevolutionary game theory, the interactions between governmental subsidies and\ntechnology adoption by power companies are modeled. A key innovation of this\nresearch is the introduction of sensitivity analyses concerning time delays and\ninitial subsidy amounts, which significantly influence the strategic decisions\nof both government and corporate entities. The findings indicate that these\nfactors are critical in determining the rate of technology adoption and the\nefficiency of the market as a whole. Due to existing data limitations, the\nstudy offers a broad overview of likely trends and recommends the inclusion of\nreal-world data for more precise modeling once the material demonstrates\nroom-temperature superconducting characteristics. The research contributes\nfoundational insights valuable for future policy design and has significant\nimplications for advancing the understanding of technology adoption and market\ndynamics.",
        "authors": [
            "Mingyang Li",
            "Maoqin Yuan",
            "Han Pengsihua",
            "Yuan Yuan",
            "Zejun Wang"
        ],
        "categories": "math.DS",
        "published": "2024-08-02T05:33:38Z",
        "updated": "2024-08-02T05:33:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.00291v2",
        "title": "Identification and Inference for Synthetic Control Methods with Spillover Effects: Estimating the Economic Cost of the Sudan Split",
        "abstract": "The synthetic control method (SCM) is widely used for causal inference with\npanel data, particularly when there are few treated units. SCM assumes the\nstable unit treatment value assumption (SUTVA), which posits that potential\noutcomes are unaffected by the treatment status of other units. However,\ninterventions often impact not only treated units but also untreated units,\nknown as spillover effects. This study introduces a novel panel data method\nthat extends SCM to allow for spillover effects and estimate both treatment and\nspillover effects. This method leverages a spatial autoregressive panel data\nmodel to account for spillover effects. We also propose Bayesian inference\nmethods using Bayesian horseshoe priors for regularization. We apply the\nproposed method to two empirical studies: evaluating the effect of the\nCalifornia tobacco tax on consumption and estimating the economic impact of the\n2011 division of Sudan on GDP per capita.",
        "authors": [
            "Shosei Sakaguchi",
            "Hayato Tagawa"
        ],
        "categories": "econ.EM",
        "published": "2024-08-01T05:33:49Z",
        "updated": "2024-10-06T05:46:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2408.00032v1",
        "title": "Methodological Foundations of Modern Causal Inference in Social Science Research",
        "abstract": "This paper serves as a literature review of methodology concerning the\n(modern) causal inference methods to address the causal estimand with\nobservational/survey data that have been or will be used in social science\nresearch. Mainly, this paper is divided into two parts: inference from\nstatistical estimand for the causal estimand, in which we reviewed the\nassumptions for causal identification and the methodological strategies\naddressing the problems if some of the assumptions are violated. We also\ndiscuss the asymptotical analysis concerning the measure from the observational\ndata to the theoretical measure and replicate the deduction of the\nefficient/doubly robust average treatment effect estimator, which is commonly\nused in current social science analysis.",
        "authors": [
            "Guanghui Pan"
        ],
        "categories": "econ.EM",
        "published": "2024-07-31T11:36:30Z",
        "updated": "2024-07-31T11:36:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.21119v1",
        "title": "Potential weights and implicit causal designs in linear regression",
        "abstract": "When do linear regressions estimate causal effects in quasi-experiments? This\npaper provides a generic diagnostic that assesses whether a given linear\nregression specification on a given dataset admits a design-based\ninterpretation. To do so, we define a notion of potential weights, which encode\ncounterfactual decisions a given regression makes to unobserved potential\noutcomes. If the specification does admit such an interpretation, this\ndiagnostic can find a vector of unit-level treatment assignment probabilities\n-- which we call an implicit design -- under which the regression estimates a\ncausal effect. This diagnostic also finds the implicit causal effect estimand.\nKnowing the implicit design and estimand adds transparency, leads to further\nsanity checks, and opens the door to design-based statistical inference. When\napplied to regression specifications studied in the causal inference\nliterature, our framework recovers and extends existing theoretical results.\nWhen applied to widely-used specifications not covered by existing causal\ninference literature, our framework generates new theoretical insights.",
        "authors": [
            "Jiafeng Chen"
        ],
        "categories": "econ.EM",
        "published": "2024-07-30T18:22:12Z",
        "updated": "2024-07-30T18:22:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.20386v2",
        "title": "On the power properties of inference for parameters with interval identified sets",
        "abstract": "This paper studies the power properties of confidence intervals (CIs) for a\npartially-identified parameter of interest with an interval identified set. We\nassume the researcher has bounds estimators to construct the CIs proposed by\nStoye (2009), referred to as CI1, CI2, and CI3. We also assume that these\nestimators are \"ordered\": the lower bound estimator is less than or equal to\nthe upper bound estimator.\n  Under these conditions, we establish two results. First, we show that CI1 and\nCI2 are equally powerful, and both dominate CI3. Second, we consider a\nfavorable situation in which there are two possible bounds estimators to\nconstruct these CIs, and one is more efficient than the other. One would expect\nthat the more efficient bounds estimator yields more powerful inference. We\nprove that this desirable result holds for CI1 and CI2, but not necessarily for\nCI3.",
        "authors": [
            "Federico A. Bugni",
            "Mengsi Gao",
            "Filip Obradovic",
            "Amilcar Velez"
        ],
        "categories": "econ.EM",
        "published": "2024-07-29T19:22:48Z",
        "updated": "2024-10-22T02:26:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.19932v2",
        "title": "Testing for the Asymmetric Optimal Hedge Ratios: With an Application to Bitcoin",
        "abstract": "Reducing financial risk is of paramount importance to investors, financial\ninstitutions, and corporations. Since the pioneering contribution of Johnson\n(1960), the optimal hedge ratio based on futures is regularly utilized. The\ncurrent paper suggests an explicit and efficient method for testing the null\nhypothesis of a symmetric optimal hedge ratio against an asymmetric alternative\none within a multivariate setting. If the null is rejected, the position\ndependent optimal hedge ratios can be estimated via the suggested model. This\napproach is expected to enhance the accuracy of the implemented hedging\nstrategies compared to the standard methods since it accounts for the fact that\nthe source of risk depends on whether the investor is a buyer or a seller of\nthe risky asset. An application is provided using spot and futures prices of\nBitcoin. The results strongly support the view that the optimal hedge ratio for\nthis cryptocurrency is position dependent. The investor that is long in Bitcoin\nhas a much higher conditional optimal hedge ratio compared to the one that is\nshort in the asset. The difference between the two conditional optimal hedge\nratios is statistically significant, which has important repercussions for\nimplementing risk management strategies.",
        "authors": [
            "Abdulnasser Hatemi-J"
        ],
        "categories": "q-fin.RM",
        "published": "2024-07-29T12:07:08Z",
        "updated": "2024-08-01T07:30:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.19618v2",
        "title": "Experimenting on Markov Decision Processes with Local Treatments",
        "abstract": "Utilizing randomized experiments to evaluate the effect of short-term\ntreatments on the short-term outcomes has been well understood and become the\ngolden standard in industrial practice. However, as service systems become\nincreasingly dynamical and personalized, much focus is shifting toward\nmaximizing long-term cumulative outcomes, such as customer lifetime value,\nthrough lifetime exposure to interventions. To bridge this gap, we investigate\nthe randomized experiments within dynamical systems modeled as Markov Decision\nProcesses (MDPs). Our goal is to assess the impact of treatment and control\npolicies on long-term cumulative rewards from relatively short-term\nobservations. We first develop optimal inference techniques for assessing the\neffects of general treatment patterns. Furthermore, recognizing that many\nreal-world treatments tend to be fine-grained and localized for practical\nefficiency and operational convenience, we then propose methods to harness this\nlocalized structure by sharing information on the non-targeted states. Our new\nestimator effectively overcomes the variance lower bound for general treatments\nwhile matching the more stringent lower bound incorporating the local treatment\nstructure. Furthermore, our estimator can optimally achieve a linear reduction\nwith the number of test arms for a major part of the variance. Finally, we\nexplore scenarios with perfect knowledge of the control arm and design\nestimators that further improve inference efficiency.",
        "authors": [
            "Shuze Chen",
            "David Simchi-Levi",
            "Chonghuan Wang"
        ],
        "categories": "stat.ME",
        "published": "2024-07-29T00:41:11Z",
        "updated": "2024-10-18T03:19:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.19509v1",
        "title": "Heterogeneous Grouping Structures in Panel Data",
        "abstract": "In this paper we examine the existence of heterogeneity within a group, in\npanels with latent grouping structure. The assumption of within group\nhomogeneity is prevalent in this literature, implying that the formation of\ngroups alleviates cross-sectional heterogeneity, regardless of the prior\nknowledge of groups. While the latter hypothesis makes inference powerful, it\ncan be often restrictive. We allow for models with richer heterogeneity that\ncan be found both in the cross-section and within a group, without imposing the\nsimple assumption that all groups must be heterogeneous. We further contribute\nto the method proposed by \\cite{su2016identifying}, by showing that the model\nparameters can be consistently estimated and the groups, while unknown, can be\nidentifiable in the presence of different types of heterogeneity. Within the\nsame framework we consider the validity of assuming both cross-sectional and\nwithin group homogeneity, using testing procedures. Simulations demonstrate\ngood finite-sample performance of the approach in both classification and\nestimation, while empirical applications across several datasets provide\nevidence of multiple clusters, as well as reject the hypothesis of within group\nhomogeneity.",
        "authors": [
            "Katerina Chrysikou",
            "George Kapetanios"
        ],
        "categories": "econ.EM",
        "published": "2024-07-28T15:07:47Z",
        "updated": "2024-07-28T15:07:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.19339v3",
        "title": "Using Total Margin of Error to Account for Non-Sampling Error in Election Polls: The Case of Nonresponse",
        "abstract": "The potential impact of non-sampling errors on election polls is well known,\nbut measurement has focused on the margin of sampling error. Survey\nstatisticians have long recommended measurement of total survey error by mean\nsquare error (MSE), which jointly measures sampling and non-sampling errors. We\nthink it reasonable to use the square root of maximum MSE to measure the total\nmargin of error (TME). Measurement of TME should encompass both sampling error\nand all forms of non-sampling error. We suggest that measurement of TME should\nbe a standard feature in the reporting of polls. To provide a clear\nillustration, and because we believe the exceedingly low response rates\ncommonly obtained by election polls to be a particularly worrisome source of\npotential error, we demonstrate how to measure the potential impact of\nnonresponse using the concept of TME. We first show how to measure TME when a\npollster lacks any knowledge of the candidate preferences of nonrespondents. We\nthen extend the analysis to settings where the pollster has partial knowledge\nthat bounds the preferences of non-respondents. In each setting, we derive a\nsimple poll estimate that approximately minimizes TME, a midpoint estimate, and\ncompare it to a conventional poll estimate.",
        "authors": [
            "Jeff Dominitz",
            "Charles F. Manski"
        ],
        "categories": "econ.EM",
        "published": "2024-07-27T20:59:54Z",
        "updated": "2024-10-31T13:48:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.18206v1",
        "title": "Starting Small: Prioritizing Safety over Efficacy in Randomized Experiments Using the Exact Finite Sample Likelihood",
        "abstract": "We use the exact finite sample likelihood and statistical decision theory to\nanswer questions of ``why?'' and ``what should you have done?'' using data from\nrandomized experiments and a utility function that prioritizes safety over\nefficacy. We propose a finite sample Bayesian decision rule and a finite sample\nmaximum likelihood decision rule. We show that in finite samples from 2 to 50,\nit is possible for these rules to achieve better performance according to\nestablished maximin and maximum regret criteria than a rule based on the\nBoole-Frechet-Hoeffding bounds. We also propose a finite sample maximum\nlikelihood criterion. We apply our rules and criterion to an actual clinical\ntrial that yielded a promising estimate of efficacy, and our results point to\nsafety as a reason for why results were mixed in subsequent trials.",
        "authors": [
            "Neil Christy",
            "A. E. Kowalski"
        ],
        "categories": "econ.EM",
        "published": "2024-07-25T17:14:55Z",
        "updated": "2024-07-25T17:14:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.17888v2",
        "title": "Enhanced power enhancements for testing many moment equalities: Beyond the $2$- and $\\infty$-norm",
        "abstract": "Contemporary testing problems in statistics are increasingly complex, i.e.,\nhigh-dimensional. Tests based on the $2$- and $\\infty$-norm have received\nconsiderable attention in such settings, as they are powerful against dense and\nsparse alternatives, respectively. The power enhancement principle of Fan et\nal. (2015) combines these two norms to construct improved tests that are\npowerful against both types of alternatives. In the context of testing whether\na candidate parameter satisfies a large number of moment equalities, we\nconstruct a test that harnesses the strength of all $p$-norms with $p\\in[2,\n\\infty]$. As a result, this test is consistent against strictly more\nalternatives than any test based on a single $p$-norm. In particular, our test\nis consistent against more alternatives than tests based on the $2$- and\n$\\infty$-norm, which is what most implementations of the power enhancement\nprinciple target.\n  We illustrate the scope of our general results by using them to construct a\ntest that simultaneously dominates the Anderson-Rubin test (based on $p=2$),\ntests based on the $\\infty$-norm and power enhancement based combinations of\nthese in terms of consistency in the linear instrumental variable model with\nmany instruments.",
        "authors": [
            "Anders Bredahl Kock",
            "David Preinerstorfer"
        ],
        "categories": "econ.EM",
        "published": "2024-07-25T09:18:05Z",
        "updated": "2024-10-22T10:41:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.17385v2",
        "title": "Causal modelling without introducing counterfactuals or abstract distributions",
        "abstract": "The most common approach to causal modelling is the potential outcomes\nframework due to Neyman and Rubin. In this framework, outcomes of\ncounterfactual treatments are assumed to be well-defined. This metaphysical\nassumption is often thought to be problematic yet indispensable. The\nconventional approach relies not only on counterfactuals but also on abstract\nnotions of distributions and assumptions of independence that are not directly\ntestable. In this paper, we construe causal inference as treatment-wise\npredictions for finite populations where all assumptions are testable; this\nmeans that one can not only test predictions themselves (without any\nfundamental problem) but also investigate sources of error when they fail. The\nnew framework highlights the model-dependence of causal claims as well as the\ndifference between statistical and scientific inference.",
        "authors": [
            "Benedikt H\u00f6ltgen",
            "Robert C. Williamson"
        ],
        "categories": "stat.ME",
        "published": "2024-07-24T16:07:57Z",
        "updated": "2024-08-14T13:01:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.16950v1",
        "title": "Identification and inference of outcome conditioned partial effects of general interventions",
        "abstract": "This paper proposes a new class of distributional causal quantities, referred\nto as the \\textit{outcome conditioned partial policy effects} (OCPPEs), to\nmeasure the \\textit{average} effect of a general counterfactual intervention of\na target covariate on the individuals in different quantile ranges of the\noutcome distribution.\n  The OCPPE approach is valuable in several aspects: (i) Unlike the\nunconditional quantile partial effect (UQPE) that is not $\\sqrt{n}$-estimable,\nan OCPPE is $\\sqrt{n}$-estimable. Analysts can use it to capture heterogeneity\nacross the unconditional distribution of $Y$ as well as obtain accurate\nestimation of the aggregated effect at the upper and lower tails of $Y$. (ii)\nThe semiparametric efficiency bound for an OCPPE is explicitly derived. (iii)\nWe propose an efficient debiased estimator for OCPPE, and provide feasible\nuniform inference procedures for the OCPPE process. (iv) The efficient doubly\nrobust score for an OCPPE can be used to optimize infinitesimal nudges to a\ncontinuous treatment by maximizing a quantile specific Empirical Welfare\nfunction. We illustrate the method by analyzing how anti-smoking policies\nimpact low percentiles of live infants' birthweights.",
        "authors": [
            "Zhengyu Zhang",
            "Zequn Jin",
            "Lihua Lin"
        ],
        "categories": "econ.EM",
        "published": "2024-07-24T02:37:02Z",
        "updated": "2024-07-24T02:37:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.16349v1",
        "title": "Bayesian modelling of VAR precision matrices using stochastic block networks",
        "abstract": "Commonly used priors for Vector Autoregressions (VARs) induce shrinkage on\nthe autoregressive coefficients. Introducing shrinkage on the error covariance\nmatrix is sometimes done but, in the vast majority of cases, without\nconsidering the network structure of the shocks and by placing the prior on the\nlower Cholesky factor of the precision matrix. In this paper, we propose a\nprior on the VAR error precision matrix directly. Our prior, which resembles a\nstandard spike and slab prior, models variable inclusion probabilities through\na stochastic block model that clusters shocks into groups. Within groups, the\nprobability of having relations across group members is higher (inducing less\nsparsity) whereas relations across groups imply a lower probability that\nmembers of each group are conditionally related. We show in simulations that\nour approach recovers the true network structure well. Using a US macroeconomic\ndata set, we illustrate how our approach can be used to cluster shocks together\nand that this feature leads to improved density forecasts.",
        "authors": [
            "Florian Huber",
            "Gary Koop",
            "Massimiliano Marcellino",
            "Tobias Scheckel"
        ],
        "categories": "econ.EM",
        "published": "2024-07-23T09:50:37Z",
        "updated": "2024-07-23T09:50:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.16037v1",
        "title": "Estimating Distributional Treatment Effects in Randomized Experiments: Machine Learning for Variance Reduction",
        "abstract": "We propose a novel regression adjustment method designed for estimating\ndistributional treatment effect parameters in randomized experiments.\nRandomized experiments have been extensively used to estimate treatment effects\nin various scientific fields. However, to gain deeper insights, it is essential\nto estimate distributional treatment effects rather than relying solely on\naverage effects. Our approach incorporates pre-treatment covariates into a\ndistributional regression framework, utilizing machine learning techniques to\nimprove the precision of distributional treatment effect estimators. The\nproposed approach can be readily implemented with off-the-shelf machine\nlearning methods and remains valid as long as the nuisance components are\nreasonably well estimated. Also, we establish the asymptotic properties of the\nproposed estimator and present a uniformly valid inference method. Through\nsimulation results and real data analysis, we demonstrate the effectiveness of\nintegrating machine learning techniques in reducing the variance of\ndistributional treatment effect estimators in finite samples.",
        "authors": [
            "Undral Byambadalai",
            "Tatsushi Oka",
            "Shota Yasui"
        ],
        "categories": "econ.EM",
        "published": "2024-07-22T20:28:29Z",
        "updated": "2024-07-22T20:28:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.15522v1",
        "title": "Big Data Analytics-Enabled Dynamic Capabilities and Market Performance: Examining the Roles of Marketing Ambidexterity and Competitor Pressure",
        "abstract": "This study, rooted in dynamic capability theory and the developing era of Big\nData Analytics, explores the transformative effect of BDA EDCs on marketing.\nAmbidexterity and firms market performance in the textile sector of Pakistans\ncities. Specifically, focusing on the firms who directly deal with customers,\ninvestigates the nuanced role of BDA EDCs in textile retail firms potential to\nnavigate market dynamics. Emphasizing the exploitation component of marketing\nambidexterity, the study investigated the mediating function of marketing\nambidexterity and the moderating influence of competitive pressure. Using a\nsurvey questionnaire, the study targets key choice makers in textile firms of\nFaisalabad, Chiniot and Lahore, Pakistan. The PLS-SEM model was employed as an\nanalytical technique, allows for a full examination of the complicated\nrelations between BDA EDCs, marketing ambidexterity, rival pressure, and market\nperformance. The study Predicting a positive impact of Big Data on marketing\nambidexterity, with a specific emphasis on exploitation. The study expects this\nexploitation-orientated marketing ambidexterity to significantly enhance the\nfirms market performance. This research contributes to the existing literature\non dynamic capabilities-based frameworks from the perspective of the retail\nsegment of textile industry. The study emphasizes the role of BDA-EDCs in the\nretail sector, imparting insights into the direct and indirect results of BDA\nEDCs on market performance inside the retail area. The study s novelty lies in\nits contextualization of BDA-EDCs in the textile zone of Faisalabad, Lahore and\nChiniot, providing a unique perspective on the effect of BDA on marketing\nambidexterity and market performance in firms. Methodologically, the study uses\nnumerous samples of retail sectors to make sure broader universality,\ncontributing realistic insights.",
        "authors": [
            "Gulfam Haider",
            "Laiba Zubair",
            "Aman Saleem"
        ],
        "categories": "econ.EM",
        "published": "2024-07-22T10:18:30Z",
        "updated": "2024-07-22T10:18:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.15276v1",
        "title": "Nonlinear Binscatter Methods",
        "abstract": "Binned scatter plots are a powerful statistical tool for empirical work in\nthe social, behavioral, and biomedical sciences. Available methods rely on a\nquantile-based partitioning estimator of the conditional mean regression\nfunction to primarily construct flexible yet interpretable visualization\nmethods, but they can also be used to estimate treatment effects, assess\nuncertainty, and test substantive domain-specific hypotheses. This paper\nintroduces novel binscatter methods based on nonlinear, possibly nonsmooth\nM-estimation methods, covering generalized linear, robust, and quantile\nregression models. We provide a host of theoretical results and practical tools\nfor local constant estimation along with piecewise polynomial and spline\napproximations, including (i) optimal tuning parameter (number of bins)\nselection, (ii) confidence bands, and (iii) formal statistical tests regarding\nfunctional form or shape restrictions. Our main results rely on novel strong\napproximations for general partitioning-based estimators covering random,\ndata-driven partitions, which may be of independent interest. We demonstrate\nour methods with an empirical application studying the relation between the\npercentage of individuals without health insurance and per capita income at the\nzip-code level. We provide general-purpose software packages implementing our\nmethods in Python, R, and Stata.",
        "authors": [
            "Matias D. Cattaneo",
            "Richard K. Crump",
            "Max H. Farrell",
            "Yingjie Feng"
        ],
        "categories": "stat.ME",
        "published": "2024-07-21T21:57:09Z",
        "updated": "2024-07-21T21:57:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.15256v3",
        "title": "Weak-instrument-robust subvector inference in instrumental variables regression: A subvector Lagrange multiplier test and properties of subvector Anderson-Rubin confidence sets",
        "abstract": "We propose a weak-instrument-robust subvector Lagrange multiplier test for\ninstrumental variables regression. We show that it is asymptotically\nsize-correct under a technical condition. This is the first\nweak-instrument-robust subvector test for instrumental variables regression to\nrecover the degrees of freedom of the commonly used non-weak-instrument-robust\nWald test. Additionally, we provide a closed-form solution for subvector\nconfidence sets obtained by inverting the subvector Anderson-Rubin test. We\nshow that they are centered around a k-class estimator. Also, we show that the\nsubvector confidence sets for single coefficients of the causal parameter are\njointly bounded if and only if Anderson's likelihood-ratio test rejects the\nhypothesis that the first-stage regression parameter is of reduced rank, that\nis, that the causal parameter is not identified. Finally, we show that if a\nconfidence set obtained by inverting the Anderson-Rubin test is bounded and\nnonempty, it is equal to a Wald-based confidence set with a data-dependent\nconfidence level. We explicitly compute this Wald-based confidence test.",
        "authors": [
            "Malte Londschien",
            "Peter B\u00fchlmann"
        ],
        "categories": "math.ST",
        "published": "2024-07-21T19:58:53Z",
        "updated": "2024-11-04T16:39:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.14914v1",
        "title": "Leveraging Uniformization and Sparsity for Computation of Continuous Time Dynamic Discrete Choice Games",
        "abstract": "Continuous-time formulations of dynamic discrete choice games offer notable\ncomputational advantages, particularly in modeling strategic interactions in\noligopolistic markets. This paper extends these benefits by addressing\ncomputational challenges in order to improve model solution and estimation. We\nfirst establish new results on the rates of convergence of the value iteration,\npolicy evaluation, and relative value iteration operators in the model, holding\nfixed player beliefs. Next, we introduce a new representation of the value\nfunction in the model based on uniformization -- a technique used in the\nanalysis of continuous time Markov chains -- which allows us to draw a direct\nanalogy to discrete time models. Furthermore, we show that uniformization also\nleads to a stable method to compute the matrix exponential, an operator\nappearing in the model's log likelihood function when only discrete time\n\"snapshot\" data are available. We also develop a new algorithm that\nconcurrently computes the matrix exponential and its derivatives with respect\nto model parameters, enhancing computational efficiency. By leveraging the\ninherent sparsity of the model's intensity matrix, combined with sparse matrix\ntechniques and precomputed addresses, we show how to significantly speed up\ncomputations. These strategies allow researchers to estimate more sophisticated\nand realistic models of strategic interactions and policy impacts in empirical\nindustrial organization.",
        "authors": [
            "Jason R. Blevins"
        ],
        "categories": "econ.EM",
        "published": "2024-07-20T15:52:54Z",
        "updated": "2024-07-20T15:52:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.14635v2",
        "title": "Predicting the Distribution of Treatment Effects: A Covariate-Adjustment Approach",
        "abstract": "Important questions for impact evaluation require knowledge not only of\naverage effects, but of the distribution of treatment effects. What proportion\nof people are harmed? Does a policy help many by a little? Or a few by a lot?\nThe inability to observe individual counterfactuals makes these empirical\nquestions challenging. I propose an approach to inference on points of the\ndistribution of treatment effects by incorporating predicted counterfactuals\nthrough covariate adjustment. I show that finite-sample inference is valid\nunder weak assumptions, for example, when data come from a Randomized\nControlled Trial (RCT), and that large-sample inference is asymptotically exact\nunder suitable conditions. Finally, I revisit five RCTs in microcredit where\naverage effects are not statistically significant and find evidence of both\npositive and negative treatment effects in household income. On average across\nstudies, at least 13.6% of households benefited, and 12.5% were negatively\naffected.",
        "authors": [
            "Bruno Fava"
        ],
        "categories": "econ.EM",
        "published": "2024-07-19T19:12:11Z",
        "updated": "2024-10-21T22:19:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.14074v1",
        "title": "Regression Adjustment for Estimating Distributional Treatment Effects in Randomized Controlled Trials",
        "abstract": "In this paper, we address the issue of estimating and inferring the\ndistributional treatment effects in randomized experiments. The distributional\ntreatment effect provides a more comprehensive understanding of treatment\neffects by characterizing heterogeneous effects across individual units, as\nopposed to relying solely on the average treatment effect. To enhance the\nprecision of distributional treatment effect estimation, we propose a\nregression adjustment method that utilizes the distributional regression and\npre-treatment information. Our method is designed to be free from restrictive\ndistributional assumptions. We establish theoretical efficiency gains and\ndevelop a practical, statistically sound inferential framework. Through\nextensive simulation studies and empirical applications, we illustrate the\nsubstantial advantages of our method, equipping researchers with a powerful\ntool for capturing the full spectrum of treatment effects in experimental\nresearch.",
        "authors": [
            "Tatsushi Oka",
            "Shota Yasui",
            "Yuta Hayakawa",
            "Undral Byambadalai"
        ],
        "categories": "econ.EM",
        "published": "2024-07-19T07:07:34Z",
        "updated": "2024-07-19T07:07:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.13613v1",
        "title": "Revisiting Randomization with the Cube Method",
        "abstract": "We propose a novel randomization approach for randomized controlled trials\n(RCTs), named the cube method. The cube method allows for the selection of\nbalanced samples across various covariate types, ensuring consistent adherence\nto balance tests and, whence, substantial precision gains when estimating\ntreatment effects. We establish several statistical properties for the\npopulation and sample average treatment effects (PATE and SATE, respectively)\nunder randomization using the cube method. The relevance of the cube method is\nparticularly striking when comparing the behavior of prevailing methods\nemployed for treatment allocation when the number of covariates to balance is\nincreasing. We formally derive and compare bounds of balancing adjustments\ndepending on the number of units $n$ and the number of covariates $p$ and show\nthat our randomization approach outperforms methods proposed in the literature\nwhen $p$ is large and $p/n$ tends to 0. We run simulation studies to illustrate\nthe substantial gains from the cube method for a large set of covariates.",
        "authors": [
            "Laurent Davezies",
            "Guillaume Hollard",
            "Pedro Vergara Merino"
        ],
        "categories": "econ.EM",
        "published": "2024-07-18T15:52:25Z",
        "updated": "2024-07-18T15:52:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.12422v1",
        "title": "Conduct Parameter Estimation in Homogeneous Goods Markets with Equilibrium Existence and Uniqueness Conditions: The Case of Log-linear Specification",
        "abstract": "We propose a constrained generalized method of moments estimator (GMM)\nincorporating theoretical conditions for the unique existence of equilibrium\nprices for estimating conduct parameters in a log-linear model with homogeneous\ngoods markets. First, we derive such conditions. Second, Monte Carlo\nsimulations confirm that in a log-linear model, incorporating the conditions\nresolves the problems of implausibly low or negative values of conduct\nparameters.",
        "authors": [
            "Yuri Matsumura",
            "Suguru Otani"
        ],
        "categories": "econ.EM",
        "published": "2024-07-17T09:17:44Z",
        "updated": "2024-07-17T09:17:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.11937v2",
        "title": "Factorial Difference-in-Differences",
        "abstract": "In many social science applications, researchers use the\ndifference-in-differences (DID) estimator to establish causal relationships,\nexploiting cross-sectional variation in a baseline factor and temporal\nvariation in exposure to an event that presumably may affect all units. This\napproach, which we term factorial DID (FDID), differs from canonical DID in\nthat it lacks a clean control group unexposed to the event after the event\noccurs. In this paper, we clarify FDID as a research design in terms of its\ndata structure, feasible estimands, and identifying assumptions that allow the\nDID estimator to recover these estimands. We frame FDID as a factorial design\nwith two factors: the baseline factor, denoted by $G$, and the exposure level\nto the event, denoted by $Z$, and define the effect modification and causal\ninteraction as the associative and causal effects of $G$ on the effect of $Z$,\nrespectively. We show that under the canonical no anticipation and parallel\ntrends assumptions, the DID estimator identifies only the effect modification\nof $G$ in FDID, and propose an additional factorial parallel trends assumption\nto identify the causal interaction. Moreover, we show that the canonical DID\nresearch design can be reframed as a special case of the FDID research design\nwith an additional exclusion restriction assumption, thereby reconciling the\ntwo approaches. We extend this framework to allow conditionally valid parallel\ntrends assumptions and multiple time periods, and clarify assumptions required\nto justify regression analysis under FDID. We illustrate these findings with\nempirical examples from economics and political science, and provide\nrecommendations for improving practice and interpretation under FDID.",
        "authors": [
            "Yiqing Xu",
            "Anqi Zhao",
            "Peng Ding"
        ],
        "categories": "stat.ME",
        "published": "2024-07-16T17:27:47Z",
        "updated": "2024-08-26T02:10:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.11765v1",
        "title": "Nowcasting R&D Expenditures: A Machine Learning Approach",
        "abstract": "Macroeconomic data are crucial for monitoring countries' performance and\ndriving policy. However, traditional data acquisition processes are slow,\nsubject to delays, and performed at a low frequency. We address this\n'ragged-edge' problem with a two-step framework. The first step is a supervised\nlearning model predicting observed low-frequency figures. We propose a\nneural-network-based nowcasting model that exploits mixed-frequency,\nhigh-dimensional data. The second step uses the elasticities derived from the\nprevious step to interpolate unobserved high-frequency figures. We apply our\nmethod to nowcast countries' yearly research and development (R&D) expenditure\nseries. These series are collected through infrequent surveys, making them\nideal candidates for this task. We exploit a range of predictors, chiefly\nInternet search volume data, and document the relevance of these data in\nimproving out-of-sample predictions. Furthermore, we leverage the high\nfrequency of our data to derive monthly estimates of R&D expenditures, which\nare currently unobserved. We compare our results with those obtained from the\nclassical regression-based and the sparse temporal disaggregation methods.\nFinally, we validate our results by reporting a strong correlation with monthly\nR&D employment data.",
        "authors": [
            "Atin Aboutorabi",
            "Ga\u00e9tan de Rassenfosse"
        ],
        "categories": "econ.EM",
        "published": "2024-07-16T14:25:38Z",
        "updated": "2024-07-16T14:25:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.10659v1",
        "title": "A nonparametric test for rough volatility",
        "abstract": "We develop a nonparametric test for deciding whether volatility of an asset\nfollows a standard semimartingale process, with paths of finite quadratic\nvariation, or a rough process with paths of infinite quadratic variation. The\ntest utilizes the fact that volatility is rough if and only if volatility\nincrements are negatively autocorrelated at high frequencies. It is based on\nthe sample autocovariance of increments of spot volatility estimates computed\nfrom high-frequency asset return data. By showing a feasible CLT for this\nstatistic under the null hypothesis of semimartingale volatility paths, we\nconstruct a test with fixed asymptotic size and an asymptotic power equal to\none. The test is derived under very general conditions for the data-generating\nprocess. In particular, it is robust to jumps with arbitrary activity and to\nthe presence of market microstructure noise. In an application of the test to\nSPY high-frequency data, we find evidence for rough volatility.",
        "authors": [
            "Carsten H. Chong",
            "Viktor Todorov"
        ],
        "categories": "math.ST",
        "published": "2024-07-15T12:19:43Z",
        "updated": "2024-07-15T12:19:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.10653v1",
        "title": "The Dynamic, the Static, and the Weak factor models and the analysis of high-dimensional time series",
        "abstract": "Several fundamental and closely interconnected issues related to factor\nmodels are reviewed and discussed: dynamic versus static loadings, rate-strong\nversus rate-weak factors, the concept of weakly common component recently\nintroduced by Gersing et al. (2023), the irrelevance of cross-sectional\nordering and the assumption of cross-sectional exchangeability, and the problem\nof undetected strong factors.",
        "authors": [
            "Matteo Barigozzi",
            "Marc Hallin"
        ],
        "categories": "econ.EM",
        "published": "2024-07-15T12:14:23Z",
        "updated": "2024-07-15T12:14:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.21025v2",
        "title": "Reinforcement Learning in High-frequency Market Making",
        "abstract": "This paper establishes a new and comprehensive theoretical analysis for the\napplication of reinforcement learning (RL) in high-frequency market making. We\nbridge the modern RL theory and the continuous-time statistical models in\nhigh-frequency financial economics. Different with most existing literature on\nmethodological research about developing various RL methods for market making\nproblem, our work is a pilot to provide the theoretical analysis. We target the\neffects of sampling frequency, and find an interesting tradeoff between error\nand complexity of RL algorithm when tweaking the values of the time increment\n$\\Delta$ $-$ as $\\Delta$ becomes smaller, the error will be smaller but the\ncomplexity will be larger. We also study the two-player case under the\ngeneral-sum game framework and establish the convergence of Nash equilibrium to\nthe continuous-time game equilibrium as $\\Delta\\rightarrow0$. The Nash\nQ-learning algorithm, which is an online multi-agent RL method, is applied to\nsolve the equilibrium. Our theories are not only useful for practitioners to\nchoose the sampling frequency, but also very general and applicable to other\nhigh-frequency financial decision making problems, e.g., optimal executions, as\nlong as the time-discretization of a continuous-time markov decision process is\nadopted. Monte Carlo simulation evidence support all of our theories.",
        "authors": [
            "Yuheng Zheng",
            "Zihan Ding"
        ],
        "categories": "q-fin.TR",
        "published": "2024-07-14T22:07:48Z",
        "updated": "2024-08-12T16:51:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.10175v1",
        "title": "Low Volatility Stock Portfolio Through High Dimensional Bayesian Cointegration",
        "abstract": "We employ a Bayesian modelling technique for high dimensional cointegration\nestimation to construct low volatility portfolios from a large number of\nstocks. The proposed Bayesian framework effectively identifies sparse and\nimportant cointegration relationships amongst large baskets of stocks across\nvarious asset spaces, resulting in portfolios with reduced volatility. Such\ncointegration relationships persist well over the out-of-sample testing time,\nproviding practical benefits in portfolio construction and optimization.\nFurther studies on drawdown and volatility minimization also highlight the\nbenefits of including cointegrated portfolios as risk management instruments.",
        "authors": [
            "Parley R Yang",
            "Alexander Y Shestopaloff"
        ],
        "categories": "stat.AP",
        "published": "2024-07-14T12:09:29Z",
        "updated": "2024-07-14T12:09:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.09759v1",
        "title": "Estimation of Integrated Volatility Functionals with Kernel Spot Volatility Estimators",
        "abstract": "For a multidimensional It\\^o semimartingale, we consider the problem of\nestimating integrated volatility functionals. Jacod and Rosenbaum (2013)\nstudied a plug-in type of estimator based on a Riemann sum approximation of the\nintegrated functional and a spot volatility estimator with a forward uniform\nkernel. Motivated by recent results that show that spot volatility estimators\nwith general two-side kernels of unbounded support are more accurate, in this\npaper, an estimator using a general kernel spot volatility estimator as the\nplug-in is considered. A biased central limit theorem for estimating the\nintegrated functional is established with an optimal convergence rate. Unbiased\ncentral limit theorems for estimators with proper de-biasing terms are also\nobtained both at the optimal convergence regime for the bandwidth and when\napplying undersmoothing. Our results show that one can significantly reduce the\nestimator's bias by adopting a general kernel instead of the standard uniform\nkernel. Our proposed bias-corrected estimators are found to maintain remarkable\nrobustness against bandwidth selection in a variety of sampling frequencies and\nfunctions.",
        "authors": [
            "Jos\u00e9 E. Figueroa-L\u00f3pez",
            "Jincheng Pang",
            "Bei Wu"
        ],
        "categories": "econ.EM",
        "published": "2024-07-13T03:39:04Z",
        "updated": "2024-07-13T03:39:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.09738v1",
        "title": "Sparse Asymptotic PCA: Identifying Sparse Latent Factors Across Time Horizon",
        "abstract": "This paper proposes a novel method for sparse latent factor modeling using a\nnew sparse asymptotic Principal Component Analysis (APCA). This approach\nanalyzes the co-movements of large-dimensional panel data systems over time\nhorizons within a general approximate factor model framework. Unlike existing\nsparse factor modeling approaches based on sparse PCA, which assume sparse\nloading matrices, our sparse APCA assumes that factor processes are sparse over\nthe time horizon, while the corresponding loading matrices are not necessarily\nsparse. This development is motivated by the observation that the assumption of\nsparse loadings may not be appropriate for financial returns, where exposure to\nmarket factors is generally universal and non-sparse. We propose a truncated\npower method to estimate the first sparse factor process and a sequential\ndeflation method for multi-factor cases. Additionally, we develop a data-driven\napproach to identify the sparsity of risk factors over the time horizon using a\nnovel cross-sectional cross-validation method. Theoretically, we establish that\nour estimators are consistent under mild conditions. Monte Carlo simulations\ndemonstrate that the proposed method performs well in finite samples.\nEmpirically, we analyze daily stock returns for a balanced panel of S&P 500\nstocks from January 2004 to December 2016. Through textual analysis, we examine\nspecific events associated with the identified sparse factors that\nsystematically influence the stock market. Our approach offers a new pathway\nfor economists to study and understand the systematic risks of economic and\nfinancial systems over time.",
        "authors": [
            "Zhaoxing Gao"
        ],
        "categories": "stat.ME",
        "published": "2024-07-13T01:32:37Z",
        "updated": "2024-07-13T01:32:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.09696v1",
        "title": "Regularizing stock return covariance matrices via multiple testing of correlations",
        "abstract": "This paper develops a large-scale inference approach for the regularization\nof stock return covariance matrices. The framework allows for the presence of\nheavy tails and multivariate GARCH-type effects of unknown form among the stock\nreturns. The approach involves simultaneous testing of all pairwise\ncorrelations, followed by setting non-statistically significant elements to\nzero. This adaptive thresholding is achieved through sign-based Monte Carlo\nresampling within multiple testing procedures, controlling either the\ntraditional familywise error rate, a generalized familywise error rate, or the\nfalse discovery proportion. Subsequent shrinkage ensures that the final\ncovariance matrix estimate is positive definite and well-conditioned while\npreserving the achieved sparsity. Compared to alternative estimators, this new\nregularization method demonstrates strong performance in simulation experiments\nand real portfolio optimization.",
        "authors": [
            "Richard Luger"
        ],
        "categories": "econ.EM",
        "published": "2024-07-12T21:39:28Z",
        "updated": "2024-07-12T21:39:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.09664v1",
        "title": "An Introduction to Permutation Processes (version 0.5)",
        "abstract": "These lecture notes were prepared for a special topics course in the\nDepartment of Statistics at the University of Washington, Seattle. They\ncomprise the first eight chapters of a book currently in progress.",
        "authors": [
            "Fang Han"
        ],
        "categories": "math.ST",
        "published": "2024-07-12T19:57:38Z",
        "updated": "2024-07-12T19:57:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.09371v2",
        "title": "Computationally Efficient Estimation of Large Probit Models",
        "abstract": "Probit models are useful for modeling correlated discrete responses in many\ndisciplines, including consumer choice data in economics and marketing.\nHowever, the Gaussian latent variable feature of probit models coupled with\nidentification constraints pose significant computational challenges for its\nestimation and inference, especially when the dimension of the discrete\nresponse variable is large. In this paper, we propose a computationally\nefficient Expectation-Maximization (EM) algorithm for estimating large probit\nmodels. Our work is distinct from existing methods in two important aspects.\nFirst, instead of simulation or sampling methods, we apply and customize\nexpectation propagation (EP), a deterministic method originally proposed for\napproximate Bayesian inference, to estimate moments of the truncated\nmultivariate normal (TMVN) in the E (expectation) step. Second, we take\nadvantage of a symmetric identification condition to transform the constrained\noptimization problem in the M (maximization) step into a one-dimensional\nproblem, which is solved efficiently using Newton's method instead of\noff-the-shelf solvers. Our method enables the analysis of correlated choice\ndata in the presence of more than 100 alternatives, which is a reasonable size\nin modern applications, such as online shopping and booking platforms, but has\nbeen difficult in practice with probit models. We apply our probit estimation\nmethod to study ordering effects in hotel search results on Expedia's online\nbooking platform.",
        "authors": [
            "Patrick Ding",
            "Guido Imbens",
            "Zhaonan Qu",
            "Yinyu Ye"
        ],
        "categories": "stat.ME",
        "published": "2024-07-12T15:52:12Z",
        "updated": "2024-09-27T15:59:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.08602v1",
        "title": "An Introduction to Causal Discovery",
        "abstract": "In social sciences and economics, causal inference traditionally focuses on\nassessing the impact of predefined treatments (or interventions) on predefined\noutcomes, such as the effect of education programs on earnings. Causal\ndiscovery, in contrast, aims to uncover causal relationships among multiple\nvariables in a data-driven manner, by investigating statistical associations\nrather than relying on predefined causal structures. This approach, more common\nin computer science, seeks to understand causality in an entire system of\nvariables, which can be visualized by causal graphs. This survey provides an\nintroduction to key concepts, algorithms, and applications of causal discovery\nfrom the perspectives of economics and social sciences. It covers fundamental\nconcepts like d-separation, causal faithfulness, and Markov equivalence,\nsketches various algorithms for causal discovery, and discusses the back-door\nand front-door criteria for identifying causal effects. The survey concludes\nwith more specific examples of causal discovery, e.g. for learning all\nvariables that directly affect an outcome of interest and/or testing\nidentification of causal effects in observational data.",
        "authors": [
            "Martin Huber"
        ],
        "categories": "econ.EM",
        "published": "2024-07-11T15:29:14Z",
        "updated": "2024-07-11T15:29:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.08510v1",
        "title": "Comparative analysis of Mixed-Data Sampling (MIDAS) model compared to Lag-Llama model for inflation nowcasting",
        "abstract": "Inflation is one of the most important economic indicators closely watched by\nboth public institutions and private agents. This study compares the\nperformance of a traditional econometric model, Mixed Data Sampling regression,\nwith one of the newest developments from the field of Artificial Intelligence,\na foundational time series forecasting model based on a Long short-term memory\nneural network called Lag-Llama, in their ability to nowcast the Harmonized\nIndex of Consumer Prices in the Euro area. Two models were compared and\nassessed whether the Lag-Llama can outperform the MIDAS regression, ensuring\nthat the MIDAS regression is evaluated under the best-case scenario using a\ndataset spanning from 2010 to 2022. The following metrics were used to evaluate\nthe models: Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE),\nMean Squared Error (MSE), correlation with the target, R-squared and adjusted\nR-squared. The results show better performance of the pre-trained Lag-Llama\nacross all metrics.",
        "authors": [
            "Adam Bahelka",
            "Harmen de Weerd"
        ],
        "categories": "econ.EM",
        "published": "2024-07-11T13:48:43Z",
        "updated": "2024-07-11T13:48:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.07988v1",
        "title": "Production function estimation using subjective expectations data",
        "abstract": "Standard methods for estimating production functions in the Olley and Pakes\n(1996) tradition require assumptions on input choices. We introduce a new\nmethod that exploits (increasingly available) data on a firm's expectations of\nits future output and inputs that allows us to obtain consistent production\nfunction parameter estimates while relaxing these input demand assumptions. In\ncontrast to dynamic panel methods, our proposed estimator can be implemented on\nvery short panels (including a single cross-section), and Monte Carlo\nsimulations show it outperforms alternative estimators when firms' material\ninput choices are subject to optimization error. Implementing a range of\nproduction function estimators on UK data, we find our proposed estimator\nyields results that are either similar to or more credible than commonly-used\nalternatives. These differences are larger in industries where material inputs\nappear harder to optimize. We show that TFP implied by our proposed estimator\nis more strongly associated with future jobs growth than existing methods,\nsuggesting that failing to adequately account for input endogeneity may\nunderestimate the degree of dynamic reallocation in the economy.",
        "authors": [
            "Agnes Norris Keiller",
            "Aureo de Paula",
            "John Van Reenen"
        ],
        "categories": "econ.EM",
        "published": "2024-07-10T18:40:24Z",
        "updated": "2024-07-10T18:40:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.07973v1",
        "title": "Reduced-Rank Matrix Autoregressive Models: A Medium $N$ Approach",
        "abstract": "Reduced-rank regressions are powerful tools used to identify co-movements\nwithin economic time series. However, this task becomes challenging when we\nobserve matrix-valued time series, where each dimension may have a different\nco-movement structure. We propose reduced-rank regressions with a tensor\nstructure for the coefficient matrix to provide new insights into co-movements\nwithin and between the dimensions of matrix-valued time series. Moreover, we\nrelate the co-movement structures to two commonly used reduced-rank models,\nnamely the serial correlation common feature and the index model. Two empirical\napplications involving U.S.\\ states and economic indicators for the Eurozone\nand North American countries illustrate how our new tools identify\nco-movements.",
        "authors": [
            "Alain Hecq",
            "Ivan Ricardo",
            "Ines Wilms"
        ],
        "categories": "econ.EM",
        "published": "2024-07-10T18:12:10Z",
        "updated": "2024-07-10T18:12:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.07251v1",
        "title": "R. A. Fisher's Exact Test Revisited",
        "abstract": "This note provides a conceptual clarification of Ronald Aylmer Fisher's\n(1935) pioneering exact test in the context of the Lady Testing Tea experiment.\nIt unveils a critical implicit assumption in Fisher's calibration: the taster\nminimizes expected misclassification given fixed probabilistic information.\nWithout similar assumptions or an explicit alternative hypothesis, the\nrationale behind Fisher's specification of the rejection region remains\nunclear.",
        "authors": [
            "Martin Mugnier"
        ],
        "categories": "econ.EM",
        "published": "2024-07-09T22:09:14Z",
        "updated": "2024-07-09T22:09:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.07217v1",
        "title": "The Hidden Subsidy of the Affordable Care Act",
        "abstract": "Under the ACA, the federal government paid a substantially larger share of\nmedical costs of newly eligible Medicaid enrollees than previously eligible\nones. States could save up to 100% of their per-enrollee costs by reclassifying\noriginal enrollees into the newly eligible group. We examine whether this\nfiscal incentive changed states' enrollment practices. We find that Medicaid\nexpansion caused large declines in the number of beneficiaries enrolled in the\noriginal Medicaid population, suggesting widespread reclassifications. In 2019\nalone, this phenomenon affected 4.4 million Medicaid enrollees at a federal\ncost of $8.3 billion. Our results imply that reclassifications inflated the\nfederal cost of Medicaid expansion by 18.2%.",
        "authors": [
            "Liam Sigaud",
            "Markus Bjoerkheim",
            "Vitor Melo"
        ],
        "categories": "econ.EM",
        "published": "2024-07-09T20:27:42Z",
        "updated": "2024-07-09T20:27:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.06883v1",
        "title": "Dealing with idiosyncratic cross-correlation when constructing confidence regions for PC factors",
        "abstract": "In this paper, we propose a computationally simple estimator of the\nasymptotic covariance matrix of the Principal Components (PC) factors valid in\nthe presence of cross-correlated idiosyncratic components. The proposed\nestimator of the asymptotic Mean Square Error (MSE) of PC factors is based on\nadaptive thresholding the sample covariances of the id iosyncratic residuals\nwith the threshold based on their individual variances. We compare the nite\nsample performance of condence regions for the PC factors obtained using the\nproposed asymptotic MSE with those of available extant asymptotic and bootstrap\nregions and show that the former beats all alternative procedures for a wide\nvariety of idiosyncratic cross-correlation structures.",
        "authors": [
            "Diego Fresoli",
            "Pilar Poncela",
            "Esther Ruiz"
        ],
        "categories": "econ.EM",
        "published": "2024-07-09T14:11:09Z",
        "updated": "2024-07-09T14:11:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.06733v1",
        "title": "Causes and Electoral Consequences of Political Assassinations: The Role of Organized Crime in Mexico",
        "abstract": "Mexico has experienced a notable surge in assassinations of political\ncandidates and mayors. This article argues that these killings are largely\ndriven by organized crime, aiming to influence candidate selection, control\nlocal governments for rent-seeking, and retaliate against government\ncrackdowns. Using a new dataset of political assassinations in Mexico from 2000\nto 2021 and instrumental variables, we address endogeneity concerns in the\nlocation and timing of government crackdowns. Our instruments include\nhistorical Chinese immigration patterns linked to opium cultivation in Mexico,\nlocal corn prices, and U.S. illicit drug prices. The findings reveal that\ncandidates in municipalities near oil pipelines face an increased risk of\nassassination due to drug trafficking organizations expanding into oil theft,\nparticularly during elections and fuel price hikes. Government arrests or\nkillings of organized crime members trigger retaliatory violence, further\nendangering incumbent mayors. This political violence has a negligible impact\non voter turnout, as it targets politicians rather than voters. However, voter\nturnout increases in areas where authorities disrupt drug smuggling, raising\nthe chances of the local party being re-elected. These results offer new\ninsights into how criminal groups attempt to capture local governments and the\nimplications for democracy under criminal governance.",
        "authors": [
            "Roxana Guti\u00e9rrez-Romero",
            "Nayely Iturbe"
        ],
        "categories": "econ.EM",
        "published": "2024-07-09T10:21:07Z",
        "updated": "2024-07-09T10:21:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.06722v1",
        "title": "Femicide Laws, Unilateral Divorce, and Abortion Decriminalization Fail to Stop Women's Killings in Mexico",
        "abstract": "This paper evaluates the effectiveness of femicide laws in combating\ngender-based killings of women, a major cause of premature female mortality\nglobally. Focusing on Mexico, a pioneer in adopting such legislation, the paper\nleverages variations in the enactment of femicide laws and associated prison\nsentences across states. Using the difference-in-difference estimator, the\nanalysis reveals that these laws have not significantly affected the incidence\nof femicides, homicides of women, or reports of women who have disappeared.\nThese findings remain robust even when accounting for differences in prison\nsentencing, whether states also implemented unilateral divorce laws, or\ndecriminalized abortion alongside femicide legislation. The results suggest\nthat legislative measures are insufficient to address violence against women in\nsettings where impunity prevails.",
        "authors": [
            "Roxana Guti\u00e9rrez-Romero"
        ],
        "categories": "econ.EM",
        "published": "2024-07-09T09:52:20Z",
        "updated": "2024-07-09T09:52:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.06387v1",
        "title": "Conditional Rank-Rank Regression",
        "abstract": "Rank-rank regressions are widely used in economic research to evaluate\nphenomena such as intergenerational income persistence or mobility. However,\nwhen covariates are incorporated to capture between-group persistence, the\nresulting coefficients can be difficult to interpret as such. We propose the\nconditional rank-rank regression, which uses conditional ranks instead of\nunconditional ranks, to measure average within-group income persistence. This\nproperty is analogous to that of the unconditional rank-rank regression that\nmeasures the overall income persistence. The difference between conditional and\nunconditional rank-rank regression coefficients therefore can measure\nbetween-group persistence. We develop a flexible estimation approach using\ndistribution regression and establish a theoretical framework for large sample\ninference. An empirical study on intergenerational income mobility in\nSwitzerland demonstrates the advantages of this approach. The study reveals\nstronger intergenerational persistence between fathers and sons compared to\nfathers and daughters, with the within-group persistence explaining 62% of the\noverall income persistence for sons and 52% for daughters. Families of small\nsize or with highly educated fathers exhibit greater persistence in passing on\ntheir economic status.",
        "authors": [
            "Victor Chernozhukov",
            "Iv\u00e1n Fern\u00e1ndez-Val",
            "Jonas Meier",
            "Aico van Vuuren",
            "Francis Vella"
        ],
        "categories": "econ.EM",
        "published": "2024-07-08T20:56:17Z",
        "updated": "2024-07-08T20:56:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.05624v1",
        "title": "Dynamic Matrix Factor Models for High Dimensional Time Series",
        "abstract": "Matrix time series, which consist of matrix-valued data observed over time,\nare prevalent in various fields such as economics, finance, and engineering.\nSuch matrix time series data are often observed in high dimensions. Matrix\nfactor models are employed to reduce the dimensionality of such data, but they\nlack the capability to make predictions without specified dynamics in the\nlatent factor process. To address this issue, we propose a two-component\ndynamic matrix factor model that extends the standard matrix factor model by\nincorporating a matrix autoregressive structure for the low-dimensional latent\nfactor process. This two-component model injects prediction capability to the\nmatrix factor model and provides deeper insights into the dynamics of\nhigh-dimensional matrix time series. We present the estimation procedures of\nthe model and their theoretical properties, as well as empirical analysis of\nthe estimation procedures via simulations, and a case study of New York city\ntaxi data, demonstrating the performance and usefulness of the model.",
        "authors": [
            "Ruofan Yu",
            "Rong Chen",
            "Han Xiao",
            "Yuefeng Han"
        ],
        "categories": "stat.ME",
        "published": "2024-07-08T05:32:46Z",
        "updated": "2024-07-08T05:32:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.05596v1",
        "title": "Methodology for Calculating CO2 Absorption by Tree Planting for Greening Projects",
        "abstract": "In order to explore the possibility of carbon credits for greening projects,\nwhich play an important role in climate change mitigation, this paper examines\na formula for estimating the amount of carbon fixation for greening activities\nin urban areas through tree planting. The usefulness of the formula studied was\nexamined by conducting calculations based on actual data through measurements\nmade by on-site surveys of a greening companie. A series of calculation results\nsuggest that this formula may be useful. Recognizing carbon credits for green\nbusinesses for the carbon sequestration of their projects is an important\nincentive not only as part of environmental improvement and climate change\naction, but also to improve the health and well-being of local communities and\nto generate economic benefits. This study is a pioneering exploration of the\nmethodology.",
        "authors": [
            "Kento Ichii",
            "Toshiki Muraoka",
            "Nobumichi Shinohara",
            "Shunsuke Managi",
            "Shutaro Takeda"
        ],
        "categories": "econ.EM",
        "published": "2024-07-08T04:18:39Z",
        "updated": "2024-07-08T04:18:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.05372v1",
        "title": "A Convexified Matching Approach to Imputation and Individualized Inference",
        "abstract": "We introduce a new convexified matching method for missing value imputation\nand individualized inference inspired by computational optimal transport. Our\nmethod integrates favorable features from mainstream imputation approaches:\noptimal matching, regression imputation, and synthetic control. We impute\ncounterfactual outcomes based on convex combinations of observed outcomes,\ndefined based on an optimal coupling between the treated and control data sets.\nThe optimal coupling problem is considered a convex relaxation to the\ncombinatorial optimal matching problem. We estimate granular-level individual\ntreatment effects while maintaining a desirable aggregate-level summary by\nproperly constraining the coupling. We construct transparent, individual\nconfidence intervals for the estimated counterfactual outcomes. We devise fast\niterative entropic-regularized algorithms to solve the optimal coupling problem\nthat scales favorably when the number of units to match is large. Entropic\nregularization plays a crucial role in both inference and computation; it helps\ncontrol the width of the individual confidence intervals and design fast\noptimization algorithms.",
        "authors": [
            "YoonHaeng Hur",
            "Tengyuan Liang"
        ],
        "categories": "econ.EM",
        "published": "2024-07-07T13:52:46Z",
        "updated": "2024-07-07T13:52:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.09565v2",
        "title": "A Short Note on Event-Study Synthetic Difference-in-Differences Estimators",
        "abstract": "I propose an event study extension of Synthetic Difference-in-Differences\n(SDID) estimators. I show that, in simple and staggered adoption designs,\nestimators from Arkhangelsky et al. (2021) can be disaggregated into dynamic\ntreatment effect estimators, comparing the lagged outcome differentials of\ntreated and synthetic controls to their pre-treatment average. Estimators\npresented in this note can be computed using the sdid_event Stata package.",
        "authors": [
            "Diego Ciccia"
        ],
        "categories": "econ.EM",
        "published": "2024-07-05T16:38:07Z",
        "updated": "2024-11-01T16:23:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.04448v1",
        "title": "Learning control variables and instruments for causal analysis in observational data",
        "abstract": "This study introduces a data-driven, machine learning-based method to detect\nsuitable control variables and instruments for assessing the causal effect of a\ntreatment on an outcome in observational data, if they exist. Our approach\ntests the joint existence of instruments, which are associated with the\ntreatment but not directly with the outcome (at least conditional on\nobservables), and suitable control variables, conditional on which the\ntreatment is exogenous, and learns the partition of instruments and control\nvariables from the observed data. The detection of sets of instruments and\ncontrol variables relies on the condition that proper instruments are\nconditionally independent of the outcome given the treatment and suitable\ncontrol variables. We establish the consistency of our method for detecting\ncontrol variables and instruments under certain regularity conditions,\ninvestigate the finite sample performance through a simulation study, and\nprovide an empirical application to labor market data from the Job Corps study.",
        "authors": [
            "Nicolas Apfel",
            "Julia Hatamyar",
            "Martin Huber",
            "Jannis Kueck"
        ],
        "categories": "econ.EM",
        "published": "2024-07-05T12:03:21Z",
        "updated": "2024-07-05T12:03:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.04437v1",
        "title": "Overeducation under different macroeconomic conditions: The case of Spanish university graduates",
        "abstract": "This paper examines the incidence and persistence of overeducation in the\nearly careers of Spanish university graduates. We investigate the role played\nby the business cycle and field of study and their interaction in shaping both\nphenomena. We also analyse the relevance of specific types of knowledge and\nskills as driving factors in reducing overeducation risk. We use data from the\nSurvey on the Labour Insertion of University Graduates (EILU) conducted by the\nSpanish National Statistics Institute in 2014 and 2019. The survey collects\nrich information on cohorts that graduated in the 2009/2010 and 2014/2015\nacademic years during the Great Recession and the subsequent economic recovery,\nrespectively. Our results show, first, the relevance of the economic scenario\nwhen graduates enter the labour market. Graduation during a recession increased\novereducation risk and persistence. Second, a clear heterogeneous pattern\noccurs across fields of study, with health sciences graduates displaying better\nperformance in terms of both overeducation incidence and persistence and less\nimpact of the business cycle. Third, we find evidence that some transversal\nskills (language, IT, management) can help to reduce overeducation risk in the\nabsence of specific knowledge required for the job, thus indicating some kind\nof compensatory role. Finally, our findings have important policy implications.\nOvereducation, and more importantly overeducation persistence, imply a\nnon-neglectable misallocation of resources. Therefore, policymakers need to\naddress this issue in the design of education and labour market policies.",
        "authors": [
            "Maite Bl\u00e1zquez Cuesta",
            "Marco A. P\u00e9rez Navarro",
            "Roc\u00edo S\u00e1nchez-Mangas"
        ],
        "categories": "econ.EM",
        "published": "2024-07-05T11:34:48Z",
        "updated": "2024-07-05T11:34:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.03725v2",
        "title": "Under the null of valid specification, pre-tests cannot make post-test inference liberal",
        "abstract": "Consider a parameter of interest, which can be consistently estimated under\nsome conditions. Suppose also that we can at least partly test these conditions\nwith specification tests. We consider the common practice of conducting\ninference on the parameter of interest conditional on not rejecting these\ntests. We show that if the tested conditions hold, conditional inference is\nvalid, though possibly conservative. This holds generally, without imposing any\nassumption on the asymptotic dependence between the estimator of the parameter\nof interest and the specification test.",
        "authors": [
            "Cl\u00e9ment de Chaisemartin",
            "Xavier D'Haultf\u0153uille"
        ],
        "categories": "econ.EM",
        "published": "2024-07-04T08:15:19Z",
        "updated": "2024-09-30T04:41:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.03616v3",
        "title": "When can weak latent factors be statistically inferred?",
        "abstract": "This article establishes a new and comprehensive estimation and inference\ntheory for principal component analysis (PCA) under the weak factor model that\nallow for cross-sectional dependent idiosyncratic components under the nearly\nminimal factor strength relative to the noise level or signal-to-noise ratio.\nOur theory is applicable regardless of the relative growth rate between the\ncross-sectional dimension $N$ and temporal dimension $T$. This more realistic\nassumption and noticeable result require completely new technical device, as\nthe commonly-used leave-one-out trick is no longer applicable to the case with\ncross-sectional dependence. Another notable advancement of our theory is on PCA\ninference $ - $ for example, under the regime where $N\\asymp T$, we show that\nthe asymptotic normality for the PCA-based estimator holds as long as the\nsignal-to-noise ratio (SNR) grows faster than a polynomial rate of $\\log N$.\nThis finding significantly surpasses prior work that required a polynomial rate\nof $N$. Our theory is entirely non-asymptotic, offering finite-sample\ncharacterizations for both the estimation error and the uncertainty level of\nstatistical inference. A notable technical innovation is our closed-form\nfirst-order approximation of PCA-based estimator, which paves the way for\nvarious statistical tests. Furthermore, we apply our theories to design\neasy-to-implement statistics for validating whether given factors fall in the\nlinear spans of unknown latent factors, testing structural breaks in the factor\nloadings for an individual unit, checking whether two units have the same risk\nexposures, and constructing confidence intervals for systematic risks. Our\nempirical studies uncover insightful correlations between our test results and\neconomic cycles.",
        "authors": [
            "Jianqing Fan",
            "Yuling Yan",
            "Yuheng Zheng"
        ],
        "categories": "stat.ME",
        "published": "2024-07-04T03:59:52Z",
        "updated": "2024-09-30T21:26:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.03279v2",
        "title": "Finely Stratified Rerandomization Designs",
        "abstract": "We study estimation and inference on causal parameters under finely\nstratified rerandomization designs, which use baseline covariates to match\nunits into groups (e.g. matched pairs), then rerandomize within-group treatment\nassignments until a balance criterion is satisfied. We show that finely\nstratified rerandomization does partially linear regression adjustment \"by\ndesign,\" providing nonparametric control over the stratified covariates and\nlinear control over the rerandomized covariates. We introduce several new\nrerandomization schemes, allowing for imbalance metrics based on nonlinear\nestimators. We also propose a novel minimax scheme that uses pilot data or\nprior information to minimize the computational cost of rerandomization,\nsubject to a strict bound on statistical efficiency. While the asymptotic\ndistribution of generalized method of moments (GMM) estimators under stratified\nrerandomization is generically non-normal, we show how to restore asymptotic\nnormality using ex-post linear adjustment tailored to the stratification. This\nenables simple asymptotically exact inference on superpopulation parameters, as\nwell as efficient conservative inference on finite population parameters.",
        "authors": [
            "Max Cytrynbaum"
        ],
        "categories": "econ.EM",
        "published": "2024-07-03T17:04:44Z",
        "updated": "2024-07-28T14:06:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.03265v2",
        "title": "Wild inference for wild SVARs with application to heteroscedasticity-based IV",
        "abstract": "Structural vector autoregressions are used to compute impulse response\nfunctions (IRF) for persistent data. Existing multiple-parameter inference\nrequires cumbersome pretesting for unit roots, cointegration, and trends with\nsubsequent stationarization. To avoid pretesting, we propose a novel\n\\emph{dependent wild bootstrap} procedure for simultaneous inference on IRF\nusing local projections (LP) estimated in levels in possibly\n\\emph{nonstationary} and \\emph{heteroscedastic} SVARs. The bootstrap also\nallows efficient smoothing of LP estimates.\n  We study IRF to US monetary policy identified using FOMC meetings count as an\ninstrument for heteroscedasticity of monetary shocks. We validate our method\nusing DSGE model simulations and alternative SVAR methods.",
        "authors": [
            "Bulat Gafarov",
            "Madina Karamysheva",
            "Andrey Polbin",
            "Anton Skrobotov"
        ],
        "categories": "econ.EM",
        "published": "2024-07-03T16:53:37Z",
        "updated": "2024-11-22T21:37:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.02262v1",
        "title": "Conditional Forecasts in Large Bayesian VARs with Multiple Equality and Inequality Constraints",
        "abstract": "Conditional forecasts, i.e. projections of a set of variables of interest on\nthe future paths of some other variables, are used routinely by empirical\nmacroeconomists in a number of applied settings. In spite of this, the existing\nalgorithms used to generate conditional forecasts tend to be very\ncomputationally intensive, especially when working with large Vector\nAutoregressions or when multiple linear equality and inequality constraints are\nimposed at once. We introduce a novel precision-based sampler that is fast,\nscales well, and yields conditional forecasts from linear equality and\ninequality constraints. We show in a simulation study that the proposed method\nproduces forecasts that are identical to those from the existing algorithms but\nin a fraction of the time. We then illustrate the performance of our method in\na large Bayesian Vector Autoregression where we simultaneously impose a mix of\nlinear equality and inequality constraints on the future trajectories of key US\nmacroeconomic indicators over the 2020--2022 period.",
        "authors": [
            "Joshua C. C. Chan",
            "Davide Pettenuzzo",
            "Aubrey Poon",
            "Dan Zhu"
        ],
        "categories": "econ.EM",
        "published": "2024-07-02T13:38:18Z",
        "updated": "2024-07-02T13:38:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.02183v1",
        "title": "How do financial variables impact public debt growth in China? An empirical study based on Markov regime-switching model",
        "abstract": "The deep financial turmoil in China caused by the COVID-19 pandemic has\nexacerbated fiscal shocks and soaring public debt levels, which raises concerns\nabout the stability and sustainability of China's public debt growth in the\nfuture. This paper employs the Markov regime-switching model with time-varying\ntransition probability (TVTP-MS) to investigate the growth pattern of China's\npublic debt and the impact of financial variables such as credit, house prices\nand stock prices on the growth of public debt. We identify two distinct regimes\nof China's public debt, i.e., the surge regime with high growth rate and high\nvolatility and the steady regime with low growth rate and low volatility. The\nmain results are twofold. On the one hand, an increase in the growth rate of\nthe financial variables helps to moderate the growth rate of public debt,\nwhereas the effects differ between the two regimes. More specifically, the\nimpacts of credit and house prices are significant in the surge regime, whereas\nstock prices affect public debt growth significantly in the steady regime. On\nthe other hand, a higher growth rate of financial variables also increases the\nprobability of public debt either staying in or switching to the steady regime.\nThese findings highlight the necessity of aligning financial adjustments with\nthe prevailing public debt regime when developing sustainable fiscal policies.",
        "authors": [
            "Tianbao Zhou",
            "Zhixin Liu",
            "Yingying Xu"
        ],
        "categories": "econ.EM",
        "published": "2024-07-02T11:43:11Z",
        "updated": "2024-07-02T11:43:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.00890v1",
        "title": "Macroeconomic Forecasting with Large Language Models",
        "abstract": "This paper presents a comparative analysis evaluating the accuracy of Large\nLanguage Models (LLMs) against traditional macro time series forecasting\napproaches. In recent times, LLMs have surged in popularity for forecasting due\nto their ability to capture intricate patterns in data and quickly adapt across\nvery different domains. However, their effectiveness in forecasting\nmacroeconomic time series data compared to conventional methods remains an area\nof interest. To address this, we conduct a rigorous evaluation of LLMs against\ntraditional macro forecasting methods, using as common ground the FRED-MD\ndatabase. Our findings provide valuable insights into the strengths and\nlimitations of LLMs in forecasting macroeconomic time series, shedding light on\ntheir applicability in real-world scenarios",
        "authors": [
            "Andrea Carriero",
            "Davide Pettenuzzo",
            "Shubhranshu Shekhar"
        ],
        "categories": "econ.EM",
        "published": "2024-07-01T01:25:26Z",
        "updated": "2024-07-01T01:25:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.19956v3",
        "title": "Three Scores and 15 Years (1948-2023) of Rao's Score Test: A Brief History",
        "abstract": "Rao (1948) introduced the score test statistic as an alternative to the\nlikelihood ratio and Wald test statistics. In spite of the optimality\nproperties of the score statistic shown in Rao and Poti (1946), the Rao score\n(RS) test remained unnoticed for almost 20 years. Today, the RS test is part of\nthe ``Holy Trinity'' of hypothesis testing and has found its place in the\nStatistics and Econometrics textbooks and related software. Reviewing the\nhistory of the RS test we note that remarkable test statistics proposed in the\nliterature earlier or around the time of Rao (1948) mostly from intuition, such\nas Pearson (1900) goodness-fit-test, Moran (1948) I test for spatial dependence\nand Durbin and Watson (1950) test for serial correlation, can be given RS test\nstatistic interpretation. At the same time, recent developments in the robust\nhypothesis testing under certain forms of misspecification, make the RS test an\nactive area of research in Statistics and Econometrics. From our brief account\nof the history the RS test we conclude that its impact in science goes far\nbeyond its calendar starting point with promising future research activities\nfor many years to come.",
        "authors": [
            "Anil K. Bera",
            "Yannis Bilias"
        ],
        "categories": "econ.EM",
        "published": "2024-06-28T14:41:13Z",
        "updated": "2024-10-06T11:15:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.19702v1",
        "title": "Vector AutoRegressive Moving Average Models: A Review",
        "abstract": "Vector AutoRegressive Moving Average (VARMA) models form a powerful and\ngeneral model class for analyzing dynamics among multiple time series. While\nVARMA models encompass the Vector AutoRegressive (VAR) models, their popularity\nin empirical applications is dominated by the latter. Can this phenomenon be\nexplained fully by the simplicity of VAR models? Perhaps many users of VAR\nmodels have not fully appreciated what VARMA models can provide. The goal of\nthis review is to provide a comprehensive resource for researchers and\npractitioners seeking insights into the advantages and capabilities of VARMA\nmodels. We start by reviewing the identification challenges inherent to VARMA\nmodels thereby encompassing classical and modern identification schemes and we\ncontinue along the same lines regarding estimation, specification and diagnosis\nof VARMA models. We then highlight the practical utility of VARMA models in\nterms of Granger Causality analysis, forecasting and structural analysis as\nwell as recent advances and extensions of VARMA models to further facilitate\ntheir adoption in practice. Finally, we discuss some interesting future\nresearch directions where VARMA models can fulfill their potentials in\napplications as compared to their subclass of VAR models.",
        "authors": [
            "Marie-Christine D\u00fcker",
            "David S. Matteson",
            "Ruey S. Tsay",
            "Ines Wilms"
        ],
        "categories": "stat.ME",
        "published": "2024-06-28T07:26:17Z",
        "updated": "2024-06-28T07:26:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.19033v1",
        "title": "Factor multivariate stochastic volatility models of high dimension",
        "abstract": "Building upon the pertinence of the factor decomposition to break the curse\nof dimensionality inherent to multivariate volatility processes, we develop a\nfactor model-based multivariate stochastic volatility (fMSV) framework that\nrelies on two viewpoints: sparse approximate factor model and sparse factor\nloading matrix. We propose a two-stage estimation procedure for the fMSV model:\nthe first stage obtains the estimators of the factor model, and the second\nstage estimates the MSV part using the estimated common factor variables. We\nderive the asymptotic properties of the estimators. Simulated experiments are\nperformed to assess the forecasting performances of the covariance matrices.\nThe empirical analysis based on vectors of asset returns illustrates that the\nforecasting performances of the fMSV models outperforms competing conditional\ncovariance models.",
        "authors": [
            "Benjamin Poignard",
            "Manabu Asai"
        ],
        "categories": "econ.EM",
        "published": "2024-06-27T09:39:10Z",
        "updated": "2024-06-27T09:39:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.18913v3",
        "title": "A Note on Identification of Match Fixed Effects as Interpretable Unobserved Match Affinity",
        "abstract": "We highlight that match fixed effects, represented by the coefficients of\ninteraction terms involving dummy variables for two elements, lack\nidentification without specific restrictions on parameters. Consequently, the\ncoefficients typically reported as relative match fixed effects by statistical\nsoftware are not interpretable. To address this, we establish normalization\nconditions that enable identification of match fixed effect parameters as\ninterpretable indicators of unobserved match affinity, facilitating comparisons\namong observed matches. Using data from middle school students in the 2007\nTrends in International Mathematics and Science Study (TIMSS), we highlight the\ndistribution of comparable match fixed effects within a specific school.",
        "authors": [
            "Suguru Otani",
            "Tohya Sugano"
        ],
        "categories": "econ.EM",
        "published": "2024-06-27T05:57:14Z",
        "updated": "2024-08-21T02:38:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2407.08750v2",
        "title": "Online Distributional Regression",
        "abstract": "Large-scale streaming data are common in modern machine learning applications\nand have led to the development of online learning algorithms. Many fields,\nsuch as supply chain management, weather and meteorology, energy markets, and\nfinance, have pivoted towards using probabilistic forecasts, which yields the\nneed not only for accurate learning of the expected value but also for learning\nthe conditional heteroskedasticity and conditional distribution moments.\nAgainst this backdrop, we present a methodology for online estimation of\nregularized, linear distributional models. The proposed algorithm is based on a\ncombination of recent developments for the online estimation of LASSO models\nand the well-known GAMLSS framework. We provide a case study on day-ahead\nelectricity price forecasting, in which we show the competitive performance of\nthe incremental estimation combined with strongly reduced computational effort.\nOur algorithms are implemented in a computationally efficient Python package.",
        "authors": [
            "Simon Hirsch",
            "Jonathan Berrisch",
            "Florian Ziel"
        ],
        "categories": "stat.ML",
        "published": "2024-06-26T16:04:49Z",
        "updated": "2024-08-21T11:43:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.17972v2",
        "title": "LABOR-LLM: Language-Based Occupational Representations with Large Language Models",
        "abstract": "Vafa et al. (2024) introduced a transformer-based econometric model, CAREER,\nthat predicts a worker's next job as a function of career history (an\n\"occupation model\"). CAREER was initially estimated (\"pre-trained\") using a\nlarge, unrepresentative resume dataset, which served as a \"foundation model,\"\nand parameter estimation was continued (\"fine-tuned\") using data from a\nrepresentative survey. CAREER had better predictive performance than\nbenchmarks. This paper considers an alternative where the resume-based\nfoundation model is replaced by a large language model (LLM). We convert\ntabular data from the survey into text files that resemble resumes and\nfine-tune the LLMs using these text files with the objective to predict the\nnext token (word). The resulting fine-tuned LLM is used as an input to an\noccupation model. Its predictive performance surpasses all prior models. We\ndemonstrate the value of fine-tuning and further show that by adding more\ncareer data from a different population, fine-tuning smaller LLMs surpasses the\nperformance of fine-tuning larger models.",
        "authors": [
            "Susan Athey",
            "Herman Brunborg",
            "Tianyu Du",
            "Ayush Kanodia",
            "Keyon Vafa"
        ],
        "categories": "cs.LG",
        "published": "2024-06-25T23:07:18Z",
        "updated": "2024-12-11T06:39:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.17708v1",
        "title": "Forecast Relative Error Decomposition",
        "abstract": "We introduce a class of relative error decomposition measures that are\nwell-suited for the analysis of shocks in nonlinear dynamic models. They\ninclude the Forecast Relative Error Decomposition (FRED), Forecast Error\nKullback Decomposition (FEKD) and Forecast Error Laplace Decomposition (FELD).\nThese measures are favourable over the traditional Forecast Error Variance\nDecomposition (FEVD) because they account for nonlinear dependence in both a\nserial and cross-sectional sense. This is illustrated by applications to\ndynamic models for qualitative data, count data, stochastic volatility and\ncyberrisk.",
        "authors": [
            "Christian Gourieroux",
            "Quinlan Lee"
        ],
        "categories": "econ.EM",
        "published": "2024-06-25T16:46:55Z",
        "updated": "2024-06-25T16:46:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.17278v1",
        "title": "Estimation and Inference for CP Tensor Factor Models",
        "abstract": "High-dimensional tensor-valued data have recently gained attention from\nresearchers in economics and finance. We consider the estimation and inference\nof high-dimensional tensor factor models, where each dimension of the tensor\ndiverges. Our focus is on a factor model that admits CP-type tensor\ndecomposition, which allows for non-orthogonal loading vectors. Based on the\ncontemporary covariance matrix, we propose an iterative simultaneous projection\nestimation method. Our estimator is robust to weak dependence among factors and\nweak correlation across different dimensions in the idiosyncratic shocks. We\nestablish an inferential theory, demonstrating both consistency and asymptotic\nnormality under relaxed assumptions. Within a unified framework, we consider\ntwo eigenvalue ratio-based estimators for the number of factors in a tensor\nfactor model and justify their consistency. Through a simulation study and two\nempirical applications featuring sorted portfolios and international trade\nflows, we illustrate the advantages of our proposed estimator over existing\nmethodologies in the literature.",
        "authors": [
            "Bin Chen",
            "Yuefeng Han",
            "Qiyang Yu"
        ],
        "categories": "stat.ME",
        "published": "2024-06-25T04:58:33Z",
        "updated": "2024-06-25T04:58:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.17056v1",
        "title": "Efficient two-sample instrumental variable estimators with change points and near-weak identification",
        "abstract": "We consider estimation and inference in a linear model with endogenous\nregressors where the parameters of interest change across two samples. If the\nfirst-stage is common, we show how to use this information to obtain more\nefficient two-sample GMM estimators than the standard split-sample GMM, even in\nthe presence of near-weak instruments. We also propose two tests to detect\nchange points in the parameters of interest, depending on whether the\nfirst-stage is common or not. We derive the limiting distribution of these\ntests and show that they have non-trivial power even under weaker and possibly\ntime-varying identification patterns. The finite sample properties of our\nproposed estimators and testing procedures are illustrated in a series of\nMonte-Carlo experiments, and in an application to the open-economy New\nKeynesian Phillips curve. Our empirical analysis using US data provides strong\nsupport for a New Keynesian Phillips curve with incomplete pass-through and\nreveals important time variation in the relationship between inflation and\nexchange rate pass-through.",
        "authors": [
            "Bertille Antoine",
            "Otilia Boldea",
            "Niccolo Zaccaria"
        ],
        "categories": "econ.EM",
        "published": "2024-06-24T18:17:34Z",
        "updated": "2024-06-24T18:17:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.16221v1",
        "title": "F-FOMAML: GNN-Enhanced Meta-Learning for Peak Period Demand Forecasting with Proxy Data",
        "abstract": "Demand prediction is a crucial task for e-commerce and physical retail\nbusinesses, especially during high-stake sales events. However, the limited\navailability of historical data from these peak periods poses a significant\nchallenge for traditional forecasting methods. In this paper, we propose a\nnovel approach that leverages strategically chosen proxy data reflective of\npotential sales patterns from similar entities during non-peak periods,\nenriched by features learned from a graph neural networks (GNNs)-based\nforecasting model, to predict demand during peak events. We formulate the\ndemand prediction as a meta-learning problem and develop the Feature-based\nFirst-Order Model-Agnostic Meta-Learning (F-FOMAML) algorithm that leverages\nproxy data from non-peak periods and GNN-generated relational metadata to learn\nfeature-specific layer parameters, thereby adapting to demand forecasts for\npeak events. Theoretically, we show that by considering domain similarities\nthrough task-specific metadata, our model achieves improved generalization,\nwhere the excess risk decreases as the number of training tasks increases.\nEmpirical evaluations on large-scale industrial datasets demonstrate the\nsuperiority of our approach. Compared to existing state-of-the-art models, our\nmethod demonstrates a notable improvement in demand prediction accuracy,\nreducing the Mean Absolute Error by 26.24% on an internal vending machine\ndataset and by 1.04% on the publicly accessible JD.com dataset.",
        "authors": [
            "Zexing Xu",
            "Linjun Zhang",
            "Sitan Yang",
            "Rasoul Etesami",
            "Hanghang Tong",
            "Huan Zhang",
            "Jiawei Han"
        ],
        "categories": "cs.LG",
        "published": "2024-06-23T21:28:50Z",
        "updated": "2024-06-23T21:28:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.15702v1",
        "title": "Testing for Restricted Stochastic Dominance under Survey Nonresponse with Panel Data: Theory and an Evaluation of Poverty in Australia",
        "abstract": "This paper lays the groundwork for a unifying approach to stochastic\ndominance testing under survey nonresponse that integrates the partial\nidentification approach to incomplete data and design-based inference for\ncomplex survey data. We propose a novel inference procedure for restricted\n$s$th-order stochastic dominance, tailored to accommodate a broad spectrum of\nnonresponse assumptions. The method uses pseudo-empirical likelihood to\nformulate the test statistic and compares it to a critical value from the\nchi-squared distribution with one degree of freedom. We detail the procedure's\nasymptotic properties under both null and alternative hypotheses, establishing\nits uniform validity under the null and consistency against various\nalternatives. Using the Household, Income and Labour Dynamics in Australia\nsurvey, we demonstrate the procedure's utility in a sensitivity analysis of\ntemporal poverty comparisons among Australian households.",
        "authors": [
            "Rami V. Tabri",
            "Mathew J. Elias"
        ],
        "categories": "econ.EM",
        "published": "2024-06-22T01:22:11Z",
        "updated": "2024-06-22T01:22:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.15667v4",
        "title": "Identification and Estimation of Causal Effects in High-Frequency Event Studies",
        "abstract": "We provide precise conditions for nonparametric identification of causal\neffects by high-frequency event study regressions, which have been used widely\nin the recent macroeconomics, financial economics and political economy\nliteratures. The high-frequency event study method regresses changes in an\noutcome variable on a measure of unexpected changes in a policy variable in a\nnarrow time window around an event or a policy announcement (e.g., a 30-minute\nwindow around an FOMC announcement). We show that, contrary to popular belief,\nthe narrow size of the window is not sufficient for identification. Rather, the\npopulation regression coefficient identifies a causal estimand when (i) the\neffect of the policy shock on the outcome does not depend on the other shocks\n(separability) and (ii) the surprise component of the news or event dominates\nall other shocks that are present in the event window (relative exogeneity).\nTechnically, the latter condition requires the policy shock to have infinite\nvariance in the event window. Under these conditions, we establish the causal\nmeaning of the event study estimand corresponding to the regression coefficient\nand the consistency and asymptotic normality of the event study estimator.\nNotably, this standard linear regression estimator is robust to general forms\nof nonlinearity. We apply our results to Nakamura and Steinsson's (2018a)\nanalysis of the real economic effects of monetary policy, providing a simple\nempirical procedure to analyze the extent to which the standard event study\nestimator adequately estimates causal effects of interest.",
        "authors": [
            "Alessandro Casini",
            "Adam McCloskey"
        ],
        "categories": "econ.EM",
        "published": "2024-06-21T21:55:35Z",
        "updated": "2024-10-29T22:34:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.15311v1",
        "title": "The disruption index suffers from citation inflation and is confounded by shifts in scholarly citation practice",
        "abstract": "Measuring the rate of innovation in academia and industry is fundamental to\nmonitoring the efficiency and competitiveness of the knowledge economy. To this\nend, a disruption index (CD) was recently developed and applied to publication\nand patent citation networks (Wu et al., Nature 2019; Park et al., Nature\n2023). Here we show that CD systematically decreases over time due to secular\ngrowth in research and patent production, following two distinct mechanisms\nunrelated to innovation -- one behavioral and the other structural. Whereas the\nbehavioral explanation reflects shifts associated with techno-social factors\n(e.g. self-citation practices), the structural explanation follows from\n`citation inflation' (CI), an inextricable feature of real citation networks\nattributable to increasing reference list lengths, which causes CD to\nsystematically decrease. We demonstrate this causal link by way of mathematical\ndeduction, computational simulation, multi-variate regression, and\nquasi-experimental comparison of the disruptiveness of PNAS versus PNAS Plus\narticles, which differ only in their lengths. Accordingly, we analyze CD data\navailable in the SciSciNet database and find that disruptiveness incrementally\nincreased from 2005-2015, and that the negative relationship between disruption\nand team-size is remarkably small in overall magnitude effect size, and shifts\nfrom negative to positive for team size $\\geq$ 8 coauthors.",
        "authors": [
            "Alexander M. Petersen",
            "Felber Arroyave",
            "Fabio Pammolli"
        ],
        "categories": "cs.DL",
        "published": "2024-06-21T17:10:01Z",
        "updated": "2024-06-21T17:10:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.15288v2",
        "title": "Difference-in-Differences when Parallel Trends Holds Conditional on Covariates",
        "abstract": "In this paper, we study difference-in-differences identification and\nestimation strategies when the parallel trends assumption holds after\nconditioning on covariates. We consider empirically relevant settings where the\ncovariates can be time-varying, time-invariant, or both. We uncover a number of\nweaknesses of commonly used two-way fixed effects (TWFE) regressions in this\ncontext, even in applications with only two time periods. In addition to some\nweaknesses due to estimating linear regression models that are similar to cases\nwith cross-sectional data, we also point out a collection of additional issues\nthat we refer to as \\textit{hidden linearity bias} that arise because the\ntransformations used to eliminate the unit fixed effect also transform the\ncovariates (e.g., taking first differences can result in the estimating\nequation only including the change in covariates over time, not their level,\nand also drop time-invariant covariates altogether). We provide simple\ndiagnostics for assessing how susceptible a TWFE regression is to hidden\nlinearity bias based on reformulating the TWFE regression as a weighting\nestimator. Finally, we propose simple alternative estimation strategies that\ncan circumvent these issues.",
        "authors": [
            "Carolina Caetano",
            "Brantly Callaway"
        ],
        "categories": "econ.EM",
        "published": "2024-06-21T16:32:32Z",
        "updated": "2024-09-10T02:03:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.15157v1",
        "title": "MIDAS-QR with 2-Dimensional Structure",
        "abstract": "Mixed frequency data has been shown to improve the performance of\ngrowth-at-risk models in the literature. Most of the research has focused on\nimposing structure on the high-frequency lags when estimating MIDAS-QR models\nakin to what is done in mean models. However, only imposing structure on the\nlag-dimension can potentially induce quantile variation that would otherwise\nnot be there. In this paper we extend the framework by introducing structure on\nboth the lag dimension and the quantile dimension. In this way we are able to\nshrink unnecessary quantile variation in the high-frequency variables. This\nleads to more gradual lag profiles in both dimensions compared to the MIDAS-QR\nand UMIDAS-QR. We show that this proposed method leads to further gains in\nnowcasting and forecasting on a pseudo-out-of-sample exercise on US data.",
        "authors": [
            "Tibor Szendrei",
            "Arnab Bhattacharjee",
            "Mark E. Schaffer"
        ],
        "categories": "econ.EM",
        "published": "2024-06-21T14:01:45Z",
        "updated": "2024-06-21T14:01:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.15522v2",
        "title": "Statistical Inference and A/B Testing in Fisher Markets and Paced Auctions",
        "abstract": "We initiate the study of statistical inference and A/B testing for two market\nequilibrium models: linear Fisher market (LFM) equilibrium and first-price\npacing equilibrium (FPPE). LFM arises from fair resource allocation systems\nsuch as allocation of food to food banks and notification opportunities to\ndifferent types of notifications. For LFM, we assume that the data observed is\ncaptured by the classical finite-dimensional Fisher market equilibrium, and its\nsteady-state behavior is modeled by a continuous limit Fisher market. The\nsecond type of equilibrium we study, FPPE, arises from internet advertising\nwhere advertisers are constrained by budgets and advertising opportunities are\nsold via first-price auctions. For platforms that use pacing-based methods to\nsmooth out the spending of advertisers, FPPE provides a hindsight-optimal\nconfiguration of the pacing method. We propose a statistical framework for the\nFPPE model, in which a continuous limit FPPE models the steady-state behavior\nof the auction platform, and a finite FPPE provides the data to estimate\nprimitives of the limit FPPE. Both LFM and FPPE have an Eisenberg-Gale convex\nprogram characterization, the pillar upon which we derive our statistical\ntheory. We start by deriving basic convergence results for the finite market to\nthe limit market. We then derive asymptotic distributions, and construct\nconfidence intervals. Furthermore, we establish the asymptotic local minimax\noptimality of estimation based on finite markets. We then show that the theory\ncan be used for conducting statistically valid A/B testing on auction\nplatforms. Synthetic and semi-synthetic experiments verify the validity and\npracticality of our theory.",
        "authors": [
            "Luofeng Liao",
            "Christian Kroer"
        ],
        "categories": "cs.GT",
        "published": "2024-06-21T02:23:52Z",
        "updated": "2024-08-08T01:49:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.14380v3",
        "title": "Estimating Treatment Effects under Recommender Interference: A Structured Neural Networks Approach",
        "abstract": "Recommender systems are essential for content-sharing platforms by curating\npersonalized content. To evaluate updates to recommender systems targeting\ncontent creators, platforms frequently rely on creator-side randomized\nexperiments. The treatment effect measures the change in outcomes when a new\nalgorithm is implemented compared to the status quo. We show that the standard\ndifference-in-means estimator can lead to biased estimates due to recommender\ninterference that arises when treated and control creators compete for\nexposure. We propose a \"recommender choice model\" that describes which item\ngets exposed from a pool containing both treated and control items. By\ncombining a structural choice model with neural networks, this framework\ndirectly models the interference pathway while accounting for rich\nviewer-content heterogeneity. We construct a debiased estimator of the\ntreatment effect and prove it is $\\sqrt n$-consistent and asymptotically normal\nwith potentially correlated samples. We validate our estimator's empirical\nperformance with a field experiment on Weixin short-video platform. In addition\nto the standard creator-side experiment, we conduct a costly double-sided\nrandomization design to obtain a benchmark estimate free from interference\nbias. We show that the proposed estimator yields results comparable to the\nbenchmark, whereas the standard difference-in-means estimator can exhibit\nsignificant bias and even produce reversed signs.",
        "authors": [
            "Ruohan Zhan",
            "Shichao Han",
            "Yuchen Hu",
            "Zhenling Jiang"
        ],
        "categories": "econ.EM",
        "published": "2024-06-20T14:53:26Z",
        "updated": "2024-07-05T13:40:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.14145v1",
        "title": "Temperature in the Iberian Peninsula: Trend, seasonality, and heterogeneity",
        "abstract": "In this paper, we propose fitting unobserved component models to represent\nthe dynamic evolution of bivariate systems of centre and log-range temperatures\nobtained monthly from minimum/maximum temperatures observed at a given\nlocation. In doing so, the centre and log-range temperature are decomposed into\npotentially stochastic trends, seasonal, and transitory components. Since our\nmodel encompasses deterministic trends and seasonal components as limiting\ncases, we contribute to the debate on whether stochastic or deterministic\ncomponents better represent the trend and seasonal components. The methodology\nis implemented to centre and log-range temperature observed in four locations\nin the Iberian Peninsula, namely, Barcelona, Coru\\~{n}a, Madrid, and Seville.\nWe show that, at each location, the centre temperature can be represented by a\nsmooth integrated random walk with time-varying slope, while a stochastic level\nbetter represents the log-range. We also show that centre and log-range\ntemperature are unrelated. The methodology is then extended to simultaneously\nmodel centre and log-range temperature observed at several locations in the\nIberian Peninsula. We fit a multi-level dynamic factor model to extract\npotential commonalities among centre (log-range) temperature while also\nallowing for heterogeneity in different areas in the Iberian Peninsula. We show\nthat, although the commonality in trends of average temperature is\nconsiderable, the regional components are also relevant.",
        "authors": [
            "C. Vladimir Rodr\u00edguez-Caballero",
            "Esther Ruiz"
        ],
        "categories": "stat.AP",
        "published": "2024-06-20T09:35:39Z",
        "updated": "2024-06-20T09:35:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.14046v2",
        "title": "Estimating Time-Varying Parameters of Various Smoothness in Linear Models via Kernel Regression",
        "abstract": "We consider estimating nonparametric time-varying parameters in linear models\nusing kernel regression. Our contributions are twofold. First, We consider a\nbroad class of time-varying parameters including deterministic smooth\nfunctions, the rescaled random walk, structural breaks, the threshold model and\ntheir mixtures. We show that those time-varying parameters can be consistently\nestimated by kernel regression. Our analysis exploits the smoothness of the\ntime-varying parameter, which is quantified by a single parameter. The second\ncontribution is to reveal that the bandwidth used in kernel regression\ndetermines the trade-off between the rate of convergence and the size of the\nclass of time-varying parameters that can be estimated. We demonstrate that an\nimproper choice of the bandwidth yields biased estimation and provide a guide\non the bandwidth selection. An empirical application shows that the\nkernel-based estimator with a particular bandwidth choice can capture the\nrandom-walk dynamics in time-varying parameters.",
        "authors": [
            "Mikihito Nishi"
        ],
        "categories": "econ.EM",
        "published": "2024-06-20T07:09:48Z",
        "updated": "2024-10-12T14:38:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.13826v1",
        "title": "Testing identification in mediation and dynamic treatment models",
        "abstract": "We propose a test for the identification of causal effects in mediation and\ndynamic treatment models that is based on two sets of observed variables,\nnamely covariates to be controlled for and suspected instruments, building on\nthe test by Huber and Kueck (2022) for single treatment models. We consider\nmodels with a sequential assignment of a treatment and a mediator to assess the\ndirect treatment effect (net of the mediator), the indirect treatment effect\n(via the mediator), or the joint effect of both treatment and mediator. We\nestablish testable conditions for identifying such effects in observational\ndata. These conditions jointly imply (1) the exogeneity of the treatment and\nthe mediator conditional on covariates and (2) the validity of distinct\ninstruments for the treatment and the mediator, meaning that the instruments do\nnot directly affect the outcome (other than through the treatment or mediator)\nand are unconfounded given the covariates. Our framework extends to\npost-treatment sample selection or attrition problems when replacing the\nmediator by a selection indicator for observing the outcome, enabling joint\ntesting of the selectivity of treatment and attrition. We propose a machine\nlearning-based test to control for covariates in a data-driven manner and\nanalyze its finite sample performance in a simulation study. Additionally, we\napply our method to Slovak labor market data and find that our testable\nimplications are not rejected for a sequence of training programs typically\nconsidered in dynamic treatment evaluations.",
        "authors": [
            "Martin Huber",
            "Kevin Kloiber",
            "Lukas Laffers"
        ],
        "categories": "econ.EM",
        "published": "2024-06-19T20:45:15Z",
        "updated": "2024-06-19T20:45:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.13395v1",
        "title": "Bayesian Inference for Multidimensional Welfare Comparisons",
        "abstract": "Using both single-index measures and stochastic dominance concepts, we show\nhow Bayesian inference can be used to make multivariate welfare comparisons. A\nfour-dimensional distribution for the well-being attributes income, mental\nhealth, education, and happiness are estimated via Bayesian Markov chain Monte\nCarlo using unit-record data taken from the Household, Income and Labour\nDynamics in Australia survey. Marginal distributions of beta and gamma mixtures\nand discrete ordinal distributions are combined using a copula. Improvements in\nboth well-being generally and poverty magnitude are assessed using posterior\nmeans of single-index measures and posterior probabilities of stochastic\ndominance. The conditions for stochastic dominance depend on the class of\nutility functions that is assumed to define a social welfare function and the\nnumber of attributes in the utility function. Three classes of utility\nfunctions are considered, and posterior probabilities of dominance are computed\nfor one, two, and four-attribute utility functions for three time intervals\nwithin the period 2001 to 2019.",
        "authors": [
            "David Gunawan",
            "William Griffiths",
            "Duangkamon Chotikapanich"
        ],
        "categories": "econ.EM",
        "published": "2024-06-19T09:38:46Z",
        "updated": "2024-06-19T09:38:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.13122v1",
        "title": "Testing for Underpowered Literatures",
        "abstract": "How many experimental studies would have come to different conclusions had\nthey been run on larger samples? I show how to estimate the expected number of\nstatistically significant results that a set of experiments would have reported\nhad their sample sizes all been counterfactually increased by a chosen factor.\nThe estimator is consistent and asymptotically normal. Unlike existing methods,\nmy approach requires no assumptions about the distribution of true effects of\nthe interventions being studied other than continuity. This method includes an\nadjustment for publication bias in the reported t-scores. An application to\nrandomized controlled trials (RCTs) published in top economics journals finds\nthat doubling every experiment's sample size would only increase the power of\ntwo-sided t-tests by 7.2 percentage points on average. This effect is small and\nis comparable to the effect for systematic replication projects in laboratory\npsychology where previous studies enabled accurate power calculations ex ante.\nThese effects are both smaller than for non-RCTs. This comparison suggests that\nRCTs are on average relatively insensitive to sample size increases. The policy\nimplication is that grant givers should generally fund more experiments rather\nthan fewer, larger ones.",
        "authors": [
            "Stefan Faridani"
        ],
        "categories": "econ.EM",
        "published": "2024-06-19T00:31:41Z",
        "updated": "2024-06-19T00:31:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.11940v1",
        "title": "Model-Based Inference and Experimental Design for Interference Using Partial Network Data",
        "abstract": "The stable unit treatment value assumption states that the outcome of an\nindividual is not affected by the treatment statuses of others, however in many\nreal world applications, treatments can have an effect on many others beyond\nthe immediately treated. Interference can generically be thought of as mediated\nthrough some network structure. In many empirically relevant situations\nhowever, complete network data (required to adjust for these spillover effects)\nare too costly or logistically infeasible to collect. Partially or indirectly\nobserved network data (e.g., subsamples, aggregated relational data (ARD),\negocentric sampling, or respondent-driven sampling) reduce the logistical and\nfinancial burden of collecting network data, but the statistical properties of\ntreatment effect adjustments from these design strategies are only beginning to\nbe explored. In this paper, we present a framework for the estimation and\ninference of treatment effect adjustments using partial network data through\nthe lens of structural causal models. We also illustrate procedures to assign\ntreatments using only partial network data, with the goal of either minimizing\nestimator variance or optimally seeding. We derive single network asymptotic\nresults applicable to a variety of choices for an underlying graph model. We\nvalidate our approach using simulated experiments on observed graphs with\napplications to information diffusion in India and Malawi.",
        "authors": [
            "Steven Wilkins Reeves",
            "Shane Lubold",
            "Arun G. Chandrasekhar",
            "Tyler H. McCormick"
        ],
        "categories": "stat.ME",
        "published": "2024-06-17T17:27:18Z",
        "updated": "2024-06-17T17:27:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.19412v1",
        "title": "Dynamically Consistent Analysis of Realized Covariations in Term Structure Models",
        "abstract": "In this article we show how to analyze the covariation of bond prices\nnonparametrically and robustly, staying consistent with a general no-arbitrage\nsetting. This is, in particular, motivated by the problem of identifying the\nnumber of statistically relevant factors in the bond market under minimal\nconditions. We apply this method in an empirical study which suggests that a\nhigh number of factors is needed to describe the term structure evolution and\nthat the term structure of volatility varies over time.",
        "authors": [
            "Dennis Schroers"
        ],
        "categories": "q-fin.ST",
        "published": "2024-06-17T15:30:18Z",
        "updated": "2024-06-17T15:30:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.11467v1",
        "title": "Resilience of international oil trade networks under extreme event shock-recovery simulations",
        "abstract": "With the frequent occurrence of black swan events, global energy security\nsituation has become increasingly complex and severe. Assessing the resilience\nof the international oil trade network (iOTN) is crucial for evaluating its\nability to withstand extreme shocks and recover thereafter, ensuring energy\nsecurity. We overcomes the limitations of discrete historical data by\ndeveloping a simulation model for extreme event shock-recovery in the iOTNs. We\nintroduce network efficiency indicator to measure oil resource allocation\nefficiency and evaluate network performance. Then, construct a resilience index\nto explore the resilience of the iOTNs from dimensions of resistance and\nrecoverability. Our findings indicate that extreme events can lead to sharp\ndeclines in performance of the iOTNs, especially when economies with\nsignificant trading positions and relations suffer shocks. The upward trend in\nrecoverability and resilience reflects the self-organizing nature of the iOTNs,\ndemonstrating its capacity for optimizing its own structure and functionality.\nUnlike traditional energy security research based solely on discrete historical\ndata or resistance indicators, our model evaluates resilience from multiple\ndimensions, offering insights for global energy governance systems while\nproviding diverse perspectives for various economies to mitigate risks and\nuphold energy security.",
        "authors": [
            "Na Wei",
            "Wen-Jie Xie",
            "Wei-Xing Zhou"
        ],
        "categories": "econ.EM",
        "published": "2024-06-17T12:28:29Z",
        "updated": "2024-06-17T12:28:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.11308v1",
        "title": "Management Decisions in Manufacturing using Causal Machine Learning -- To Rework, or not to Rework?",
        "abstract": "In this paper, we present a data-driven model for estimating optimal rework\npolicies in manufacturing systems. We consider a single production stage within\na multistage, lot-based system that allows for optional rework steps. While the\nrework decision depends on an intermediate state of the lot and system, the\nfinal product inspection, and thus the assessment of the actual yield, is\ndelayed until production is complete. Repair steps are applied uniformly to the\nlot, potentially improving some of the individual items while degrading others.\nThe challenge is thus to balance potential yield improvement with the rework\ncosts incurred. Given the inherently causal nature of this decision problem, we\npropose a causal model to estimate yield improvement. We apply methods from\ncausal machine learning, in particular double/debiased machine learning (DML)\ntechniques, to estimate conditional treatment effects from data and derive\npolicies for rework decisions. We validate our decision model using real-world\ndata from opto-electronic semiconductor manufacturing, achieving a yield\nimprovement of 2 - 3% during the color-conversion process of white\nlight-emitting diodes (LEDs).",
        "authors": [
            "Philipp Schwarz",
            "Oliver Schacht",
            "Sven Klaassen",
            "Daniel Gr\u00fcnbaum",
            "Sebastian Imhof",
            "Martin Spindler"
        ],
        "categories": "cs.LG",
        "published": "2024-06-17T08:14:40Z",
        "updated": "2024-06-17T08:14:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.11046v1",
        "title": "Impact of the Availability of ChatGPT on Software Development: A Synthetic Difference in Differences Estimation using GitHub Data",
        "abstract": "Advancements in Artificial Intelligence, particularly with ChatGPT, have\nsignificantly impacted software development. Utilizing novel data from GitHub\nInnovation Graph, we hypothesize that ChatGPT enhances software production\nefficiency. Utilizing natural experiments where some governments banned\nChatGPT, we employ Difference-in-Differences (DID), Synthetic Control (SC), and\nSynthetic Difference-in-Differences (SDID) methods to estimate its effects. Our\nfindings indicate a significant positive impact on the number of git pushes,\nrepositories, and unique developers per 100,000 people, particularly for\nhigh-level, general purpose, and shell scripting languages. These results\nsuggest that AI tools like ChatGPT can substantially boost developer\nproductivity, though further analysis is needed to address potential downsides\nsuch as low quality code and privacy concerns.",
        "authors": [
            "Alexander Quispe",
            "Rodrigo Grijalba"
        ],
        "categories": "cs.SE",
        "published": "2024-06-16T19:11:15Z",
        "updated": "2024-06-16T19:11:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.10837v3",
        "title": "EM Estimation of Conditional Matrix Variate $t$ Distributions",
        "abstract": "Conditional matrix variate student $t$ distribution was introduced by\nBattulga (2024a). In this paper, we propose a new version of the conditional\nmatrix variate student $t$ distribution. The paper provides EM algorithms,\nwhich estimate parameters of the conditional matrix variate student $t$\ndistributions, including general cases and special cases with Minnesota prior.",
        "authors": [
            "Battulga Gankhuu"
        ],
        "categories": "econ.EM",
        "published": "2024-06-16T08:15:54Z",
        "updated": "2024-10-02T08:12:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.09521v1",
        "title": "Randomization Inference: Theory and Applications",
        "abstract": "We review approaches to statistical inference based on randomization.\nPermutation tests are treated as an important special case. Under a certain\ngroup invariance property, referred to as the ``randomization hypothesis,''\nrandomization tests achieve exact control of the Type I error rate in finite\nsamples. Although this unequivocal precision is very appealing, the range of\nproblems that satisfy the randomization hypothesis is somewhat limited. We show\nthat randomization tests are often asymptotically, or approximately, valid and\nefficient in settings that deviate from the conditions required for\nfinite-sample error control. When randomization tests fail to offer even\nasymptotic Type 1 error control, their asymptotic validity may be restored by\nconstructing an asymptotically pivotal test statistic. Randomization tests can\nthen provide exact error control for tests of highly structured hypotheses with\ngood performance in a wider class of problems. We give a detailed overview of\nseveral prominent applications of randomization tests, including two-sample\npermutation tests, regression, and conformal inference.",
        "authors": [
            "David M. Ritzwoller",
            "Joseph P. Romano",
            "Azeem M. Shaikh"
        ],
        "categories": "econ.EM",
        "published": "2024-06-13T18:17:03Z",
        "updated": "2024-06-13T18:17:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.09473v1",
        "title": "Multidimensional clustering in judge designs",
        "abstract": "Estimates in judge designs run the risk of being biased due to the many judge\nidentities that are implicitly or explicitly used as instrumental variables.\nThe usual method to analyse judge designs, via a leave-out mean instrument,\neliminates this many instrument bias only in case the data are clustered in at\nmost one dimension. What is left out in the mean defines this clustering\ndimension. How most judge designs cluster their standard errors, however,\nimplies that there are additional clustering dimensions, which makes that a\nmany instrument bias remains. We propose two estimators that are many\ninstrument bias free, also in multidimensional clustered judge designs. The\nfirst generalises the one dimensional cluster jackknife instrumental variable\nestimator, by removing from this estimator the additional bias terms due to the\nextra dependence in the data. The second models all but one clustering\ndimensions by fixed effects and we show how these numerous fixed effects can be\nremoved without introducing extra bias. A Monte-Carlo experiment and the\nrevisitation of two judge designs show the empirical relevance of properly\naccounting for multidimensional clustering in estimation.",
        "authors": [
            "Johannes W. Ligtenberg",
            "Tiemen Woutersen"
        ],
        "categories": "econ.EM",
        "published": "2024-06-13T07:44:19Z",
        "updated": "2024-06-13T07:44:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.08880v1",
        "title": "Jackknife inference with two-way clustering",
        "abstract": "For linear regression models with cross-section or panel data, it is natural\nto assume that the disturbances are clustered in two dimensions. However, the\nfinite-sample properties of two-way cluster-robust tests and confidence\nintervals are often poor. We discuss several ways to improve inference with\ntwo-way clustering. Two of these are existing methods for avoiding, or at least\nameliorating, the problem of undefined standard errors when a cluster-robust\nvariance matrix estimator (CRVE) is not positive definite. One is a new method\nthat always avoids the problem. More importantly, we propose a family of new\ntwo-way CRVEs based on the cluster jackknife. Simulations for models with\ntwo-way fixed effects suggest that, in many cases, the cluster-jackknife CRVE\ncombined with our new method yields surprisingly accurate inferences. We\nprovide a simple software package, twowayjack for Stata, that implements our\nrecommended variance estimator.",
        "authors": [
            "James G. MacKinnon",
            "Morten \u00d8rregaard Nielsen",
            "Matthew D. Webb"
        ],
        "categories": "econ.EM",
        "published": "2024-06-13T07:31:46Z",
        "updated": "2024-06-13T07:31:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.08419v2",
        "title": "Identification and Inference on Treatment Effects under Covariate-Adaptive Randomization and Imperfect Compliance",
        "abstract": "Randomized controlled trials (RCTs) frequently utilize covariate-adaptive\nrandomization (CAR) (e.g., stratified block randomization) and commonly suffer\nfrom imperfect compliance. This paper studies the identification and inference\nfor the average treatment effect (ATE) and the average treatment effect on the\ntreated (ATT) in such RCTs with a binary treatment.\n  We first develop characterizations of the identified sets for both estimands.\nSince data are generally not i.i.d. under CAR, these characterizations do not\nfollow from existing results. We then provide consistent estimators of the\nidentified sets and asymptotically valid confidence intervals for the\nparameters. Our asymptotic analysis leads to concrete practical recommendations\nregarding how to estimate the treatment assignment probabilities that enter in\nestimated bounds. In the case of the ATE, using sample analog assignment\nfrequencies is more efficient than using the true assignment probabilities. On\nthe contrary, using the true assignment probabilities is preferable for the\nATT.",
        "authors": [
            "Federico A. Bugni",
            "Mengsi Gao",
            "Filip Obradovic",
            "Amilcar Velez"
        ],
        "categories": "econ.EM",
        "published": "2024-06-12T17:03:16Z",
        "updated": "2024-06-20T18:01:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.08279v1",
        "title": "Positive and negative word of mouth in the United States",
        "abstract": "Word of mouth is a process by which consumers transmit positive or negative\nsentiment to other consumers about a business. While this process has long been\nrecognized as a type of promotion for businesses, the value of word of mouth is\nquestionable. This study will examine the various correlates of word of mouth\nto demographic variables, including the role of the trust of business owners.\nEducation level, region of residence, and income level were found to be\nsignificant predictors of positive word of mouth. Although the results\ngenerally suggest that the majority of respondents do not engage in word of\nmouth, there are valuable insights to be learned.",
        "authors": [
            "Shawn Berry"
        ],
        "categories": "econ.EM",
        "published": "2024-06-12T14:42:37Z",
        "updated": "2024-06-12T14:42:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.08041v1",
        "title": "HARd to Beat: The Overlooked Impact of Rolling Windows in the Era of Machine Learning",
        "abstract": "We investigate the predictive abilities of the heterogeneous autoregressive\n(HAR) model compared to machine learning (ML) techniques across an\nunprecedented dataset of 1,455 stocks. Our analysis focuses on the role of\nfitting schemes, particularly the training window and re-estimation frequency,\nin determining the HAR model's performance. Despite extensive hyperparameter\ntuning, ML models fail to surpass the linear benchmark set by HAR when\nutilizing a refined fitting approach for the latter. Moreover, the simplicity\nof HAR allows for an interpretable model with drastically lower computational\ncosts. We assess performance using QLIKE, MSE, and realized utility metrics,\nfinding that HAR consistently outperforms its ML counterparts when both rely\nsolely on realized volatility and VIX as predictors. Our results underscore the\nimportance of a correctly specified fitting scheme. They suggest that properly\nfitted HAR models provide superior forecasting accuracy, establishing robust\nguidelines for their practical application and use as a benchmark. This study\nnot only reaffirms the efficacy of the HAR model but also provides a critical\nperspective on the practical limitations of ML approaches in realized\nvolatility forecasting.",
        "authors": [
            "Francesco Audrino",
            "Jonathan Chassot"
        ],
        "categories": "q-fin.ST",
        "published": "2024-06-12T09:50:41Z",
        "updated": "2024-06-12T09:50:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.07809v1",
        "title": "Did Harold Zuercher Have Time-Separable Preferences?",
        "abstract": "This paper proposes an empirical model of dynamic discrete choice to allow\nfor non-separable time preferences, generalizing the well-known Rust (1987)\nmodel. Under weak conditions, we show the existence of value functions and\nhence well-defined optimal choices. We construct a contraction mapping of the\nvalue function and propose an estimation method similar to Rust's nested fixed\npoint algorithm. Finally, we apply the framework to the bus engine replacement\ndata. We improve the fit of the data with our general model and reject the null\nhypothesis that Harold Zuercher has separable time preferences. Misspecifying\nan agent's preference as time-separable when it is not leads to biased\ninferences about structure parameters (such as the agent's risk attitudes) and\nmisleading policy recommendations.",
        "authors": [
            "Jay Lu",
            "Yao Luo",
            "Kota Saito",
            "Yi Xin"
        ],
        "categories": "econ.EM",
        "published": "2024-06-12T02:03:57Z",
        "updated": "2024-06-12T02:03:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.06860v1",
        "title": "Cluster GARCH",
        "abstract": "We introduce a novel multivariate GARCH model with flexible convolution-t\ndistributions that is applicable in high-dimensional systems. The model is\ncalled Cluster GARCH because it can accommodate cluster structures in the\nconditional correlation matrix and in the tail dependencies. The expressions\nfor the log-likelihood function and its derivatives are tractable, and the\nlatter facilitate a score-drive model for the dynamic correlation structure. We\napply the Cluster GARCH model to daily returns for 100 assets and find it\noutperforms existing models, both in-sample and out-of-sample. Moreover, the\nconvolution-t distribution provides a better empirical performance than the\nconventional multivariate t-distribution.",
        "authors": [
            "Chen Tong",
            "Peter Reinhard Hansen",
            "Ilya Archakov"
        ],
        "categories": "econ.EM",
        "published": "2024-06-11T00:28:02Z",
        "updated": "2024-06-11T00:28:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.06804v1",
        "title": "Robustness to Missing Data: Breakdown Point Analysis",
        "abstract": "Missing data is pervasive in econometric applications, and rarely is it\nplausible that the data are missing (completely) at random. This paper proposes\na methodology for studying the robustness of results drawn from incomplete\ndatasets. Selection is measured as the squared Hellinger divergence between the\ndistributions of complete and incomplete observations, which has a natural\ninterpretation. The breakdown point is defined as the minimal amount of\nselection needed to overturn a given result. Reporting point estimates and\nlower confidence intervals of the breakdown point is a simple, concise way to\ncommunicate the robustness of a result. An estimator of the breakdown point of\na result drawn from a generalized method of moments model is proposed and shown\nroot-n consistent and asymptotically normal under mild assumptions. Lower\nconfidence intervals of the breakdown point are simple to construct. The paper\nconcludes with a simulation study illustrating the finite sample performance of\nthe estimators in several common models.",
        "authors": [
            "Daniel Ober-Reynolds"
        ],
        "categories": "econ.EM",
        "published": "2024-06-10T21:17:29Z",
        "updated": "2024-06-10T21:17:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.06768v1",
        "title": "Data-Driven Switchback Experiments: Theoretical Tradeoffs and Empirical Bayes Designs",
        "abstract": "We study the design and analysis of switchback experiments conducted on a\nsingle aggregate unit. The design problem is to partition the continuous time\nspace into intervals and switch treatments between intervals, in order to\nminimize the estimation error of the treatment effect. We show that the\nestimation error depends on four factors: carryover effects, periodicity,\nserially correlated outcomes, and impacts from simultaneous experiments. We\nderive a rigorous bias-variance decomposition and show the tradeoffs of the\nestimation error from these factors. The decomposition provides three new\ninsights in choosing a design: First, balancing the periodicity between treated\nand control intervals reduces the variance; second, switching less frequently\nreduces the bias from carryover effects while increasing the variance from\ncorrelated outcomes, and vice versa; third, randomizing interval start and end\npoints reduces both bias and variance from simultaneous experiments. Combining\nthese insights, we propose a new empirical Bayes design approach. This approach\nuses prior data and experiments for designing future experiments. We illustrate\nthis approach using real data from a ride-sharing platform, yielding a design\nthat reduces MSE by 33% compared to the status quo design used on the platform.",
        "authors": [
            "Ruoxuan Xiong",
            "Alex Chin",
            "Sean J. Taylor"
        ],
        "categories": "stat.ME",
        "published": "2024-06-10T20:06:53Z",
        "updated": "2024-06-10T20:06:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.05987v3",
        "title": "Data-Driven Real-time Coupon Allocation in the Online Platform",
        "abstract": "Traditionally, firms have offered coupons to customer groups at predetermined\ndiscount rates. However, advancements in machine learning and the availability\nof abundant customer data now enable platforms to provide real-time customized\ncoupons to individuals. In this study, we partner with Meituan, a leading\nshopping platform, to develop a real-time, end-to-end coupon allocation system\nthat is fast and effective in stimulating demand while adhering to marketing\nbudgets when faced with uncertain traffic from a diverse customer base.\nLeveraging comprehensive customer and product features, we estimate Conversion\nRates (CVR) under various coupon values and employ isotonic regression to\nensure the monotonicity of predicted CVRs with respect to coupon value. Using\ncalibrated CVR predictions as input, we propose a Lagrangian Dual-based\nalgorithm that efficiently determines optimal coupon values for each arriving\ncustomer within 50 milliseconds. We theoretically and numerically investigate\nthe model performance under parameter misspecifications and apply a control\nloop to adapt to real-time updated information, thereby better adhering to the\nmarketing budget. Finally, we demonstrate through large-scale field experiments\nand observational data that our proposed coupon allocation algorithm\noutperforms traditional approaches in terms of both higher conversion rates and\nincreased revenue. As of May 2024, Meituan has implemented our framework to\ndistribute coupons to over 100 million users across more than 110 major cities\nin China, resulting in an additional CNY 8 million in annual profit. We\ndemonstrate how to integrate a machine learning prediction model for estimating\ncustomer CVR, a Lagrangian Dual-based coupon value optimizer, and a control\nsystem to achieve real-time coupon delivery while dynamically adapting to\nrandom customer arrival patterns.",
        "authors": [
            "Jinglong Dai",
            "Hanwei Li",
            "Weiming Zhu",
            "Jianfeng Lin",
            "Binqiang Huang"
        ],
        "categories": "econ.EM",
        "published": "2024-06-10T03:06:09Z",
        "updated": "2024-06-17T07:49:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.05633v1",
        "title": "Heterogeneous Treatment Effects in Panel Data",
        "abstract": "We address a core problem in causal inference: estimating heterogeneous\ntreatment effects using panel data with general treatment patterns. Many\nexisting methods either do not utilize the potential underlying structure in\npanel data or have limitations in the allowable treatment patterns. In this\nwork, we propose and evaluate a new method that first partitions observations\ninto disjoint clusters with similar treatment effects using a regression tree,\nand then leverages the (assumed) low-rank structure of the panel data to\nestimate the average treatment effect for each cluster. Our theoretical results\nestablish the convergence of the resulting estimates to the true treatment\neffects. Computation experiments with semi-synthetic data show that our method\nachieves superior accuracy compared to alternative approaches, using a\nregression tree with no more than 40 leaves. Hence, our method provides more\naccurate and interpretable estimates than alternative methods.",
        "authors": [
            "Retsef Levi",
            "Elisabeth Paulson",
            "Georgia Perakis",
            "Emily Zhang"
        ],
        "categories": "stat.ML",
        "published": "2024-06-09T04:02:08Z",
        "updated": "2024-06-09T04:02:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.05548v1",
        "title": "Causal Interpretation of Regressions With Ranks",
        "abstract": "In studies of educational production functions or intergenerational mobility,\nit is common to transform the key variables into percentile ranks. Yet, it\nremains unclear what the regression coefficient estimates with ranks of the\noutcome or the treatment. In this paper, we derive effective causal estimands\nfor a broad class of commonly-used regression methods, including the ordinary\nleast squares (OLS), two-stage least squares (2SLS), difference-in-differences\n(DiD), and regression discontinuity designs (RDD). Specifically, we introduce a\nnovel primitive causal estimand, the Rank Average Treatment Effect (rank-ATE),\nand prove that it serves as the building block of the effective estimands of\nall the aforementioned econometrics methods. For 2SLS, DiD, and RDD, we show\nthat direct applications to outcome ranks identify parameters that are\ndifficult to interpret. To address this issue, we develop alternative methods\nto identify more interpretable causal parameters.",
        "authors": [
            "Lihua Lei"
        ],
        "categories": "econ.EM",
        "published": "2024-06-08T18:42:46Z",
        "updated": "2024-06-08T18:42:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.04191v2",
        "title": "Strong Approximations for Empirical Processes Indexed by Lipschitz Functions",
        "abstract": "This paper presents new uniform Gaussian strong approximations for empirical\nprocesses indexed by classes of functions based on $d$-variate random vectors\n($d\\geq1$). First, a uniform Gaussian strong approximation is established for\ngeneral empirical processes indexed by possibly Lipschitz functions, improving\non previous results in the literature. In the setting considered by Rio (1994),\nand if the function class is Lipschitzian, our result improves the\napproximation rate $n^{-1/(2d)}$ to $n^{-1/\\max\\{d,2\\}}$, up to a\n$\\operatorname{polylog}(n)$ term, where $n$ denotes the sample size.\nRemarkably, we establish a valid uniform Gaussian strong approximation at the\nrate $n^{-1/2}\\log n$ for $d=2$, which was previously known to be valid only\nfor univariate ($d=1$) empirical processes via the celebrated Hungarian\nconstruction (Koml\\'os et al., 1975). Second, a uniform Gaussian strong\napproximation is established for multiplicative separable empirical processes\nindexed by possibly Lipschitz functions, which addresses some outstanding\nproblems in the literature (Chernozhukov et al., 2014, Section 3). Finally, two\nother uniform Gaussian strong approximation results are presented when the\nfunction class is a sequence of Haar basis based on quasi-uniform partitions.\nApplications to nonparametric density and regression estimation are discussed.",
        "authors": [
            "Matias D. Cattaneo",
            "Ruiqi Rae Yu"
        ],
        "categories": "math.ST",
        "published": "2024-06-06T15:47:06Z",
        "updated": "2024-11-12T22:01:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.04133v1",
        "title": "GLOBUS: Global building renovation potential by 2070",
        "abstract": "Surpassing the two large emission sectors of transportation and industry, the\nbuilding sector accounted for 34% and 37% of global energy consumption and\ncarbon emissions in 2021, respectively. The building sector, the final piece to\nbe addressed in the transition to net-zero carbon emissions, requires a\ncomprehensive, multisectoral strategy for reducing emissions. Until now, the\nabsence of data on global building floorspace has impeded the measurement of\nbuilding carbon intensity (carbon emissions per floorspace) and the\nidentification of ways to achieve carbon neutrality for buildings. For this\nstudy, we develop a global building stock model (GLOBUS) to fill that data gap.\nOur study's primary contribution lies in providing a dataset of global building\nstock turnover using scenarios that incorporate various levels of building\nrenovation. By unifying the evaluation indicators, the dataset empowers\nbuilding science researchers to perform comparative analyses based on\nfloorspace. Specifically, the building stock dataset establishes a reference\nfor measuring carbon emission intensity and decarbonization intensity of\nbuildings within different countries. Further, we emphasize the sufficiency of\nexisting buildings by incorporating building renovation into the model.\nRenovation can minimize the need to expand the building stock, thereby\nbolstering decarbonization of the building sector.",
        "authors": [
            "Shufan Zhang",
            "Minda Ma",
            "Nan Zhou",
            "Jinyue Yan"
        ],
        "categories": "econ.EM",
        "published": "2024-06-06T14:51:39Z",
        "updated": "2024-06-06T14:51:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.03971v1",
        "title": "Comments on B. Hansen's Reply to \"A Comment on: `A Modern Gauss-Markov Theorem'\", and Some Related Discussion",
        "abstract": "In P\\\"otscher and Preinerstorfer (2022) and in the abridged version\nP\\\"otscher and Preinerstorfer (2024, published in Econometrica) we have tried\nto clear up the confusion introduced in Hansen (2022a) and in the earlier\nversions Hansen (2021a,b). Unfortunatelly, Hansen's (2024) reply to P\\\"otscher\nand Preinerstorfer (2024) further adds to the confusion. While we are already\nsomewhat tired of the matter, for the sake of the econometrics community we\nfeel compelled to provide clarification. We also add a comment on Portnoy\n(2023), a \"correction\" to Portnoy (2022), as well as on Lei and Wooldridge\n(2022).",
        "authors": [
            "Benedikt M. P\u00f6tscher"
        ],
        "categories": "econ.EM",
        "published": "2024-06-06T11:33:24Z",
        "updated": "2024-06-06T11:33:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.03321v1",
        "title": "Decision synthesis in monetary policy",
        "abstract": "The macroeconomy is a sophisticated dynamic system involving significant\nuncertainties that complicate modelling. In response, decision makers consider\nmultiple models that provide different predictions and policy recommendations\nwhich are then synthesized into a policy decision. In this setting, we\nintroduce and develop Bayesian predictive decision synthesis (BPDS) to\nformalize monetary policy decision processes. BPDS draws on recent developments\nin model combination and statistical decision theory that yield new\nopportunities in combining multiple models, emphasizing the integration of\ndecision goals, expectations and outcomes into the model synthesis process. Our\ncase study concerns central bank policy decisions about target interest rates\nwith a focus on implications for multi-step macroeconomic forecasting.",
        "authors": [
            "Tony Chernis",
            "Gary Koop",
            "Emily Tallman",
            "Mike West"
        ],
        "categories": "stat.ME",
        "published": "2024-06-05T14:33:51Z",
        "updated": "2024-06-05T14:33:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.03053v2",
        "title": "Identification of structural shocks in Bayesian VEC models with two-state Markov-switching heteroskedasticity",
        "abstract": "We develop a Bayesian framework for cointegrated structural VAR models\nidentified by two-state Markovian breaks in conditional covariances. The\nresulting structural VEC specification with Markov-switching heteroskedasticity\n(SVEC-MSH) is formulated in the so-called B-parameterization, in which the\nprior distribution is specified directly for the matrix of the instantaneous\nreactions of the endogenous variables to structural innovations. We discuss\nsome caveats pertaining to the identification conditions presented earlier in\nthe literature on stationary structural VAR-MSH models, and revise the\nrestrictions to actually ensure the unique global identification through the\ntwo-state heteroskedasticity. To enable the posterior inference in the proposed\nmodel, we design an MCMC procedure, combining the Gibbs sampler and the\nMetropolis-Hastings algorithm. The methodology is illustrated both with a\nsimulated as well as real-world data examples.",
        "authors": [
            "Justyna Wr\u00f3blewska",
            "\u0141ukasz Kwiatkowski"
        ],
        "categories": "econ.EM",
        "published": "2024-06-05T08:27:44Z",
        "updated": "2024-06-07T06:20:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.03022v1",
        "title": "Is local opposition taking the wind out of the energy transition?",
        "abstract": "Local opposition to the installation of renewable energy sources is a\npotential threat to the energy transition. Local communities tend to oppose the\nconstruction of energy plants due to the associated negative externalities (the\nso-called 'not in my backyard' or NIMBY phenomenon) according to widespread\nbelief, mostly based on anecdotal evidence. Using administrative data on wind\nturbine installation and electoral outcomes across municipalities located in\nthe South of Italy during 2000-19, we estimate the impact of wind turbines'\ninstallation on incumbent regional governments' electoral support during the\nnext elections. Our main findings, derived by a wind-speed based instrumental\nvariable strategy, point in the direction of a mild and not statistically\nsignificant electoral backlash for right-wing regional administrations and of a\nstrong and statistically significant positive reinforcement for left-wing\nregional administrations. Based on our analysis, the hypothesis of an electoral\neffect of NIMBY type of behavior in connection with the development of wind\nturbines appears not to be supported by the data.",
        "authors": [
            "Federica Daniele",
            "Guido de Blasio",
            "Alessandra Pasquini"
        ],
        "categories": "econ.EM",
        "published": "2024-06-05T07:38:55Z",
        "updated": "2024-06-05T07:38:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.02835v3",
        "title": "When does IV identification not restrict outcomes?",
        "abstract": "Many identification results in instrumental variables (IV) models hold\nwithout requiring any restrictions on the distribution of potential outcomes,\nor how those outcomes are correlated with selection behavior. This enables IV\nmodels to allow for arbitrary heterogeneity in treatment effects and the\npossibility of selection on gains in the outcome. I provide a necessary and\nsufficient condition for treatment effects to be point identified in a manner\nthat does not restrict outcomes when the instruments take a finite number of\nvalues. The condition generalizes the well-known LATE monotonicity assumption,\nand unifies a wide variety of other known IV identification results. The result\nalso yields a brute-force approach to reveal all selection models that allow\nfor point identification of treatment effects without restricting outcomes, and\nthen enumerate all of the identified parameters within each such selection\nmodel. The search uncovers new selection models that yield identification,\nprovides impossibility results for others, and offers opportunities to relax\nassumptions on selection used in existing literature.",
        "authors": [
            "Leonard Goff"
        ],
        "categories": "econ.EM",
        "published": "2024-06-05T01:17:36Z",
        "updated": "2024-09-24T04:19:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.02525v1",
        "title": "The Impact of Acquisition on Product Quality in the Console Gaming Industry",
        "abstract": "The console gaming industry, a dominant force in the global entertainment\nsector, has witnessed a wave of consolidation in recent years, epitomized by\nMicrosoft's high-profile acquisitions of Activision Blizzard and Zenimax. This\nstudy investigates the repercussions of such mergers on consumer welfare and\ninnovation within the gaming landscape, focusing on product quality as a key\nmetric. Through a comprehensive analysis employing a difference-in-difference\nmodel, the research evaluates the effects of acquisition on game review\nratings, drawing from a dataset comprising over 16,000 console games released\nbetween 2000 and 2023. The research addresses key assumptions underlying the\ndifference-in-difference methodology, including parallel trends and spillover\neffects, to ensure the robustness of the findings. The DID results suggest a\npositive and statistically significant impact of acquisition on game review\nratings, when controlling for genre and release year. The study contributes to\nthe literature by offering empirical evidence on the direct consequences of\nindustry consolidation on consumer welfare and competition dynamics within the\ngaming sector.",
        "authors": [
            "Shivam Somani"
        ],
        "categories": "econ.EM",
        "published": "2024-06-04T17:48:15Z",
        "updated": "2024-06-04T17:48:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.02241v1",
        "title": "Enabling Decision-Making with the Modified Causal Forest: Policy Trees for Treatment Assignment",
        "abstract": "Decision-making plays a pivotal role in shaping outcomes in various\ndisciplines, such as medicine, economics, and business. This paper provides\nguidance to practitioners on how to implement a decision tree designed to\naddress treatment assignment policies using an interpretable and non-parametric\nalgorithm. Our Policy Tree is motivated on the method proposed by Zhou, Athey,\nand Wager (2023), distinguishing itself for the policy score calculation,\nincorporating constraints, and handling categorical and continuous variables.\nWe demonstrate the usage of the Policy Tree for multiple, discrete treatments\non data sets from different fields. The Policy Tree is available in Python's\nopen-source package mcf (Modified Causal Forest).",
        "authors": [
            "Hugo Bodory",
            "Federica Mascolo",
            "Michael Lechner"
        ],
        "categories": "econ.EM",
        "published": "2024-06-04T12:02:49Z",
        "updated": "2024-06-04T12:02:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.02152v1",
        "title": "A sequential test procedure for the choice of the number of regimes in multivariate nonlinear models",
        "abstract": "This paper proposes a sequential test procedure for determining the number of\nregimes in nonlinear multivariate autoregressive models. The procedure relies\non linearity and no additional nonlinearity tests for both multivariate smooth\ntransition and threshold autoregressive models. We conduct a simulation study\nto evaluate the finite-sample properties of the proposed test in small samples.\nOur findings indicate that the test exhibits satisfactory size properties, with\nthe rescaled version of the Lagrange Multiplier test statistics demonstrating\nthe best performance in most simulation settings. The sequential procedure is\nalso applied to two empirical cases, the US monthly interest rates and\nIcelandic river flows. In both cases, the detected number of regimes aligns\nwell with the existing literature.",
        "authors": [
            "Andrea Bucci"
        ],
        "categories": "econ.EM",
        "published": "2024-06-04T09:40:03Z",
        "updated": "2024-06-04T09:40:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.01002v1",
        "title": "Random Subspace Local Projections",
        "abstract": "We show how random subspace methods can be adapted to estimating local\nprojections with many controls. Random subspace methods have their roots in the\nmachine learning literature and are implemented by averaging over regressions\nestimated over different combinations of subsets of these controls. We document\nthree key results: (i) Our approach can successfully recover the impulse\nresponse functions across Monte Carlo experiments representative of different\nmacroeconomic settings and identification schemes. (ii) Our results suggest\nthat random subspace methods are more accurate than other dimension reduction\nmethods if the underlying large dataset has a factor structure similar to\ntypical macroeconomic datasets such as FRED-MD. (iii) Our approach leads to\ndifferences in the estimated impulse response functions relative to benchmark\nmethods when applied to two widely studied empirical applications.",
        "authors": [
            "Viet Hoang Dinh",
            "Didier Nibbering",
            "Benjamin Wong"
        ],
        "categories": "econ.EM",
        "published": "2024-06-03T05:23:32Z",
        "updated": "2024-06-03T05:23:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.00941v1",
        "title": "A Robust Residual-Based Test for Structural Changes in Factor Models",
        "abstract": "In this paper, we propose an easy-to-implement residual-based specification\ntesting procedure for detecting structural changes in factor models, which is\npowerful against both smooth and abrupt structural changes with unknown break\ndates. The proposed test is robust against the over-specified number of\nfactors, and serially and cross-sectionally correlated error processes. A new\ncentral limit theorem is given for the quadratic forms of panel data with\ndependence over both dimensions, thereby filling a gap in the literature. We\nestablish the asymptotic properties of the proposed test statistic, and\naccordingly develop a simulation-based scheme to select critical value in order\nto improve finite sample performance. Through extensive simulations and a\nreal-world application, we confirm our theoretical results and demonstrate that\nthe proposed test exhibits desirable size and power in practice.",
        "authors": [
            "Bin Peng",
            "Liangjun Su",
            "Yayi Yan"
        ],
        "categories": "econ.EM",
        "published": "2024-06-03T02:38:39Z",
        "updated": "2024-06-03T02:38:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.00827v2",
        "title": "LaLonde (1986) after Nearly Four Decades: Lessons Learned",
        "abstract": "In 1986, Robert LaLonde published an article that compared nonexperimental\nestimates to experimental benchmarks (LaLonde 1986). He concluded that the\nnonexperimental methods at the time could not systematically replicate\nexperimental benchmarks, casting doubt on the credibility of these methods.\nFollowing LaLonde's critical assessment, there have been significant\nmethodological advances and practical changes, including (i) an emphasis on\nestimators based on unconfoundedness, (ii) a focus on the importance of overlap\nin covariate distributions, (iii) the introduction of propensity score-based\nmethods leading to doubly robust estimators, (iv) a greater emphasis on\nvalidation exercises to bolster research credibility, and (v) methods for\nestimating and exploiting treatment effect heterogeneity. To demonstrate the\npractical lessons from these advances, we reexamine the LaLonde data and the\nImbens-Rubin-Sacerdote lottery data. We show that modern methods, when applied\nin contexts with sufficient covariate overlap, yield robust estimates for the\nadjusted differences between the treatment and control groups. However, this\ndoes not mean that these estimates are valid. To assess their credibility,\nvalidation exercises (such as placebo tests) are essential, whereas goodness of\nfit tests alone are inadequate. Our findings highlight the importance of\nclosely examining the assignment process, carefully inspecting overlap, and\nconducting validation exercises when analyzing causal effects with\nnonexperimental data.",
        "authors": [
            "Guido Imbens",
            "Yiqing Xu"
        ],
        "categories": "econ.EM",
        "published": "2024-06-02T18:22:13Z",
        "updated": "2024-06-08T06:47:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.00700v1",
        "title": "On the modelling and prediction of high-dimensional functional time series",
        "abstract": "We propose a two-step procedure to model and predict high-dimensional\nfunctional time series, where the number of function-valued time series $p$ is\nlarge in relation to the length of time series $n$. Our first step performs an\neigenanalysis of a positive definite matrix, which leads to a one-to-one linear\ntransformation for the original high-dimensional functional time series, and\nthe transformed curve series can be segmented into several groups such that any\ntwo subseries from any two different groups are uncorrelated both\ncontemporaneously and serially. Consequently in our second step those groups\nare handled separately without the information loss on the overall linear\ndynamic structure. The second step is devoted to establishing a\nfinite-dimensional dynamical structure for all the transformed functional time\nseries within each group. Furthermore the finite-dimensional structure is\nrepresented by that of a vector time series. Modelling and forecasting for the\noriginal high-dimensional functional time series are realized via those for the\nvector time series in all the groups. We investigate the theoretical properties\nof our proposed methods, and illustrate the finite-sample performance through\nboth extensive simulation and two real datasets.",
        "authors": [
            "Jinyuan Chang",
            "Qin Fang",
            "Xinghao Qiao",
            "Qiwei Yao"
        ],
        "categories": "stat.ME",
        "published": "2024-06-02T10:35:58Z",
        "updated": "2024-06-02T10:35:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.00650v1",
        "title": "Cluster-robust jackknife and bootstrap inference for binary response models",
        "abstract": "We study cluster-robust inference for binary response models. Inference based\non the most commonly-used cluster-robust variance matrix estimator (CRVE) can\nbe very unreliable. We study several alternatives. Conceptually the simplest of\nthese, but also the most computationally demanding, involves jackknifing at the\ncluster level. We also propose a linearized version of the cluster-jackknife\nvariance matrix estimator as well as linearized versions of the wild cluster\nbootstrap. The linearizations are based on empirical scores and are\ncomputationally efficient. Throughout we use the logit model as a leading\nexample. We also discuss a new Stata software package called logitjack which\nimplements these procedures. Simulation results strongly favor the new methods,\nand two empirical examples suggest that it can be important to use them in\npractice.",
        "authors": [
            "James G. MacKinnon",
            "Morten \u00d8rregaard Nielsen",
            "Matthew D. Webb"
        ],
        "categories": "econ.EM",
        "published": "2024-06-02T07:30:55Z",
        "updated": "2024-06-02T07:30:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.00610v1",
        "title": "Portfolio Optimization with Robust Covariance and Conditional Value-at-Risk Constraints",
        "abstract": "The measure of portfolio risk is an important input of the Markowitz\nframework. In this study, we explored various methods to obtain a robust\ncovariance estimators that are less susceptible to financial data noise. We\nevaluated the performance of large-cap portfolio using various forms of Ledoit\nShrinkage Covariance and Robust Gerber Covariance matrix during the period of\n2012 to 2022. Out-of-sample performance indicates that robust covariance\nestimators can outperform the market capitalization-weighted benchmark\nportfolio, particularly during bull markets. The Gerber covariance with\nMean-Absolute-Deviation (MAD) emerged as the top performer. However, robust\nestimators do not manage tail risk well under extreme market conditions, for\nexample, Covid-19 period. When we aim to control for tail risk, we should add\nconstraint on Conditional Value-at-Risk (CVaR) to make more conservative\ndecision on risk exposure. Additionally, we incorporated unsupervised\nclustering algorithm K-means to the optimization algorithm (i.e. Nested\nClustering Optimization, NCO). It not only helps mitigate numerical instability\nof the optimization algorithm, but also contributes to lower drawdown as well.",
        "authors": [
            "Qiqin Zhou"
        ],
        "categories": "q-fin.PM",
        "published": "2024-06-02T03:50:20Z",
        "updated": "2024-06-02T03:50:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.00472v1",
        "title": "Financial Deepening and Economic Growth in Select Emerging Markets with Currency Board Systems: Theory and Evidence",
        "abstract": "This paper investigates some indicators of financial development in select\ncountries with currency board systems and raises some questions about the\nconnection between financial development and growth in currency board systems.\nMost of those cases are long past episodes of what we would now call emerging\nmarkets. However, the paper also looks at Hong Kong, the currency board system\nthat is one of the world's largest and most advanced financial markets. The\nglobal financial crisis of 2008 09 created doubts about the efficiency of\nfinancial markets in advanced economies, including in Hong Kong, and unsettled\nthe previous consensus that a large financial sector would be more stable than\na smaller one.",
        "authors": [
            "Yujuan Qiu"
        ],
        "categories": "econ.EM",
        "published": "2024-06-01T15:56:33Z",
        "updated": "2024-06-01T15:56:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.00442v1",
        "title": "Optimizing hydrogen and e-methanol production through Power-to-X integration in biogas plants",
        "abstract": "The European Union strategy for net zero emissions relies on developing\nhydrogen and electro fuels infrastructure. These fuels will be crucial as\nenergy carriers and balancing agents for renewable energy variability. Large\nscale production requires more renewable capacity, and various Power to X (PtX)\nconcepts are emerging in renewable rich countries. However, sourcing renewable\ncarbon to scale carbon based electro fuels is a significant challenge. This\nstudy explores a PtX hub that sources renewable CO2 from biogas plants,\nintegrating renewable energy, hydrogen production, and methanol synthesis on\nsite. This concept creates an internal market for energy and materials,\ninterfacing with the external energy system. The size and operation of the PtX\nhub were optimized, considering integration with local energy systems and a\npotential hydrogen grid. The levelized costs of hydrogen and methanol were\nestimated for a 2030 start, considering new legislation on renewable fuels of\nnon biological origin (RFNBOs). Our results show the PtX hub can rely mainly on\non site renewable energy, selling excess electricity to the grid. A local\nhydrogen grid connection improves operations, and the behind the meter market\nlowers energy prices, buffering against market variability. We found methanol\ncosts could be below 650 euros per ton and hydrogen production costs below 3\neuros per kg, with standalone methanol plants costing 23 per cent more. The CO2\nrecovery to methanol production ratio is crucial, with over 90 per cent\nrecovery requiring significant investment in CO2 and H2 storage. Overall, our\nfindings support planning PtX infrastructures integrated with the agricultural\nsector as a cost effective way to access renewable carbon.",
        "authors": [
            "Alberto Alamia",
            "Behzad Partoon",
            "Eoghan Rattigan",
            "Gorm Brunn Andresen"
        ],
        "categories": "econ.EM",
        "published": "2024-06-01T13:40:36Z",
        "updated": "2024-06-01T13:40:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.00326v2",
        "title": "From day-ahead to mid and long-term horizons with econometric electricity price forecasting models",
        "abstract": "The recent energy crisis starting in 2021 led to record-high gas, coal,\ncarbon and power prices, with electricity reaching up to 40 times the\npre-crisis average. This had dramatic consequences for operational and risk\nmanagement prompting the need for robust econometric models for mid to\nlong-term electricity price forecasting. After a comprehensive literature\nanalysis, we identify key challenges and address them with novel approaches: 1)\nFundamental information is incorporated by constraining coefficients with\nbounds derived from fundamental models offering interpretability; 2) Short-term\nregressors such as load and renewables can be used in long-term forecasts by\nincorporating their seasonal expectations to stabilize the model; 3) Unit root\nbehavior of power prices, induced by fuel prices, can be managed by estimating\nsame-day relationships and projecting them forward. We develop interpretable\nmodels for a range of forecasting horizons from one day to one year ahead,\nproviding guidelines on robust modeling frameworks and key explanatory\nvariables for each horizon. Our study, focused on Europe's largest energy\nmarket, Germany, analyzes hourly electricity prices using regularized\nregression methods and generalized additive models.",
        "authors": [
            "Paul Ghelasi",
            "Florian Ziel"
        ],
        "categories": "stat.AP",
        "published": "2024-06-01T07:00:51Z",
        "updated": "2024-08-25T16:21:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.20715v1",
        "title": "Transforming Japan Real Estate",
        "abstract": "The Japanese real estate market, valued over 35 trillion USD, offers\nsignificant investment opportunities. Accurate rent and price forecasting could\nprovide a substantial competitive edge. This paper explores using alternative\ndata variables to predict real estate performance in 1100 Japanese\nmunicipalities. A comprehensive house price index was created, covering all\nmunicipalities from 2005 to the present, using a dataset of over 5 million\ntransactions. This core dataset was enriched with economic factors spanning\ndecades, allowing for price trajectory predictions.\n  The findings show that alternative data variables can indeed forecast real\nestate performance effectively. Investment signals based on these variables\nyielded notable returns with low volatility. For example, the net migration\nratio delivered an annualized return of 4.6% with a Sharpe ratio of 1.5.\nTaxable income growth and new dwellings ratio also performed well, with\nannualized returns of 4.1% (Sharpe ratio of 1.3) and 3.3% (Sharpe ratio of\n0.9), respectively. When combined with transformer models to predict\nrisk-adjusted returns 4 years in advance, the model achieved an R-squared score\nof 0.28, explaining nearly 30% of the variation in future municipality prices.\n  These results highlight the potential of alternative data variables in real\nestate investment. They underscore the need for further research to identify\nmore predictive factors. Nonetheless, the evidence suggests that such data can\nprovide valuable insights into real estate price drivers, enabling more\ninformed investment decisions in the Japanese market.",
        "authors": [
            "Diabul Haque"
        ],
        "categories": "cs.CE",
        "published": "2024-05-31T09:12:28Z",
        "updated": "2024-05-31T09:12:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.20191v1",
        "title": "Multidimensional spatiotemporal clustering -- An application to environmental sustainability scores in Europe",
        "abstract": "The assessment of corporate sustainability performance is extremely relevant\nin facilitating the transition to a green and low-carbon intensity economy.\nHowever, companies located in different areas may be subject to different\nsustainability and environmental risks and policies. Henceforth, the main\nobjective of this paper is to investigate the spatial and temporal pattern of\nthe sustainability evaluations of European firms. We leverage on a large\ndataset containing information about companies' sustainability performances,\nmeasured by MSCI ESG ratings, and geographical coordinates of firms in Western\nEurope between 2013 and 2023. By means of a modified version of the Chavent et\nal. (2018) hierarchical algorithm, we conduct a spatial clustering analysis,\ncombining sustainability and spatial information, and a spatiotemporal\nclustering analysis, which combines the time dynamics of multiple\nsustainability features and spatial dissimilarities, to detect groups of firms\nwith homogeneous sustainability performance. We are able to build\ncross-national and cross-industry clusters with remarkable differences in terms\nof sustainability scores. Among other results, in the spatio-temporal analysis,\nwe observe a high degree of geographical overlap among clusters, indicating\nthat the temporal dynamics in sustainability assessment are relevant within a\nmultidimensional approach. Our findings help to capture the diversity of ESG\nratings across Western Europe and may assist practitioners and policymakers in\nevaluating companies facing different sustainability-linked risks in different\nareas.",
        "authors": [
            "Caterina Morelli",
            "Simone Boccaletti",
            "Paolo Maranzano",
            "Philipp Otto"
        ],
        "categories": "stat.AP",
        "published": "2024-05-30T15:56:06Z",
        "updated": "2024-05-30T15:56:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.19920v2",
        "title": "The ARR2 prior: flexible predictive prior definition for Bayesian auto-regressions",
        "abstract": "We present the ARR2 prior, a joint prior over the auto-regressive components\nin Bayesian time-series models and their induced $R^2$. Compared to other\npriors designed for times-series models, the ARR2 prior allows for flexible and\nintuitive shrinkage. We derive the prior for pure auto-regressive models, and\nextend it to auto-regressive models with exogenous inputs, and state-space\nmodels. Through both simulations and real-world modelling exercises, we\ndemonstrate the efficacy of the ARR2 prior in improving sparse and reliable\ninference, while showing greater inference quality and predictive performance\nthan other shrinkage priors. An open-source implementation of the prior is\nprovided.",
        "authors": [
            "David Kohns",
            "Noa Kallioinen",
            "Yann McLatchie",
            "Aki Vehtari"
        ],
        "categories": "stat.CO",
        "published": "2024-05-30T10:32:59Z",
        "updated": "2024-05-31T20:19:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.19897v1",
        "title": "The Political Resource Curse Redux",
        "abstract": "In the study of the Political Resource Curse (Brollo et al.,2013), the\nauthors identified a new channel to investigate whether the windfalls of\nresources are unambiguously beneficial to society, both with theory and\nempirical evidence. This paper revisits the framework with a new dataset.\nSpecifically, we implemented a regression discontinuity design and\ndifference-in-difference specification",
        "authors": [
            "Hanyuan Jiang"
        ],
        "categories": "econ.EM",
        "published": "2024-05-30T09:53:51Z",
        "updated": "2024-05-30T09:53:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.19849v1",
        "title": "Modelling and Forecasting Energy Market Volatility Using GARCH and Machine Learning Approach",
        "abstract": "This paper presents a comparative analysis of univariate and multivariate\nGARCH-family models and machine learning algorithms in modeling and forecasting\nthe volatility of major energy commodities: crude oil, gasoline, heating oil,\nand natural gas. It uses a comprehensive dataset incorporating financial,\nmacroeconomic, and environmental variables to assess predictive performance and\ndiscusses volatility persistence and transmission across these commodities.\nAspects of volatility persistence and transmission, traditionally examined by\nGARCH-class models, are jointly explored using the SHAP (Shapley Additive\nexPlanations) method. The findings reveal that machine learning models\ndemonstrate superior out-of-sample forecasting performance compared to\ntraditional GARCH models. Machine learning models tend to underpredict, while\nGARCH models tend to overpredict energy market volatility, suggesting a hybrid\nuse of both types of models. There is volatility transmission from crude oil to\nthe gasoline and heating oil markets. The volatility transmission in the\nnatural gas market is less prevalent.",
        "authors": [
            "Seulki Chung"
        ],
        "categories": "econ.EM",
        "published": "2024-05-30T08:54:57Z",
        "updated": "2024-05-30T08:54:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.19463v1",
        "title": "Stochastic Optimization Algorithms for Instrumental Variable Regression with Streaming Data",
        "abstract": "We develop and analyze algorithms for instrumental variable regression by\nviewing the problem as a conditional stochastic optimization problem. In the\ncontext of least-squares instrumental variable regression, our algorithms\nneither require matrix inversions nor mini-batches and provides a fully online\napproach for performing instrumental variable regression with streaming data.\nWhen the true model is linear, we derive rates of convergence in expectation,\nthat are of order $\\mathcal{O}(\\log T/T)$ and $\\mathcal{O}(1/T^{1-\\iota})$ for\nany $\\iota>0$, respectively under the availability of two-sample and one-sample\noracles, respectively, where $T$ is the number of iterations. Importantly,\nunder the availability of the two-sample oracle, our procedure avoids\nexplicitly modeling and estimating the relationship between confounder and the\ninstrumental variables, demonstrating the benefit of the proposed approach over\nrecent works based on reformulating the problem as minimax optimization\nproblems. Numerical experiments are provided to corroborate the theoretical\nresults.",
        "authors": [
            "Xuxing Chen",
            "Abhishek Roy",
            "Yifan Hu",
            "Krishnakumar Balasubramanian"
        ],
        "categories": "stat.ML",
        "published": "2024-05-29T19:21:55Z",
        "updated": "2024-05-29T19:21:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.19317v1",
        "title": "Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal Best Arm Identification",
        "abstract": "This study investigates a local asymptotic minimax optimal strategy for\nfixed-budget best arm identification (BAI). We propose the Adaptive Generalized\nNeyman Allocation (AGNA) strategy and show that its worst-case upper bound of\nthe probability of misidentifying the best arm aligns with the worst-case lower\nbound under the small-gap regime, where the gap between the expected outcomes\nof the best and suboptimal arms is small. Our strategy corresponds to a\ngeneralization of the Neyman allocation for two-armed bandits (Neyman, 1934;\nKaufmann et al., 2016) and a refinement of existing strategies such as the ones\nproposed by Glynn & Juneja (2004) and Shin et al. (2018). Compared to Komiyama\net al. (2022), which proposes a minimax rate-optimal strategy, our proposed\nstrategy has a tighter upper bound that exactly matches the lower bound,\nincluding the constant terms, by restricting the class of distributions to the\nclass of small-gap distributions. Our result contributes to the longstanding\nopen issue about the existence of asymptotically optimal strategies in\nfixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.",
        "authors": [
            "Masahiro Kato"
        ],
        "categories": "cs.LG",
        "published": "2024-05-29T17:43:13Z",
        "updated": "2024-05-29T17:43:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.19225v3",
        "title": "Synthetic Potential Outcomes and Causal Mixture Identifiability",
        "abstract": "Heterogeneous data from multiple populations, sub-groups, or sources is often\nrepresented as a ``mixture model'' with a single latent class influencing all\nof the observed covariates. Heterogeneity can be resolved at multiple levels by\ngrouping populations according to different notions of similarity. This paper\nproposes grouping with respect to the causal response of an intervention or\nperturbation on the system. This definition is distinct from previous notions,\nsuch as similar covariate values (e.g. clustering) or similar correlations\nbetween covariates (e.g. Gaussian mixture models). To solve the problem, we\n``synthetically sample'' from a counterfactual distribution using higher-order\nmulti-linear moments of the observable data. To understand how these ``causal\nmixtures'' fit in with more classical notions, we develop a hierarchy of\nmixture identifiability.",
        "authors": [
            "Bijan Mazaheri",
            "Chandler Squires",
            "Caroline Uhler"
        ],
        "categories": "cs.LG",
        "published": "2024-05-29T16:05:57Z",
        "updated": "2024-12-12T07:12:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.18987v2",
        "title": "Transmission Channel Analysis in Dynamic Models",
        "abstract": "We propose a framework for the analysis of transmission channels in a large\nclass of dynamic models. To this end, we formulate our approach both using\ngraph theory and potential outcomes, which we show to be equivalent. Our\nmethod, labelled Transmission Channel Analysis (TCA), allows for the\ndecomposition of total effects captured by impulse response functions into the\neffects flowing along transmission channels, thereby providing a quantitative\nassessment of the strength of various transmission channels. We establish that\nthis requires no additional identification assumptions beyond the\nidentification of the structural shock whose effects the researcher wants to\ndecompose. Additionally, we prove that impulse response functions are\nsufficient statistics for the computation of transmission effects. We\ndemonstrate the empirical relevance of TCA for policy evaluation by decomposing\nthe effects of policy shocks arising from a variety of popular macroeconomic\nmodels.",
        "authors": [
            "Enrico Wegner",
            "Lenard Lieb",
            "Stephan Smeekes",
            "Ines Wilms"
        ],
        "categories": "econ.EM",
        "published": "2024-05-29T11:04:23Z",
        "updated": "2024-07-12T13:16:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.18531v1",
        "title": "Difference-in-Discontinuities: Estimation, Inference and Validity Tests",
        "abstract": "This paper investigates the econometric theory behind the newly developed\ndifference-in-discontinuities design (DiDC). Despite its increasing use in\napplied research, there are currently limited studies of its properties. The\nmethod combines elements of regression discontinuity (RDD) and\ndifference-in-differences (DiD) designs, allowing researchers to eliminate the\neffects of potential confounders at the discontinuity. We formalize the\ndifference-in-discontinuity theory by stating the identification assumptions\nand proposing a nonparametric estimator, deriving its asymptotic properties and\nexamining the scenarios in which the DiDC has desirable bias properties when\ncompared to the standard RDD. We also provide comprehensive tests for one of\nthe identification assumption of the DiDC. Monte Carlo simulation studies show\nthat the estimators have good performance in finite samples. Finally, we\nrevisit Grembi et al. (2016), that studies the effects of relaxing fiscal rules\non public finance outcomes in Italian municipalities. The results show that the\nproposed estimator exhibits substantially smaller confidence intervals for the\nestimated effects.",
        "authors": [
            "Pedro Picchetti",
            "Cristine C. X. Pinto",
            "Stephanie T. Shinoki"
        ],
        "categories": "econ.EM",
        "published": "2024-05-28T18:55:35Z",
        "updated": "2024-05-28T18:55:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.18089v1",
        "title": "Semi-nonparametric models of multidimensional matching: an optimal transport approach",
        "abstract": "This paper proposes empirically tractable multidimensional matching models,\nfocusing on worker-job matching. We generalize the parametric model proposed by\nLindenlaub (2017), which relies on the assumption of joint normality of\nobserved characteristics of workers and jobs. In our paper, we allow\nunrestricted distributions of characteristics and show identification of the\nproduction technology, and equilibrium wage and matching functions using tools\nfrom optimal transport theory. Given identification, we propose efficient,\nconsistent, asymptotically normal sieve estimators. We revisit Lindenlaub's\nempirical application and show that, between 1990 and 2010, the U.S. economy\nexperienced much larger technological progress favoring cognitive abilities\nthan the original findings suggest. Furthermore, our flexible model\nspecifications provide a significantly better fit for patterns in the evolution\nof wage inequality.",
        "authors": [
            "Dongwoo Kim",
            "Young Jun Lee"
        ],
        "categories": "econ.EM",
        "published": "2024-05-28T11:50:46Z",
        "updated": "2024-05-28T11:50:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.17787v2",
        "title": "Dyadic Regression with Sample Selection",
        "abstract": "This paper addresses the sample selection problem in panel dyadic regression\nanalysis. Dyadic data often include many zeros in the main outcomes due to the\nunderlying network formation process. This not only contaminates popular\nestimators used in practice but also complicates the inference due to the\ndyadic dependence structure. We extend Kyriazidou (1997)'s approach to dyadic\ndata and characterize the asymptotic distribution of our proposed estimator.\nThe convergence rates are $\\sqrt{n}$ or $\\sqrt{n^{2}h_{n}}$, depending on the\ndegeneracy of the H\\'{a}jek projection part of the estimator, where $n$ is the\nnumber of nodes and $h_{n}$ is a bandwidth. We propose a bias-corrected\nconfidence interval and a variance estimator that adapts to the degeneracy. A\nMonte Carlo simulation shows the good finite sample performance of our\nestimator and highlights the importance of bias correction in both asymptotic\nregimes when the fraction of zeros in outcomes varies. We illustrate our\nprocedure using data from Moretti and Wilson (2017)'s paper on migration.",
        "authors": [
            "Kensuke Sakamoto"
        ],
        "categories": "econ.EM",
        "published": "2024-05-28T03:33:26Z",
        "updated": "2024-07-03T15:11:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.17290v1",
        "title": "Count Data Models with Heterogeneous Peer Effects under Rational Expectations",
        "abstract": "This paper develops a micro-founded peer effect model for count responses\nusing a game of incomplete information. The model incorporates heterogeneity in\npeer effects through agents' groups based on observed characteristics.\nParameter identification is established using the identification condition of\nlinear models, which relies on the presence of friends' friends who are not\ndirect friends in the network. I show that this condition extends to a large\nclass of nonlinear models. The model parameters are estimated using the nested\npseudo-likelihood approach, controlling for network endogeneity. I present an\nempirical application on students' participation in extracurricular activities.\nI find that females are more responsive to their peers than males, whereas male\npeers do not influence male students. An easy-to-use R packag--named\nCDatanet--is available for implementing the model.",
        "authors": [
            "Aristide Houndetoungan"
        ],
        "categories": "econ.EM",
        "published": "2024-05-27T15:54:47Z",
        "updated": "2024-05-27T15:54:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.17254v3",
        "title": "Estimating treatment-effect heterogeneity across sites, in multi-site randomized experiments with few units per site",
        "abstract": "In multi-site randomized trials with many sites and few randomization units\nper site, an Empirical-Bayes estimator can be used to estimate the variance of\nthe treatment effect across sites. When this estimator indicates that treatment\neffects do vary, we propose estimators of the coefficients from regressions of\nsite-level effects on site-level characteristics that are unobserved but can be\nunbiasedly estimated, such as sites' average outcome without treatment, or\nsite-specific treatment effects on mediator variables. In experiments with\nimperfect compliance, we show that the sign of the correlation between local\naverage treatment effects (LATEs) and site-level characteristics is identified,\nand we propose a partly testable assumption under which the variance of LATEs\nis identified. We use our results to revisit Behaghel et al (2014), who study\nthe effect of counseling programs on job seekers' job-finding rate, in 200 job\nplacement agencies in France. We find considerable treatment-effect\nheterogeneity, both for intention to treat and LATE effects, and the treatment\neffect is negatively correlated with sites' job-finding rate without treatment.",
        "authors": [
            "Cl\u00e9ment de Chaisemartin",
            "Antoine Deeb"
        ],
        "categories": "econ.EM",
        "published": "2024-05-27T15:10:11Z",
        "updated": "2024-12-11T14:01:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.17237v2",
        "title": "Mixing it up: Inflation at risk",
        "abstract": "Assessing the contribution of various risk factors to future inflation risks\nwas crucial for guiding monetary policy during the recent high inflation\nperiod. However, existing methodologies often provide limited insights by\nfocusing solely on specific percentiles of the forecast distribution. In\ncontrast, this paper introduces a comprehensive framework that examines how\neconomic indicators impact the entire forecast distribution of macroeconomic\nvariables, facilitating the decomposition of the overall risk outlook into its\nunderlying drivers. Additionally, the framework allows for the construction of\nrisk measures that align with central bank preferences, serving as valuable\nsummary statistics. Applied to the recent inflation surge, the framework\nreveals that U.S. inflation risk was primarily influenced by the recovery of\nthe U.S. business cycle and surging commodity prices, partially mitigated by\nadjustments in monetary policy and credit spreads.",
        "authors": [
            "Maximilian Schr\u00f6der"
        ],
        "categories": "econ.EM",
        "published": "2024-05-27T14:52:31Z",
        "updated": "2024-05-28T15:03:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.17225v1",
        "title": "Quantifying the Reliance of Black-Box Decision-Makers on Variables of Interest",
        "abstract": "This paper introduces a framework for measuring how much black-box\ndecision-makers rely on variables of interest. The framework adapts a\npermutation-based measure of variable importance from the explainable machine\nlearning literature. With an emphasis on applicability, I present some of the\nframework's theoretical and computational properties, explain how reliance\ncomputations have policy implications, and work through an illustrative\nexample. In the empirical application to interruptions by Supreme Court\nJustices during oral argument, I find that the effect of gender is more muted\ncompared to the existing literature's estimate; I then use this paper's\nframework to compare Justices' reliance on gender and alignment to their\nreliance on experience, which are incomparable using regression coefficients.",
        "authors": [
            "Daniel Vebman"
        ],
        "categories": "econ.EM",
        "published": "2024-05-27T14:42:04Z",
        "updated": "2024-05-27T14:42:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.17178v1",
        "title": "Statistical Mechanism Design: Robust Pricing, Estimation, and Inference",
        "abstract": "This paper tackles challenges in pricing and revenue projections due to\nconsumer uncertainty. We propose a novel data-based approach for firms facing\nunknown consumer type distributions. Unlike existing methods, we assume firms\nonly observe a finite sample of consumers' types. We introduce\n\\emph{empirically optimal mechanisms}, a simple and intuitive class of\nsample-based mechanisms with strong finite-sample revenue guarantees.\nFurthermore, we leverage our results to develop a toolkit for statistical\ninference on profits. Our approach allows to reliably estimate the profits\nassociated for any particular mechanism, to construct confidence intervals, and\nto, more generally, conduct valid hypothesis testing.",
        "authors": [
            "Duarte Gon\u00e7alves",
            "Bruno A. Furtado"
        ],
        "categories": "econ.TH",
        "published": "2024-05-27T13:59:16Z",
        "updated": "2024-05-27T13:59:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.17166v1",
        "title": "Cross-border cannibalization: Spillover effects of wind and solar energy on interconnected European electricity markets",
        "abstract": "The average revenue, or market value, of wind and solar energy tends to fall\nwith increasing market shares, as is now evident across European electricity\nmarkets. At the same time, these markets have become more interconnected. In\nthis paper, we empirically study the multiple cross-border effects on the value\nof renewable energy: on one hand, interconnection is a flexibility resource\nthat allows to export energy when it is locally abundant, benefitting\nrenewables. On the other hand, wind and solar radiation are correlated across\nspace, so neighboring supply adds to the local one to depress domestic prices.\nWe estimate both effects, using spatial panel regression on electricity market\ndata from 2015 to 2023 from 30 European bidding zones. We find that domestic\nwind and solar value is not only depressed by domestic, but also by neighboring\nrenewables expansion. The better interconnected a market is, the smaller the\neffect of domestic but the larger the effect of neighboring renewables. While\nwind value is stabilized by interconnection, solar value is not. If wind market\nshare increases both at home and in neighboring markets by one percentage\npoint, the value factor of wind energy is reduced by just above 1 percentage\npoints. For solar, this number is almost 4 percentage points.",
        "authors": [
            "Clemens Stiewe",
            "Alice Lixuan Xu",
            "Anselm Eicke",
            "Lion Hirth"
        ],
        "categories": "econ.EM",
        "published": "2024-05-27T13:42:35Z",
        "updated": "2024-05-27T13:42:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.16547v1",
        "title": "Estimating Dyadic Treatment Effects with Unknown Confounders",
        "abstract": "This paper proposes a statistical inference method for assessing treatment\neffects with dyadic data. Under the assumption that the treatments follow an\nexchangeable distribution, our approach allows for the presence of any\nunobserved confounding factors that potentially cause endogeneity of treatment\nchoice without requiring additional information other than the treatments and\noutcomes. Building on the literature of graphon estimation in network data\nanalysis, we propose a neighborhood kernel smoothing method for estimating\ndyadic average treatment effects. We also develop a permutation inference\nmethod for testing the sharp null hypothesis. Under certain regularity\nconditions, we derive the rate of convergence of the proposed estimator and\ndemonstrate the size control property of our test. We apply our method to\ninternational trade data to assess the impact of free trade agreements on\nbilateral trade flows.",
        "authors": [
            "Tadao Hoshino",
            "Takahide Yanagi"
        ],
        "categories": "econ.EM",
        "published": "2024-05-26T12:32:14Z",
        "updated": "2024-05-26T12:32:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.16467v1",
        "title": "Two-way fixed effects instrumental variable regressions in staggered DID-IV designs",
        "abstract": "Many studies run two-way fixed effects instrumental variable (TWFEIV)\nregressions, leveraging variation in the timing of policy adoption across units\nas an instrument for treatment. This paper studies the properties of the TWFEIV\nestimator in staggered instrumented difference-in-differences (DID-IV) designs.\nWe show that in settings with the staggered adoption of the instrument across\nunits, the TWFEIV estimator can be decomposed into a weighted average of all\npossible two-group/two-period Wald-DID estimators. Under staggered DID-IV\ndesigns, a causal interpretation of the TWFEIV estimand hinges on the stable\neffects of the instrument on the treatment and the outcome over time. We\nillustrate the use of our decomposition theorem for the TWFEIV estimator\nthrough an empirical application.",
        "authors": [
            "Sho Miyaji"
        ],
        "categories": "econ.EM",
        "published": "2024-05-26T07:27:38Z",
        "updated": "2024-05-26T07:27:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.15721v1",
        "title": "Dynamic Latent-Factor Model with High-Dimensional Asset Characteristics",
        "abstract": "We develop novel estimation procedures with supporting econometric theory for\na dynamic latent-factor model with high-dimensional asset characteristics, that\nis, the number of characteristics is on the order of the sample size. Utilizing\nthe Double Selection Lasso estimator, our procedure employs regularization to\neliminate characteristics with low signal-to-noise ratios yet maintains\nasymptotically valid inference for asset pricing tests. The crypto asset class\nis well-suited for applying this model given the limited number of tradable\nassets and years of data as well as the rich set of available asset\ncharacteristics. The empirical results present out-of-sample pricing abilities\nand risk-adjusted returns for our novel estimator as compared to benchmark\nmethods. We provide an inference procedure for measuring the risk premium of an\nobservable nontradable factor, and employ this to find that the\ninflation-mimicking portfolio in the crypto asset class has positive risk\ncompensation.",
        "authors": [
            "Adam Baybutt"
        ],
        "categories": "econ.EM",
        "published": "2024-05-24T17:09:28Z",
        "updated": "2024-05-24T17:09:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.15716v1",
        "title": "Empirical Crypto Asset Pricing",
        "abstract": "We motivate the study of the crypto asset class with eleven empirical facts,\nand study the drivers of crypto asset returns through the lens of univariate\nfactors. We argue crypto assets are a new, attractive, and independent asset\nclass. In a novel and rigorously built panel of crypto assets, we examine\npricing ability of sixty three asset characteristics to find rich signal\ncontent across the characteristics and at several future horizons. Only\nunivariate financial factors (i.e., functions of previous returns) were\nassociated with statistically significant long-short strategies, suggestive of\nspeculatively driven returns as opposed to more fundamental pricing factors.",
        "authors": [
            "Adam Baybutt"
        ],
        "categories": "econ.EM",
        "published": "2024-05-24T17:03:59Z",
        "updated": "2024-05-24T17:03:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.15579v1",
        "title": "Generating density nowcasts for U.S. GDP growth with deep learning: Bayes by Backprop and Monte Carlo dropout",
        "abstract": "Recent results in the literature indicate that artificial neural networks\n(ANNs) can outperform the dynamic factor model (DFM) in terms of the accuracy\nof GDP nowcasts. Compared to the DFM, the performance advantage of these highly\nflexible, nonlinear estimators is particularly evident in periods of recessions\nand structural breaks. From the perspective of policy-makers, however, nowcasts\nare the most useful when they are conveyed with uncertainty attached to them.\nWhile the DFM and other classical time series approaches analytically derive\nthe predictive (conditional) distribution for GDP growth, ANNs can only produce\npoint nowcasts based on their default training procedure (backpropagation). To\nfill this gap, first in the literature, we adapt two different deep learning\nalgorithms that enable ANNs to generate density nowcasts for U.S. GDP growth:\nBayes by Backprop and Monte Carlo dropout. The accuracy of point nowcasts,\ndefined as the mean of the empirical predictive distribution, is evaluated\nrelative to a naive constant growth model for GDP and a benchmark DFM\nspecification. Using a 1D CNN as the underlying ANN architecture, both\nalgorithms outperform those benchmarks during the evaluation period (2012:Q1 --\n2022:Q4). Furthermore, both algorithms are able to dynamically adjust the\nlocation (mean), scale (variance), and shape (skew) of the empirical predictive\ndistribution. The results indicate that both Bayes by Backprop and Monte Carlo\ndropout can effectively augment the scope and functionality of ANNs, rendering\nthem a fully compatible and competitive alternative for classical time series\napproaches.",
        "authors": [
            "Krist\u00f3f N\u00e9meth",
            "D\u00e1niel Hadh\u00e1zi"
        ],
        "categories": "econ.EM",
        "published": "2024-05-24T14:06:08Z",
        "updated": "2024-05-24T14:06:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.15042v1",
        "title": "Modularity, Higher-Order Recombination, and New Venture Success",
        "abstract": "Modularity is critical for the emergence and evolution of complex social,\nnatural, and technological systems robust to exploratory failure. We consider\nthis in the context of emerging business organizations, which can be understood\nas complex systems. We build a theory of organizational emergence as\nhigher-order, modular recombination wherein successful start-ups assemble novel\ncombinations of successful modular components, rather than engage in the\nlower-order combination of disparate, singular components. Lower-order\ncombinations are critical for long-term socio-economic transformation, but\nmanifest diffuse benefits requiring support as public goods. Higher-order\ncombinations facilitate rapid experimentation and attract private funding. We\nevaluate this with U.S. venture-funded start-ups over 45 years using company\ndescriptions. We build a dynamic semantic space with word embedding models\nconstructed from evolving business discourse, which allow us to measure the\nmodularity of and distance between new venture components. Using event history\nmodels, we demonstrate how ventures more likely achieve successful IPOs and\nhigh-priced acquisitions when they combine diverse modules of clustered\ncomponents. We demonstrate how higher-order combination enables venture success\nby accelerating firm development and diversifying investment, and we reflect on\nits implications for social innovation.",
        "authors": [
            "Likun Cao",
            "Ziwen Chen",
            "James Evans"
        ],
        "categories": "econ.EM",
        "published": "2024-05-23T20:42:31Z",
        "updated": "2024-05-23T20:42:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.14104v2",
        "title": "On the Identifying Power of Monotonicity for Average Treatment Effects",
        "abstract": "In the context of a binary outcome, treatment, and instrument, Balke and\nPearl (1993, 1997) establish that the monotonicity condition of Imbens and\nAngrist (1994) has no identifying power beyond instrument exogeneity for\naverage potential outcomes and average treatment effects in the sense that\nadding it to instrument exogeneity does not decrease the identified sets for\nthose parameters whenever those restrictions are consistent with the\ndistribution of the observable data. This paper shows that this phenomenon\nholds in a broader setting with a multi-valued outcome, treatment, and\ninstrument, under an extension of the monotonicity condition that we refer to\nas generalized monotonicity. We further show that this phenomenon holds for any\nrestriction on treatment response that is stronger than generalized\nmonotonicity provided that these stronger restrictions do not restrict\npotential outcomes. Importantly, many models of potential treatments previously\nconsidered in the literature imply generalized monotonicity, including the\ntypes of monotonicity restrictions considered by Kline and Walters (2016),\nKirkeboen et al. (2016), and Heckman and Pinto (2018), and the restriction that\ntreatment selection is determined by particular classes of additive random\nutility models. We show through a series of examples that restrictions on\npotential treatments can provide identifying power beyond instrument exogeneity\nfor average potential outcomes and average treatment effects when the\nrestrictions imply that the generalized monotonicity condition is violated. In\nthis way, our results shed light on the types of restrictions required for help\nin identifying average potential outcomes and average treatment effects.",
        "authors": [
            "Yuehao Bai",
            "Shunzhuang Huang",
            "Sarah Moon",
            "Azeem M. Shaikh",
            "Edward J. Vytlacil"
        ],
        "categories": "econ.EM",
        "published": "2024-05-23T02:14:12Z",
        "updated": "2024-08-23T15:23:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.13945v1",
        "title": "Exogenous Consideration and Extended Random Utility",
        "abstract": "In a consideration set model, an individual maximizes utility among the\nconsidered alternatives. I relate a consideration set additive random utility\nmodel to classic discrete choice and the extended additive random utility\nmodel, in which utility can be $-\\infty$ for infeasible alternatives. When\nobservable utility shifters are bounded, all three models are observationally\nequivalent. Moreover, they have the same counterfactual bounds and welfare\nformulas for changes in utility shifters like price. For attention\ninterventions, welfare cannot change in the full consideration model but is\ncompletely unbounded in the limited consideration model. The identified set for\nconsideration set probabilities has a minimal width for any bounded support of\nshifters, but with unbounded support it is a point: identification \"towards\"\ninfinity does not resemble identification \"at\" infinity.",
        "authors": [
            "Roy Allen"
        ],
        "categories": "econ.EM",
        "published": "2024-05-22T19:20:42Z",
        "updated": "2024-05-22T19:20:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.13926v1",
        "title": "Some models are useful, but for how long?: A decision theoretic approach to choosing when to refit large-scale prediction models",
        "abstract": "Large-scale prediction models (typically using tools from artificial\nintelligence, AI, or machine learning, ML) are increasingly ubiquitous across a\nvariety of industries and scientific domains. Such methods are often paired\nwith detailed data from sources such as electronic health records, wearable\nsensors, and omics data (high-throughput technology used to understand\nbiology). Despite their utility, implementing AI and ML tools at the scale\nnecessary to work with this data introduces two major challenges. First, it can\ncost tens of thousands of dollars to train a modern AI/ML model at scale.\nSecond, once the model is trained, its predictions may become less relevant as\npatient and provider behavior change, and predictions made for one geographical\narea may be less accurate for another. These two challenges raise a fundamental\nquestion: how often should you refit the AI/ML model to optimally trade-off\nbetween cost and relevance? Our work provides a framework for making decisions\nabout when to {\\it refit} AI/ML models when the goal is to maintain valid\nstatistical inference (e.g. estimating a treatment effect in a clinical trial).\nDrawing on portfolio optimization theory, we treat the decision of {\\it\nrecalibrating} versus {\\it refitting} the model as a choice between\n''investing'' in one of two ''assets.'' One asset, recalibrating the model\nbased on another model, is quick and relatively inexpensive but bears\nuncertainty from sampling and the possibility that the other model is not\nrelevant to current circumstances. The other asset, {\\it refitting} the model,\nis costly but removes the irrelevance concern (though not the risk of sampling\nerror). We explore the balancing act between these two potential investments in\nthis paper.",
        "authors": [
            "Kentaro Hoffman",
            "Stephen Salerno",
            "Jeff Leek",
            "Tyler McCormick"
        ],
        "categories": "stat.ME",
        "published": "2024-05-22T18:57:38Z",
        "updated": "2024-05-22T18:57:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.13224v1",
        "title": "Integrating behavioral experimental findings into dynamical models to inform social change interventions",
        "abstract": "Addressing global challenges -- from public health to climate change -- often\ninvolves stimulating the large-scale adoption of new products or behaviors.\nResearch traditions that focus on individual decision making suggest that\nachieving this objective requires better identifying the drivers of individual\nadoption choices. On the other hand, computational approaches rooted in\ncomplexity science focus on maximizing the propagation of a given product or\nbehavior throughout social networks of interconnected adopters. The integration\nof these two perspectives -- although advocated by several research communities\n-- has remained elusive so far. Here we show how achieving this integration\ncould inform seeding policies to facilitate the large-scale adoption of a given\nbehavior or product. Drawing on complex contagion and discrete choice theories,\nwe propose a method to estimate individual-level thresholds to adoption, and\nvalidate its predictive power in two choice experiments. By integrating the\nestimated thresholds into computational simulations, we show that\nstate-of-the-art seeding methods for social influence maximization might be\nsuboptimal if they neglect individual-level behavioral drivers, which can be\ncorrected through the proposed experimental method.",
        "authors": [
            "Radu Tanase",
            "Ren\u00e9 Algesheimer",
            "Manuel S. Mariani"
        ],
        "categories": "physics.soc-ph",
        "published": "2024-05-21T22:17:17Z",
        "updated": "2024-05-21T22:17:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.12467v1",
        "title": "Conditional Choice Probability Estimation of Dynamic Discrete Choice Models with 2-period Finite Dependence",
        "abstract": "This paper extends the work of Arcidiacono and Miller (2011, 2019) by\nintroducing a novel characterization of finite dependence within dynamic\ndiscrete choice models, demonstrating that numerous models display 2-period\nfinite dependence. We recast finite dependence as a problem of sequentially\nsearching for weights and introduce a computationally efficient method for\ndetermining these weights by utilizing the Kronecker product structure embedded\nin state transitions. With the estimated weights, we develop a computationally\nattractive Conditional Choice Probability estimator with 2-period finite\ndependence. The computational efficacy of our proposed estimator is\ndemonstrated through Monte Carlo simulations.",
        "authors": [
            "Yu Hao",
            "Hiroyuki Kasahara"
        ],
        "categories": "econ.EM",
        "published": "2024-05-21T02:50:07Z",
        "updated": "2024-05-21T02:50:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.12180v1",
        "title": "Estimating the Impact of Social Distance Policy in Mitigating COVID-19 Spread with Factor-Based Imputation Approach",
        "abstract": "We identify the effectiveness of social distancing policies in reducing the\ntransmission of the COVID-19 spread. We build a model that measures the\nrelative frequency and geographic distribution of the virus growth rate and\nprovides hypothetical infection distribution in the states that enacted the\nsocial distancing policies, where we control time-varying, observed and\nunobserved, state-level heterogeneities. Using panel data on infection and\ndeaths in all US states from February 20 to April 20, 2020, we find that\nstay-at-home orders and other types of social distancing policies significantly\nreduced the growth rate of infection and deaths. We show that the effects are\ntime-varying and range from the weakest at the beginning of policy intervention\nto the strongest by the end of our sample period. We also found that social\ndistancing policies were more effective in states with higher income, better\neducation, more white people, more democratic voters, and higher CNN\nviewership.",
        "authors": [
            "Difang Huang",
            "Ying Liang",
            "Boyao Wu",
            "Yanyi Ye"
        ],
        "categories": "econ.EM",
        "published": "2024-05-20T17:06:33Z",
        "updated": "2024-05-20T17:06:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.12083v4",
        "title": "Instrumented Difference-in-Differences with Heterogeneous Treatment Effects",
        "abstract": "Many studies exploit variation in the timing of policy adoption across units\nas an instrument for treatment. This paper formalizes the underlying\nidentification strategy as an instrumented difference-in-differences (DID-IV).\nIn this design, a Wald-DID estimand, which scales the DID estimand of the\noutcome by the DID estimand of the treatment, captures the local average\ntreatment effect on the treated (LATET). In contrast to Fuzzy DID design\nconsidered in de Chaisemartin and D'Haultfoeuille (2018), our DID-IV design\ndoes not ex ante require strong restrictions on treatment adoption behavior\nacross units. Additionally, our target parameter, the LATET, is policy-relevant\nif the instrument is based on the policy change of interest to the researcher.\nWe extend the canonical DID-IV design to multiple period settings with the\nstaggered adoption of the instrument across units, calling it a staggered\nDID-IV design, and propose an estimation method that is robust to treatment\neffect heterogeneity. We illustrate our findings in the setting of Oreopoulos\n(2006), estimating returns to schooling in the United Kingdom. In this\napplication, the two-way fixed effects instrumental variable regression, which\nis the conventional approach to implement a staggered DID-IV design, yields a\nnegative estimate, whereas our estimation method indicates a substantial gain\nfrom schooling.",
        "authors": [
            "Sho Miyaji"
        ],
        "categories": "econ.EM",
        "published": "2024-05-20T14:52:39Z",
        "updated": "2024-07-23T02:15:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.11954v1",
        "title": "Comparing predictive ability in presence of instability over a very short time",
        "abstract": "We consider forecast comparison in the presence of instability when this\naffects only a short period of time. We demonstrate that global tests do not\nperform well in this case, as they were not designed to capture very\nshort-lived instabilities, and their power vanishes altogether when the\nmagnitude of the shock is very large. We then discuss and propose approaches\nthat are more suitable to detect such situations, such as nonparametric methods\n(S test or MAX procedure). We illustrate these results in different Monte Carlo\nexercises and in evaluating the nowcast of the quarterly US nominal GDP from\nthe Survey of Professional Forecasters (SPF) against a naive benchmark of no\ngrowth, over the period that includes the GDP instability brought by the\nCovid-19 crisis. We recommend that the forecaster should not pool the sample,\nbut exclude the short periods of high local instability from the evaluation\nexercise.",
        "authors": [
            "Fabrizio Iacone",
            "Luca Rossini",
            "Andrea Viselli"
        ],
        "categories": "econ.EM",
        "published": "2024-05-20T11:19:27Z",
        "updated": "2024-05-20T11:19:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.14893v2",
        "title": "Revisiting Day-ahead Electricity Price: Simple Model Save Millions",
        "abstract": "Accurate day-ahead electricity price forecasting is essential for residential\nwelfare, yet current methods often fall short in forecast accuracy. We observe\nthat commonly used time series models struggle to utilize the prior correlation\nbetween price and demand-supply, which, we found, can contribute a lot to a\nreliable electricity price forecaster. Leveraging this prior, we propose a\nsimple piecewise linear model that significantly enhances forecast accuracy by\ndirectly deriving prices from readily forecastable demand-supply values.\nExperiments in the day-ahead electricity markets of Shanxi province and ISO New\nEngland reveal that such forecasts could potentially save residents millions of\ndollars a year compared to existing methods. Our findings underscore the value\nof suitably integrating time series modeling with economic prior for enhanced\nelectricity price forecasting accuracy.",
        "authors": [
            "Linian Wang",
            "Jianghong Liu",
            "Huibin Zhang",
            "Leye Wang"
        ],
        "categories": "cs.LG",
        "published": "2024-05-20T08:27:14Z",
        "updated": "2024-08-19T14:42:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.11759v2",
        "title": "Testing Sign Congruence Between Two Parameters",
        "abstract": "We test the null hypothesis that two parameters $(\\mu_1,\\mu_2)$ have the same\nsign, assuming that (asymptotically) normal estimators\n$(\\hat{\\mu}_1,\\hat{\\mu}_2)$ are available. Examples of this problem include the\nanalysis of heterogeneous treatment effects, causal interpretation of\nreduced-form estimands, meta-studies, and mediation analysis. A number of tests\nwere recently proposed. We recommend a test that is simple and rejects more\noften than many of these recent proposals. Like all other tests in the\nliterature, it is conservative if the truth is near $(0,0)$ and therefore also\nbiased. To clarify whether these features are avoidable, we also provide a test\nthat is unbiased and has exact size control on the boundary of the null\nhypothesis, but which has counterintuitive properties and hence we do not\nrecommend. We use the test to improve p-values in Kowalski (2022) from\ninformation contained in that paper's main text and to establish statistical\nsignificance of some key estimates in Dippel et al. (2021).",
        "authors": [
            "Douglas L. Miller",
            "Francesca Molinari",
            "J\u00f6rg Stoye"
        ],
        "categories": "econ.EM",
        "published": "2024-05-20T03:37:38Z",
        "updated": "2024-06-12T14:45:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.15600v2",
        "title": "Transfer Learning for Spatial Autoregressive Models with Application to U.S. Presidential Election Prediction",
        "abstract": "It is important to incorporate spatial geographic information into U.S.\npresidential election analysis, especially for swing states. The state-level\nanalysis also faces significant challenges of limited spatial data\navailability. To address the challenges of spatial dependence and small sample\nsizes in predicting U.S. presidential election results using spatially\ndependent data, we propose a novel transfer learning framework within the SAR\nmodel, called as tranSAR. Classical SAR model estimation often loses accuracy\nwith small target data samples. Our framework enhances estimation and\nprediction by leveraging information from similar source data. We introduce a\ntwo-stage algorithm, consisting of a transferring stage and a debiasing stage,\nto estimate parameters and establish theoretical convergence rates for the\nestimators. Additionally, if the informative source data are unknown, we\npropose a transferable source detection algorithm using spatial residual\nbootstrap to maintain spatial dependence and derive its detection consistency.\nSimulation studies show our algorithm substantially improves the classical\ntwo-stage least squares estimator. We demonstrate our method's effectiveness in\npredicting outcomes in U.S. presidential swing states, where it outperforms\ntraditional methods. In addition, our tranSAR model predicts that the\nDemocratic party will win the 2024 U.S. presidential election.",
        "authors": [
            "Hao Zeng",
            "Wei Zhong",
            "Xingbai Xu"
        ],
        "categories": "stat.ML",
        "published": "2024-05-20T03:14:15Z",
        "updated": "2024-09-07T08:59:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.10655v1",
        "title": "Macroeconomic Factors, Industrial Indexes and Bank Spread in Brazil",
        "abstract": "The main objective of this paper is to Identify which macroe conomic factors\nand industrial indexes influenced the total Brazilian banking spread between\nMarch 2011 and March 2015. This paper considers subclassification of industrial\nactivities in Brazil. Monthly time series data were used in multivariate linear\nregression models using Eviews (7.0). Eighteen variables were considered as\ncandidates to be determinants. Variables which positively influenced bank\nspread are; Default, IPIs (Industrial Production Indexes) for capital goods,\nintermediate goods, du rable consumer goods, semi-durable and non-durable\ngoods, the Selic, GDP, unemployment rate and EMBI +. Variables which influence\nnegatively are; Consumer and general consumer goods IPIs, IPCA, the balance of\nthe loan portfolio and the retail sales index. A p-value of 05% was considered.\nThe main conclusion of this work is that the progress of industry, job creation\nand consumption can reduce bank spread. Keywords: Credit. Bank spread.\nMacroeconomics. Industrial Production Indexes. Finance.",
        "authors": [
            "Carlos Alberto Durigan Junior",
            "Andr\u00e9 Taue Saito",
            "Daniel Reed Bergmann",
            "Nuno Manoel Martins Dias Fouto"
        ],
        "categories": "econ.EM",
        "published": "2024-05-17T09:41:57Z",
        "updated": "2024-05-17T09:41:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.10539v1",
        "title": "Overcoming Medical Overuse with AI Assistance: An Experimental Investigation",
        "abstract": "This study evaluates the effectiveness of Artificial Intelligence (AI) in\nmitigating medical overtreatment, a significant issue characterized by\nunnecessary interventions that inflate healthcare costs and pose risks to\npatients. We conducted a lab-in-the-field experiment at a medical school,\nutilizing a novel medical prescription task, manipulating monetary incentives\nand the availability of AI assistance among medical students using a\nthree-by-two factorial design. We tested three incentive schemes: Flat\n(constant pay regardless of treatment quantity), Progressive (pay increases\nwith the number of treatments), and Regressive (penalties for overtreatment) to\nassess their influence on the adoption and effectiveness of AI assistance. Our\nfindings demonstrate that AI significantly reduced overtreatment rates by up to\n62% in the Regressive incentive conditions where (prospective) physician and\npatient interests were most aligned. Diagnostic accuracy improved by 17% to\n37%, depending on the incentive scheme. Adoption of AI advice was high, with\napproximately half of the participants modifying their decisions based on AI\ninput across all settings. For policy implications, we quantified the monetary\n(57%) and non-monetary (43%) incentives of overtreatment and highlighted AI's\npotential to mitigate non-monetary incentives and enhance social welfare. Our\nresults provide valuable insights for healthcare administrators considering AI\nintegration into healthcare systems.",
        "authors": [
            "Ziyi Wang",
            "Lijia Wei",
            "Lian Xue"
        ],
        "categories": "econ.GN",
        "published": "2024-05-17T04:47:36Z",
        "updated": "2024-05-17T04:47:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.10469v1",
        "title": "Simulation-Based Benchmarking of Reinforcement Learning Agents for Personalized Retail Promotions",
        "abstract": "The development of open benchmarking platforms could greatly accelerate the\nadoption of AI agents in retail. This paper presents comprehensive simulations\nof customer shopping behaviors for the purpose of benchmarking reinforcement\nlearning (RL) agents that optimize coupon targeting. The difficulty of this\nlearning problem is largely driven by the sparsity of customer purchase events.\nWe trained agents using offline batch data comprising summarized customer\npurchase histories to help mitigate this effect. Our experiments revealed that\ncontextual bandit and deep RL methods that are less prone to over-fitting the\nsparse reward distributions significantly outperform static policies. This\nstudy offers a practical framework for simulating AI agents that optimize the\nentire retail customer journey. It aims to inspire the further development of\nsimulation tools for retail AI systems.",
        "authors": [
            "Yu Xia",
            "Sriram Narayanamoorthy",
            "Zhengyuan Zhou",
            "Joshua Mabry"
        ],
        "categories": "cs.AI",
        "published": "2024-05-16T23:27:21Z",
        "updated": "2024-05-16T23:27:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.10449v1",
        "title": "Optimal Text-Based Time-Series Indices",
        "abstract": "We propose an approach to construct text-based time-series indices in an\noptimal way--typically, indices that maximize the contemporaneous relation or\nthe predictive performance with respect to a target variable, such as\ninflation. We illustrate our methodology with a corpus of news articles from\nthe Wall Street Journal by optimizing text-based indices focusing on tracking\nthe VIX index and inflation expectations. Our results highlight the superior\nperformance of our approach compared to existing indices.",
        "authors": [
            "David Ardia",
            "Keven Bluteau"
        ],
        "categories": "econ.EM",
        "published": "2024-05-16T21:20:45Z",
        "updated": "2024-05-16T21:20:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.10198v1",
        "title": "Comprehensive Causal Machine Learning",
        "abstract": "Uncovering causal effects at various levels of granularity provides\nsubstantial value to decision makers. Comprehensive machine learning approaches\nto causal effect estimation allow to use a single causal machine learning\napproach for estimation and inference of causal mean effects for all levels of\ngranularity. Focusing on selection-on-observables, this paper compares three\nsuch approaches, the modified causal forest (mcf), the generalized random\nforest (grf), and double machine learning (dml). It also provides proven\ntheoretical guarantees for the mcf and compares the theoretical properties of\nthe approaches. The findings indicate that dml-based methods excel for average\ntreatment effects at the population level (ATE) and group level (GATE) with few\ngroups, when selection into treatment is not too strong. However, for finer\ncausal heterogeneity, explicitly outcome-centred forest-based approaches are\nsuperior. The mcf has three additional benefits: (i) It is the most robust\nestimator in cases when dml-based approaches underperform because of\nsubstantial selectivity; (ii) it is the best estimator for GATEs when the\nnumber of groups gets larger; and (iii), it is the only estimator that is\ninternally consistent, in the sense that low-dimensional causal ATEs and GATEs\nare obtained as aggregates of finer-grained causal parameters.",
        "authors": [
            "Michael Lechner",
            "Jana Mareckova"
        ],
        "categories": "econ.EM",
        "published": "2024-05-16T15:39:09Z",
        "updated": "2024-05-16T15:39:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.09509v2",
        "title": "Double Robustness of Local Projections and Some Unpleasant VARithmetic",
        "abstract": "We consider impulse response inference in a locally misspecified vector\nautoregression (VAR) model. The conventional local projection (LP) confidence\ninterval has correct coverage even when the misspecification is so large that\nit can be detected with probability approaching 1. This result follows from a\n\"double robustness\" property analogous to that of popular partially linear\nregression estimators. In contrast, the conventional VAR confidence interval\nwith short-to-moderate lag length can severely undercover, even for\nmisspecification that is small, economically plausible, and difficult to detect\nstatistically. There is no free lunch: the VAR confidence interval has robust\ncoverage only if the lag length is so large that the interval is as wide as the\nLP interval.",
        "authors": [
            "Jos\u00e9 Luis Montiel Olea",
            "Mikkel Plagborg-M\u00f8ller",
            "Eric Qian",
            "Christian K. Wolf"
        ],
        "categories": "econ.EM",
        "published": "2024-05-15T17:02:00Z",
        "updated": "2024-08-05T13:50:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.09500v1",
        "title": "Identifying Heterogeneous Decision Rules From Choices When Menus Are Unobserved",
        "abstract": "Given only aggregate choice data and limited information about how menus are\ndistributed across the population, we describe what can be inferred robustly\nabout the distribution of preferences (or more general decision rules). We\nstrengthen and generalize existing results on such identification and provide\nan alternative analytical approach to study the problem. We show further that\nour model and results are applicable, after suitable reinterpretation, to other\ncontexts. One application is to the robust identification of the distribution\nof updating rules given only the population distribution of beliefs and limited\ninformation about heterogeneous information sources.",
        "authors": [
            "Larry G Epstein",
            "Kaushil Patel"
        ],
        "categories": "econ.TH",
        "published": "2024-05-15T16:49:19Z",
        "updated": "2024-05-15T16:49:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2406.07564v1",
        "title": "Optimizing Sales Forecasts through Automated Integration of Market Indicators",
        "abstract": "Recognizing that traditional forecasting models often rely solely on\nhistorical demand, this work investigates the potential of data-driven\ntechniques to automatically select and integrate market indicators for\nimproving customer demand predictions. By adopting an exploratory methodology,\nwe integrate macroeconomic time series, such as national GDP growth, from the\n\\textit{Eurostat} database into \\textit{Neural Prophet} and \\textit{SARIMAX}\nforecasting models. Suitable time series are automatically identified through\ndifferent state-of-the-art feature selection methods and applied to sales data\nfrom our industrial partner. It could be shown that forecasts can be\nsignificantly enhanced by incorporating external information. Notably, the\npotential of feature selection methods stands out, especially due to their\ncapability for automation without expert knowledge and manual selection effort.\nIn particular, the Forward Feature Selection technique consistently yielded\nsuperior forecasting accuracy for both SARIMAX and Neural Prophet across\ndifferent company sales datasets. In the comparative analysis of the errors of\nthe selected forecasting models, namely Neural Prophet and SARIMAX, it is\nobserved that neither model demonstrates a significant superiority over the\nother.",
        "authors": [
            "Lina D\u00f6ring",
            "Felix Grumbach",
            "Pascal Reusch"
        ],
        "categories": "econ.EM",
        "published": "2024-05-15T08:11:41Z",
        "updated": "2024-05-15T08:11:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.08806v1",
        "title": "Bounds on the Distribution of a Sum of Two Random Variables: Revisiting a problem of Kolmogorov with application to Individual Treatment Effects",
        "abstract": "We revisit the following problem, proposed by Kolmogorov: given prescribed\nmarginal distributions $F$ and $G$ for random variables $X,Y$ respectively,\ncharacterize the set of compatible distribution functions for the sum $Z=X+Y$.\nBounds on the distribution function for $Z$ were given by Markarov (1982), and\nFrank et al. (1987), the latter using copula theory. However, though they\nobtain the same bounds, they make different assertions concerning their\nsharpness. In addition, their solutions leave some open problems in the case\nwhen the given marginal distribution functions are discontinuous. These issues\nhave led to some confusion and erroneous statements in subsequent literature,\nwhich we correct.\n  Kolmogorov's problem is closely related to inferring possible distributions\nfor individual treatment effects $Y_1 - Y_0$ given the marginal distributions\nof $Y_1$ and $Y_0$; the latter being identified from a randomized experiment.\nWe use our new insights to sharpen and correct results due to Fan and Park\n(2010) concerning individual treatment effects, and to fill some other logical\ngaps.",
        "authors": [
            "Zhehao Zhang",
            "Thomas S. Richardson"
        ],
        "categories": "math.ST",
        "published": "2024-05-14T17:53:08Z",
        "updated": "2024-05-14T17:53:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.08796v2",
        "title": "Variational Bayes and non-Bayesian Updating",
        "abstract": "I show how variational Bayes can be used as a microfoundation for a popular\nmodel of non-Bayesian updating.",
        "authors": [
            "Tomasz Strzalecki"
        ],
        "categories": "econ.TH",
        "published": "2024-05-14T17:45:53Z",
        "updated": "2024-05-21T17:43:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.08687v1",
        "title": "Latent group structure in linear panel data models with endogenous regressors",
        "abstract": "This paper concerns the estimation of linear panel data models with\nendogenous regressors and a latent group structure in the coefficients. We\nconsider instrumental variables estimation of the group-specific coefficient\nvector. We show that direct application of the Kmeans algorithm to the\ngeneralized method of moments objective function does not yield unique\nestimates. We newly develop and theoretically justify two-stage estimation\nmethods that apply the Kmeans algorithm to a regression of the dependent\nvariable on predicted values of the endogenous regressors. The results of Monte\nCarlo simulations demonstrate that two-stage estimation with the first stage\nmodeled using a latent group structure achieves good classification accuracy,\neven if the true first-stage regression is fully heterogeneous. We apply our\nestimation methods to revisiting the relationship between income and democracy.",
        "authors": [
            "Junho Choi",
            "Ryo Okui"
        ],
        "categories": "econ.EM",
        "published": "2024-05-14T15:09:11Z",
        "updated": "2024-05-14T15:09:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.08284v1",
        "title": "Predicting NVIDIA's Next-Day Stock Price: A Comparative Analysis of LSTM, MLP, ARIMA, and ARIMA-GARCH Models",
        "abstract": "Forecasting stock prices remains a considerable challenge in financial\nmarkets, bearing significant implications for investors, traders, and financial\ninstitutions. Amid the ongoing AI revolution, NVIDIA has emerged as a key\nplayer driving innovation across various sectors. Given its prominence, we\nchose NVIDIA as the subject of our study.",
        "authors": [
            "Yiluan Xing",
            "Chao Yan",
            "Cathy Chang Xie"
        ],
        "categories": "econ.EM",
        "published": "2024-05-14T02:50:23Z",
        "updated": "2024-05-14T02:50:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.08222v2",
        "title": "Random Utility Models with Skewed Random Components: the Smallest versus Largest Extreme Value Distribution",
        "abstract": "At the core of most random utility models (RUMs) is an individual agent with\na random utility component following a largest extreme value Type I (LEVI)\ndistribution. What if, instead, the random component follows its mirror image\n-- the smallest extreme value Type I (SEVI) distribution? Differences between\nthese specifications, closely tied to the random component's skewness, can be\nquite profound. For the same preference parameters, the two RUMs, equivalent\nwith only two choice alternatives, diverge progressively as the number of\nalternatives increases, resulting in substantially different estimates and\npredictions for key measures, such as elasticities and market shares.\n  The LEVI model imposes the well-known independence-of-irrelevant-alternatives\nproperty, while SEVI does not. Instead, the SEVI choice probability for a\nparticular option involves enumerating all subsets that contain this option.\nThe SEVI model, though more complex to estimate, is shown to have\ncomputationally tractable closed-form choice probabilities. Much of the paper\ndelves into explicating the properties of the SEVI model and exploring\nimplications of the random component's skewness.\n  Conceptually, the difference between the LEVI and SEVI models centers on\nwhether information, known only to the agent, is more likely to increase or\ndecrease the systematic utility parameterized using observed attributes. LEVI\ndoes the former; SEVI the latter. An immediate implication is that if choice is\ncharacterized by SEVI random components, then the observed choice is more\nlikely to correspond to the systematic-utility-maximizing choice than if\ncharacterized by LEVI. Examining standard empirical examples from different\napplied areas, we find that the SEVI model outperforms the LEVI model,\nsuggesting the relevance of its inclusion in applied researchers' toolkits.",
        "authors": [
            "Richard T. Carson",
            "Derrick H. Sun",
            "Yixiao Sun"
        ],
        "categories": "econ.EM",
        "published": "2024-05-13T22:24:29Z",
        "updated": "2024-05-22T01:37:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.07860v3",
        "title": "Simultaneous Inference for Local Structural Parameters with Random Forests",
        "abstract": "We construct simultaneous confidence intervals for solutions to conditional\nmoment equations. The intervals are built around a class of nonparametric\nregression algorithms based on subsampled kernels. This class encompasses\nvarious forms of subsampled random forest regression, including Generalized\nRandom Forests (Athey et al., 2019). Although simultaneous validity is often\ndesirable in practice -- for example, for fine-grained characterization of\ntreatment effect heterogeneity -- only confidence intervals that confer\npointwise guarantees were previously available. Our work closes this gap. As a\nby-product, we obtain several new order-explicit results on the concentration\nand normal approximation of high-dimensional U-statistics.",
        "authors": [
            "David M. Ritzwoller",
            "Vasilis Syrgkanis"
        ],
        "categories": "econ.EM",
        "published": "2024-05-13T15:46:11Z",
        "updated": "2024-09-09T18:33:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.07420v2",
        "title": "Robust Inference for High-Dimensional Panel Data Models",
        "abstract": "In this paper, we propose a robust estimation and inferential method for\nhigh-dimensional panel data models. Specifically, (1) we investigate the case\nwhere the number of regressors can grow faster than the sample size, (2) we pay\nparticular attention to non-Gaussian, serially and cross-sectionally correlated\nand heteroskedastic error processes, and (3) we develop an estimation method\nfor high-dimensional long-run covariance matrix using a thresholded estimator.\n  Methodologically and technically, we develop two Nagaev-types of\nconcentration inequalities: one for a partial sum and the other for a quadratic\nform, subject to a set of easily verifiable conditions. Leveraging these two\ninequalities, we also derive a non-asymptotic bound for the LASSO estimator,\nachieve asymptotic normality via the node-wise LASSO regression, and establish\na sharp convergence rate for the thresholded heteroskedasticity and\nautocorrelation consistent (HAC) estimator.\n  Our study thus provides the relevant literature with a complete toolkit for\nconducting inference about the parameters of interest involved in a\nhigh-dimensional panel data framework. We also demonstrate the practical\nrelevance of these theoretical results by investigating a high-dimensional\npanel data model with interactive fixed effects. Moreover, we conduct extensive\nnumerical studies using simulated and real data examples.",
        "authors": [
            "Jiti Gao",
            "Bin Peng",
            "Yayi Yan"
        ],
        "categories": "econ.EM",
        "published": "2024-05-13T01:39:25Z",
        "updated": "2024-08-14T23:19:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.07292v2",
        "title": "Kernel Three Pass Regression Filter",
        "abstract": "We forecast a single time series using a high-dimensional set of predictors.\nWhen these predictors share common underlying dynamics, an approximate latent\nfactor model provides a powerful characterization of their co-movements\nBai(2003). These latent factors succinctly summarize the data and can also be\nused for prediction, alleviating the curse of dimensionality in\nhigh-dimensional prediction exercises, see Stock & Watson (2002a). However,\nforecasting using these latent factors suffers from two potential drawbacks.\nFirst, not all pervasive factors among the set of predictors may be relevant,\nand using all of them can lead to inefficient forecasts. The second shortcoming\nis the assumption of linear dependence of predictors on the underlying factors.\nThe first issue can be addressed by using some form of supervision, which leads\nto the omission of irrelevant information. One example is the three-pass\nregression filter proposed by Kelly & Pruitt (2015). We extend their framework\nto cases where the form of dependence might be nonlinear by developing a new\nestimator, which we refer to as the Kernel Three-Pass Regression Filter\n(K3PRF). This alleviates the aforementioned second shortcoming. The estimator\nis computationally efficient and performs well empirically. The short-term\nperformance matches or exceeds that of established models, while the long-term\nperformance shows significant improvement.",
        "authors": [
            "Rajveer Jat",
            "Daanish Padha"
        ],
        "categories": "econ.EM",
        "published": "2024-05-12T14:09:12Z",
        "updated": "2024-06-07T06:37:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.07134v1",
        "title": "On the Ollivier-Ricci curvature as fragility indicator of the stock markets",
        "abstract": "Recently, an indicator for stock market fragility and crash size in terms of\nthe Ollivier-Ricci curvature has been proposed. We study analytical and\nempirical properties of such indicator, test its elasticity with respect to\ndifferent parameters and provide heuristics for the parameters involved. We\nshow when and how the indicator accurately describes a financial crisis. We\nalso propose an alternate method for calculating the indicator using a specific\nsub-graph with special curvature properties.",
        "authors": [
            "Joaqu\u00edn S\u00e1nchez Garc\u00eda",
            "Sebastian Gherghe"
        ],
        "categories": "econ.EM",
        "published": "2024-05-12T02:13:28Z",
        "updated": "2024-05-12T02:13:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.06850v1",
        "title": "Identifying Peer Effects in Networks with Unobserved Effort and Isolated Students",
        "abstract": "Peer influence on effort devoted to some activity is often studied using\nproxy variables when actual effort is unobserved. For instance, in education,\nacademic effort is often proxied by GPA. We propose an alternative approach\nthat circumvents this approximation. Our framework distinguishes unobserved\nshocks to GPA that do not affect effort from preference shocks that do affect\neffort levels. We show that peer effects estimates obtained using our approach\ncan differ significantly from classical estimates (where effort is\napproximated) if the network includes isolated students. Applying our approach\nto data on high school students in the United States, we find that peer effect\nestimates relying on GPA as a proxy for effort are 40% lower than those\nobtained using our approach.",
        "authors": [
            "Aristide Houndetoungan",
            "Cristelle Kouame",
            "Michael Vlassopoulos"
        ],
        "categories": "econ.EM",
        "published": "2024-05-10T23:24:30Z",
        "updated": "2024-05-10T23:24:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.06779v2",
        "title": "Generalization Issues in Experiments Involving Multidimensional Decisions",
        "abstract": "Can the causal effects estimated in an experiment be generalized to\nreal-world scenarios? This question lies at the heart of social science\nstudies. External validity primarily assesses whether experimental effects\npersist across different settings, implicitly presuming the consistency of\nexperimental effects with their real-life counterparts. However, we argue that\nthis presumed consistency may not always hold, especially in experiments\ninvolving multi-dimensional decision processes, such as conjoint experiments.\nWe introduce a formal model to elucidate how attention and salience effects\nlead to three types of inconsistencies between experimental findings and\nreal-world phenomena: amplified effect magnitude, effect sign reversal, and\neffect importance reversal. We derive testable hypotheses from each theoretical\noutcome and test these hypotheses using data from various existing conjoint\nexperiments and our own experiments. Drawing on our theoretical framework, we\npropose several recommendations for experimental design aimed at enhancing the\ngeneralizability of survey experiment findings.",
        "authors": [
            "Jiawei Fu",
            "Xiaojun Li"
        ],
        "categories": "econ.EM",
        "published": "2024-05-10T19:10:18Z",
        "updated": "2024-09-01T17:09:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.06156v1",
        "title": "A Sharp Test for the Judge Leniency Design",
        "abstract": "We propose a new specification test to assess the validity of the judge\nleniency design. We characterize a set of sharp testable implications, which\nexploit all the relevant information in the observed data distribution to\ndetect violations of the judge leniency design assumptions. The proposed sharp\ntest is asymptotically valid and consistent and will not make discordant\nrecommendations. When the judge's leniency design assumptions are rejected, we\npropose a way to salvage the model using partial monotonicity and exclusion\nassumptions, under which a variant of the Local Instrumental Variable (LIV)\nestimand can recover the Marginal Treatment Effect. Simulation studies show our\ntest outperforms existing non-sharp tests by significant margins. We apply our\ntest to assess the validity of the judge leniency design using data from\nStevenson (2018), and it rejects the validity for three crime categories:\nrobbery, drug selling, and drug possession.",
        "authors": [
            "Mohamed Coulibaly",
            "Yu-Chin Hsu",
            "Ismael Mourifi\u00e9",
            "Yuanyuan Wan"
        ],
        "categories": "econ.EM",
        "published": "2024-05-10T00:36:25Z",
        "updated": "2024-05-10T00:36:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.05759v1",
        "title": "Advancing Distribution Decomposition Methods Beyond Common Supports: Applications to Racial Wealth Disparities",
        "abstract": "I generalize state-of-the-art approaches that decompose differences in the\ndistribution of a variable of interest between two groups into a portion\nexplained by covariates and a residual portion. The method that I propose\nrelaxes the overlapping supports assumption, allowing the groups being compared\nto not necessarily share exactly the same covariate support. I illustrate my\nmethod revisiting the black-white wealth gap in the U.S. as a function of labor\nincome and other variables. Traditionally used decomposition methods would trim\n(or assign zero weight to) observations that lie outside the common covariate\nsupport region. On the other hand, by allowing all observations to contribute\nto the existing wealth gap, I find that otherwise trimmed observations\ncontribute from 3% to 19% to the overall wealth gap, at different portions of\nthe wealth distribution.",
        "authors": [
            "Bernardo Modenesi"
        ],
        "categories": "econ.EM",
        "published": "2024-05-09T13:28:07Z",
        "updated": "2024-05-09T13:28:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.05534v1",
        "title": "Sequential Validation of Treatment Heterogeneity",
        "abstract": "We use the martingale construction of Luedtke and van der Laan (2016) to\ndevelop tests for the presence of treatment heterogeneity. The resulting\nsequential validation approach can be instantiated using various validation\nmetrics, such as BLPs, GATES, QINI curves, etc., and provides an alternative to\ncross-validation-like cross-fold application of these metrics.",
        "authors": [
            "Stefan Wager"
        ],
        "categories": "econ.EM",
        "published": "2024-05-09T04:14:55Z",
        "updated": "2024-05-09T04:14:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.05220v1",
        "title": "Causal Duration Analysis with Diff-in-Diff",
        "abstract": "In economic program evaluation, it is common to obtain panel data in which\noutcomes are indicators that an individual has reached an absorbing state. For\nexample, they may indicate whether an individual has exited a period of\nunemployment, passed an exam, left a marriage, or had their parole revoked. The\nparallel trends assumption that underpins difference-in-differences generally\nfails in such settings. We suggest identifying conditions that are analogous to\nthose of difference-in-differences but apply to hazard rates rather than mean\noutcomes. These alternative assumptions motivate estimators that retain the\nsimplicity and transparency of standard diff-in-diff, and we suggest analogous\nspecification tests. Our approach can be adapted to general linear restrictions\nbetween the hazard rates of different groups, motivating duration analogues of\nthe triple differences and synthetic control methods. We apply our procedures\nto examine the impact of a policy that increased the generosity of unemployment\nbenefits, using a cross-cohort comparison.",
        "authors": [
            "Ben Deaner",
            "Hyejin Ku"
        ],
        "categories": "econ.EM",
        "published": "2024-05-08T17:13:34Z",
        "updated": "2024-05-08T17:13:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.04973v1",
        "title": "SVARs with breaks: Identification and inference",
        "abstract": "In this paper we propose a class of structural vector autoregressions (SVARs)\ncharacterized by structural breaks (SVAR-WB). Together with standard\nrestrictions on the parameters and on functions of them, we also consider\nconstraints across the different regimes. Such constraints can be either (a) in\nthe form of stability restrictions, indicating that not all the parameters or\nimpulse responses are subject to structural changes, or (b) in terms of\ninequalities regarding particular characteristics of the SVAR-WB across the\nregimes. We show that all these kinds of restrictions provide benefits in terms\nof identification. We derive conditions for point and set identification of the\nstructural parameters of the SVAR-WB, mixing equality, sign, rank and stability\nrestrictions, as well as constraints on forecast error variances (FEVs). As\npoint identification, when achieved, holds locally but not globally, there will\nbe a set of isolated structural parameters that are observationally equivalent\nin the parametric space. In this respect, both common frequentist and Bayesian\napproaches produce unreliable inference as the former focuses on just one of\nthese observationally equivalent points, while for the latter on a\nnon-vanishing sensitivity to the prior. To overcome these issues, we propose\nalternative approaches for estimation and inference that account for all\nadmissible observationally equivalent structural parameters. Moreover, we\ndevelop a pure Bayesian and a robust Bayesian approach for doing inference in\nset-identified SVAR-WBs. Both the theory of identification and inference are\nillustrated through a set of examples and an empirical application on the\ntransmission of US monetary policy over the great inflation and great\nmoderation regimes.",
        "authors": [
            "Emanuele Bacchiocchi",
            "Toru Kitagawa"
        ],
        "categories": "econ.EM",
        "published": "2024-05-08T11:26:32Z",
        "updated": "2024-05-08T11:26:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.04816v3",
        "title": "Testing the Fairness-Accuracy Improvability of Algorithms",
        "abstract": "Many organizations use algorithms that have a disparate impact, i.e., the\nbenefits or harms of the algorithm fall disproportionately on certain social\ngroups. Addressing an algorithm's disparate impact can be challenging, however,\nbecause it is often unclear whether it is possible to reduce this impact\nwithout sacrificing other objectives of the organization, such as accuracy or\nprofit. Establishing the improvability of algorithms with respect to multiple\ncriteria is of both conceptual and practical interest: in many settings,\ndisparate impact that would otherwise be prohibited under US federal law is\npermissible if it is necessary to achieve a legitimate business interest. The\nquestion is how a policy-maker can formally substantiate, or refute, this\n\"necessity\" defense. In this paper, we provide an econometric framework for\ntesting the hypothesis that it is possible to improve on the fairness of an\nalgorithm without compromising on other pre-specified objectives. Our proposed\ntest is simple to implement and can be applied under any exogenous constraint\non the algorithm space. We establish the large-sample validity and consistency\nof our test, and illustrate its practical application by evaluating a\nhealthcare algorithm originally considered by Obermeyer et al. (2019). In this\napplication, we reject the null hypothesis that it is not possible to reduce\nthe algorithm's disparate impact without compromising the accuracy of its\npredictions.",
        "authors": [
            "Eric Auerbach",
            "Annie Liang",
            "Kyohei Okumura",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2024-05-08T05:34:18Z",
        "updated": "2024-07-03T18:22:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.04465v4",
        "title": "Two-way Fixed Effects and Differences-in-Differences Estimators in Heterogeneous Adoption Designs",
        "abstract": "We consider treatment-effect estimation under a parallel trends assumption,\nin designs where no unit is treated at period one, all units receive a strictly\npositive dose at period two, and the dose varies across units. There are\ntherefore no true control groups in such cases. First, we develop a test of the\nassumption that the treatment effect is mean independent of the treatment,\nunder which the commonly-used two-way-fixed-effects estimator is consistent.\nWhen this test is rejected or lacks power, we propose alternative estimators,\nrobust to heterogeneous effects. If there are units with a period-two treatment\narbitrarily close to zero, the robust estimator is a difference-in-difference\nusing units with a period-two treatment below a bandwidth as controls. Without\nsuch units, we propose non-parametric bounds, and an estimator relying on a\nparametric specification of treatment-effect heterogeneity. We use our results\nto revisit Pierce and Schott (2016) and Enikolopov et al. (2011).",
        "authors": [
            "Cl\u00e9ment de Chaisemartin",
            "Diego Ciccia Xavier D'Haultf\u0153uille",
            "Felix Knau"
        ],
        "categories": "econ.EM",
        "published": "2024-05-07T16:29:37Z",
        "updated": "2024-11-19T21:39:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.04365v1",
        "title": "Detailed Gender Wage Gap Decompositions: Controlling for Worker Unobserved Heterogeneity Using Network Theory",
        "abstract": "Recent advances in the literature of decomposition methods in economics have\nallowed for the identification and estimation of detailed wage gap\ndecompositions. In this context, building reliable counterfactuals requires\nusing tighter controls to ensure that similar workers are correctly identified\nby making sure that important unobserved variables such as skills are\ncontrolled for, as well as comparing only workers with similar observable\ncharacteristics. This paper contributes to the wage decomposition literature in\ntwo main ways: (i) developing an economic principled network based approach to\ncontrol for unobserved worker skills heterogeneity in the presence of potential\ndiscrimination; and (ii) extending existing generic decomposition tools to\naccommodate for potential lack of overlapping supports in covariates between\ngroups being compared, which is likely to be the norm in more detailed\ndecompositions. We illustrate the methodology by decomposing the gender wage\ngap in Brazil.",
        "authors": [
            "Jamie Fogel",
            "Bernardo Modenesi"
        ],
        "categories": "econ.EM",
        "published": "2024-05-07T14:45:43Z",
        "updated": "2024-05-07T14:45:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.03910v1",
        "title": "A Primer on the Analysis of Randomized Experiments and a Survey of some Recent Advances",
        "abstract": "The past two decades have witnessed a surge of new research in the analysis\nof randomized experiments. The emergence of this literature may seem surprising\ngiven the widespread use and long history of experiments as the \"gold standard\"\nin program evaluation, but this body of work has revealed many subtle aspects\nof randomized experiments that may have been previously unappreciated. This\narticle provides an overview of some of these topics, primarily focused on\nstratification, regression adjustment, and cluster randomization.",
        "authors": [
            "Yuehao Bai",
            "Azeem M. Shaikh",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2024-05-06T23:59:24Z",
        "updated": "2024-05-06T23:59:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.03826v1",
        "title": "A quantile-based nonadditive fixed effects model",
        "abstract": "I propose a quantile-based nonadditive fixed effects panel model to study\nheterogeneous causal effects. Similar to standard fixed effects (FE) model, my\nmodel allows arbitrary dependence between regressors and unobserved\nheterogeneity, but it generalizes the additive separability of standard FE to\nallow the unobserved heterogeneity to enter nonseparably. Similar to structural\nquantile models, my model's random coefficient vector depends on an unobserved,\nscalar ''rank'' variable, in which outcomes (excluding an additive noise term)\nare monotonic at a particular value of the regressor vector, which is much\nweaker than the conventional monotonicity assumption that must hold at all\npossible values. This rank is assumed to be stable over time, which is often\nmore economically plausible than the panel quantile studies that assume\nindividual rank is iid over time. It uncovers the heterogeneous causal effects\nas functions of the rank variable. I provide identification and estimation\nresults, establishing uniform consistency and uniform asymptotic normality of\nthe heterogeneous causal effect function estimator. Simulations show reasonable\nfinite-sample performance and show my model complements fixed effects quantile\nregression. Finally, I illustrate the proposed methods by examining the causal\neffect of a country's oil wealth on its military defense spending.",
        "authors": [
            "Xin Liu"
        ],
        "categories": "econ.EM",
        "published": "2024-05-06T20:16:30Z",
        "updated": "2024-05-06T20:16:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.03021v1",
        "title": "Tuning parameter selection in econometrics",
        "abstract": "I review some of the main methods for selecting tuning parameters in\nnonparametric and $\\ell_1$-penalized estimation. For the nonparametric\nestimation, I consider the methods of Mallows, Stein, Lepski, cross-validation,\npenalization, and aggregation in the context of series estimation. For the\n$\\ell_1$-penalized estimation, I consider the methods based on the theory of\nself-normalized moderate deviations, bootstrap, Stein's unbiased risk\nestimation, and cross-validation in the context of Lasso estimation. I explain\nthe intuition behind each of the methods and discuss their comparative\nadvantages. I also give some extensions.",
        "authors": [
            "Denis Chetverikov"
        ],
        "categories": "econ.EM",
        "published": "2024-05-05T18:08:24Z",
        "updated": "2024-05-05T18:08:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.02480v1",
        "title": "A Network Simulation of OTC Markets with Multiple Agents",
        "abstract": "We present a novel agent-based approach to simulating an over-the-counter\n(OTC) financial market in which trades are intermediated solely by market\nmakers and agent visibility is constrained to a network topology. Dynamics,\nsuch as changes in price, result from agent-level interactions that\nubiquitously occur via market maker agents acting as liquidity providers. Two\nadditional agents are considered: trend investors use a deep convolutional\nneural network paired with a deep Q-learning framework to inform trading\ndecisions by analysing price history; and value investors use a static\nprice-target to determine their trade directions and sizes. We demonstrate that\nour novel inclusion of a network topology with market makers facilitates\nexplorations into various market structures. First, we present the model and an\noverview of its mechanics. Second, we validate our findings via comparison to\nthe real-world: we demonstrate a fat-tailed distribution of price changes,\nauto-correlated volatility, a skew negatively correlated to market maker\npositioning, predictable price-history patterns and more. Finally, we\ndemonstrate that our network-based model can lend insights into the effect of\nmarket-structure on price-action. For example, we show that markets with\nsparsely connected intermediaries can have a critical point of fragmentation,\nbeyond which the market forms distinct clusters and arbitrage becomes rapidly\npossible between the prices of different market makers. A discussion is\nprovided on future work that would be beneficial.",
        "authors": [
            "James T. Wilkinson",
            "Jacob Kelter",
            "John Chen",
            "Uri Wilensky"
        ],
        "categories": "econ.EM",
        "published": "2024-05-03T20:45:00Z",
        "updated": "2024-05-03T20:45:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.02217v4",
        "title": "Identifying and exploiting alpha in linear asset pricing models with strong, semi-strong, and latent factors",
        "abstract": "The risk premia of traded factors are the sum of factor means and a parameter\nvector we denote by {\\phi} which is identified from the cross section\nregression of alpha of individual securities on the vector of factor loadings.\nIf phi is non-zero one can construct \"phi-portfolios\" which exploit the\nsystematic components of non-zero alpha. We show that for known values of betas\nand when phi is non-zero there exist phi-portfolios that dominate mean-variance\nportfolios. The paper then proposes a two-step bias corrected estimator of phi\nand derives its asymptotic distribution allowing for idiosyncratic pricing\nerrors, weak missing factors, and weak error cross-sectional dependence. Small\nsample results from extensive Monte Carlo experiments show that the proposed\nestimator has the correct size with good power properties. The paper also\nprovides an empirical application to a large number of U.S. securities with\nrisk factors selected from a large number of potential risk factors according\nto their strength and constructs phi-portfolios and compares their Sharpe\nratios to mean variance and S&P 500 portfolio.",
        "authors": [
            "M. Hashem Pesaran",
            "Ron P. Smith"
        ],
        "categories": "econ.EM",
        "published": "2024-05-03T16:22:21Z",
        "updated": "2024-10-22T08:12:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.02087v1",
        "title": "Testing for an Explosive Bubble using High-Frequency Volatility",
        "abstract": "Based on a continuous-time stochastic volatility model with a linear drift,\nwe develop a test for explosive behavior in financial asset prices at a low\nfrequency when prices are sampled at a higher frequency. The test exploits the\nvolatility information in the high-frequency data. The method consists of\ndevolatizing log-asset price increments with realized volatility measures and\nperforming a supremum-type recursive Dickey-Fuller test on the devolatized\nsample. The proposed test has a nuisance-parameter-free asymptotic distribution\nand is easy to implement. We study the size and power properties of the test in\nMonte Carlo simulations. A real-time date-stamping strategy based on the\ndevolatized sample is proposed for the origination and conclusion dates of the\nexplosive regime. Conditions under which the real-time date-stamping strategy\nis consistent are established. The test and the date-stamping strategy are\napplied to study explosive behavior in cryptocurrency and stock markets.",
        "authors": [
            "H. Peter Boswijk",
            "Jun Yu",
            "Yang Zu"
        ],
        "categories": "econ.EM",
        "published": "2024-05-03T13:23:09Z",
        "updated": "2024-05-03T13:23:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.01913v1",
        "title": "Unleashing the Power of AI: Transforming Marketing Decision-Making in Heavy Machinery with Machine Learning, Radar Chart Simulation, and Markov Chain Analysis",
        "abstract": "This pioneering research introduces a novel approach for decision-makers in\nthe heavy machinery industry, specifically focusing on production management.\nThe study integrates machine learning techniques like Ridge Regression, Markov\nchain analysis, and radar charts to optimize North American Crawler Cranes\nmarket production processes. Ridge Regression enables growth pattern\nidentification and performance assessment, facilitating comparisons and\naddressing industry challenges. Markov chain analysis evaluates risk factors,\naiding in informed decision-making and risk management. Radar charts simulate\nbenchmark product designs, enabling data-driven decisions for production\noptimization. This interdisciplinary approach equips decision-makers with\ntransformative insights, enhancing competitiveness in the heavy machinery\nindustry and beyond. By leveraging these techniques, companies can\nrevolutionize their production management strategies, driving success in\ndiverse markets.",
        "authors": [
            "Tian Tian",
            "Jiahao Deng"
        ],
        "categories": "econ.EM",
        "published": "2024-05-03T08:12:14Z",
        "updated": "2024-05-03T08:12:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.01645v1",
        "title": "Synthetic Controls with spillover effects: A comparative study",
        "abstract": "Iterative Synthetic Control Method is introduced in this study, a\nmodification of the Synthetic Control Method (SCM) designed to improve its\npredictive performance by utilizing control units affected by the treatment in\nquestion. This method is then compared to other SCM modifications: SCM without\nany modifications, SCM after removing all spillover-affected units, Inclusive\nSCM, and the SP SCM model. For the comparison, Monte Carlo simulations are\nutilized, generating artificial datasets with known counterfactuals and\ncomparing the predictive performance of the methods. Generally, the Inclusive\nSCM performed best in all settings and is relatively simple to implement. The\nIterative SCM, introduced in this paper, was in close seconds, with a small\ndifference in performance and a simpler implementation.",
        "authors": [
            "Andrii Melnychuk"
        ],
        "categories": "econ.EM",
        "published": "2024-05-02T18:05:53Z",
        "updated": "2024-05-02T18:05:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.01484v2",
        "title": "Designing Algorithmic Recommendations to Achieve Human-AI Complementarity",
        "abstract": "Algorithms frequently assist, rather than replace, human decision-makers.\nHowever, the design and analysis of algorithms often focus on predicting\noutcomes and do not explicitly model their effect on human decisions. This\ndiscrepancy between the design and role of algorithmic assistants becomes\nparticularly concerning in light of empirical evidence that suggests that\nalgorithmic assistants again and again fail to improve human decisions. In this\narticle, we formalize the design of recommendation algorithms that assist human\ndecision-makers without making restrictive ex-ante assumptions about how\nrecommendations affect decisions. We formulate an algorithmic-design problem\nthat leverages the potential-outcomes framework from causal inference to model\nthe effect of recommendations on a human decision-maker's binary treatment\nchoice. Within this model, we introduce a monotonicity assumption that leads to\nan intuitive classification of human responses to the algorithm. Under this\nassumption, we can express the human's response to algorithmic recommendations\nin terms of their compliance with the algorithm and the active decision they\nwould take if the algorithm sends no recommendation. We showcase the utility of\nour framework using an online experiment that simulates a hiring task. We argue\nthat our approach can make sense of the relative performance of different\nrecommendation algorithms in the experiment and can help design solutions that\nrealize human-AI complementarity. Finally, we leverage our approach to derive\nminimax optimal recommendation algorithms that can be implemented with machine\nlearning using limited training data.",
        "authors": [
            "Bryce McLaughlin",
            "Jann Spiess"
        ],
        "categories": "cs.HC",
        "published": "2024-05-02T17:15:30Z",
        "updated": "2024-10-30T03:56:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.01463v2",
        "title": "Dynamic Local Average Treatment Effects",
        "abstract": "We consider Dynamic Treatment Regimes (DTRs) with One Sided Noncompliance\nthat arise in applications such as digital recommendations and adaptive medical\ntrials. These are settings where decision makers encourage individuals to take\ntreatments over time, but adapt encouragements based on previous\nencouragements, treatments, states, and outcomes. Importantly, individuals may\nnot comply with encouragements based on unobserved confounders. For settings\nwith binary treatments and encouragements, we provide nonparametric\nidentification, estimation, and inference for Dynamic Local Average Treatment\nEffects (LATEs), which are expected values of multiple time period treatment\ncontrasts for the respective complier subpopulations. Under standard\nassumptions in the Instrumental Variable and DTR literature, we show that one\ncan identify Dynamic LATEs that correspond to treating at single time steps.\nUnder an additional cross-period effect-compliance independence assumption,\nwhich is satisfied in Staggered Adoption settings and a generalization of them,\nwhich we define as Staggered Compliance settings, we identify Dynamic LATEs for\ntreating in multiple time periods.",
        "authors": [
            "Ravi B. Sojitra",
            "Vasilis Syrgkanis"
        ],
        "categories": "econ.EM",
        "published": "2024-05-02T16:52:09Z",
        "updated": "2024-05-13T20:42:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.01281v1",
        "title": "Demistifying Inference after Adaptive Experiments",
        "abstract": "Adaptive experiments such as multi-arm bandits adapt the treatment-allocation\npolicy and/or the decision to stop the experiment to the data observed so far.\nThis has the potential to improve outcomes for study participants within the\nexperiment, to improve the chance of identifying best treatments after the\nexperiment, and to avoid wasting data. Seen as an experiment (rather than just\na continually optimizing system) it is still desirable to draw statistical\ninferences with frequentist guarantees. The concentration inequalities and\nunion bounds that generally underlie adaptive experimentation algorithms can\nyield overly conservative inferences, but at the same time the asymptotic\nnormality we would usually appeal to in non-adaptive settings can be imperiled\nby adaptivity. In this article we aim to explain why, how, and when adaptivity\nis in fact an issue for inference and, when it is, understand the various ways\nto fix it: reweighting to stabilize variances and recover asymptotic normality,\nalways-valid inference based on joint normality of an asymptotic limiting\nsequence, and characterizing and inverting the non-normal distributions induced\nby adaptivity.",
        "authors": [
            "Aur\u00e9lien Bibaut",
            "Nathan Kallus"
        ],
        "categories": "stat.ME",
        "published": "2024-05-02T13:39:51Z",
        "updated": "2024-05-02T13:39:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.00953v2",
        "title": "Asymptotic Properties of the Distributional Synthetic Controls",
        "abstract": "As an alternative to synthetic control, the distributional Synthetic Control\n(DSC) proposed by Gunsilius (2023) provides estimates for quantile treatment\neffect and thus enabling researchers to comprehensively understand the impact\nof interventions in causal inference. But the asymptotic properties of DSC have\nnot been built. In this paper, we first establish the DSC estimator's\nasymptotic optimality in the essence that the treatment effect estimator given\nby DSC achieves the lowest possible squared prediction error among all\npotential estimators from averaging quantiles of control units. We then\nestablish the convergence rate of the DSC weights. A significant aspect of our\nresearch is that we find the DSC synthesis forms an optimal weighted average,\nparticularly in situations where it is impractical to perfectly fit the treated\nunit's quantiles through the weighted average of the control units' quantiles.\nSimulation results verify our theoretical insights.",
        "authors": [
            "Lu Zhang",
            "Xiaomeng Zhang",
            "Xinyu Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-05-02T02:30:26Z",
        "updated": "2024-08-20T11:11:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.00910v1",
        "title": "De-Biasing Models of Biased Decisions: A Comparison of Methods Using Mortgage Application Data",
        "abstract": "Prediction models can improve efficiency by automating decisions such as the\napproval of loan applications. However, they may inherit bias against protected\ngroups from the data they are trained on. This paper adds counterfactual\n(simulated) ethnic bias to real data on mortgage application decisions, and\nshows that this bias is replicated by a machine learning model (XGBoost) even\nwhen ethnicity is not used as a predictive variable. Next, several other\nde-biasing methods are compared: averaging over prohibited variables, taking\nthe most favorable prediction over prohibited variables (a novel method), and\njointly minimizing errors as well as the association between predictions and\nprohibited variables. De-biasing can recover some of the original decisions,\nbut the results are sensitive to whether the bias is effected through a proxy.",
        "authors": [
            "Nicholas Tenev"
        ],
        "categories": "cs.LG",
        "published": "2024-05-01T23:46:44Z",
        "updated": "2024-05-01T23:46:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.00424v2",
        "title": "Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution",
        "abstract": "Ridge regression is an indispensable tool in big data analysis. Yet its\ninherent bias poses a significant and longstanding challenge, compromising both\nstatistical efficiency and scalability across various applications. To tackle\nthis critical issue, we introduce an iterative strategy to correct bias\neffectively when the dimension $p$ is less than the sample size $n$. For $p>n$,\nour method optimally mitigates the bias such that any remaining bias in the\nproposed de-biased estimator is unattainable through linear transformations of\nthe response data. To address the remaining bias when $p>n$, we employ a\nRidge-Screening (RS) method, producing a reduced model suitable for bias\ncorrection. Crucially, under certain conditions, the true model is nested\nwithin our selected one, highlighting RS as a novel variable selection\napproach. Through rigorous analysis, we establish the asymptotic properties and\nvalid inferences of our de-biased ridge estimators for both $p<n$ and $p>n$,\nwhere, both $p$ and $n$ may increase towards infinity, along with the number of\niterations. We further validate these results using simulated and real-world\ndata examples. Our method offers a transformative solution to the bias\nchallenge in ridge regression inferences across various disciplines.",
        "authors": [
            "Zhaoxing Gao",
            "Ruey S. Tsay"
        ],
        "categories": "econ.EM",
        "published": "2024-05-01T10:05:19Z",
        "updated": "2024-07-24T15:59:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2405.00161v3",
        "title": "Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory",
        "abstract": "Analyses of heterogeneous treatment effects (HTE) are common in applied\ncausal inference research. However, when outcomes are latent variables assessed\nvia psychometric instruments such as educational tests, standard methods ignore\nthe potential HTE that may exist among the individual items of the outcome\nmeasure. Failing to account for ``item-level'' HTE (IL-HTE) can lead to both\nestimated standard errors that are too small and identification challenges in\nthe estimation of treatment-by-covariate interaction effects. We demonstrate\nhow Item Response Theory (IRT) models that estimate a treatment effect for each\nassessment item can both address these challenges and provide new insights into\nHTE generally. This study articulates the theoretical rationale for the IL-HTE\nmodel and demonstrates its practical value using 73 data sets from 46\nrandomized controlled trials containing 5.8 million item responses in\neconomics, education, and health research. Our results show that the IL-HTE\nmodel reveals item-level variation masked by single-number scores, provides\nmore meaningful standard errors in many settings, allows for estimates of the\ngeneralizability of causal effects to untested items, resolves identification\nproblems in the estimation of interaction effects, and provides estimates of\nstandardized treatment effect sizes corrected for attenuation due to\nmeasurement error.",
        "authors": [
            "Joshua B. Gilbert",
            "Zachary Himmelsbach",
            "James Soland",
            "Mridul Joshi",
            "Benjamin W. Domingue"
        ],
        "categories": "econ.EM",
        "published": "2024-04-30T19:24:56Z",
        "updated": "2024-08-26T13:17:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.19707v2",
        "title": "Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models",
        "abstract": "Linear structural vector autoregressive models can be identified\nstatistically without imposing restrictions on the model if the shocks are\nmutually independent and at most one of them is Gaussian. We show that this\nresult extends to structural threshold and smooth transition vector\nautoregressive models incorporating a time-varying impact matrix defined as a\nweighted sum of the impact matrices of the regimes. We also discuss labelling\nof the shocks, maximum likelihood estimation of the parameters, and\nstationarity the model. The introduced methods are implemented to the\naccompanying R package sstvars. Our empirical application studies the effects\nof the climate policy uncertainty shock on the U.S. macroeconomy. In a\nstructural logistic smooth transition vector autoregressive model consisting of\ntwo regimes, we find that a positive climate policy uncertainty shock decreases\nproduction in times of low economic policy uncertainty but slightly increases\nit in times of high economic policy uncertainty.",
        "authors": [
            "Savi Virolainen"
        ],
        "categories": "econ.EM",
        "published": "2024-04-30T16:59:38Z",
        "updated": "2024-06-13T11:36:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.19495v2",
        "title": "Percentage Coefficient (bp) -- Effect Size Analysis (Theory Paper 1)",
        "abstract": "Percentage coefficient (bp) has emerged in recent publications as an\nadditional and alternative estimator of effect size for regression analysis.\nThis paper retraces the theory behind the estimator. It's posited that an\nestimator must first serve the fundamental function of enabling researchers and\nreaders to comprehend an estimand, the target of estimation. It may then serve\nthe instrumental function of enabling researchers and readers to compare two or\nmore estimands. Defined as the regression coefficient when dependent variable\n(DV) and independent variable (IV) are both on conceptual 0-1 percentage\nscales, percentage coefficients (bp) feature 1) clearly comprehendible\ninterpretation and 2) equitable scales for comparison. The coefficient (bp)\nserves the two functions effectively and efficiently. It thus serves needs\nunserved by other indicators, such as raw coefficient (bw) and standardized\nbeta.\n  Another premise of the functionalist theory is that \"effect\" is not a\nmonolithic concept. Rather, it is a collection of concepts, each of which\nmeasures a component of the conglomerate called \"effect\", thereby serving a\nsubfunction. Regression coefficient (b), for example, indicates the unit change\nin DV associated with a one-unit increase in IV, thereby measuring one aspect\ncalled unit effect, aka efficiency. Percentage coefficient (bp) indicates the\npercentage change in DV associated with a whole scale increase in IV. It is not\nmeant to be an all-encompassing indicator of an all-encompassing concept, but\nrather a comprehendible and comparable indicator of efficiency, a key aspect of\neffect.",
        "authors": [
            "Xinshu Zhao",
            "Dianshi Moses Li",
            "Ze Zack Lai",
            "Piper Liping Liu",
            "Song Harris Ao",
            "Fei You"
        ],
        "categories": "stat.AP",
        "published": "2024-04-30T12:27:49Z",
        "updated": "2024-05-06T15:01:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.19145v2",
        "title": "Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty",
        "abstract": "Bootstrap is a popular methodology for simulating input uncertainty. However,\nit can be computationally expensive when the number of samples is large. We\npropose a new approach called \\textbf{Orthogonal Bootstrap} that reduces the\nnumber of required Monte Carlo replications. We decomposes the target being\nsimulated into two parts: the \\textit{non-orthogonal part} which has a\nclosed-form result known as Infinitesimal Jackknife and the \\textit{orthogonal\npart} which is easier to be simulated. We theoretically and numerically show\nthat Orthogonal Bootstrap significantly reduces the computational cost of\nBootstrap while improving empirical accuracy and maintaining the same width of\nthe constructed interval.",
        "authors": [
            "Kaizhao Liu",
            "Jose Blanchet",
            "Lexing Ying",
            "Yiping Lu"
        ],
        "categories": "stat.ME",
        "published": "2024-04-29T23:08:03Z",
        "updated": "2024-05-01T02:10:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.19144v1",
        "title": "A Locally Robust Semiparametric Approach to Examiner IV Designs",
        "abstract": "I propose a locally robust semiparametric framework for estimating causal\neffects using the popular examiner IV design, in the presence of many examiners\nand possibly many covariates relative to the sample size. The key ingredient of\nthis approach is an orthogonal moment function that is robust to biases and\nlocal misspecification from the first step estimation of the examiner IV. I\nderive the orthogonal moment function and show that it delivers multiple\nrobustness where the outcome model or at least one of the first step components\nis misspecified but the estimating equation remains valid. The proposed\nframework not only allows for estimation of the examiner IV in the presence of\nmany examiners and many covariates relative to sample size, using a wide range\nof nonparametric and machine learning techniques including LASSO, Dantzig,\nneural networks and random forests, but also delivers root-n consistent\nestimation of the parameter of interest under mild assumptions.",
        "authors": [
            "Lonjezo Sithole"
        ],
        "categories": "econ.EM",
        "published": "2024-04-29T23:05:03Z",
        "updated": "2024-04-29T23:05:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.18268v1",
        "title": "Optimal Treatment Allocation under Constraints",
        "abstract": "In optimal policy problems where treatment effects vary at the individual\nlevel, optimally allocating treatments to recipients is complex even when\npotential outcomes are known. We present an algorithm for multi-arm treatment\nallocation problems that is guaranteed to find the optimal allocation in\nstrongly polynomial time, and which is able to handle arbitrary potential\noutcomes as well as constraints on treatment requirement and capacity. Further,\nstarting from an arbitrary allocation, we show how to optimally re-allocate\ntreatments in a Pareto-improving manner. To showcase our results, we use data\nfrom Danish nurse home visiting for infants. We estimate nurse specific\ntreatment effects for children born 1959-1967 in Copenhagen, comparing nurses\nagainst each other. We exploit random assignment of newborn children to nurses\nwithin a district to obtain causal estimates of nurse-specific treatment\neffects using causal machine learning. Using these estimates, and treating the\nDanish nurse home visiting program as a case of an optimal treatment allocation\nproblem (where a treatment is a nurse), we document room for significant\nproductivity improvements by optimally re-allocating nurses to children. Our\nestimates suggest that optimal allocation of nurses to children could have\nimproved average yearly earnings by USD 1,815 and length of education by around\ntwo months.",
        "authors": [
            "Torben S. D. Johansen"
        ],
        "categories": "econ.EM",
        "published": "2024-04-28T18:18:14Z",
        "updated": "2024-04-28T18:18:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.18207v1",
        "title": "Testing for Asymmetric Information in Insurance with Deep Learning",
        "abstract": "The positive correlation test for asymmetric information developed by\nChiappori and Salanie (2000) has been applied in many insurance markets. Most\nof the literature focuses on the special case of constant correlation; it also\nrelies on restrictive parametric specifications for the choice of coverage and\nthe occurrence of claims. We relax these restrictions by estimating conditional\ncovariances and correlations using deep learning methods. We test the positive\ncorrelation property by using the intersection test of Chernozhukov, Lee, and\nRosen (2013) and the \"sorted groups\" test of Chernozhukov, Demirer, Duflo, and\nFernandez-Val (2023). Our results confirm earlier findings that the correlation\nbetween risk and coverage is small. Random forests and gradient boosting trees\nproduce similar results to neural networks.",
        "authors": [
            "Serguei Maliar",
            "Bernard Salanie"
        ],
        "categories": "econ.EM",
        "published": "2024-04-28T14:59:46Z",
        "updated": "2024-04-28T14:59:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.17885v1",
        "title": "Sequential monitoring for explosive volatility regimes",
        "abstract": "In this paper, we develop two families of sequential monitoring procedure to\n(timely) detect changes in a GARCH(1,1) model. Whilst our methodologies can be\napplied for the general analysis of changepoints in GARCH(1,1) sequences, they\nare in particular designed to detect changes from stationarity to explosivity\nor vice versa, thus allowing to check for volatility bubbles. Our statistics\ncan be applied irrespective of whether the historical sample is stationary or\nnot, and indeed without prior knowledge of the regime of the observations\nbefore and after the break. In particular, we construct our detectors as the\nCUSUM process of the quasi-Fisher scores of the log likelihood function. In\norder to ensure timely detection, we then construct our boundary function\n(exceeding which would indicate a break) by including a weighting sequence\nwhich is designed to shorten the detection delay in the presence of a\nchangepoint. We consider two types of weights: a lighter set of weights, which\nensures timely detection in the presence of changes occurring early, but not\ntoo early after the end of the historical sample; and a heavier set of weights,\ncalled Renyi weights which is designed to ensure timely detection in the\npresence of changepoints occurring very early in the monitoring horizon. In\nboth cases, we derive the limiting distribution of the detection delays,\nindicating the expected delay for each set of weights. Our theoretical results\nare validated via a comprehensive set of simulations, and an empirical\napplication to daily returns of individual stocks.",
        "authors": [
            "Lajos Horvath",
            "Lorenzo Trapani",
            "Shixuan Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-04-27T12:45:22Z",
        "updated": "2024-04-27T12:45:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.17693v1",
        "title": "A Survey Selection Correction using Nonrandom Followup with an Application to the Gender Entrepreneurship Gap",
        "abstract": "Selection into samples undermines efforts to describe populations and to\nestimate relationships between variables. We develop a simple method for\ncorrecting for sample selection that explains differences in survey responses\nbetween early and late respondents with correlation between potential responses\nand preference for survey response. Our method relies on researchers observing\nthe number of data collection attempts prior to each individual's survey\nresponse rather than covariates that affect response rates without affecting\npotential responses. Applying our method to a survey of entrepreneurial\naspirations among undergraduates at University of Wisconsin-Madison, we find\nsuggestive evidence that the entrepreneurial aspiration rate is larger among\nsurvey respondents than the population, as well as the male-female gender gap\nin the entrepreneurial aspiration rate, which we estimate as 21 percentage\npoints in the sample and 19 percentage points in the population. Our results\nsuggest that the male-female gap in entrepreneurial aspirations arises prior to\ndirect exposure to the labor market.",
        "authors": [
            "Clint Harris",
            "Jon Eckhardt",
            "Brent Goldfarb"
        ],
        "categories": "econ.EM",
        "published": "2024-04-26T20:45:02Z",
        "updated": "2024-04-26T20:45:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.17049v1",
        "title": "Overidentification in Shift-Share Designs",
        "abstract": "This paper studies the testability of identifying restrictions commonly\nemployed to assign a causal interpretation to two stage least squares (TSLS)\nestimators based on Bartik instruments. For homogeneous effects models applied\nto short panels, our analysis yields testable implications previously noted in\nthe literature for the two major available identification strategies. We\npropose overidentification tests for these restrictions that remain valid in\nhigh dimensional regimes and are robust to heteroskedasticity and clustering.\nWe further show that homogeneous effect models in short panels, and their\ncorresponding overidentification tests, are of central importance by\nestablishing that: (i) In heterogenous effects models, interpreting TSLS as a\npositively weighted average of treatment effects can impose implausible\nassumptions on the distribution of the data; and (ii) Alternative identifying\nstrategies relying on long panels can prove uninformative in short panel\napplications. We highlight the empirical relevance of our results by examining\nthe viability of Bartik instruments for identifying the effect of rising\nChinese import competition on US local labor markets.",
        "authors": [
            "Jinyong Hahn",
            "Guido Kuersteiner",
            "Andres Santos",
            "Wavid Willigrod"
        ],
        "categories": "econ.EM",
        "published": "2024-04-25T21:16:02Z",
        "updated": "2024-04-25T21:16:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.16961v3",
        "title": "A joint test of unconfoundedness and common trends",
        "abstract": "This paper introduces an overidentification test of two alternative\nassumptions to identify the average treatment effect on the treated in a\ntwo-period panel data setting: unconfoundedness and common trends. Under the\nunconfoundedness assumption, treatment assignment and post-treatment outcomes\nare independent, conditional on control variables and pre-treatment outcomes,\nwhich motivates including pre-treatment outcomes in the set of controls.\nConversely, under the common trends assumption, the trend and the treatment\nassignment are independent, conditional on control variables. This motivates\nemploying a Difference-in-Differences (DiD) approach by comparing the\ndifferences between pre- and post-treatment outcomes of the treatment and\ncontrol group. Given the non-nested nature of these assumptions and their often\nambiguous plausibility in empirical settings, we propose a joint test using a\ndoubly robust statistic that can be combined with machine learning to control\nfor observed confounders in a data-driven manner. We discuss various causal\nmodels that imply the satisfaction of either common trends, unconfoundedness,\nor both assumptions jointly, and we investigate the finite sample properties of\nour test through a simulation study. Additionally, we apply the proposed method\nto five empirical examples using publicly available datasets and find the test\nto reject the null hypothesis in two cases.",
        "authors": [
            "Martin Huber",
            "Eva-Maria Oe\u00df"
        ],
        "categories": "econ.EM",
        "published": "2024-04-25T18:19:36Z",
        "updated": "2024-06-24T16:49:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.15495v2",
        "title": "Correlations versus noise in the NFT market",
        "abstract": "The non-fungible token (NFT) market emerges as a recent trading innovation\nleveraging blockchain technology, mirroring the dynamics of the cryptocurrency\nmarket. The current study is based on the capitalization changes and\ntransaction volumes across a large number of token collections on the Ethereum\nplatform. In order to deepen the understanding of the market dynamics, the\ncollection-collection dependencies are examined by using the multivariate\nformalism of detrended correlation coefficient and correlation matrix. It\nappears that correlation strength is lower here than that observed in\npreviously studied markets. Consequently, the eigenvalue spectra of the\ncorrelation matrix more closely follow the Marchenko-Pastur distribution,\nstill, some departures indicating the existence of correlations remain. The\ncomparison of results obtained from the correlation matrix built from the\nPearson coefficients and, independently, from the detrended cross-correlation\ncoefficients suggests that the global correlations in the NFT market arise from\nhigher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for\ncapitalization variability exhibit a scale-free character while, for the number\nof transactions, they are somewhat more decentralized.",
        "authors": [
            "Marcin W\u0105torek",
            "Pawe\u0142 Szyd\u0142o",
            "Jaros\u0142aw Kwapie\u0144",
            "Stanis\u0142aw Dro\u017cd\u017c"
        ],
        "categories": "q-fin.ST",
        "published": "2024-04-23T20:15:12Z",
        "updated": "2024-07-07T19:45:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.14603v1",
        "title": "Quantifying the Internal Validity of Weighted Estimands",
        "abstract": "In this paper we study a class of weighted estimands, which we define as\nparameters that can be expressed as weighted averages of the underlying\nheterogeneous treatment effects. The popular ordinary least squares (OLS),\ntwo-stage least squares (2SLS), and two-way fixed effects (TWFE) estimands are\nall special cases within our framework. Our focus is on answering two questions\nconcerning weighted estimands. First, under what conditions can they be\ninterpreted as the average treatment effect for some (possibly latent)\nsubpopulation? Second, when these conditions are satisfied, what is the upper\nbound on the size of that subpopulation, either in absolute terms or relative\nto a target population of interest? We argue that this upper bound provides a\nvaluable diagnostic for empirical research. When a given weighted estimand\ncorresponds to the average treatment effect for a small subset of the\npopulation of interest, we say its internal validity is low. Our paper develops\npractical tools to quantify the internal validity of weighted estimands.",
        "authors": [
            "Alexandre Poirier",
            "Tymon S\u0142oczy\u0144ski"
        ],
        "categories": "econ.EM",
        "published": "2024-04-22T21:59:33Z",
        "updated": "2024-04-22T21:59:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.13986v2",
        "title": "Stochastic Volatility in Mean: Efficient Analysis by a Generalized Mixture Sampler",
        "abstract": "In this paper we consider the simulation-based Bayesian analysis of\nstochastic volatility in mean (SVM) models. Extending the highly efficient\nMarkov chain Monte Carlo mixture sampler for the SV model proposed in Kim et\nal. (1998) and Omori et al. (2007), we develop an accurate approximation of the\nnon-central chi-squared distribution as a mixture of thirty normal\ndistributions. Under this mixture representation, we sample the parameters and\nlatent volatilities in one block. We also detail a correction of the small\napproximation error by using additional Metropolis-Hastings steps. The proposed\nmethod is extended to the SVM model with leverage. The methodology and models\nare applied to excess holding yields and S&P500 returns in empirical studies,\nand the SVM models are shown to outperform other volatility models based on\nmarginal likelihoods.",
        "authors": [
            "Daichi Hiraki",
            "Siddhartha Chib",
            "Yasuhiro Omori"
        ],
        "categories": "econ.EM",
        "published": "2024-04-22T08:49:14Z",
        "updated": "2024-11-20T14:42:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.13735v1",
        "title": "Identification and Estimation of Nonseparable Triangular Equations with Mismeasured Instruments",
        "abstract": "In this paper, I study the nonparametric identification and estimation of the\nmarginal effect of an endogenous variable $X$ on the outcome variable $Y$,\ngiven a potentially mismeasured instrument variable $W^*$, without assuming\nlinearity or separability of the functions governing the relationship between\nobservables and unobservables. To address the challenges arising from the\nco-existence of measurement error and nonseparability, I first employ the\ndeconvolution technique from the measurement error literature to identify the\njoint distribution of $Y, X, W^*$ using two error-laden measurements of $W^*$.\nI then recover the structural derivative of the function of interest and the\n\"Local Average Response\" (LAR) from the joint distribution via the \"unobserved\ninstrument\" approach in Matzkin (2016). I also propose nonparametric estimators\nfor these parameters and derive their uniform rates of convergence. Monte Carlo\nexercises show evidence that the estimators I propose have good finite sample\nperformance.",
        "authors": [
            "Shaomin Wu"
        ],
        "categories": "econ.EM",
        "published": "2024-04-21T18:22:08Z",
        "updated": "2024-04-21T18:22:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.13356v1",
        "title": "How do applied researchers use the Causal Forest? A methodological review of a method",
        "abstract": "This paper conducts a methodological review of papers using the causal forest\nmachine learning method for flexibly estimating heterogeneous treatment\neffects. It examines 133 peer-reviewed papers. It shows that the emerging best\npractice relies heavily on the approach and tools created by the original\nauthors of the causal forest such as their grf package and the approaches given\nby them in examples. Generally researchers use the causal forest on a\nrelatively low-dimensional dataset relying on randomisation or observed\ncontrols to identify effects. There are several common ways to then communicate\nresults -- by mapping out the univariate distribution of individual-level\ntreatment effect estimates, displaying variable importance results for the\nforest and graphing the distribution of treatment effects across covariates\nthat are important either for theoretical reasons or because they have high\nvariable importance. Some deviations from this common practice are interesting\nand deserve further development and use. Others are unnecessary or even\nharmful.",
        "authors": [
            "Patrick Rehill"
        ],
        "categories": "econ.EM",
        "published": "2024-04-20T11:39:56Z",
        "updated": "2024-04-20T11:39:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.13198v1",
        "title": "An economically-consistent discrete choice model with flexible utility specification based on artificial neural networks",
        "abstract": "Random utility maximisation (RUM) models are one of the cornerstones of\ndiscrete choice modelling. However, specifying the utility function of RUM\nmodels is not straightforward and has a considerable impact on the resulting\ninterpretable outcomes and welfare measures. In this paper, we propose a new\ndiscrete choice model based on artificial neural networks (ANNs) named\n\"Alternative-Specific and Shared weights Neural Network (ASS-NN)\", which\nprovides a further balance between flexible utility approximation from the data\nand consistency with two assumptions: RUM theory and fungibility of money\n(i.e., \"one euro is one euro\"). Therefore, the ASS-NN can derive\neconomically-consistent outcomes, such as marginal utilities or willingness to\npay, without explicitly specifying the utility functional form. Using a Monte\nCarlo experiment and empirical data from the Swissmetro dataset, we show that\nASS-NN outperforms (in terms of goodness of fit) conventional multinomial logit\n(MNL) models under different utility specifications. Furthermore, we show how\nthe ASS-NN is used to derive marginal utilities and willingness to pay\nmeasures.",
        "authors": [
            "Jose Ignacio Hernandez",
            "Niek Mouter",
            "Sander van Cranenburgh"
        ],
        "categories": "stat.ML",
        "published": "2024-04-19T22:13:12Z",
        "updated": "2024-04-19T22:13:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.12997v2",
        "title": "On the Asymmetric Volatility Connectedness",
        "abstract": "Connectedness measures the degree at which a time-series variable spills over\nvolatility to other variables compared to the rate that it is receiving. The\nidea is based on the percentage of variance decomposition from one variable to\nthe others, which is estimated by making use of a VAR model. Diebold and Yilmaz\n(2012, 2014) suggested estimating this simple and useful measure of percentage\nrisk spillover impact. Their method is symmetric by nature, however. The\ncurrent paper offers an alternative asymmetric approach for measuring the\nvolatility spillover direction, which is based on estimating the asymmetric\nvariance decompositions introduced by Hatemi-J (2011, 2014). This approach\naccounts explicitly for the asymmetric property in the estimations, which\naccords better with reality. An application is provided to capture the\npotential asymmetric volatility spillover impacts between the three largest\nfinancial markets in the world.",
        "authors": [
            "Abdulnasser Hatemi-J"
        ],
        "categories": "econ.EM",
        "published": "2024-04-19T16:50:22Z",
        "updated": "2024-05-06T07:09:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.12882v1",
        "title": "The modified conditional sum-of-squares estimator for fractionally integrated models",
        "abstract": "In this paper, we analyse the influence of estimating a constant term on the\nbias of the conditional sum-of-squares (CSS) estimator in a stationary or\nnon-stationary type-II ARFIMA ($p_1$,$d$,$p_2$) model. We derive expressions\nfor the estimator's bias and show that the leading term can be easily removed\nby a simple modification of the CSS objective function. We call this new\nestimator the modified conditional sum-of-squares (MCSS) estimator. We show\ntheoretically and by means of Monte Carlo simulations that its performance\nrelative to that of the CSS estimator is markedly improved even for small\nsample sizes. Finally, we revisit three classical short datasets that have in\nthe past been described by ARFIMA($p_1$,$d$,$p_2$) models with constant term,\nnamely the post-second World War real GNP data, the extended Nelson-Plosser\ndata, and the Nile data.",
        "authors": [
            "Mustafa R. K\u0131l\u0131n\u00e7",
            "Michael Massmann"
        ],
        "categories": "econ.EM",
        "published": "2024-04-19T13:30:50Z",
        "updated": "2024-04-19T13:30:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.12581v1",
        "title": "Two-step Estimation of Network Formation Models with Unobserved Heterogeneities and Strategic Interactions",
        "abstract": "In this paper, I characterize the network formation process as a static game\nof incomplete information, where the latent payoff of forming a link between\ntwo individuals depends on the structure of the network, as well as private\ninformation on agents' attributes. I allow agents' private unobserved\nattributes to be correlated with observed attributes through individual fixed\neffects. Using data from a single large network, I propose a two-step estimator\nfor the model primitives. In the first step, I estimate agents' equilibrium\nbeliefs of other people's choice probabilities. In the second step, I plug in\nthe first-step estimator to the conditional choice probability expression and\nestimate the model parameters and the unobserved individual fixed effects\ntogether using Joint MLE. Assuming that the observed attributes are discrete, I\nshowed that the first step estimator is uniformly consistent with rate\n$N^{-1/4}$, where $N$ is the total number of linking proposals. I also show\nthat the second-step estimator converges asymptotically to a normal\ndistribution at the same rate.",
        "authors": [
            "Shaomin Wu"
        ],
        "categories": "econ.EM",
        "published": "2024-04-19T02:20:39Z",
        "updated": "2024-04-19T02:20:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.12462v1",
        "title": "Axiomatic modeling of fixed proportion technologies",
        "abstract": "Understanding input substitution and output transformation possibilities is\ncritical for efficient resource allocation and firm strategy. There are\nimportant examples of fixed proportion technologies where certain inputs are\nnon-substitutable and/or certain outputs are non-transformable. However, there\nis widespread confusion about the appropriate modeling of fixed proportion\ntechnologies in data envelopment analysis. We point out and rectify several\nmisconceptions in the existing literature, and show how fixed proportion\ntechnologies can be correctly incorporated into the axiomatic framework. A\nMonte Carlo study is performed to demonstrate the proposed solution.",
        "authors": [
            "Xun Zhou",
            "Timo Kuosmanen"
        ],
        "categories": "econ.EM",
        "published": "2024-04-18T18:42:45Z",
        "updated": "2024-04-18T18:42:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.11839v1",
        "title": "(Empirical) Bayes Approaches to Parallel Trends",
        "abstract": "We consider Bayes and Empirical Bayes (EB) approaches for dealing with\nviolations of parallel trends. In the Bayes approach, the researcher specifies\na prior over both the pre-treatment violations of parallel trends\n$\\delta_{pre}$ and the post-treatment violations $\\delta_{post}$. The\nresearcher then updates their posterior about the post-treatment bias\n$\\delta_{post}$ given an estimate of the pre-trends $\\delta_{pre}$. This allows\nthem to form posterior means and credible sets for the treatment effect of\ninterest, $\\tau_{post}$. In the EB approach, the prior on the violations of\nparallel trends is learned from the pre-treatment observations. We illustrate\nthese approaches in two empirical applications.",
        "authors": [
            "Soonwoo Kwon",
            "Jonathan Roth"
        ],
        "categories": "econ.EM",
        "published": "2024-04-18T01:40:03Z",
        "updated": "2024-04-18T01:40:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.11767v1",
        "title": "Regret Analysis in Threshold Policy Design",
        "abstract": "Threshold policies are targeting mechanisms that assign treatments based on\nwhether an observable characteristic exceeds a certain threshold. They are\nwidespread across multiple domains, such as welfare programs, taxation, and\nclinical medicine. This paper addresses the problem of designing threshold\npolicies using experimental data, when the goal is to maximize the population\nwelfare. First, I characterize the regret (a measure of policy optimality) of\nthe Empirical Welfare Maximizer (EWM) policy, popular in the literature. Next,\nI introduce the Smoothed Welfare Maximizer (SWM) policy, which improves the\nEWM's regret convergence rate under an additional smoothness condition. The two\npolicies are compared studying how differently their regrets depend on the\npopulation distribution, and investigating their finite sample performances\nthrough Monte Carlo simulations. In many contexts, the welfare guaranteed by\nthe novel SWM policy is larger than with the EWM. An empirical illustration\ndemonstrates how the treatment recommendation of the two policies may in\npractice notably differ.",
        "authors": [
            "Federico Crippa"
        ],
        "categories": "econ.EM",
        "published": "2024-04-17T21:50:12Z",
        "updated": "2024-04-17T21:50:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.11739v1",
        "title": "Testing Mechanisms",
        "abstract": "Economists are often interested in the mechanisms by which a particular\ntreatment affects an outcome. This paper develops tests for the ``sharp null of\nfull mediation'' that the treatment $D$ operates on the outcome $Y$ only\nthrough a particular conjectured mechanism (or set of mechanisms) $M$. A key\nobservation is that if $D$ is randomly assigned and has a monotone effect on\n$M$, then $D$ is a valid instrumental variable for the local average treatment\neffect (LATE) of $M$ on $Y$. Existing tools for testing the validity of the\nLATE assumptions can thus be used to test the sharp null of full mediation when\n$M$ and $D$ are binary. We develop a more general framework that allows one to\ntest whether the effect of $D$ on $Y$ is fully explained by a potentially\nmulti-valued and multi-dimensional set of mechanisms $M$, allowing for\nrelaxations of the monotonicity assumption. We further provide methods for\nlower-bounding the size of the alternative mechanisms when the sharp null is\nrejected. An advantage of our approach relative to existing tools for mediation\nanalysis is that it does not require stringent assumptions about how $M$ is\nassigned; on the other hand, our approach helps to answer different questions\nthan traditional mediation analysis by focusing on the sharp null rather than\nestimating average direct and indirect effects. We illustrate the usefulness of\nthe testable implications in two empirical applications.",
        "authors": [
            "Soonwoo Kwon",
            "Jonathan Roth"
        ],
        "categories": "econ.EM",
        "published": "2024-04-17T20:48:00Z",
        "updated": "2024-04-17T20:48:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.11324v1",
        "title": "Weighted-Average Least Squares for Negative Binomial Regression",
        "abstract": "Model averaging methods have become an increasingly popular tool for\nimproving predictions and dealing with model uncertainty, especially in\nBayesian settings. Recently, frequentist model averaging methods such as\ninformation theoretic and least squares model averaging have emerged. This work\nfocuses on the issue of covariate uncertainty where managing the computational\nresources is key: The model space grows exponentially with the number of\ncovariates such that averaged models must often be approximated.\nWeighted-average least squares (WALS), first introduced for (generalized)\nlinear models in the econometric literature, combines Bayesian and frequentist\naspects and additionally employs a semiorthogonal transformation of the\nregressors to reduce the computational burden. This paper extends WALS for\ngeneralized linear models to the negative binomial (NB) regression model for\noverdispersed count data. A simulation experiment and an empirical application\nusing data on doctor visits were conducted to compare the predictive power of\nWALS for NB regression to traditional estimators. The results show that WALS\nfor NB improves on the maximum likelihood estimator in sparse situations and is\ncompetitive with lasso while being computationally more efficient.",
        "authors": [
            "Kevin Huynh"
        ],
        "categories": "econ.EM",
        "published": "2024-04-17T12:35:55Z",
        "updated": "2024-04-17T12:35:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.11235v3",
        "title": "Bayesian Markov-Switching Vector Autoregressive Process",
        "abstract": "This study introduces marginal density functions of the general Bayesian\nMarkov-Switching Vector Autoregressive (MS-VAR) process. In the case of the\nBayesian MS-VAR process, we provide closed-form density functions and\nMonte-Carlo simulation algorithms, including the importance sampling method.\nThe Monte-Carlo simulation method departs from the previous simulation methods\nbecause it removes the duplication in a regime vector.",
        "authors": [
            "Battulga Gankhuu"
        ],
        "categories": "econ.EM",
        "published": "2024-04-17T10:37:52Z",
        "updated": "2024-09-26T10:09:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.11198v1",
        "title": "Forecasting with panel data: Estimation uncertainty versus parameter heterogeneity",
        "abstract": "We provide a comprehensive examination of the predictive accuracy of panel\nforecasting methods based on individual, pooling, fixed effects, and Bayesian\nestimation, and propose optimal weights for forecast combination schemes. We\nconsider linear panel data models, allowing for weakly exogenous regressors and\ncorrelated heterogeneity. We quantify the gains from exploiting panel data and\ndemonstrate how forecasting performance depends on the degree of parameter\nheterogeneity, whether such heterogeneity is correlated with the regressors,\nthe goodness of fit of the model, and the cross-sectional ($N$) and time ($T$)\ndimensions. Monte Carlo simulations and empirical applications to house prices\nand CPI inflation show that forecast combination and Bayesian forecasting\nmethods perform best overall and rarely produce the least accurate forecasts\nfor individual series.",
        "authors": [
            "M. Hashem Pesaran",
            "Andreas Pick",
            "Allan Timmermann"
        ],
        "categories": "econ.EM",
        "published": "2024-04-17T09:30:41Z",
        "updated": "2024-04-17T09:30:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.11092v1",
        "title": "Estimation for conditional moment models based on martingale difference divergence",
        "abstract": "We provide a new estimation method for conditional moment models via the\nmartingale difference divergence (MDD).Our MDD-based estimation method is\nformed in the framework of a continuum of unconditional moment restrictions.\nUnlike the existing estimation methods in this framework, the MDD-based\nestimation method adopts a non-integrable weighting function, which could grab\nmore information from unconditional moment restrictions than the integrable\nweighting function to enhance the estimation efficiency. Due to the nature of\nshift-invariance in MDD, our MDD-based estimation method can not identify the\nintercept parameters. To overcome this identification issue, we further provide\na two-step estimation procedure for the model with intercept parameters. Under\nregularity conditions, we establish the asymptotics of the proposed estimators,\nwhich are not only easy-to-implement with analytic asymptotic variances, but\nalso applicable to time series data with an unspecified form of conditional\nheteroskedasticity. Finally, we illustrate the usefulness of the proposed\nestimators by simulations and two real examples.",
        "authors": [
            "Kunyang Song",
            "Feiyu Jiang",
            "Ke Zhu"
        ],
        "categories": "econ.EM",
        "published": "2024-04-17T06:16:31Z",
        "updated": "2024-04-17T06:16:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.11057v1",
        "title": "Partial Identification of Heteroskedastic Structural VARs: Theory and Bayesian Inference",
        "abstract": "We consider structural vector autoregressions identified through stochastic\nvolatility. Our focus is on whether a particular structural shock is identified\nby heteroskedasticity without the need to impose any sign or exclusion\nrestrictions. Three contributions emerge from our exercise: (i) a set of\nconditions under which the matrix containing structural parameters is partially\nor globally unique; (ii) a statistical procedure to assess the validity of the\nconditions mentioned above; and (iii) a shrinkage prior distribution for\nconditional variances centred on a hypothesis of homoskedasticity. Such a prior\nensures that the evidence for identifying a structural shock comes only from\nthe data and is not favoured by the prior. We illustrate our new methods using\na U.S. fiscal structural model.",
        "authors": [
            "Helmut L\u00fctkepohl",
            "Fei Shang",
            "Luis Uzeda",
            "Tomasz Wo\u017aniak"
        ],
        "categories": "econ.EM",
        "published": "2024-04-17T04:21:58Z",
        "updated": "2024-04-17T04:21:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.10111v1",
        "title": "From Predictive Algorithms to Automatic Generation of Anomalies",
        "abstract": "Machine learning algorithms can find predictive signals that researchers fail\nto notice; yet they are notoriously hard-to-interpret. How can we extract\ntheoretical insights from these black boxes? History provides a clue. Facing a\nsimilar problem -- how to extract theoretical insights from their intuitions --\nresearchers often turned to ``anomalies:'' constructed examples that highlight\nflaws in an existing theory and spur the development of new ones. Canonical\nexamples include the Allais paradox and the Kahneman-Tversky choice experiments\nfor expected utility theory. We suggest anomalies can extract theoretical\ninsights from black box predictive algorithms. We develop procedures to\nautomatically generate anomalies for an existing theory when given a predictive\nalgorithm. We cast anomaly generation as an adversarial game between a theory\nand a falsifier, the solutions to which are anomalies: instances where the\nblack box algorithm predicts - were we to collect data - we would likely\nobserve violations of the theory. As an illustration, we generate anomalies for\nexpected utility theory using a large, publicly available dataset on real\nlottery choices. Based on an estimated neural network that predicts lottery\nchoices, our procedures recover known anomalies and discover new ones for\nexpected utility theory. In incentivized experiments, subjects violate expected\nutility theory on these algorithmically generated anomalies; moreover, the\nviolation rates are similar to observed rates for the Allais paradox and Common\nratio effect.",
        "authors": [
            "Sendhil Mullainathan",
            "Ashesh Rambachan"
        ],
        "categories": "econ.EM",
        "published": "2024-04-15T19:50:46Z",
        "updated": "2024-04-15T19:50:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.09528v2",
        "title": "Overfitting Reduction in Convex Regression",
        "abstract": "Convex regression is a method for estimating the convex function from a data\nset. This method has played an important role in operations research,\neconomics, machine learning, and many other areas. However, it has been\nempirically observed that convex regression produces inconsistent estimates of\nconvex functions and extremely large subgradients near the boundary as the\nsample size increases. In this paper, we provide theoretical evidence of this\noverfitting behavior. To eliminate this behavior, we propose two new estimators\nby placing a bound on the subgradients of the convex function. We further show\nthat our proposed estimators can reduce overfitting by proving that they\nconverge to the underlying true convex function and that their subgradients\nconverge to the gradient of the underlying function, both uniformly over the\ndomain with probability one as the sample size is increasing to infinity. An\napplication to Finnish electricity distribution firms confirms the superior\nperformance of the proposed methods in predictive power over the existing\nmethods.",
        "authors": [
            "Zhiqiang Liao",
            "Sheng Dai",
            "Eunji Lim",
            "Timo Kuosmanen"
        ],
        "categories": "stat.ME",
        "published": "2024-04-15T07:48:59Z",
        "updated": "2024-10-16T08:19:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.09467v5",
        "title": "The Role of Carbon Pricing in Food Inflation: Evidence from Canadian Provinces",
        "abstract": "In the search for political-economic tools for greenhouse gas mitigation,\ncarbon pricing, which includes carbon tax and cap-and-trade, is implemented by\nmany governments. However, the inflating food prices in carbon-pricing\ncountries, such as Canada, have led many to believe such policies harm food\naffordability. This study aims to identify changes in food prices induced by\ncarbon pricing using the case of Canadian provinces. Using the staggered\ndifference-in-difference (DiD) approach, we find an overall deflationary effect\nof carbon pricing on food prices (measured by monthly provincial food CPI). The\naverage reductions in food CPI compared to before carbon pricing are $2\\%$ and\n$4\\%$ within and beyond two years of implementation. We further find that the\ndeflationary effects are partially driven by lower consumption with no\nsignificant change via farm input costs. Evidence in this paper suggests no\ninflationary effect of carbon pricing in Canadian provinces, thus giving no\nsupport to the growing voices against carbon pricing policies.",
        "authors": [
            "Jiansong Xu"
        ],
        "categories": "econ.EM",
        "published": "2024-04-15T05:37:43Z",
        "updated": "2024-05-22T15:55:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.09309v3",
        "title": "Julia as a universal platform for statistical software development",
        "abstract": "The julia package integrates the Julia programming language into Stata. Users\ncan transfer data between Stata and Julia, issue Julia commands to analyze and\nplot, and pass results back to Stata. Julia's econometric ecosystem is not as\nmature as Stata's or R's or Python's. But Julia is an excellent environment for\ndeveloping high-performance numerical applications, which can then be called\nfrom many platforms. For example, the boottest program for wild bootstrap-based\ninference (Roodman et al. 2019) and fwildclusterboot for R (Fischer and Roodman\n2021) can both call the same Julia back end. And the program reghdfejl mimics\nreghdfe (Correia 2016) in fitting linear models with high-dimensional fixed\neffects but calls a Julia package for tenfold acceleration on hard problems.\nreghdfejl also supports nonlinear fixed-effect models that cannot otherwise be\nfit in Stata--though preliminarily, as the Julia package for that purpose is\nimmature.",
        "authors": [
            "David Roodman"
        ],
        "categories": "econ.EM",
        "published": "2024-04-14T17:37:30Z",
        "updated": "2024-08-17T20:53:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.09117v1",
        "title": "Identifying Causal Effects under Kink Setting: Theory and Evidence",
        "abstract": "This paper develops a generalized framework for identifying causal impacts in\na reduced-form manner under kinked settings when agents can manipulate their\nchoices around the threshold. The causal estimation using a bunching framework\nwas initially developed by Diamond and Persson (2017) under notched settings.\nMany empirical applications of bunching designs involve kinked settings. We\npropose a model-free causal estimator in kinked settings with sharp bunching\nand then extend to the scenarios with diffuse bunching, misreporting,\noptimization frictions, and heterogeneity. The estimation method is mostly\nnon-parametric and accounts for the interior response under kinked settings.\nApplying the proposed approach, we estimate how medical subsidies affect\noutpatient behaviors in China.",
        "authors": [
            "Yi Lu",
            "Jianguo Wang",
            "Huihua Xie"
        ],
        "categories": "econ.EM",
        "published": "2024-04-14T02:02:13Z",
        "updated": "2024-04-14T02:02:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.08839v4",
        "title": "Multiply-Robust Causal Change Attribution",
        "abstract": "Comparing two samples of data, we observe a change in the distribution of an\noutcome variable. In the presence of multiple explanatory variables, how much\nof the change can be explained by each possible cause? We develop a new\nestimation strategy that, given a causal model, combines regression and\nre-weighting methods to quantify the contribution of each causal mechanism. Our\nproposed methodology is multiply robust, meaning that it still recovers the\ntarget parameter under partial misspecification. We prove that our estimator is\nconsistent and asymptotically normal. Moreover, it can be incorporated into\nexisting frameworks for causal attribution, such as Shapley values, which will\ninherit the consistency and large-sample distribution properties. Our method\ndemonstrates excellent performance in Monte Carlo simulations, and we show its\nusefulness in an empirical application. Our method is implemented as part of\nthe Python library DoWhy (arXiv:2011.04216, arXiv:2206.06821).",
        "authors": [
            "Victor Quintas-Martinez",
            "Mohammad Taha Bahadori",
            "Eduardo Santiago",
            "Jeff Mu",
            "Dominik Janzing",
            "David Heckerman"
        ],
        "categories": "stat.ME",
        "published": "2024-04-12T22:57:01Z",
        "updated": "2024-09-05T18:36:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.08816v3",
        "title": "Measuring the Quality of Answers in Political Q&As with Large Language Models",
        "abstract": "This paper proposes a novel methodology for assessing the quality of answers\nin political question-and-answer sessions. Our approach consists of measuring\nthe quality of an answer based on how accurately it can be identified among all\nobserved answers given the question. This reflects the relevance and depth of\nengagement of the answer to the question. Similarly to semantic search, this\nmeasurement approach can be implemented by training a language model on the\ncorpus of observed questions and answers without additional labeled data. We\nshowcase and validate our methodology using data from the Question Period in\nthe Canadian House of Commons. Our analysis reveals that while some answers\nhave a weak semantic connection with questions, hinting at some evasion or\nobfuscation, answers are generally relevant, far surpassing what would be\nexpected from random replies. Besides, our findings provide valuable insights\ninto the correlates of answer quality. We find significant variations based on\nthe party affiliation of the members of Parliament posing the questions.\nFinally, we uncover a meaningful correlation between the quality of answers and\nthe topic of the questions.",
        "authors": [
            "R. Michael Alvarez",
            "Jacob Morrier"
        ],
        "categories": "cs.CL",
        "published": "2024-04-12T21:16:53Z",
        "updated": "2024-11-27T23:27:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.08365v2",
        "title": "Estimation and Inference for Three-Dimensional Panel Data Models",
        "abstract": "Hierarchical panel data models have recently garnered significant attention.\nThis study contributes to the relevant literature by introducing a novel\nthree-dimensional (3D) hierarchical panel data model, which integrates panel\nregression with three sets of latent factor structures: one set of global\nfactors and two sets of local factors. Instead of aggregating latent factors\nfrom various nodes, as seen in the literature of distributed principal\ncomponent analysis (PCA), we propose an estimation approach capable of\nrecovering the parameters of interest and disentangling latent factors at\ndifferent levels and across different dimensions. We establish an asymptotic\ntheory and provide a bootstrap procedure to obtain inference for the parameters\nof interest while accommodating various types of cross-sectional dependence and\ntime series autocorrelation. Finally, we demonstrate the applicability of our\nframework by examining productivity convergence in manufacturing industries\nworldwide.",
        "authors": [
            "Guohua Feng",
            "Jiti Gao",
            "Fei Liu",
            "Bin Peng"
        ],
        "categories": "econ.EM",
        "published": "2024-04-12T10:09:06Z",
        "updated": "2024-09-12T00:19:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.08129v1",
        "title": "One Factor to Bind the Cross-Section of Returns",
        "abstract": "We propose a new non-linear single-factor asset pricing model\n$r_{it}=h(f_{t}\\lambda_{i})+\\epsilon_{it}$. Despite its parsimony, this model\nrepresents exactly any non-linear model with an arbitrary number of factors and\nloadings -- a consequence of the Kolmogorov-Arnold representation theorem. It\nfeatures only one pricing component $h(f_{t}\\lambda_{I})$, comprising a\nnonparametric link function of the time-dependent factor and factor loading\nthat we jointly estimate with sieve-based estimators. Using 171 assets across\nmajor classes, our model delivers superior cross-sectional performance with a\nlow-dimensional approximation of the link function. Most known finance and\nmacro factors become insignificant controlling for our single-factor.",
        "authors": [
            "Nicola Borri",
            "Denis Chetverikov",
            "Yukun Liu",
            "Aleh Tsyvinski"
        ],
        "categories": "q-fin.GN",
        "published": "2024-04-11T21:10:24Z",
        "updated": "2024-04-11T21:10:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.08105v2",
        "title": "Uniform Inference in High-Dimensional Threshold Regression Models",
        "abstract": "We develop uniform inference for high-dimensional threshold regression\nparameters, allowing for either cross-sectional or time series data. We first\nestablish Oracle inequalities for prediction errors and $\\ell_1$ estimation\nerrors for the Lasso estimator of the slope parameters and the threshold\nparameter, accommodating heteroskedastic non-subgaussian error terms and\nnon-subgaussian covariates. Next, we derive the asymptotic distribution of\ntests involving an increasing number of slope parameters by debiasing (or\ndesparsifying) the Lasso estimator in cases with no threshold effect and with a\nfixed threshold effect. We show that the asymptotic distributions in both cases\nare the same, allowing us to perform uniform inference without specifying\nwhether the true model is a linear or threshold regression. Finally, we\ndemonstrate the consistent performance of our estimator in both cases through\nsimulation studies, and we apply the proposed estimator to analyze two\nempirical applications.",
        "authors": [
            "Jiatong Li",
            "Hongqiang Yan"
        ],
        "categories": "econ.EM",
        "published": "2024-04-11T19:56:22Z",
        "updated": "2024-08-30T06:25:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.07684v3",
        "title": "Merger Analysis with Latent Price",
        "abstract": "Standard empirical tools for merger analysis assume price data, which may not\nbe readily available. This paper characterizes sufficient conditions for\nidentifying the unilateral effects of mergers without price data. I show that\ndata on merging firms' revenues, margins, and revenue diversion ratios are\nsufficient for identifying the gross upward pricing pressure indices,\ncompensating marginal cost reductions, and first-order welfare effects. Revenue\ndiversion ratios can be identified from consumer expenditure data via the\nHotz-Miller inversion under the standard discrete-continuous demand assumption.\nMerger simulations are also feasible with CES demand if data on all firms'\nmargins and revenues are available. I use the proposed framework to evaluate\nthe Albertsons/Safeway merger (2015) and the Staples/Office Depot merger\n(2016).",
        "authors": [
            "Paul S. Koh"
        ],
        "categories": "econ.EM",
        "published": "2024-04-11T12:23:57Z",
        "updated": "2024-07-30T19:01:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.06471v1",
        "title": "Regression Discontinuity Design with Spillovers",
        "abstract": "Researchers who estimate treatment effects using a regression discontinuity\ndesign (RDD) typically assume that there are no spillovers between the treated\nand control units. This may be unrealistic. We characterize the estimand of RDD\nin a setting where spillovers occur between units that are close in their\nvalues of the running variable. Under the assumption that spillovers are\nlinear-in-means, we show that the estimand depends on the ratio of two terms:\n(1) the radius over which spillovers occur and (2) the choice of bandwidth used\nfor the local linear regression. Specifically, RDD estimates direct treatment\neffect when radius is of larger order than the bandwidth, and total treatment\neffect when radius is of smaller order than the bandwidth. In the more\nrealistic regime where radius is of similar order as the bandwidth, the RDD\nestimand is a mix of the above effects. To recover direct and spillover\neffects, we propose incorporating estimated spillover terms into local linear\nregression -- the local analog of peer effects regression. We also clarify the\nsettings under which the donut-hole RD is able to eliminate the effects of\nspillovers.",
        "authors": [
            "Eric Auerbach",
            "Yong Cai",
            "Ahnaf Rafi"
        ],
        "categories": "econ.EM",
        "published": "2024-04-09T17:21:41Z",
        "updated": "2024-04-09T17:21:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.05349v2",
        "title": "Common Trends and Long-Run Identification in Nonlinear Structural VARs",
        "abstract": "While it is widely recognised that linear (structural) VARs may fail to\ncapture important aspects of economic time series, the use of nonlinear SVARs\nhas to date been almost entirely confined to the modelling of stationary time\nseries, because of a lack of understanding as to how common stochastic trends\nmay be accommodated within nonlinear models. This has unfortunately\ncircumscribed the range of series to which such models can be applied -- and/or\nrequired that these series be first transformed to stationarity, a potential\nsource of misspecification -- and prevented the use of long-run identifying\nrestrictions in these models. To address these problems, we develop a flexible\nclass of additively time-separable nonlinear SVARs, which subsume models with\nthreshold-type endogenous regime switching, both of the piecewise linear and\nsmooth transition varieties. We extend the Granger--Johansen representation\ntheorem to this class of models, obtaining conditions that specialise exactly\nto the usual ones when the model is linear. We further show that, as a\ncorollary, these models are capable of supporting the same kinds of long-run\nidentifying restrictions as are available in linearly cointegrated SVARs.",
        "authors": [
            "James A. Duffy",
            "Sophocles Mavroeidis"
        ],
        "categories": "econ.EM",
        "published": "2024-04-08T09:37:29Z",
        "updated": "2024-09-10T17:12:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.05209v1",
        "title": "Maximally Forward-Looking Core Inflation",
        "abstract": "Timely monetary policy decision-making requires timely core inflation\nmeasures. We create a new core inflation series that is explicitly designed to\nsucceed at that goal. Precisely, we introduce the Assemblage Regression, a\ngeneralized nonnegative ridge regression problem that optimizes the price\nindex's subcomponent weights such that the aggregate is maximally predictive of\nfuture headline inflation. Ordering subcomponents according to their rank in\neach period switches the algorithm to be learning supervised trimmed inflation\n- or, put differently, the maximally forward-looking summary statistic of the\nrealized price changes distribution. In an extensive out-of-sample forecasting\nexperiment for the US and the euro area, we find substantial improvements for\nsignaling medium-term inflation developments in both the pre- and post-Covid\nyears. Those coming from the supervised trimmed version are particularly\nstriking, and are attributable to a highly asymmetric trimming which contrasts\nwith conventional indicators. We also find that this metric was indicating\nfirst upward pressures on inflation as early as mid-2020 and quickly captured\nthe turning point in 2022. We also consider extensions, like assembling\ninflation from geographical regions, trimmed temporal aggregation, and building\ncore measures specialized for either upside or downside inflation risks.",
        "authors": [
            "Philippe Goulet Coulombe",
            "Karin Klieber",
            "Christophe Barrette",
            "Maximilian Goebel"
        ],
        "categories": "econ.EM",
        "published": "2024-04-08T05:39:41Z",
        "updated": "2024-04-08T05:39:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.05178v1",
        "title": "Estimating granular house price distributions in the Australian market using Gaussian mixtures",
        "abstract": "A new methodology is proposed to approximate the time-dependent house price\ndistribution at a fine regional scale using Gaussian mixtures. The means,\nvariances and weights of the mixture components are related to time, location\nand dwelling type through a non linear function trained by a deep functional\napproximator. Price indices are derived as means, medians, quantiles or other\nfunctions of the estimated distributions. Price densities for larger regions,\nsuch as a city, are calculated via a weighted sum of the component density\nfunctions. The method is applied to a data set covering all of Australia at a\nfine spatial and temporal resolution. In addition to enabling a detailed\nexploration of the data, the proposed index yields lower prediction errors in\nthe practical task of individual dwelling price projection from previous sales\nvalues within the three major Australian cities. The estimated quantiles are\nalso found to be well calibrated empirically, capturing the complexity of house\nprice distributions.",
        "authors": [
            "Willem P Sijp",
            "Anastasios Panagiotelis"
        ],
        "categories": "econ.EM",
        "published": "2024-04-08T04:01:43Z",
        "updated": "2024-04-08T04:01:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.05021v1",
        "title": "Context-dependent Causality (the Non-Nonotonic Case)",
        "abstract": "We develop a novel identification strategy as well as a new estimator for\ncontext-dependent causal inference in non-parametric triangular models with\nnon-separable disturbances. Departing from the common practice, our analysis\ndoes not rely on the strict monotonicity assumption. Our key contribution lies\nin leveraging on diffusion models to formulate the structural equations as a\nsystem evolving from noise accumulation to account for the influence of the\nlatent context (confounder) on the outcome. Our identifiability strategy\ninvolves a system of Fredholm integral equations expressing the distributional\nrelationship between a latent context variable and a vector of observables.\nThese integral equations involve an unknown kernel and are governed by a set of\nstructural form functions, inducing a non-monotonic inverse problem. We prove\nthat if the kernel density can be represented as an infinite mixture of\nGaussians, then there exists a unique solution for the unknown function. This\nis a significant result, as it shows that it is possible to solve a\nnon-monotonic inverse problem even when the kernel is unknown. On the\nmethodological front we leverage on a novel and enriched Contaminated\nGenerative Adversarial (Neural) Networks (CONGAN) which we provide as a\nsolution to the non-monotonic inverse problem.",
        "authors": [
            "Nir Billfeld",
            "Moshe Kim"
        ],
        "categories": "econ.EM",
        "published": "2024-04-07T17:25:24Z",
        "updated": "2024-04-07T17:25:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.04985v1",
        "title": "Towards a generalized accessibility measure for transportation equity and efficiency",
        "abstract": "Locational measures of accessibility are widely used in urban and\ntransportation planning to understand the impact of the transportation system\non influencing people's access to places. However, there is a considerable lack\nof measurement standards and publicly available data. We propose a generalized\nmeasure of locational accessibility that has a comprehensible form for\ntransportation planning analysis. This metric combines the cumulative\nopportunities approach with gravity-based measures and is capable of catering\nto multiple trip purposes, travel modes, cost thresholds, and scales of\nanalysis. Using data from multiple publicly available datasets, this metric is\ncomputed by trip purpose and travel time threshold for all block groups in the\nUnited States, and the data is made publicly accessible. Further, case studies\nof three large metropolitan areas reveal substantial inefficiencies in\ntransportation infrastructure, with the most inefficiency observed in sprawling\nand non-core urban areas, especially for bicycling. Subsequently, it is shown\nthat targeted investment in facilities can contribute to a more equitable\ndistribution of accessibility to essential shopping and service facilities. By\nassigning greater weights to socioeconomically disadvantaged neighborhoods, the\nproposed metric formally incorporates equity considerations into transportation\nplanning, contributing to a more equitable distribution of accessibility to\nessential services and facilities.",
        "authors": [
            "Rajat Verma",
            "Mithun Debnath",
            "Shagun Mittal",
            "Satish V. Ukkusuri"
        ],
        "categories": "econ.EM",
        "published": "2024-04-07T15:06:43Z",
        "updated": "2024-04-07T15:06:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.04979v2",
        "title": "CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference",
        "abstract": "Social science research often hinges on the relationship between categorical\nvariables and outcomes. We introduce CAVIAR, a novel method for embedding\ncategorical variables that assume values in a high-dimensional ambient space\nbut are sampled from an underlying manifold. Our theoretical and numerical\nanalyses outline challenges posed by such categorical variables in causal\ninference. Specifically, dynamically varying and sparse levels can lead to\nviolations of the Donsker conditions and a failure of the estimation\nfunctionals to converge to a tight Gaussian process. Traditional approaches,\nincluding the exclusion of rare categorical levels and principled variable\nselection models like LASSO, fall short. CAVIAR embeds the data into a\nlower-dimensional global coordinate system. The mapping can be derived from\nboth structured and unstructured data, and ensures stable and robust estimates\nthrough dimensionality reduction. In a dataset of direct-to-consumer apparel\nsales, we illustrate how high-dimensional categorical variables, such as zip\ncodes, can be succinctly represented, facilitating inference and analysis.",
        "authors": [
            "Anirban Mukherjee",
            "Hannah Hanwen Chang"
        ],
        "categories": "econ.EM",
        "published": "2024-04-07T14:47:07Z",
        "updated": "2024-04-11T16:11:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.04974v1",
        "title": "Neural Network Modeling for Forecasting Tourism Demand in Stopi\u0107a Cave: A Serbian Cave Tourism Study",
        "abstract": "For modeling the number of visits in Stopi\\'{c}a cave (Serbia) we consider\nthe classical Auto-regressive Integrated Moving Average (ARIMA) model, Machine\nLearning (ML) method Support Vector Regression (SVR), and hybrid NeuralPropeth\nmethod which combines classical and ML concepts. The most accurate predictions\nwere obtained with NeuralPropeth which includes the seasonal component and\ngrowing trend of time-series. In addition, non-linearity is modeled by shallow\nNeural Network (NN), and Google Trend is incorporated as an exogenous variable.\nModeling tourist demand represents great importance for management structures\nand decision-makers due to its applicability in establishing sustainable\ntourism utilization strategies in environmentally vulnerable destinations such\nas caves. The data provided insights into the tourist demand in Stopi\\'{c}a\ncave and preliminary data for addressing the issues of carrying capacity within\nthe most visited cave in Serbia.",
        "authors": [
            "Buda Baji\u0107",
            "Sr\u0111an Mili\u0107evi\u0107",
            "Aleksandar Anti\u0107",
            "Slobodan Markovi\u0107",
            "Nemanja Tomi\u0107"
        ],
        "categories": "econ.EM",
        "published": "2024-04-07T14:33:06Z",
        "updated": "2024-04-07T14:33:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.04700v1",
        "title": "Stratifying on Treatment Status",
        "abstract": "We investigate the estimation of treatment effects from a sample that is\nstratified on the binary treatment status. In the case of unconfounded\nassignment where the potential outcomes are independent of the treatment given\ncovariates, we show that standard estimators of the average treatment effect\nare inconsistent. In the case of an endogenous treatment and a binary\ninstrument, we show that the IV estimator is inconsistent for the local average\ntreatment effect. In both cases, we propose simple alternative estimators that\nare consistent in stratified samples, assuming that the fraction treated in the\npopulation is known or can be estimated.",
        "authors": [
            "Jinyong Hahn",
            "John Ham",
            "Geert Ridder",
            "Shuyang Sheng"
        ],
        "categories": "econ.EM",
        "published": "2024-04-06T18:03:20Z",
        "updated": "2024-04-06T18:03:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.04590v1",
        "title": "Absolute Technical Efficiency Indices",
        "abstract": "Technical efficiency indices (TEIs) can be estimated using the traditional\nstochastic frontier analysis approach, which yields relative indices that do\nnot allow self-interpretations. In this paper, we introduce a single-step\nestimation procedure for TEIs that eliminates the need to identify best\npractices and avoids imposing restrictive hypotheses on the error term. The\nresulting indices are absolute and allow for individual interpretation. In our\nmodel, we estimate a distance function using the inverse coefficient of\nresource utilization, rather than treating it as unobservable. We employ a\nTobit model with a translog distance function as our econometric framework.\nApplying this model to a sample of 19 airline companies from 2012 to 2021, we\nfind that: (1) Absolute technical efficiency varies considerably between\ncompanies with medium-haul European airlines being technically the most\nefficient, while Asian airlines are the least efficient; (2) Our estimated TEIs\nare consistent with the observed data with a decline in efficiency especially\nduring the Covid-19 crisis and Brexit period; (3) All airlines contained in our\nsample would be able to increase their average technical efficiency by 0.209%\nif they reduced their average kerosene consumption by 1%; (4) Total factor\nproductivity (TFP) growth slowed between 2013 and 2019 due to a decrease in\nDisembodied Technical Change (DTC) and a small effect from Scale Economies\n(SE). Toward the end of our study period, TFP growth seemed increasingly driven\nby the SE effect, with a sharp decline in 2020 followed by an equally sharp\nrecovery in 2021 for most airlines.",
        "authors": [
            "Montacer Ben Cheikh Larbi",
            "Sina Belkhiria"
        ],
        "categories": "econ.EM",
        "published": "2024-04-06T11:10:56Z",
        "updated": "2024-04-06T11:10:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.04494v4",
        "title": "Fast and simple inner-loop algorithms of static / dynamic BLP estimations",
        "abstract": "This study investigates computationally efficient inner-loop algorithms for\nestimating static/dynamic BLP models. It provides the following ideas to reduce\nthe number of inner-loop iterations: (1). Add a term concerning the outside\noption share in the BLP contraction mapping; (2). Analytically represent mean\nproduct utilities as a function of value functions and solve for the value\nfunctions (for dynamic BLP); (3). Combine an acceleration method of fixed point\niterations, especially Anderson acceleration. They are independent and easy to\nimplement. This study shows good performance of these methods by numerical\nexperiments.",
        "authors": [
            "Takeshi Fukasawa"
        ],
        "categories": "econ.EM",
        "published": "2024-04-06T03:52:43Z",
        "updated": "2024-10-21T01:14:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.03737v1",
        "title": "Forecasting with Neuro-Dynamic Programming",
        "abstract": "Economic forecasting is concerned with the estimation of some variable like\ngross domestic product (GDP) in the next period given a set of variables that\ndescribes the current situation or state of the economy, including industrial\nproduction, retail trade turnover or economic confidence. Neuro-dynamic\nprogramming (NDP) provides tools to deal with forecasting and other sequential\nproblems with such high-dimensional states spaces. Whereas conventional\nforecasting methods penalises the difference (or loss) between predicted and\nactual outcomes, NDP favours the difference between temporally successive\npredictions, following an interactive and trial-and-error approach. Past data\nprovides a guidance to train the models, but in a different way from ordinary\nleast squares (OLS) and other supervised learning methods, signalling the\nadjustment costs between sequential states. We found that it is possible to\ntrain a GDP forecasting model with data concerned with other countries that\nperforms better than models trained with past data from the tested country\n(Portugal). In addition, we found that non-linear architectures to approximate\nthe value function of a sequential problem, namely, neural networks can perform\nbetter than a simple linear architecture, lowering the out-of-sample mean\nabsolute forecast error (MAE) by 32% from an OLS model.",
        "authors": [
            "Pedro Afonso Fernandes"
        ],
        "categories": "econ.EM",
        "published": "2024-04-04T18:10:57Z",
        "updated": "2024-04-04T18:10:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.03319v1",
        "title": "Early warning systems for financial markets of emerging economies",
        "abstract": "We develop and apply a new online early warning system (EWS) for what is\nknown in machine learning as concept drift, in economics as a regime shift and\nin statistics as a change point. The system goes beyond linearity assumed in\nmany conventional methods, and is robust to heavy tails and tail-dependence in\nthe data, making it particularly suitable for emerging markets. The key\ncomponent is an effective change-point detection mechanism for conditional\nentropy of the data, rather than for a particular indicator of interest.\nCombined with recent advances in machine learning methods for high-dimensional\nrandom forests, the mechanism is capable of finding significant shifts in\ninformation transfer between interdependent time series when traditional\nmethods fail. We explore when this happens using simulations and we provide\nillustrations by applying the method to Uzbekistan's commodity and equity\nmarkets as well as to Russia's equity market in 2021-2023.",
        "authors": [
            "Artem Kraevskiy",
            "Artem Prokhorov",
            "Evgeniy Sokolovskiy"
        ],
        "categories": "econ.EM",
        "published": "2024-04-04T09:35:42Z",
        "updated": "2024-04-04T09:35:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.03235v1",
        "title": "Marginal Treatment Effects and Monotonicity",
        "abstract": "How robust are analyses based on marginal treatment effects (MTE) to\nviolations of Imbens and Angrist (1994) monotonicity? In this note, I present\nweaker forms of monotonicity under which popular MTE-based estimands still\nidentify the parameters of interest.",
        "authors": [
            "Henrik Sigstad"
        ],
        "categories": "econ.EM",
        "published": "2024-04-04T06:49:48Z",
        "updated": "2024-04-04T06:49:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.02671v3",
        "title": "Bayesian Bi-level Sparse Group Regressions for Macroeconomic Density Forecasting",
        "abstract": "We propose a Machine Learning approach for optimal macroeconomic density\nforecasting in a high-dimensional setting where the underlying model exhibits a\nknown group structure. Our approach is general enough to encompass specific\nforecasting models featuring either many covariates, or unknown nonlinearities,\nor series sampled at different frequencies. By relying on the novel concept of\nbi-level sparsity in time-series econometrics, we construct density forecasts\nbased on a prior that induces sparsity both at the group level and within\ngroups. We demonstrate the consistency of both posterior and predictive\ndistributions. We show that the posterior distribution contracts at the\nminimax-optimal rate and, asymptotically, puts mass on a set that includes the\nsupport of the model. Our theory allows for correlation between groups, while\npredictors in the same group can be characterized by strong covariation as well\nas common characteristics and patterns. Finite sample performance is\nillustrated through comprehensive Monte Carlo experiments and a real-data\nnowcasting exercise of the US GDP growth rate.",
        "authors": [
            "Matteo Mogliani",
            "Anna Simoni"
        ],
        "categories": "econ.EM",
        "published": "2024-04-03T12:12:18Z",
        "updated": "2024-11-15T10:04:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.02584v1",
        "title": "Moran's I 2-Stage Lasso: for Models with Spatial Correlation and Endogenous Variables",
        "abstract": "We propose a novel estimation procedure for models with endogenous variables\nin the presence of spatial correlation based on Eigenvector Spatial Filtering.\nThe procedure, called Moran's $I$ 2-Stage Lasso (Mi-2SL), uses a two-stage\nLasso estimator where the Standardised Moran's I is used to set the Lasso\ntuning parameter. Unlike existing spatial econometric methods, this has the key\nbenefit of not requiring the researcher to explicitly model the spatial\ncorrelation process, which is of interest in cases where they are only\ninterested in removing the resulting bias when estimating the direct effect of\ncovariates. We show the conditions necessary for consistent and asymptotically\nnormal parameter estimation assuming the support (relevant) set of eigenvectors\nis known. Our Monte Carlo simulation results also show that Mi-2SL performs\nwell against common alternatives in the presence of spatial correlation. Our\nempirical application replicates Cadena and Kovak (2016) instrumental variables\nestimates using Mi-2SL and shows that in that case, Mi-2SL can boost the\nperformance of the first stage.",
        "authors": [
            "Sylvain Barde",
            "Rowan Cherodian",
            "Guy Tchuente"
        ],
        "categories": "econ.EM",
        "published": "2024-04-03T09:09:07Z",
        "updated": "2024-04-03T09:09:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.02400v2",
        "title": "Improved Semi-Parametric Bounds for Tail Probability and Expected Loss: Theory and Applications",
        "abstract": "Many management decisions involve accumulated random realizations for which\nthe expected value and variance are assumed to be known. We revisit the tail\nbehavior of such quantities when individual realizations are independent, and\nwe develop new sharper bounds on the tail probability and expected linear loss.\nThe underlying distribution is semi-parametric in the sense that it remains\nunrestricted other than the assumed mean and variance. Our bounds complement\nwell-established results in the literature, including those based on\naggregation, which often fail to take full account of independence and use less\nelegant proofs. New insights include a proof that in the non-identical case,\nthe distributions attaining the bounds have the equal range property, and that\nthe impact of each random variable on the expected value of the sum can be\nisolated using an extension of the Korkine identity. We show that the new\nbounds %not only complement the extant results but also open up abundant\npractical applications, including improved pricing of product bundles, more\nprecise option pricing, more efficient insurance design, and better inventory\nmanagement. For example, we establish a new solution to the optimal bundling\nproblem, yielding a 17\\% uplift in per-bundle profits, and a new solution to\nthe inventory problem, yielding a 5.6\\% cost reduction for a model with 20\nretailers.",
        "authors": [
            "Zhaolin Li",
            "Artem Prokhorov"
        ],
        "categories": "econ.EM",
        "published": "2024-04-03T02:06:54Z",
        "updated": "2024-05-03T00:26:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.02228v2",
        "title": "Seemingly unrelated Bayesian additive regression trees for cost-effectiveness analyses in healthcare",
        "abstract": "In recent years, theoretical results and simulation evidence have shown\nBayesian additive regression trees to be a highly-effective method for\nnonparametric regression. Motivated by cost-effectiveness analyses in health\neconomics, where interest lies in jointly modelling the costs of healthcare\ntreatments and the associated health-related quality of life experienced by a\npatient, we propose a multivariate extension of BART applicable in regression\nand classification analyses with several correlated outcome variables. Our\nframework overcomes some key limitations of existing multivariate BART models\nby allowing each individual response to be associated with different ensembles\nof trees, while still handling dependencies between the outcomes. In the case\nof continuous outcomes, our model is essentially a nonparametric version of\nseemingly unrelated regression. Likewise, our proposal for binary outcomes is a\nnonparametric generalisation of the multivariate probit model. We give\nsuggestions for easily interpretable prior distributions, which allow\nspecification of both informative and uninformative priors. We provide detailed\ndiscussions of MCMC sampling methods to conduct posterior inference. Our\nmethods are implemented in the R package `suBART'. We showcase their\nperformance through extensive simulations and an application to an empirical\ncase study from health economics. By also accommodating propensity scores in a\nmanner befitting a causal analysis, we find substantial evidence for a novel\ntrauma care intervention's cost-effectiveness.",
        "authors": [
            "Jonas Esser",
            "Mateus Maia",
            "Andrew C. Parnell",
            "Judith Bosmans",
            "Hanneke van Dongen",
            "Thomas Klausch",
            "Keefe Murphy"
        ],
        "categories": "stat.ME",
        "published": "2024-04-02T18:32:51Z",
        "updated": "2024-06-10T23:04:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.02141v3",
        "title": "Robustly estimating heterogeneity in factorial data using Rashomon Partitions",
        "abstract": "Many statistical analyses, in both observational data and randomized control\ntrials, ask: how does the outcome of interest vary with combinations of\nobservable covariates? How do various drug combinations affect health outcomes,\nor how does technology adoption depend on incentives and demographics? Our goal\nis to partition this factorial space into \"pools\" of covariate combinations\nwhere the outcome differs across the pools (but not within a pool). Existing\napproaches (i) search for a single \"optimal\" partition under assumptions about\nthe association between covariates or (ii) sample from the entire set of\npossible partitions. Both these approaches ignore the reality that, especially\nwith correlation structure in covariates, many ways to partition the covariate\nspace may be statistically indistinguishable, despite very different\nimplications for policy or science. We develop an alternative perspective,\ncalled Rashomon Partition Sets (RPSs). Each item in the RPS partitions the\nspace of covariates using a tree-like geometry. RPSs incorporate all partitions\nthat have posterior values near the maximum a posteriori partition, even if\nthey offer substantively different explanations, and do so using a prior that\nmakes no assumptions about associations between covariates. This prior is the\n$\\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate\nthe posterior of any measurable function of the feature effects vector on\noutcomes, conditional on being in the RPS. We also characterize approximation\nerror relative to the entire posterior and provide bounds on the size of the\nRPS. Simulations demonstrate this framework allows for robust conclusions\nrelative to conventional regularization techniques. We apply our method to\nthree empirical settings: price effects on charitable giving, chromosomal\nstructure (telomere length), and the introduction of microfinance.",
        "authors": [
            "Aparajithan Venkateswaran",
            "Anirudh Sankar",
            "Arun G. Chandrasekhar",
            "Tyler H. McCormick"
        ],
        "categories": "stat.ME",
        "published": "2024-04-02T17:53:28Z",
        "updated": "2024-08-13T19:15:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.01641v1",
        "title": "The impact of geopolitical risk on the international agricultural market: Empirical analysis based on the GJR-GARCH-MIDAS model",
        "abstract": "The current international landscape is turbulent and unstable, with frequent\noutbreaks of geopolitical conflicts worldwide. Geopolitical risk has emerged as\na significant threat to regional and global peace, stability, and economic\nprosperity, causing serious disruptions to the global food system and food\nsecurity. Focusing on the international food market, this paper builds\ndifferent dimensions of geopolitical risk measures based on the random matrix\ntheory and constructs single- and two-factor GJR-GARCH-MIDAS models with fixed\ntime span and rolling window, respectively, to investigate the impact of\ngeopolitical risk on food market volatility. The findings indicate that\nmodeling based on rolling window performs better in describing the overall\nvolatility of the wheat, maize, soybean, and rice markets, and the two-factor\nmodels generally exhibit stronger explanatory power in most cases. In terms of\nshort-term fluctuations, all four staple food markets demonstrate obvious\nvolatility clustering and high volatility persistence, without significant\nasymmetry. Regarding long-term volatility, the realized volatility of wheat,\nmaize, and soybean significantly exacerbates their long-run market volatility.\nAdditionally, geopolitical risks of different dimensions show varying\ndirections and degrees of effects in explaining the long-term market volatility\nof the four staple food commodities. This study contributes to the\nunderstanding of the macro-drivers of food market fluctuations, provides useful\ninformation for investment using agricultural futures, and offers valuable\ninsights into maintaining the stable operation of food markets and safeguarding\nglobal food security.",
        "authors": [
            "Yun-Shi Dai",
            "Peng-Fei Dai",
            "Wei-Xing Zhou"
        ],
        "categories": "econ.EM",
        "published": "2024-04-02T05:13:39Z",
        "updated": "2024-04-02T05:13:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.01566v2",
        "title": "Heterogeneous Treatment Effects and Causal Mechanisms",
        "abstract": "The credibility revolution advances the use of research designs that permit\nidentification and estimation of causal effects. However, understanding which\nmechanisms produce measured causal effects remains a challenge. A dominant\ncurrent approach to the quantitative evaluation of mechanisms relies on the\ndetection of heterogeneous treatment effects with respect to pre-treatment\ncovariates. This paper develops a framework to understand when the existence of\nsuch heterogeneous treatment effects can support inferences about the\nactivation of a mechanism. We show first that this design cannot provide\nevidence of mechanism activation without an additional, generally implicit,\nassumption. Further, even when this assumption is satisfied, if a measured\noutcome is produced by a non-linear transformation of a directly-affected\noutcome of theoretical interest, heterogeneous treatment effects are not\ninformative of mechanism activation. We provide novel guidance for\ninterpretation and research design in light of these findings.",
        "authors": [
            "Jiawei Fu",
            "Tara Slough"
        ],
        "categories": "econ.EM",
        "published": "2024-04-02T02:00:42Z",
        "updated": "2024-06-15T13:53:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.01495v1",
        "title": "Estimating Heterogeneous Effects: Applications to Labor Economics",
        "abstract": "A growing number of applications involve settings where, in order to infer\nheterogeneous effects, a researcher compares various units. Examples of\nresearch designs include children moving between different neighborhoods,\nworkers moving between firms, patients migrating from one city to another, and\nbanks offering loans to different firms. We present a unified framework for\nthese settings, based on a linear model with normal random coefficients and\nnormal errors. Using the model, we discuss how to recover the mean and\ndispersion of effects, other features of their distribution, and to construct\npredictors of the effects. We provide moment conditions on the model's\nparameters, and outline various estimation strategies. A main objective of the\npaper is to clarify some of the underlying assumptions by highlighting their\neconomic content, and to discuss and inform some of the key practical choices.",
        "authors": [
            "Stephane Bonhomme",
            "Angela Denis"
        ],
        "categories": "econ.EM",
        "published": "2024-04-01T21:34:00Z",
        "updated": "2024-04-01T21:34:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.00864v1",
        "title": "Convolution-t Distributions",
        "abstract": "We introduce a new class of multivariate heavy-tailed distributions that are\nconvolutions of heterogeneous multivariate t-distributions. Unlike commonly\nused heavy-tailed distributions, the multivariate convolution-t distributions\nembody cluster structures with flexible nonlinear dependencies and\nheterogeneous marginal distributions. Importantly, convolution-t distributions\nhave simple density functions that facilitate estimation and likelihood-based\ninference. The characteristic features of convolution-t distributions are found\nto be important in an empirical analysis of realized volatility measures and\nhelp identify their underlying factor structure.",
        "authors": [
            "Peter Reinhard Hansen",
            "Chen Tong"
        ],
        "categories": "econ.EM",
        "published": "2024-04-01T02:08:59Z",
        "updated": "2024-04-01T02:08:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.00784v1",
        "title": "Estimating sample paths of Gauss-Markov processes from noisy data",
        "abstract": "I derive the pointwise conditional means and variances of an arbitrary\nGauss-Markov process, given noisy observations of points on a sample path.\nThese moments depend on the process's mean and covariance functions, and on the\nconditional moments of the sampled points. I study the Brownian motion and\nbridge as special cases.",
        "authors": [
            "Benjamin Davies"
        ],
        "categories": "math.ST",
        "published": "2024-03-31T20:06:53Z",
        "updated": "2024-03-31T20:06:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.00221v4",
        "title": "Robust Learning for Optimal Dynamic Treatment Regimes with Observational Data",
        "abstract": "Public policies and medical interventions often involve dynamics in their\ntreatment assignments, where individuals receive a series of interventions over\nmultiple stages. We study the statistical learning of optimal dynamic treatment\nregimes (DTRs) that guide the optimal treatment assignment for each individual\nat each stage based on the individual's evolving history. We propose a doubly\nrobust, classification-based approach to learning the optimal DTR using\nobservational data under the assumption of sequential ignorability. This\napproach learns the optimal DTR through backward induction. At each step, it\nconstructs an augmented inverse probability weighting (AIPW) estimator of the\npolicy value function and maximizes it to learn the optimal policy for the\ncorresponding stage. We show that the resulting DTR can achieve an optimal\nconvergence rate of $n^{-1/2}$ for welfare regret under mild convergence\nconditions on estimators of the nuisance components.",
        "authors": [
            "Shosei Sakaguchi"
        ],
        "categories": "stat.ME",
        "published": "2024-03-30T02:33:39Z",
        "updated": "2024-11-20T05:50:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.00164v1",
        "title": "Sequential Synthetic Difference in Differences",
        "abstract": "We study the estimation of treatment effects of a binary policy in\nenvironments with a staggered treatment rollout. We propose a new estimator --\nSequential Synthetic Difference in Difference (Sequential SDiD) -- and\nestablish its theoretical properties in a linear model with interactive fixed\neffects. Our estimator is based on sequentially applying the original SDiD\nestimator proposed in Arkhangelsky et al. (2021) to appropriately aggregated\ndata. To establish the theoretical properties of our method, we compare it to\nan infeasible OLS estimator based on the knowledge of the subspaces spanned by\nthe interactive fixed effects. We show that this OLS estimator has a sequential\nrepresentation and use this result to show that it is asymptotically equivalent\nto the Sequential SDiD estimator. This result implies the asymptotic normality\nof our estimator along with corresponding efficiency guarantees. The method\ndeveloped in this paper presents a natural alternative to the conventional DiD\nstrategies in staggered adoption designs.",
        "authors": [
            "Dmitry Arkhangelsky",
            "Aleksei Samkov"
        ],
        "categories": "econ.EM",
        "published": "2024-03-29T21:43:32Z",
        "updated": "2024-03-29T21:43:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.19439v1",
        "title": "Dynamic Analyses of Contagion Risk and Module Evolution on the SSE A-Shares Market Based on Minimum Information Entropy",
        "abstract": "The interactive effect is significant in the Chinese stock market,\nexacerbating the abnormal market volatilities and risk contagion. Based on\ndaily stock returns in the Shanghai Stock Exchange (SSE) A-shares, this paper\ndivides the period between 2005 and 2018 into eight bull and bear market stages\nto investigate interactive patterns in the Chinese financial market. We employ\nthe LASSO method to construct the stock network and further use the Map\nEquation method to analyze the evolution of modules in the SSE A-shares market.\nEmpirical results show: (1) The connected effect is more significant in bear\nmarkets than bull markets; (2) A system module can be found in the network\nduring the first four stages, and the industry aggregation effect leads to\nmodule differentiation in the last four stages; (3) Some stocks have leading\neffects on others throughout eight periods, and medium- and small-cap stocks\nwith poor financial conditions are more likely to become risk sources,\nespecially in bear markets. Our conclusions are beneficial to improving\ninvestment strategies and making regulatory policies.",
        "authors": [
            "Muzi Chen",
            "Yuhang Wang",
            "Boyao Wu",
            "Difang Huang"
        ],
        "categories": "econ.EM",
        "published": "2024-03-28T14:07:40Z",
        "updated": "2024-03-28T14:07:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.19363v1",
        "title": "Dynamic Correlation of Market Connectivity, Risk Spillover and Abnormal Volatility in Stock Price",
        "abstract": "The connectivity of stock markets reflects the information efficiency of\ncapital markets and contributes to interior risk contagion and spillover\neffects. We compare Shanghai Stock Exchange A-shares (SSE A-shares) during\ntranquil periods, with high leverage periods associated with the 2015 subprime\nmortgage crisis. We use Pearson correlations of returns, the maximum strongly\nconnected subgraph, and $3\\sigma$ principle to iteratively determine the\nthreshold value for building a dynamic correlation network of SSE A-shares.\nAnalyses are carried out based on the networking structure, intra-sector\nconnectivity, and node status, identifying several contributions. First,\ncompared with tranquil periods, the SSE A-shares network experiences a more\nsignificant small-world and connective effect during the subprime mortgage\ncrisis and the high leverage period in 2015. Second, the finance, energy and\nutilities sectors have a stronger intra-industry connectivity than other\nsectors. Third, HUB nodes drive the growth of the SSE A-shares market during\nbull periods, while stocks have a think-tail degree distribution in bear\nperiods and show distinct characteristics in terms of market value and finance.\nGranger linear and non-linear causality networks are also considered for the\ncomparison purpose. Studies on the evolution of inter-cycle connectivity in the\nSSE A-share market may help investors improve portfolios and develop more\nrobust risk management policies.",
        "authors": [
            "Muzi Chen",
            "Nan Li",
            "Lifen Zheng",
            "Difang Huang",
            "Boyao Wu"
        ],
        "categories": "econ.EM",
        "published": "2024-03-28T12:23:13Z",
        "updated": "2024-03-28T12:23:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.18503v2",
        "title": "Distributional Treatment Effect with Latent Rank Invariance",
        "abstract": "Treatment effect heterogeneity is of a great concern when evaluating the\ntreatment. However, even with a simple case of a binary random treatment, the\ndistribution of treatment effect is difficult to identify due to the\nfundamental limitation that we cannot observe both treated potential outcome\nand untreated potential outcome for a given individual. This paper assumes a\nconditional independence assumption that the two potential outcomes are\nindependent of each other given a scalar latent variable. Using two proxy\nvariables, we identify conditional distribution of the potential outcomes given\nthe latent variable. To pin down the location of the latent variable, we assume\nstrict monotonicty on some functional of the conditional distribution; with\nspecific example of strictly increasing conditional expectation, we label the\nlatent variable as 'latent rank' and motivate the identifying assumption as\n'latent rank invariance.'",
        "authors": [
            "Myungkou Shin"
        ],
        "categories": "econ.EM",
        "published": "2024-03-27T12:29:32Z",
        "updated": "2024-06-06T16:10:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.18248v2",
        "title": "Statistical Inference of Optimal Allocations I: Regularities and their Implications",
        "abstract": "In this paper, we develop a functional differentiability approach for solving\nstatistical optimal allocation problems. We first derive Hadamard\ndifferentiability of the value function through a detailed analysis of the\ngeneral properties of the sorting operator. Central to our framework are the\nconcept of Hausdorff measure and the area and coarea integration formulas from\ngeometric measure theory. Building on our Hadamard differentiability results,\nwe demonstrate how the functional delta method can be used to directly derive\nthe asymptotic properties of the value function process for binary constrained\noptimal allocation problems, as well as the two-step ROC curve estimator.\nMoreover, leveraging profound insights from geometric functional analysis on\nconvex and local Lipschitz functionals, we obtain additional generic Fr\\'echet\ndifferentiability results for the value functions of optimal allocation\nproblems. These compelling findings motivate us to study carefully the first\norder approximation of the optimal social welfare. In this paper, we then\npresent a double / debiased estimator for the value functions. Importantly, the\nconditions outlined in the Hadamard differentiability section validate the\nmargin assumption from the statistical classification literature employing\nplug-in methods that justifies a faster convergence rate.",
        "authors": [
            "Kai Feng",
            "Han Hong"
        ],
        "categories": "econ.EM",
        "published": "2024-03-27T04:39:13Z",
        "updated": "2024-04-07T08:40:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.17777v1",
        "title": "Deconvolution from two order statistics",
        "abstract": "Economic data are often contaminated by measurement errors and truncated by\nranking. This paper shows that the classical measurement error model with\nindependent and additive measurement errors is identified nonparametrically\nusing only two order statistics of repeated measurements. The identification\nresult confirms a hypothesis by Athey and Haile (2002) for a symmetric\nascending auction model with unobserved heterogeneity. Extensions allow for\nheterogeneous measurement errors, broadening the applicability to additional\nempirical settings, including asymmetric auctions and wage offer models. We\nadapt an existing simulated sieve estimator and illustrate its performance in\nfinite samples.",
        "authors": [
            "JoonHwan Cho",
            "Yao Luo",
            "Ruli Xiao"
        ],
        "categories": "econ.EM",
        "published": "2024-03-26T15:09:55Z",
        "updated": "2024-03-26T15:09:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.17624v2",
        "title": "The inclusive Synthetic Control Method",
        "abstract": "We introduce the inclusive synthetic control method (iSCM), a modification of\nsynthetic control methods that includes units in the donor pool potentially\naffected, directly or indirectly, by an intervention. This method is ideal for\nsituations where including treated units in the donor pool is essential or\nwhere donor units may experience spillover effects. The iSCM is straightforward\nto implement with most synthetic control estimators. As an empirical\nillustration, we re-estimate the causal effect of German reunification on GDP\nper capita, accounting for spillover effects from West Germany to Austria.",
        "authors": [
            "Roberta Di Stefano",
            "Giovanni Mellace"
        ],
        "categories": "econ.EM",
        "published": "2024-03-26T11:57:51Z",
        "updated": "2024-11-14T10:56:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.16844v1",
        "title": "Resistant Inference in Instrumental Variable Models",
        "abstract": "The classical tests in the instrumental variable model can behave arbitrarily\nif the data is contaminated. For instance, one outlying observation can be\nenough to change the outcome of a test. We develop a framework to construct\ntesting procedures that are robust to weak instruments, outliers and\nheavy-tailed errors in the instrumental variable model. The framework is\nconstructed upon M-estimators. By deriving the influence functions of the\nclassical weak instrument robust tests, such as the Anderson-Rubin test, K-test\nand the conditional likelihood ratio (CLR) test, we prove their unbounded\nsensitivity to infinitesimal contamination. Therefore, we construct\ncontamination resistant/robust alternatives. In particular, we show how to\nconstruct a robust CLR statistic based on Mallows type M-estimators and show\nthat its asymptotic distribution is the same as that of the (classical) CLR\nstatistic. The theoretical results are corroborated by a simulation study.\nFinally, we revisit three empirical studies affected by outliers and\ndemonstrate how the new robust tests can be used in practice.",
        "authors": [
            "Jens Klooster",
            "Mikhail Zhelonkin"
        ],
        "categories": "econ.EM",
        "published": "2024-03-25T15:04:32Z",
        "updated": "2024-03-25T15:04:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.16773v2",
        "title": "Privacy-Protected Spatial Autoregressive Model",
        "abstract": "Spatial autoregressive (SAR) models are important tools for studying network\neffects. However, with an increasing emphasis on data privacy, data providers\noften implement privacy protection measures that make classical SAR models\ninapplicable. In this study, we introduce a privacy-protected SAR model with\nnoise-added response and covariates to meet privacy-protection requirements.\nHowever, in this scenario, the traditional quasi-maximum likelihood estimator\nbecomes infeasible because the likelihood function cannot be directly\nformulated. To address this issue, we first consider an explicit expression for\nthe likelihood function with only noise-added responses. Then, we develop\ntechniques to correct the biases for derivatives introduced by noise.\nCorrespondingly, a Newton-Raphson-type algorithm is proposed to obtain the\nestimator, leading to a corrected likelihood estimator. To further enhance\ncomputational efficiency, we introduce a corrected least squares estimator\nbased on the idea of bias correction. These two estimation methods ensure both\ndata security and the attainment of statistically valid estimators. Theoretical\nanalysis of both estimators is carefully conducted, statistical inference\nmethods and model extensions are discussed. The finite sample performances of\ndifferent methods are demonstrated through extensive simulations and the\nanalysis of a real dataset.",
        "authors": [
            "Danyang Huang",
            "Ziyi Kong",
            "Shuyuan Wu",
            "Hansheng Wang"
        ],
        "categories": "stat.ME",
        "published": "2024-03-25T13:51:22Z",
        "updated": "2024-07-27T15:02:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.16673v2",
        "title": "Quasi-randomization tests for network interference",
        "abstract": "Network interference amounts to the treatment status of one unit affecting\nthe potential outcome of other units in the population. Testing for spillover\neffects in this setting makes the null hypothesis non-sharp. An interesting\napproach to tackling the non-sharp nature of the null hypothesis in this setup\nis constructing conditional randomization tests such that the null is sharp on\nthe restricted population. Such approaches can pose computational challenges as\nfinding these appropriate sub-populations based on experimental design can\ninvolve solving an NP-hard problem. In this paper, we view the network amongst\nthe population as a random variable instead of being fixed. We propose a new\napproach that builds a conditional quasi-randomization test. We build the\n(non-sharp) null distribution of no spillover effects using random graph null\nmodels. We show that our method is exactly valid in finite samples under mild\nassumptions. Our method displays enhanced power over other methods,\nsubstantially improving cluster randomized trials. We illustrate our\nmethodology to test for interference in a weather insurance adoption experiment\nrun in rural China.",
        "authors": [
            "Supriya Tiwari",
            "Pallavi Basu"
        ],
        "categories": "stat.ME",
        "published": "2024-03-25T12:11:28Z",
        "updated": "2024-10-31T12:50:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.16413v1",
        "title": "Optimal testing in a class of nonregular models",
        "abstract": "This paper studies optimal hypothesis testing for nonregular statistical\nmodels with parameter-dependent support. We consider both one-sided and\ntwo-sided hypothesis testing and develop asymptotically uniformly most powerful\ntests based on the likelihood ratio process. The proposed one-sided test\ninvolves randomization to achieve asymptotic size control, some tuning constant\nto avoid discontinuities in the limiting likelihood ratio process, and a\nuser-specified alternative hypothetical value to achieve the asymptotic\noptimality. Our two-sided test becomes asymptotically uniformly most powerful\nwithout imposing further restrictions such as unbiasedness. Simulation results\nillustrate desirable power properties of the proposed tests.",
        "authors": [
            "Yuya Shimizu",
            "Taisuke Otsu"
        ],
        "categories": "math.ST",
        "published": "2024-03-25T04:14:46Z",
        "updated": "2024-03-25T04:14:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.16177v1",
        "title": "The Informativeness of Combined Experimental and Observational Data under Dynamic Selection",
        "abstract": "This paper addresses the challenge of estimating the Average Treatment Effect\non the Treated Survivors (ATETS; Vikstrom et al., 2018) in the absence of\nlong-term experimental data, utilizing available long-term observational data\ninstead. We establish two theoretical results. First, it is impossible to\nobtain informative bounds for the ATETS with no model restriction and no\nauxiliary data. Second, to overturn this negative result, we explore as a\npromising avenue the recent econometric developments in combining experimental\nand observational data (e.g., Athey et al., 2020, 2019); we indeed find that\nexploiting short-term experimental data can be informative without imposing\nclassical model restrictions. Furthermore, building on Chesher and Rosen\n(2017), we explore how to systematically derive sharp identification bounds,\nexploiting both the novel data-combination principles and classical model\nrestrictions. Applying the proposed method, we explore what can be learned\nabout the long-run effects of job training programs on employment without\nlong-term experimental data.",
        "authors": [
            "Yechan Park",
            "Yuya Sasaki"
        ],
        "categories": "econ.EM",
        "published": "2024-03-24T14:37:36Z",
        "updated": "2024-03-24T14:37:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2404.07222v3",
        "title": "Liquidity Jump, Liquidity Diffusion, and Treatment on Wash Trading of Crypto Assets",
        "abstract": "We propose that the liquidity of an asset includes two components: liquidity\njump and liquidity diffusion. We show that liquidity diffusion has a higher\ncorrelation with crypto wash trading than liquidity jump and demonstrate that\ntreatment on wash trading significantly reduces the level of liquidity\ndiffusion, but only marginally reduces that of liquidity jump. We confirm that\nthe autoregressive models are highly effective in modeling the\nliquidity-adjusted return with and without the treatment on wash trading. We\nargue that treatment on wash trading is unnecessary in modeling established\ncrypto assets that trade in unregulated but mainstream exchanges.",
        "authors": [
            "Qi Deng",
            "Zhong-guo Zhou"
        ],
        "categories": "q-fin.ST",
        "published": "2024-03-24T02:46:04Z",
        "updated": "2024-05-11T04:09:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.15934v1",
        "title": "Debiased Machine Learning when Nuisance Parameters Appear in Indicator Functions",
        "abstract": "This paper studies debiased machine learning when nuisance parameters appear\nin indicator functions. An important example is maximized average welfare under\noptimal treatment assignment rules. For asymptotically valid inference for a\nparameter of interest, the current literature on debiased machine learning\nrelies on Gateaux differentiability of the functions inside moment conditions,\nwhich does not hold when nuisance parameters appear in indicator functions. In\nthis paper, we propose smoothing the indicator functions, and develop an\nasymptotic distribution theory for this class of models. The asymptotic\nbehavior of the proposed estimator exhibits a trade-off between bias and\nvariance due to smoothing. We study how a parameter which controls the degree\nof smoothing can be chosen optimally to minimize an upper bound of the\nasymptotic mean squared error. A Monte Carlo simulation supports the asymptotic\ndistribution theory, and an empirical example illustrates the implementation of\nthe method.",
        "authors": [
            "Gyungbae Park"
        ],
        "categories": "econ.EM",
        "published": "2024-03-23T21:28:42Z",
        "updated": "2024-03-23T21:28:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.15910v2",
        "title": "Difference-in-Differences with Unpoolable Data",
        "abstract": "Difference-in-differences (DID) is commonly used to estimate treatment\neffects but is infeasible in settings where data are unpoolable due to privacy\nconcerns or legal restrictions on data sharing, particularly across\njurisdictions. In this study, we identify and relax the assumption of data\npoolability in DID estimation. We propose an innovative approach to estimate\nDID with unpoolable data (UN-DID) which can accommodate covariates, multiple\ngroups, and staggered adoption. Through analytical proofs and Monte Carlo\nsimulations, we show that UN-DID and conventional DID estimates of the average\ntreatment effect and standard errors are equal and unbiased in settings without\ncovariates. With covariates, both methods produce estimates that are unbiased,\nequivalent, and converge to the true value. The estimates differ slightly but\nthe statistical inference and substantive conclusions remain the same. Two\nempirical examples with real-world data further underscore UN-DID's utility.\nThe UN-DID method allows the estimation of cross-jurisdictional treatment\neffects with unpoolable data, enabling better counterfactuals to be used and\nnew research questions to be answered.",
        "authors": [
            "Sunny Karim",
            "Matthew D. Webb",
            "Nichole Austin",
            "Erin Strumpf"
        ],
        "categories": "econ.EM",
        "published": "2024-03-23T18:50:50Z",
        "updated": "2024-10-09T02:50:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.15258v1",
        "title": "Tests for almost stochastic dominance",
        "abstract": "We introduce a 2-dimensional stochastic dominance (2DSD) index to\ncharacterize both strict and almost stochastic dominance. Based on this index,\nwe derive an estimator for the minimum violation ratio (MVR), also known as the\ncritical parameter, of the almost stochastic ordering condition between two\nvariables. We determine the asymptotic properties of the empirical 2DSD index\nand MVR for the most frequently used stochastic orders. We also provide\nconditions under which the bootstrap estimators of these quantities are\nstrongly consistent. As an application, we develop consistent bootstrap testing\nprocedures for almost stochastic dominance. The performance of the tests is\nchecked via simulations and the analysis of real data.",
        "authors": [
            "Amparo Ba\u00edllo",
            "Javier C\u00e1rcamo",
            "Carlos Mora-Corral"
        ],
        "categories": "econ.EM",
        "published": "2024-03-22T14:58:48Z",
        "updated": "2024-03-22T14:58:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.15220v1",
        "title": "Modelling with Discretized Variables",
        "abstract": "This paper deals with econometric models in which the dependent variable,\nsome explanatory variables, or both are observed as censored interval data.\nThis discretization often happens due to confidentiality of sensitive variables\nlike income. Models using these variables cannot point identify regression\nparameters as the conditional moments are unknown, which led the literature to\nuse interval estimates. Here, we propose a discretization method through which\nthe regression parameters can be point identified while preserving data\nconfidentiality. We demonstrate the asymptotic properties of the OLS estimator\nfor the parameters in multivariate linear regressions for cross-sectional data.\nThe theoretical findings are supported by Monte Carlo experiments and\nillustrated with an application to the Australian gender wage gap.",
        "authors": [
            "Felix Chan",
            "Laszlo Matyas",
            "Agoston Reguly"
        ],
        "categories": "econ.EM",
        "published": "2024-03-22T14:09:59Z",
        "updated": "2024-03-22T14:09:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.15111v1",
        "title": "Fast TTC Computation",
        "abstract": "This paper proposes a fast Markov Matrix-based methodology for computing Top\nTrading Cycles (TTC) that delivers O(1) computational speed, that is speed\nindependent of the number of agents and objects in the system. The proposed\nmethodology is well suited for complex large-dimensional problems like housing\nchoice. The methodology retains all the properties of TTC, namely,\nPareto-efficiency, individual rationality and strategy-proofness.",
        "authors": [
            "Irene Aldridge"
        ],
        "categories": "econ.EM",
        "published": "2024-03-22T11:07:35Z",
        "updated": "2024-03-22T11:07:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.14385v2",
        "title": "Estimating Causal Effects with Double Machine Learning -- A Method Evaluation",
        "abstract": "The estimation of causal effects with observational data continues to be a\nvery active research area. In recent years, researchers have developed new\nframeworks which use machine learning to relax classical assumptions necessary\nfor the estimation of causal effects. In this paper, we review one of the most\nprominent methods - \"double/debiased machine learning\" (DML) - and empirically\nevaluate it by comparing its performance on simulated data relative to more\ntraditional statistical methods, before applying it to real-world data. Our\nfindings indicate that the application of a suitably flexible machine learning\nalgorithm within DML improves the adjustment for various nonlinear confounding\nrelationships. This advantage enables a departure from traditional functional\nform assumptions typically necessary in causal effect estimation. However, we\ndemonstrate that the method continues to critically depend on standard\nassumptions about causal structure and identification. When estimating the\neffects of air pollution on housing prices in our application, we find that DML\nestimates are consistently larger than estimates of less flexible methods. From\nour overall results, we provide actionable recommendations for specific choices\nresearchers must make when applying DML in practice.",
        "authors": [
            "Jonathan Fuhr",
            "Philipp Berens",
            "Dominik Papies"
        ],
        "categories": "stat.ML",
        "published": "2024-03-21T13:21:33Z",
        "updated": "2024-04-30T10:42:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.14216v2",
        "title": "A Gaussian smooth transition vector autoregressive model: An application to the macroeconomic effects of severe weather shocks",
        "abstract": "We introduce a new smooth transition vector autoregressive model with a\nGaussian conditional distribution and transition weights that, for a $p$th\norder model, depend on the full distribution of the preceding $p$ observations.\nSpecifically, the transition weight of each regime increases in its relative\nweighted likelihood. This data-driven approach facilitates capturing complex\nswitching dynamics, enhancing the identification of gradual regime shifts. In\nan empirical application to the macroeconomic effects of a severe weather\nshock, we find that in monthly U.S. data from 1961:1 to 2022:3, the impacts of\nthe shock are stronger in the regime prevailing in the early part of the sample\nand in certain crisis periods than in the regime dominating the latter part of\nthe sample. This suggests overall adaptation of the U.S. economy to increased\nsevere weather over time.",
        "authors": [
            "Markku Lanne",
            "Savi Virolainen"
        ],
        "categories": "econ.EM",
        "published": "2024-03-21T08:13:07Z",
        "updated": "2024-07-04T10:40:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.14036v2",
        "title": "Fused LASSO as Non-Crossing Quantile Regression",
        "abstract": "Quantile crossing has been an ever-present thorn in the side of quantile\nregression. This has spurred research into obtaining densities and coefficients\nthat obey the quantile monotonicity property. While important contributions,\nthese papers do not provide insight into how exactly these constraints\ninfluence the estimated coefficients. This paper extends non-crossing\nconstraints and shows that by varying a single hyperparameter ($\\alpha$) one\ncan obtain commonly used quantile estimators. Namely, we obtain the quantile\nregression estimator of Koenker and Bassett (1978) when $\\alpha=0$, the non\ncrossing quantile regression estimator of Bondell et al. (2010) when\n$\\alpha=1$, and the composite quantile regression estimator of Koenker (1984)\nand Zou and Yuan (2008) when $\\alpha\\rightarrow\\infty$. As such, we show that\nnon-crossing constraints are simply a special type of fused-shrinkage.",
        "authors": [
            "Tibor Szendrei",
            "Arnab Bhattacharjee",
            "Mark E. Schaffer"
        ],
        "categories": "econ.EM",
        "published": "2024-03-20T23:20:44Z",
        "updated": "2024-08-09T21:52:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.13738v1",
        "title": "Policy Relevant Treatment Effects with Multidimensional Unobserved Heterogeneity",
        "abstract": "This paper provides a framework for the policy relevant treatment effects\nusing instrumental variables. In the framework, a treatment selection may or\nmay not satisfy the classical monotonicity condition and can accommodate\nmultidimensional unobserved heterogeneity. We can bound the target parameter by\nextracting information from identifiable estimands. We also provide a more\nconservative yet computationally simpler bound by applying a convex relaxation\nmethod. Linear shape restrictions can be easily incorporated to further improve\nthe bounds. Numerical and simulation results illustrate the informativeness of\nour convex-relaxation bounds, i.e., that our bounds are sufficiently tight.",
        "authors": [
            "Takuya Ura",
            "Lina Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-03-20T16:44:57Z",
        "updated": "2024-03-20T16:44:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.13725v1",
        "title": "Robust Inference in Locally Misspecified Bipartite Networks",
        "abstract": "This paper introduces a methodology to conduct robust inference in bipartite\nnetworks under local misspecification. We focus on a class of dyadic network\nmodels with misspecified conditional moment restrictions. The framework of\nmisspecification is local, as the effect of misspecification varies with the\nsample size. We utilize this local asymptotic approach to construct a robust\nestimator that is minimax optimal for the mean square error within a\nneighborhood of misspecification. Additionally, we introduce bias-aware\nconfidence intervals that account for the effect of the local misspecification.\nThese confidence intervals have the correct asymptotic coverage for the true\nparameter of interest under sparse network asymptotics. Monte Carlo experiments\ndemonstrate that the robust estimator performs well in finite samples and\nsparse networks. As an empirical illustration, we study the formation of a\nscientific collaboration network among economists.",
        "authors": [
            "Luis E. Candelaria",
            "Yichong Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-03-20T16:36:04Z",
        "updated": "2024-03-20T16:36:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.13361v1",
        "title": "Multifractal wavelet dynamic mode decomposition modeling for marketing time series",
        "abstract": "Marketing is the way we ensure our sales are the best in the market, our\nprices the most accessible, and our clients satisfied, thus ensuring our brand\nhas the widest distribution. This requires sophisticated and advanced\nunderstanding of the whole related network. Indeed, marketing data may exist in\ndifferent forms such as qualitative and quantitative data. However, in the\nliterature, it is easily noted that large bibliographies may be collected about\nqualitative studies, while only a few studies adopt a quantitative point of\nview. This is a major drawback that results in marketing science still focusing\non design, although the market is strongly dependent on quantities such as\nmoney and time. Indeed, marketing data may form time series such as brand sales\nin specified periods, brand-related prices over specified periods, market\nshares, etc. The purpose of the present work is to investigate some marketing\nmodels based on time series for various brands. This paper aims to combine the\ndynamic mode decomposition and wavelet decomposition to study marketing series\ndue to both prices, and volume sales in order to explore the effect of the time\nscale on the persistence of brand sales in the market and on the forecasting of\nsuch persistence, according to the characteristics of the brand and the related\nmarket competition or competitors. Our study is based on a sample of Saudi\nbrands during the period 22 November 2017 to 30 December 2021.",
        "authors": [
            "Mohamed Elshazli A. Zidan",
            "Anouar Ben Mabrouk",
            "Nidhal Ben Abdallah",
            "Tawfeeq M. Alanazi"
        ],
        "categories": "q-fin.MF",
        "published": "2024-03-20T07:43:11Z",
        "updated": "2024-03-20T07:43:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.12653v1",
        "title": "Composite likelihood estimation of stationary Gaussian processes with a view toward stochastic volatility",
        "abstract": "We develop a framework for composite likelihood inference of parametric\ncontinuous-time stationary Gaussian processes. We derive the asymptotic theory\nof the associated maximum composite likelihood estimator. We implement our\napproach on a pair of models that has been proposed to describe the random\nlog-spot variance of financial asset returns. A simulation study shows that it\ndelivers good performance in these settings and improves upon a\nmethod-of-moments estimation. In an application, we inspect the dynamic of an\nintraday measure of spot variance computed with high-frequency data from the\ncryptocurrency market. The empirical evidence supports a mechanism, where the\nshort- and long-term correlation structure of stochastic volatility are\ndecoupled in order to capture its properties at different time scales.",
        "authors": [
            "Mikkel Bennedsen",
            "Kim Christensen",
            "Peter Christensen"
        ],
        "categories": "econ.EM",
        "published": "2024-03-19T11:37:30Z",
        "updated": "2024-03-19T11:37:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.12456v1",
        "title": "Inflation Target at Risk: A Time-varying Parameter Distributional Regression",
        "abstract": "Macro variables frequently display time-varying distributions, driven by the\ndynamic and evolving characteristics of economic, social, and environmental\nfactors that consistently reshape the fundamental patterns and relationships\ngoverning these variables. To better understand the distributional dynamics\nbeyond the central tendency, this paper introduces a novel semi-parametric\napproach for constructing time-varying conditional distributions, relying on\nthe recent advances in distributional regression. We present an efficient\nprecision-based Markov Chain Monte Carlo algorithm that simultaneously\nestimates all model parameters while explicitly enforcing the monotonicity\ncondition on the conditional distribution function. Our model is applied to\nconstruct the forecasting distribution of inflation for the U.S., conditional\non a set of macroeconomic and financial indicators. The risks of future\ninflation deviating excessively high or low from the desired range are\ncarefully evaluated. Moreover, we provide a thorough discussion about the\ninterplay between inflation and unemployment rates during the Global Financial\nCrisis, COVID, and the third quarter of 2023.",
        "authors": [
            "Yunyun Wang",
            "Tatsushi Oka",
            "Dan Zhu"
        ],
        "categories": "econ.EM",
        "published": "2024-03-19T05:27:24Z",
        "updated": "2024-03-19T05:27:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.11954v3",
        "title": "Robust Estimation and Inference for Categorical Data",
        "abstract": "While there is a rich literature on robust methodologies for contamination in\ncontinuously distributed data, contamination in categorical data is largely\noverlooked. This is regrettable because many datasets are categorical and\noftentimes suffer from contamination. Examples include inattentive responding\nand bot responses in questionnaires or zero-inflated count data. We propose a\nnovel class of contamination-robust estimators of models for categorical data,\ncoined $C$-estimators (``$C$'' for categorical). We show that the countable and\npossibly finite sample space of categorical data results in non-standard\ntheoretical properties. Notably, in contrast to classic robustness theory,\n$C$-estimators can be simultaneously robust \\textit{and} fully efficient at the\npostulated model. In addition, a certain particularly robust specification\nfails to be asymptotically Gaussian at the postulated model, but is\nasymptotically Gaussian in the presence of contamination. We furthermore\npropose a diagnostic test to identify categorical outliers and demonstrate the\nenhanced robustness of $C$-estimators in a simulation study.",
        "authors": [
            "Max Welz"
        ],
        "categories": "stat.ME",
        "published": "2024-03-18T16:51:56Z",
        "updated": "2024-12-12T12:43:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.11333v1",
        "title": "Identification of Information Structures in Bayesian Games",
        "abstract": "To what extent can an external observer observing an equilibrium action\ndistribution in an incomplete information game infer the underlying information\nstructure? We investigate this issue in a general linear-quadratic-Gaussian\nframework. A simple class of canonical information structures is offered and\nproves rich enough to rationalize any possible equilibrium action distribution\nthat can arise under an arbitrary information structure. We show that the class\nis parsimonious in the sense that the relevant parameters can be uniquely\npinned down by an observed equilibrium outcome, up to some qualifications. Our\nresult implies, for example, that the accuracy of each agent's signal about the\nstate is identified, as measured by how much observing the signal reduces the\nstate variance. Moreover, we show that a canonical information structure\ncharacterizes the lower bound on the amount by which each agent's signal can\nreduce the state variance, across all observationally equivalent information\nstructures. The lower bound is tight, for example, when the actual information\nstructure is uni-dimensional, or when there are no strategic interactions among\nagents, but in general, there is a gap since agents' strategic motives confound\ntheir private information about fundamental and strategic uncertainty.",
        "authors": [
            "Masaki Miyashita"
        ],
        "categories": "econ.TH",
        "published": "2024-03-17T20:25:38Z",
        "updated": "2024-03-17T20:25:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.11309v1",
        "title": "Nonparametric Identification and Estimation with Non-Classical Errors-in-Variables",
        "abstract": "This paper considers nonparametric identification and estimation of the\nregression function when a covariate is mismeasured. The measurement error need\nnot be classical. Employing the small measurement error approximation, we\nestablish nonparametric identification under weak and easy-to-interpret\nconditions on the instrumental variable. The paper also provides nonparametric\nestimators of the regression function and derives their rates of convergence.",
        "authors": [
            "Kirill S. Evdokimov",
            "Andrei Zeleneev"
        ],
        "categories": "econ.EM",
        "published": "2024-03-17T19:09:05Z",
        "updated": "2024-03-17T19:09:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.11016v2",
        "title": "Comprehensive OOS Evaluation of Predictive Algorithms with Statistical Decision Theory",
        "abstract": "We argue that comprehensive out-of-sample (OOS) evaluation using statistical\ndecision theory (SDT) should replace the current practice of K-fold and Common\nTask Framework validation in machine learning (ML) research. SDT provides a\nformal framework for performing comprehensive OOS evaluation across all\npossible (1) training samples, (2) populations that may generate training data,\nand (3) populations of prediction interest. Regarding feature (3), we emphasize\nthat SDT requires the practitioner to directly confront the possibility that\nthe future may not look like the past and to account for a possible need to\nextrapolate from one population to another when building a predictive\nalgorithm. SDT is simple in abstraction, but it is often computationally\ndemanding to implement. We discuss progress in tractable implementation of SDT\nwhen prediction accuracy is measured by mean square error or by\nmisclassification rate. We summarize research studying settings in which the\ntraining data will be generated from a subpopulation of the population of\nprediction interest. We consider conditional prediction with alternative\nrestrictions on the state space of possible populations that may generate\ntraining data. We present an illustrative application of the methodology to the\nproblem of predicting patient illness to inform clinical decision making. We\nconclude by calling on ML researchers to join with econometricians and\nstatisticians in expanding the domain within which implementation of SDT is\ntractable.",
        "authors": [
            "Jeff Dominitz",
            "Charles F. Manski"
        ],
        "categories": "econ.EM",
        "published": "2024-03-16T20:59:49Z",
        "updated": "2024-05-25T14:34:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.10907v2",
        "title": "Macroeconomic Spillovers of Weather Shocks across U.S. States",
        "abstract": "We estimate the short-run effects of severe weather shocks on local economic\nactivity and cross-border spillovers operating through economic linkages\nbetween U.S. states. We measure weather shocks using emergency declarations\ntriggered by natural disasters and estimate their impacts with a monthly Global\nVector Autoregressive (GVAR) model for U.S. states. Impulse responses highlight\ncountry-wide effects of weather shocks hitting individual regions. Taking into\naccount economic interconnections between states allows capturing much stronger\nspillovers than those associated with mere spatial adjacency. Also,\ngeographical heterogeneity is critical for assessing country-wide effects of\nweather shocks, and network effects amplify the local impacts of shocks.",
        "authors": [
            "Emanuele Bacchiocchi",
            "Andrea Bastianin",
            "Graziano Moramarco"
        ],
        "categories": "econ.EM",
        "published": "2024-03-16T11:54:13Z",
        "updated": "2024-04-15T09:44:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.10618v1",
        "title": "Limits of Approximating the Median Treatment Effect",
        "abstract": "Average Treatment Effect (ATE) estimation is a well-studied problem in causal\ninference. However, it does not necessarily capture the heterogeneity in the\ndata, and several approaches have been proposed to tackle the issue, including\nestimating the Quantile Treatment Effects. In the finite population setting\ncontaining $n$ individuals, with treatment and control values denoted by the\npotential outcome vectors $\\mathbf{a}, \\mathbf{b}$, much of the prior work\nfocused on estimating median$(\\mathbf{a}) -$ median$(\\mathbf{b})$, where\nmedian($\\mathbf x$) denotes the median value in the sorted ordering of all the\nvalues in vector $\\mathbf x$. It is known that estimating the difference of\nmedians is easier than the desired estimand of median$(\\mathbf{a-b})$, called\nthe Median Treatment Effect (MTE). The fundamental problem of causal inference\n-- for every individual $i$, we can only observe one of the potential outcome\nvalues, i.e., either the value $a_i$ or $b_i$, but not both, makes estimating\nMTE particularly challenging. In this work, we argue that MTE is not estimable\nand detail a novel notion of approximation that relies on the sorted order of\nthe values in $\\mathbf{a-b}$. Next, we identify a quantity called variability\nthat exactly captures the complexity of MTE estimation. By drawing connections\nto instance-optimality studied in theoretical computer science, we show that\nevery algorithm for estimating the MTE obtains an approximation error that is\nno better than the error of an algorithm that computes variability. Finally, we\nprovide a simple linear time algorithm for computing the variability exactly.\nUnlike much prior work, a particular highlight of our work is that we make no\nassumptions about how the potential outcome vectors are generated or how they\nare correlated, except that the potential outcome values are $k$-ary, i.e.,\ntake one of $k$ discrete values.",
        "authors": [
            "Raghavendra Addanki",
            "Siddharth Bhandari"
        ],
        "categories": "cs.LG",
        "published": "2024-03-15T18:30:06Z",
        "updated": "2024-03-15T18:30:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.10352v1",
        "title": "Goodness-of-Fit for Conditional Distributions: An Approach Using Principal Component Analysis and Component Selection",
        "abstract": "This paper introduces a novel goodness-of-fit test technique for parametric\nconditional distributions. The proposed tests are based on a residual marked\nempirical process, for which we develop a conditional Principal Component\nAnalysis. The obtained components provide a basis for various types of new\ntests in addition to the omnibus one. Component tests that based on each\ncomponent serve as experts in detecting certain directions. Smooth tests that\nassemble a few components are also of great use in practice. To further improve\ntesting efficiency, we introduce a component selection approach, aiming to\nidentify the most contributory components. The finite sample performance of the\nproposed tests is illustrated through Monte Carlo experiments.",
        "authors": [
            "Cui Rui",
            "Li Yuhao"
        ],
        "categories": "econ.EM",
        "published": "2024-03-15T14:37:46Z",
        "updated": "2024-03-15T14:37:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.10239v1",
        "title": "A Big Data Approach to Understand Sub-national Determinants of FDI in Africa",
        "abstract": "Various macroeconomic and institutional factors hinder FDI inflows, including\ncorruption, trade openness, access to finance, and political instability.\nExisting research mostly focuses on country-level data, with limited\nexploration of firm-level data, especially in developing countries. Recognizing\nthis gap, recent calls for research emphasize the need for qualitative data\nanalysis to delve into FDI determinants, particularly at the regional level.\nThis paper proposes a novel methodology, based on text mining and social\nnetwork analysis, to get information from more than 167,000 online news\narticles to quantify regional-level (sub-national) attributes affecting FDI\nownership in African companies. Our analysis extends information on obstacles\nto industrial development as mapped by the World Bank Enterprise Surveys.\nFindings suggest that regional (sub-national) structural and institutional\ncharacteristics can play an important role in determining foreign ownership.",
        "authors": [
            "A. Fronzetti Colladon",
            "R. Vestrelli",
            "S. Bait",
            "M. M. Schiraldi"
        ],
        "categories": "cs.CL",
        "published": "2024-03-15T12:12:54Z",
        "updated": "2024-03-15T12:12:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.08753v1",
        "title": "Invalid proxies and volatility changes",
        "abstract": "When in proxy-SVARs the covariance matrix of VAR disturbances is subject to\nexogenous, permanent, nonrecurring breaks that generate target impulse response\nfunctions (IRFs) that change across volatility regimes, even strong, exogenous\nexternal instruments can result in inconsistent estimates of the dynamic causal\neffects of interest if the breaks are not properly accounted for. In such\ncases, it is essential to explicitly incorporate the shifts in unconditional\nvolatility in order to point-identify the target structural shocks and possibly\nrestore consistency. We demonstrate that, under a necessary and sufficient rank\ncondition that leverages moments implied by changes in volatility, the target\nIRFs can be point-identified and consistently estimated. Importantly, standard\nasymptotic inference remains valid in this context despite (i) the covariance\nbetween the proxies and the instrumented structural shocks being local-to-zero,\nas in Staiger and Stock (1997), and (ii) the potential failure of instrument\nexogeneity. We introduce a novel identification strategy that appropriately\ncombines external instruments with \"informative\" changes in volatility, thus\nobviating the need to assume proxy relevance and exogeneity in estimation. We\nillustrate the effectiveness of the suggested method by revisiting a fiscal\nproxy-SVAR previously estimated in the literature, complementing the fiscal\ninstruments with information derived from the massive reduction in volatility\nobserved in the transition from the Great Inflation to the Great Moderation\nregimes.",
        "authors": [
            "Giovanni Angelini",
            "Luca Fanelli",
            "Luca Neri"
        ],
        "categories": "econ.EM",
        "published": "2024-03-13T17:51:47Z",
        "updated": "2024-03-13T17:51:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.08183v3",
        "title": "Identifying Treatment and Spillover Effects Using Exposure Contrasts",
        "abstract": "To report spillover effects, a common practice is to regress outcomes on\nstatistics capturing treatment variation among neighboring units. This paper\nstudies the causal interpretation of nonparametric analogs of these estimands,\nwhich we refer to as exposure contrasts. We demonstrate that their signs can be\ninconsistent with those of the unit-level effects of interest even under\nunconfounded assignment. We then provide interpretable restrictions under which\nexposure contrasts are sign preserving and therefore have causal\ninterpretations. We discuss the implications of our results for\ncluster-randomized trials, network experiments, and observational settings with\npeer effects in selection into treatment.",
        "authors": [
            "Michael P. Leung"
        ],
        "categories": "econ.EM",
        "published": "2024-03-13T02:12:03Z",
        "updated": "2024-12-02T22:44:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.08130v2",
        "title": "Imputation of Counterfactual Outcomes when the Errors are Predictable",
        "abstract": "A crucial input into causal inference is the imputed counterfactual outcome.\nImputation error can arise because of sampling uncertainty from estimating the\nprediction model using the untreated observations, or from out-of-sample\ninformation not captured by the model. While the literature has focused on\nsampling uncertainty, it vanishes with the sample size. Often overlooked is the\npossibility that the out-of-sample error can be informative about the missing\ncounterfactual outcome if it is mutually or serially correlated. Motivated by\nthe best linear unbiased predictor (\\blup) of \\citet{goldberger:62} in a time\nseries setting, we propose an improved predictor of potential outcome when the\nerrors are correlated. The proposed \\pup\\; is practical as it is not restricted\nto linear models, can be used with consistent estimators already developed, and\nimproves mean-squared error for a large class of strong mixing error processes.\nIgnoring predictability in the errors can distort conditional inference.\nHowever, the precise impact will depend on the choice of estimator as well as\nthe realized values of the residuals.",
        "authors": [
            "Silvia Goncalves",
            "Serena Ng"
        ],
        "categories": "econ.EM",
        "published": "2024-03-12T23:38:29Z",
        "updated": "2024-05-17T16:25:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.07236v5",
        "title": "Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Model",
        "abstract": "It is well known that the relationship between variables at the individual\nlevel can be different from the relationship between those same variables\naggregated over individuals. In this paper, I develop a methodology to\npartially identify linear combinations of conditional mean outcomes for\nindividual-level outcomes of interest without imposing parametric assumptions\nwhen the researcher only has access to aggregate data. I construct identified\nsets using an optimization program that allows for researchers to impose\nadditional shape and data restrictions. I also provide consistency results and\nconstruct an inference procedure that is valid with data that only provides\nmarginal information about each variable. I apply the methodology to simulated\nand real-world data sets and find that the estimated identified sets are too\nwide to be useful, but become narrower as more assumptions are imposed and data\naggregated at a finer level is available.",
        "authors": [
            "Sarah Moon"
        ],
        "categories": "econ.EM",
        "published": "2024-03-12T01:14:35Z",
        "updated": "2024-05-03T22:07:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.06879v2",
        "title": "Partially identified heteroskedastic SVARs",
        "abstract": "This paper studies the identification of Structural Vector Autoregressions\n(SVARs) exploiting a break in the variances of the structural shocks.\nPoint-identification for this class of models relies on an eigen-decomposition\ninvolving the covariance matrices of reduced-form errors and requires that all\nthe eigenvalues are distinct. This point-identification, however, fails in the\npresence of multiplicity of eigenvalues. This occurs in an empirically relevant\nscenario where, for instance, only a subset of structural shocks had the break\nin their variances, or where a group of variables shows a variance shift of the\nsame amount. Together with zero or sign restrictions on the structural\nparameters and impulse responses, we derive the identified sets for impulse\nresponses and show how to compute them. We perform inference on the impulse\nresponse functions, building on the robust Bayesian approach developed for set\nidentified SVARs. To illustrate our proposal, we present an empirical example\nbased on the literature on the global crude oil market where the identification\nis expected to fail due to multiplicity of eigenvalues.",
        "authors": [
            "Emanuele Bacchiocchi",
            "Andrea Bastianin",
            "Toru Kitagawa",
            "Elisabetta Mirto"
        ],
        "categories": "econ.EM",
        "published": "2024-03-11T16:33:56Z",
        "updated": "2024-03-15T10:33:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.06657v1",
        "title": "Data-Driven Tuning Parameter Selection for High-Dimensional Vector Autoregressions",
        "abstract": "Lasso-type estimators are routinely used to estimate high-dimensional time\nseries models. The theoretical guarantees established for Lasso typically\nrequire the penalty level to be chosen in a suitable fashion often depending on\nunknown population quantities. Furthermore, the resulting estimates and the\nnumber of variables retained in the model depend crucially on the chosen\npenalty level. However, there is currently no theoretically founded guidance\nfor this choice in the context of high-dimensional time series. Instead one\nresorts to selecting the penalty level in an ad hoc manner using, e.g.,\ninformation criteria or cross-validation. We resolve this problem by\nconsidering estimation of the perhaps most commonly employed multivariate time\nseries model, the linear vector autoregressive (VAR) model, and propose a\nweighted Lasso estimator with penalization chosen in a fully data-driven way.\nThe theoretical guarantees that we establish for the resulting estimation and\nprediction error match those currently available for methods based on\ninfeasible choices of penalization. We thus provide a first solution for\nchoosing the penalization in high-dimensional time series models.",
        "authors": [
            "Anders Bredahl Kock",
            "Rasmus S\u00f8ndergaard Pedersen",
            "Jesper Riis-Vestergaard S\u00f8rensen"
        ],
        "categories": "econ.EM",
        "published": "2024-03-11T12:24:19Z",
        "updated": "2024-03-11T12:24:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.06246v1",
        "title": "Estimating Factor-Based Spot Volatility Matrices with Noisy and Asynchronous High-Frequency Data",
        "abstract": "We propose a new estimator of high-dimensional spot volatility matrices\nsatisfying a low-rank plus sparse structure from noisy and asynchronous\nhigh-frequency data collected for an ultra-large number of assets. The noise\nprocesses are allowed to be temporally correlated, heteroskedastic,\nasymptotically vanishing and dependent on the efficient prices. We define a\nkernel-weighted pre-averaging method to jointly tackle the microstructure noise\nand asynchronicity issues, and we obtain uniformly consistent estimates for\nlatent prices. We impose a continuous-time factor model with time-varying\nfactor loadings on the price processes, and estimate the common factors and\nloadings via a local principal component analysis. Assuming a uniform sparsity\ncondition on the idiosyncratic volatility structure, we combine the POET and\nkernel-smoothing techniques to estimate the spot volatility matrices for both\nthe latent prices and idiosyncratic errors. Under some mild restrictions, the\nestimated spot volatility matrices are shown to be uniformly consistent under\nvarious matrix norms. We provide Monte-Carlo simulation and empirical studies\nto examine the numerical performance of the developed estimation methodology.",
        "authors": [
            "Degui Li",
            "Oliver Linton",
            "Haoxuan Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-03-10T16:08:35Z",
        "updated": "2024-03-10T16:08:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.05999v1",
        "title": "Locally Regular and Efficient Tests in Non-Regular Semiparametric Models",
        "abstract": "This paper considers hypothesis testing in semiparametric models which may be\nnon-regular. I show that C($\\alpha$) style tests are locally regular under mild\nconditions, including in cases where locally regular estimators do not exist,\nsuch as models which are (semi-parametrically) weakly identified. I\ncharacterise the appropriate limit experiment in which to study local\n(asymptotic) optimality of tests in the non-regular case, permitting the\ngeneralisation of classical power bounds to this case. I give conditions under\nwhich these power bounds are attained by the proposed C($\\alpha$) style tests.\nThe application of the theory to a single index model and an instrumental\nvariables model is worked out in detail.",
        "authors": [
            "Adam Lee"
        ],
        "categories": "econ.EM",
        "published": "2024-03-09T20:09:40Z",
        "updated": "2024-03-09T20:09:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.05850v1",
        "title": "Estimating Causal Effects of Discrete and Continuous Treatments with Binary Instruments",
        "abstract": "We propose an instrumental variable framework for identifying and estimating\naverage and quantile effects of discrete and continuous treatments with binary\ninstruments. The basis of our approach is a local copula representation of the\njoint distribution of the potential outcomes and unobservables determining\ntreatment assignment. This representation allows us to introduce an identifying\nassumption, so-called copula invariance, that restricts the local dependence of\nthe copula with respect to the treatment propensity. We show that copula\ninvariance identifies treatment effects for the entire population and other\nsubpopulations such as the treated. The identification results are constructive\nand lead to straightforward semiparametric estimation procedures based on\ndistribution regression. An application to the effect of sleep on well-being\nuncovers interesting patterns of heterogeneity.",
        "authors": [
            "Victor Chernozhukov",
            "Iv\u00e1n Fern\u00e1ndez-Val",
            "Sukjin Han",
            "Kaspar W\u00fcthrich"
        ],
        "categories": "econ.EM",
        "published": "2024-03-09T09:20:35Z",
        "updated": "2024-03-09T09:20:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.05803v1",
        "title": "Semiparametric Inference for Regression-Discontinuity Designs",
        "abstract": "Treatment effects in regression discontinuity designs (RDDs) are often\nestimated using local regression methods. However, global approximation methods\nare generally deemed inefficient. In this paper, we propose a semiparametric\nframework tailored for estimating treatment effects in RDDs. Our global\napproach conceptualizes the identification of treatment effects within RDDs as\na partially linear modeling problem, with the linear component capturing the\ntreatment effect. Furthermore, we utilize the P-spline method to approximate\nthe nonparametric function and develop procedures for inferring treatment\neffects within this framework. We demonstrate through Monte Carlo simulations\nthat the proposed method performs well across various scenarios. Furthermore,\nwe illustrate using real-world datasets that our global approach may result in\nmore reliable inference.",
        "authors": [
            "Rong J. B. Zhu",
            "Weiwei Jiang"
        ],
        "categories": "econ.EM",
        "published": "2024-03-09T05:42:02Z",
        "updated": "2024-03-09T05:42:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.05704v4",
        "title": "Non-robustness of diffusion estimates on networks with measurement error",
        "abstract": "Network diffusion models are used to study things like disease transmission,\ninformation spread, and technology adoption. However, small amounts of\nmismeasurement are extremely likely in the networks constructed to\noperationalize these models. We show that estimates of diffusions are highly\nnon-robust to this measurement error. First, we show that even when measurement\nerror is vanishingly small, such that the share of missed links is close to\nzero, forecasts about the extent of diffusion will greatly underestimate the\ntruth. Second, a small mismeasurement in the identity of the initial seed\ngenerates a large shift in the locations of expected diffusion path. We show\nthat both of these results still hold when the vanishing measurement error is\nonly local in nature. Such non-robustness in forecasting exists even under\nconditions where the basic reproductive number is consistently estimable.\nPossible solutions, such as estimating the measurement error or implementing\nwidespread detection efforts, still face difficulties because the number of\nmissed links are so small. Finally, we conduct Monte Carlo simulations on\nsimulated networks, and real networks from three settings: travel data from the\nCOVID-19 pandemic in the western US, a mobile phone marketing campaign in rural\nIndia, and in an insurance experiment in China.",
        "authors": [
            "Arun G. Chandrasekhar",
            "Paul Goldsmith-Pinkham",
            "Tyler H. McCormick",
            "Samuel Thau",
            "Jerry Wei"
        ],
        "categories": "econ.EM",
        "published": "2024-03-08T22:36:03Z",
        "updated": "2024-06-11T18:43:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.04766v1",
        "title": "Nonparametric Regression under Cluster Sampling",
        "abstract": "This paper develops a general asymptotic theory for nonparametric kernel\nregression in the presence of cluster dependence. We examine nonparametric\ndensity estimation, Nadaraya-Watson kernel regression, and local linear\nestimation. Our theory accommodates growing and heterogeneous cluster sizes. We\nderive asymptotic conditional bias and variance, establish uniform consistency,\nand prove asymptotic normality. Our findings reveal that under heterogeneous\ncluster sizes, the asymptotic variance includes a new term reflecting\nwithin-cluster dependence, which is overlooked when cluster sizes are presumed\nto be bounded. We propose valid approaches for bandwidth selection and\ninference, introduce estimators of the asymptotic variance, and demonstrate\ntheir consistency. In simulations, we verify the effectiveness of the\ncluster-robust bandwidth selection and show that the derived cluster-robust\nconfidence interval improves the coverage ratio. We illustrate the application\nof these methods using a policy-targeting dataset in development economics.",
        "authors": [
            "Yuya Shimizu"
        ],
        "categories": "econ.EM",
        "published": "2024-03-07T18:59:12Z",
        "updated": "2024-03-07T18:59:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.04354v1",
        "title": "A Logarithmic Mean Divisia Index Decomposition of CO$_2$ Emissions from Energy Use in Romania",
        "abstract": "Carbon emissions have become a specific alarming indicators and intricate\nchallenges that lead an extended argue about climate change. The growing trend\nin the utilization of fossil fuels for the economic progress and simultaneously\nreducing the carbon quantity has turn into a substantial and global challenge.\nThe aim of this paper is to examine the driving factors of CO$_2$ emissions\nfrom energy sector in Romania during the period 2008-2022 emissions using the\nlog mean Divisia index (LMDI) method and takes into account five items: CO$_2$\nemissions, primary energy resources, energy consumption, gross domestic product\nand population, the driving forces of CO$_2$ emissions, based on which it was\ncalculated the contribution of carbon intensity, energy mixes, generating\nefficiency, economy, and population. The results indicate that generating\nefficiency effect -90968.57 is the largest inhibiting index while economic\neffect is the largest positive index 69084.04 having the role of increasing\nCO$_2$ emissions.",
        "authors": [
            "Mariana Carmelia Balanica-Dragomir",
            "Gabriel Murariu",
            "Lucian Puiu Georgescu"
        ],
        "categories": "econ.EM",
        "published": "2024-03-07T09:37:31Z",
        "updated": "2024-03-07T09:37:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.04328v3",
        "title": "A dual approach to nonparametric characterization for random utility models",
        "abstract": "This paper develops a novel characterization for random utility models (RUM),\nwhich turns out to be a dual representation of the characterization by Kitamura\nand Stoye (2018, ECMA). For a given family of budgets and its \"patch\"\nrepresentation \\'a la Kitamura and Stoye, we construct a matrix $\\Xi$ of which\neach row vector indicates the structure of possible revealed preference\nrelations in each subfamily of budgets. Then, it is shown that a stochastic\ndemand system on the patches of budget lines, say $\\pi$, is consistent with a\nRUM, if and only if $\\Xi\\pi \\geq \\mathbb{1}$, where the RHS is the vector of\n$1$'s. In addition to providing a concise quantifier-free characterization,\nespecially when $\\pi$ is inconsistent with RUMs, the vector $\\Xi\\pi$ also\ncontains information concerning (1) sub-families of budgets in which cyclical\nchoices must occur with positive probabilities, and (2) the maximal possible\nweights on rational choice patterns in a population. The notion of Chv\\'atal\nrank of polytopes and the duality theorem in linear programming play key roles\nto obtain these results.",
        "authors": [
            "Nobuo Koida",
            "Koji Shirai"
        ],
        "categories": "econ.TH",
        "published": "2024-03-07T08:48:17Z",
        "updated": "2024-06-18T14:04:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.04236v1",
        "title": "Regularized DeepIV with Model Selection",
        "abstract": "In this paper, we study nonparametric estimation of instrumental variable\n(IV) regressions. While recent advancements in machine learning have introduced\nflexible methods for IV estimation, they often encounter one or more of the\nfollowing limitations: (1) restricting the IV regression to be uniquely\nidentified; (2) requiring minimax computation oracle, which is highly unstable\nin practice; (3) absence of model selection procedure. In this paper, we\npresent the first method and analysis that can avoid all three limitations,\nwhile still enabling general function approximation. Specifically, we propose a\nminimax-oracle-free method called Regularized DeepIV (RDIV) regression that can\nconverge to the least-norm IV solution. Our method consists of two stages:\nfirst, we learn the conditional distribution of covariates, and by utilizing\nthe learned distribution, we learn the estimator by minimizing a\nTikhonov-regularized loss function. We further show that our method allows\nmodel selection procedures that can achieve the oracle rates in the\nmisspecified regime. When extended to an iterative estimator, our method\nmatches the current state-of-the-art convergence rate. Our method is a Tikhonov\nregularized variant of the popular DeepIV method with a non-parametric MLE\nfirst-stage estimator, and our results provide the first rigorous guarantees\nfor this empirically used method, showcasing the importance of regularization\nwhich was absent from the original work.",
        "authors": [
            "Zihao Li",
            "Hui Lan",
            "Vasilis Syrgkanis",
            "Mengdi Wang",
            "Masatoshi Uehara"
        ],
        "categories": "cs.LG",
        "published": "2024-03-07T05:38:56Z",
        "updated": "2024-03-07T05:38:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.04131v5",
        "title": "Extracting Mechanisms from Heterogeneous Effects: An Identification Strategy for Mediation Analysis",
        "abstract": "Understanding causal mechanisms is crucial for explaining and generalizing\nempirical phenomena. Causal mediation analysis offers statistical techniques to\nquantify the mediation effects. However, current methods often require multiple\nignorability assumptions or sophisticated research designs. In this paper, we\nintroduce a novel identification strategy that enables the simultaneous\nidentification and estimation of treatment and mediation effects. By combining\nexplicit and implicit mediation analysis, this strategy exploits heterogeneous\ntreatment effects through a new decomposition of total treatment effects. Monte\nCarlo simulations demonstrate that the method is more accurate and precise\nacross various scenarios. To illustrate the efficiency and efficacy of our\nmethod, we apply it to estimate the causal mediation effects in two studies\nwith distinct data structures, focusing on common pool resource governance and\nvoting information. Additionally, we have developed statistical software to\nfacilitate the implementation of our method.",
        "authors": [
            "Jiawei Fu"
        ],
        "categories": "stat.ME",
        "published": "2024-03-07T01:12:41Z",
        "updated": "2024-10-30T17:54:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.03589v2",
        "title": "Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices",
        "abstract": "This study designs an adaptive experiment for efficiently estimating average\ntreatment effects (ATEs). In each round of our adaptive experiment, an\nexperimenter sequentially samples an experimental unit, assigns a treatment,\nand observes the corresponding outcome immediately. At the end of the\nexperiment, the experimenter estimates an ATE using the gathered samples. The\nobjective is to estimate the ATE with a smaller asymptotic variance. Existing\nstudies have designed experiments that adaptively optimize the propensity score\n(treatment-assignment probability). As a generalization of such an approach, we\npropose optimizing the covariate density as well as the propensity score.\nFirst, we derive the efficient covariate density and propensity score that\nminimize the semiparametric efficiency bound and find that optimizing both\ncovariate density and propensity score minimizes the semiparametric efficiency\nbound more effectively than optimizing only the propensity score. Next, we\ndesign an adaptive experiment using the efficient covariate density and\npropensity score sequentially estimated during the experiment. Lastly, we\npropose an ATE estimator whose asymptotic variance aligns with the minimized\nsemiparametric efficiency bound.",
        "authors": [
            "Masahiro Kato",
            "Akihiro Oga",
            "Wataru Komatsubara",
            "Ryo Inokuchi"
        ],
        "categories": "stat.ME",
        "published": "2024-03-06T10:24:44Z",
        "updated": "2024-06-18T18:20:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.03299v3",
        "title": "Demystifying and avoiding the OLS \"weighting problem\": Unmodeled heterogeneity and straightforward solutions",
        "abstract": "Researchers have long run regressions of an outcome variable (Y) on a\ntreatment (D) and covariates (X) to estimate treatment effects. Even absent\nunobserved confounding, the regression coefficient on D in this setup reports a\nconditional variance weighted average of strata-wise average effects, not\ngenerally equal to the average treatment effect (ATE). Numerous proposals have\nbeen offered to cope with this \"weighting problem\", including interpretational\ntools to help characterize the weights and diagnostic aids to help researchers\nassess the potential severity of this problem. We make two contributions that\ntogether suggest an alternative direction for researchers and this literature.\nOur first contribution is conceptual, demystifying these weights. Simply put,\nunder heterogeneous treatment effects (and varying probability of treatment),\nthe linear regression of Y on D and X will be misspecified. The \"weights\" of\nregression offer one characterization for the coefficient from regression that\nhelps to clarify how it will depart from the ATE. We also derive a more general\nexpression for the weights than what is usually referenced. Our second\ncontribution is practical: as these weights simply characterize\nmisspecification bias, we suggest simply avoiding them through an approach that\ntolerate heterogeneous effects. A wide range of longstanding alternatives\n(regression-imputation/g-computation, interacted regression, and balancing\nweights) relax specification assumptions to allow heterogeneous effects. We\nmake explicit the assumption of \"separate linearity\", under which each\npotential outcome is separately linear in X. This relaxation of conventional\nlinearity offers a common justification for all of these methods and avoids the\nweighting problem, at an efficiency cost that will be small when there are few\ncovariates relative to sample size.",
        "authors": [
            "Chad Hazlett",
            "Tanvi Shinkre"
        ],
        "categories": "stat.ME",
        "published": "2024-03-05T20:00:48Z",
        "updated": "2024-11-21T00:57:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.03240v1",
        "title": "Triple/Debiased Lasso for Statistical Inference of Conditional Average Treatment Effects",
        "abstract": "This study investigates the estimation and the statistical inference about\nConditional Average Treatment Effects (CATEs), which have garnered attention as\na metric representing individualized causal effects. In our data-generating\nprocess, we assume linear models for the outcomes associated with binary\ntreatments and define the CATE as a difference between the expected outcomes of\nthese linear models. This study allows the linear models to be\nhigh-dimensional, and our interest lies in consistent estimation and\nstatistical inference for the CATE. In high-dimensional linear regression, one\ntypical approach is to assume sparsity. However, in our study, we do not assume\nsparsity directly. Instead, we consider sparsity only in the difference of the\nlinear models. We first use a doubly robust estimator to approximate this\ndifference and then regress the difference on covariates with Lasso\nregularization. Although this regression estimator is consistent for the CATE,\nwe further reduce the bias using the techniques in double/debiased machine\nlearning (DML) and debiased Lasso, leading to $\\sqrt{n}$-consistency and\nconfidence intervals. We refer to the debiased estimator as the triple/debiased\nLasso (TDL), applying both DML and debiased Lasso techniques. We confirm the\nsoundness of our proposed method through simulation studies.",
        "authors": [
            "Masahiro Kato"
        ],
        "categories": "stat.ME",
        "published": "2024-03-05T18:29:36Z",
        "updated": "2024-03-05T18:29:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.02591v2",
        "title": "Matrix-based Prediction Approach for Intraday Instantaneous Volatility Vector",
        "abstract": "In this paper, we introduce a novel method for predicting intraday\ninstantaneous volatility based on Ito semimartingale models using\nhigh-frequency financial data. Several studies have highlighted stylized\nvolatility time series features, such as interday auto-regressive dynamics and\nthe intraday U-shaped pattern. To accommodate these volatility features, we\npropose an interday-by-intraday instantaneous volatility matrix process that\ncan be decomposed into low-rank conditional expected instantaneous volatility\nand noise matrices. To predict the low-rank conditional expected instantaneous\nvolatility matrix, we propose the Two-sIde Projected-PCA (TIP-PCA) procedure.\nWe establish asymptotic properties of the proposed estimators and conduct a\nsimulation study to assess the finite sample performance of the proposed\nprediction method. Finally, we apply the TIP-PCA method to an out-of-sample\ninstantaneous volatility vector prediction study using high-frequency data from\nthe S&P 500 index and 11 sector index funds.",
        "authors": [
            "Sung Hoon Choi",
            "Donggyu Kim"
        ],
        "categories": "econ.EM",
        "published": "2024-03-05T02:07:51Z",
        "updated": "2024-12-05T16:03:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.02467v1",
        "title": "Applied Causal Inference Powered by ML and AI",
        "abstract": "An introduction to the emerging fusion of machine learning and causal\ninference. The book presents ideas from classical structural equation models\n(SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and\nstructural causal models (SCMs), and covers Double/Debiased Machine Learning\nmethods to do inference in such models using modern predictive tools.",
        "authors": [
            "Victor Chernozhukov",
            "Christian Hansen",
            "Nathan Kallus",
            "Martin Spindler",
            "Vasilis Syrgkanis"
        ],
        "categories": "econ.EM",
        "published": "2024-03-04T20:28:28Z",
        "updated": "2024-03-04T20:28:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.02144v1",
        "title": "Improved Tests for Mediation",
        "abstract": "Testing for a mediation effect is important in many disciplines, but is made\ndifficult - even asymptotically - by the influence of nuisance parameters.\nClassical tests such as likelihood ratio (LR) and Wald (Sobel) tests have very\npoor power properties in parts of the parameter space, and many attempts have\nbeen made to produce improved tests, with limited success. In this paper we\nshow that augmenting the critical region of the LR test can produce a test with\nmuch improved behavior everywhere. In fact, we first show that there exists a\ntest of this type that is (asymptotically) exact for certain test levels\n$\\alpha $, including the common choices $\\alpha =.01,.05,.10.$ The critical\nregion of this exact test has some undesirable properties. We go on to show\nthat there is a very simple class of augmented LR critical regions which\nprovides tests that are nearly exact, and avoid the issues inherent in the\nexact test. We suggest an optimal and coherent member of this class, provide\nthe table needed to implement the test and to report p-values if desired.\nSimulation confirms validity with non-Gaussian disturbances, under\nheteroskedasticity, and in a nonlinear (logit) model. A short application of\nthe method to an entrepreneurial attitudes study is included for illustration.",
        "authors": [
            "Grant Hillier",
            "Kees Jan van Garderen",
            "Noud van Giersbergen"
        ],
        "categories": "econ.EM",
        "published": "2024-03-04T15:54:19Z",
        "updated": "2024-03-04T15:54:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.01585v2",
        "title": "Calibrating doubly-robust estimators with unbalanced treatment assignment",
        "abstract": "Machine learning methods, particularly the double machine learning (DML)\nestimator (Chernozhukov et al., 2018), are increasingly popular for the\nestimation of the average treatment effect (ATE). However, datasets often\nexhibit unbalanced treatment assignments where only a few observations are\ntreated, leading to unstable propensity score estimations. We propose a simple\nextension of the DML estimator which undersamples data for propensity score\nmodeling and calibrates scores to match the original distribution. The paper\nprovides theoretical results showing that the estimator retains the DML\nestimator's asymptotic properties. A simulation study illustrates the finite\nsample performance of the estimator.",
        "authors": [
            "Daniele Ballinari"
        ],
        "categories": "econ.EM",
        "published": "2024-03-03T18:40:11Z",
        "updated": "2024-06-11T07:11:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.01386v2",
        "title": "Minimax-Regret Sample Selection in Randomized Experiments",
        "abstract": "Randomized controlled trials are often run in settings with many\nsubpopulations that may have differential benefits from the treatment being\nevaluated. We consider the problem of sample selection, i.e., whom to enroll in\na randomized trial, such as to optimize welfare in a heterogeneous population.\nWe formalize this problem within the minimax-regret framework, and derive\noptimal sample-selection schemes under a variety of conditions. Using data from\na COVID-19 vaccine trial, we also highlight how different objectives and\ndecision rules can lead to meaningfully different guidance regarding optimal\nsample allocation.",
        "authors": [
            "Yuchen Hu",
            "Henry Zhu",
            "Emma Brunskill",
            "Stefan Wager"
        ],
        "categories": "stat.ME",
        "published": "2024-03-03T03:43:48Z",
        "updated": "2024-06-25T09:24:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.01318v2",
        "title": "High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media",
        "abstract": "Motivated by the empirical observation of power-law distributions in the\ncredits (e.g., \"likes\") of viral social media posts, we introduce a\nhigh-dimensional tail index regression model and propose methods for estimation\nand inference of its parameters. First, we present a regularized estimator,\nestablish its consistency, and derive its convergence rate. Second, we\nintroduce a debiasing technique for the regularized estimator to facilitate\ninference and prove its asymptotic normality. Third, we extend our approach to\nhandle large-scale online streaming data using stochastic gradient descent.\nSimulation studies corroborate our theoretical findings. We apply these methods\nto the text analysis of viral posts on X (formerly Twitter) related to LGBTQ+\ntopics.",
        "authors": [
            "Yuya Sasaki",
            "Jing Tao",
            "Yulong Wang"
        ],
        "categories": "stat.ML",
        "published": "2024-03-02T21:37:40Z",
        "updated": "2024-10-06T21:12:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.00458v1",
        "title": "Prices and preferences in the electric vehicle market",
        "abstract": "Although electric vehicles are less polluting than gasoline powered vehicles,\nadoption is challenged by higher procurement prices. Existing discourse\nemphasizes EV battery costs as being principally responsible for this price\ndifferential and widespread adoption is routinely conditioned upon battery\ncosts declining. We scrutinize such reasoning by sourcing data on EV attributes\nand market conditions between 2011 and 2023. Our findings are fourfold. First,\nEV prices are influenced principally by the number of amenities, additional\nfeatures, and dealer-installed accessories sold as standard on an EV, and to a\nlesser extent, by EV horsepower. Second, EV range is negatively correlated with\nEV price implying that range anxiety concerns may be less consequential than\nexisting discourse suggests. Third, battery capacity is positively correlated\nwith EV price, due to more capacity being synonymous with the delivery of more\nhorsepower. Collectively, this suggests that higher procurement prices for EVs\nreflects consumer preference for vehicles that are feature dense and more\npowerful. Fourth and finally, accommodating these preferences have produced\nvehicles with lower fuel economy, a shift that reduces envisioned lifecycle\nemissions benefits by at least 3.26 percent, subject to the battery pack\nchemistry leveraged and the carbon intensity of the electrical grid. These\nfindings warrant attention as decarbonization efforts increasingly emphasize\nelectrification as a pathway for complying with domestic and international\nclimate agreements.",
        "authors": [
            "Chung Yi See",
            "Vasco Rato Santos",
            "Lucas Woodley",
            "Megan Yeo",
            "Daniel Palmer",
            "Shuheng Zhang",
            "and Ashley Nunes"
        ],
        "categories": "econ.EM",
        "published": "2024-03-01T11:30:25Z",
        "updated": "2024-03-01T11:30:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.00422v1",
        "title": "Inference for Interval-Identified Parameters Selected from an Estimated Set",
        "abstract": "Interval identification of parameters such as average treatment effects,\naverage partial effects and welfare is particularly common when using\nobservational data and experimental data with imperfect compliance due to the\nendogeneity of individuals' treatment uptake. In this setting, a treatment or\npolicy will typically become an object of interest to the researcher when it is\neither selected from the estimated set of best-performers or arises from a\ndata-dependent selection rule. In this paper, we develop new inference tools\nfor interval-identified parameters chosen via these forms of selection. We\ndevelop three types of confidence intervals for data-dependent and\ninterval-identified parameters, discuss how they apply to several examples of\ninterest and prove their uniform asymptotic validity under weak assumptions.",
        "authors": [
            "Sukjin Han",
            "Adam McCloskey"
        ],
        "categories": "econ.EM",
        "published": "2024-03-01T10:17:08Z",
        "updated": "2024-03-01T10:17:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2403.00347v2",
        "title": "Set-Valued Control Functions",
        "abstract": "The control function approach allows the researcher to identify various\ncausal effects of interest. While powerful, it requires a strong invertibility\nassumption, which limits its applicability. This paper expands the scope of the\nnonparametric control function approach by allowing the control function to be\nset-valued and derive sharp bounds on structural parameters. The proposed\ngeneralization accommodates a wide range of selection processes involving\ndiscrete endogenous variables, random coefficients, treatment selections with\ninterference, and dynamic treatment selections.",
        "authors": [
            "Sukjin Han",
            "Hiroaki Kaido"
        ],
        "categories": "econ.EM",
        "published": "2024-03-01T08:17:23Z",
        "updated": "2024-03-18T13:57:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.19425v1",
        "title": "Testing Information Ordering for Strategic Agents",
        "abstract": "A key primitive of a strategic environment is the information available to\nplayers. Specifying a priori an information structure is often difficult for\nempirical researchers. We develop a test of information ordering that allows\nresearchers to examine if the true information structure is at least as\ninformative as a proposed baseline. We construct a computationally tractable\ntest statistic by utilizing the notion of Bayes Correlated Equilibrium (BCE) to\ntranslate the ordering of information structures into an ordering of functions.\nWe apply our test to examine whether hubs provide informational advantages to\ncertain airlines in addition to market power.",
        "authors": [
            "Sukjin Han",
            "Hiroaki Kaido",
            "Lorenzo Magnolfi"
        ],
        "categories": "econ.EM",
        "published": "2024-02-29T18:22:38Z",
        "updated": "2024-02-29T18:22:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.19399v3",
        "title": "An Empirical Analysis of Scam Tokens on Ethereum Blockchain",
        "abstract": "This article presents an empirical investigation into the determinants of\ntotal revenue generated by counterfeit tokens on Uniswap. It offers a detailed\noverview of the counterfeit token fraud process, along with a systematic\nsummary of characteristics associated with such fraudulent activities observed\nin Uniswap. The study primarily examines the relationship between revenue from\ncounterfeit token scams and their defining characteristics, and analyzes the\ninfluence of market economic factors such as return on market capitalization\nand price return on Ethereum. Key findings include a significant increase in\noverall transactions of counterfeit tokens on their first day of fraud, and a\nrise in upfront fraud costs leading to corresponding increases in revenue.\nFurthermore, a negative correlation is identified between the total revenue of\ncounterfeit tokens and the volatility of Ethereum market capitalization return,\nwhile price return volatility on Ethereum is found to have a positive impact on\ncounterfeit token revenue, albeit requiring further investigation for a\ncomprehensive understanding. Additionally, the number of subscribers for the\nreal token correlates positively with the realized volume of scam tokens,\nindicating that a larger community following the legitimate token may\ninadvertently contribute to the visibility and success of counterfeit tokens.\nConversely, the number of Telegram subscribers exhibits a negative impact on\nthe realized volume of scam tokens, suggesting that a higher level of scrutiny\nor awareness within Telegram communities may act as a deterrent to fraudulent\nactivities. Finally, the timing of when the scam token is introduced on the\nEthereum blockchain may have a negative impact on its success. Notably, the\ncumulative amount scammed by only 42 counterfeit tokens amounted to almost\n11214 Ether.",
        "authors": [
            "Vahidin Jeleskovic"
        ],
        "categories": "q-fin.TR",
        "published": "2024-02-29T17:57:05Z",
        "updated": "2024-03-05T23:47:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.19268v2",
        "title": "Extremal quantiles of intermediate orders under two-way clustering",
        "abstract": "This paper investigates extremal quantiles under two-way cluster dependence.\nWe demonstrate that the limiting distribution of the unconditional intermediate\norder quantiles in the tails converges to a Gaussian distribution. This is\nremarkable as two-way cluster dependence entails potential non-Gaussianity in\ngeneral, but extremal quantiles do not suffer from this issue. Building upon\nthis result, we extend our analysis to extremal quantile regressions of\nintermediate order.",
        "authors": [
            "Harold D. Chiang",
            "Ryutah Kato",
            "Yuya Sasaki"
        ],
        "categories": "math.ST",
        "published": "2024-02-29T15:38:58Z",
        "updated": "2024-03-04T15:31:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.18392v2",
        "title": "Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators",
        "abstract": "The growing demand for personalized decision-making has led to a surge of\ninterest in estimating the Conditional Average Treatment Effect (CATE). Various\ntypes of CATE estimators have been developed with advancements in machine\nlearning and causal inference. However, selecting the desirable CATE estimator\nthrough a conventional model validation procedure remains impractical due to\nthe absence of counterfactual outcomes in observational data. Existing\napproaches for CATE estimator selection, such as plug-in and pseudo-outcome\nmetrics, face two challenges. First, they must determine the metric form and\nthe underlying machine learning models for fitting nuisance parameters (e.g.,\noutcome function, propensity function, and plug-in learner). Second, they lack\na specific focus on selecting a robust CATE estimator. To address these\nchallenges, this paper introduces a Distributionally Robust Metric (DRM) for\nCATE estimator selection. The proposed DRM is nuisance-free, eliminating the\nneed to fit models for nuisance parameters, and it effectively prioritizes the\nselection of a distributionally robust CATE estimator. The experimental results\nvalidate the effectiveness of the DRM method in selecting CATE estimators that\nare robust to the distribution shift incurred by covariate shift and hidden\nconfounders.",
        "authors": [
            "Yiyan Huang",
            "Cheuk Hang Leung",
            "Siyi Wang",
            "Yijun Li",
            "Qi Wu"
        ],
        "categories": "cs.LG",
        "published": "2024-02-28T15:12:24Z",
        "updated": "2024-10-31T11:08:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.17374v1",
        "title": "Quasi-Bayesian Estimation and Inference with Control Functions",
        "abstract": "We consider a quasi-Bayesian method that combines a frequentist estimation in\nthe first stage and a Bayesian estimation/inference approach in the second\nstage. The study is motivated by structural discrete choice models that use the\ncontrol function methodology to correct for endogeneity bias. In this scenario,\nthe first stage estimates the control function using some frequentist\nparametric or nonparametric approach. The structural equation in the second\nstage, associated with certain complicated likelihood functions, can be more\nconveniently dealt with using a Bayesian approach. This paper studies the\nasymptotic properties of the quasi-posterior distributions obtained from the\nsecond stage. We prove that the corresponding quasi-Bayesian credible set does\nnot have the desired coverage in large samples. Nonetheless, the quasi-Bayesian\npoint estimator remains consistent and is asymptotically equivalent to a\nfrequentist two-stage estimator. We show that one can obtain valid inference by\nbootstrapping the quasi-posterior that takes into account the first-stage\nestimation uncertainty.",
        "authors": [
            "Ruixuan Liu",
            "Zhengfei Yu"
        ],
        "categories": "econ.EM",
        "published": "2024-02-27T10:11:43Z",
        "updated": "2024-02-27T10:11:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.17103v1",
        "title": "Causal Orthogonalization: Multicollinearity, Economic Interpretability, and the Gram-Schmidt Process",
        "abstract": "This paper considers the problem of interpreting orthogonalization model\ncoefficients. We derive a causal economic interpretation of the Gram-Schmidt\northogonalization process and provide the conditions for its equivalence to\ntotal effects from a recursive Directed Acyclic Graph. We extend the\nGram-Schmidt process to groups of simultaneous regressors common in economic\ndata sets and derive its finite sample properties, finding its coefficients to\nbe unbiased, stable, and more efficient than those from Ordinary Least Squares.\nFinally, we apply the estimator to childhood reading comprehension scores,\ncontrolling for such highly collinear characteristics as race, education, and\nincome. The model expands Bohren et al.'s decomposition of systemic\ndiscrimination into channel-specific effects and improves its coefficient\nsignificance levels.",
        "authors": [
            "Robin M. Cross",
            "Steven T. Buccola"
        ],
        "categories": "econ.EM",
        "published": "2024-02-27T00:36:58Z",
        "updated": "2024-02-27T00:36:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.17042v2",
        "title": "Towards Generalizing Inferences from Trials to Target Populations",
        "abstract": "Randomized Controlled Trials (RCTs) are pivotal in generating internally\nvalid estimates with minimal assumptions, serving as a cornerstone for\nresearchers dedicated to advancing causal inference methods. However, extending\nthese findings beyond the experimental cohort to achieve externally valid\nestimates is crucial for broader scientific inquiry. This paper delves into the\nforefront of addressing these external validity challenges, encapsulating the\nessence of a multidisciplinary workshop held at the Institute for Computational\nand Experimental Research in Mathematics (ICERM), Brown University, in Fall\n2023. The workshop congregated experts from diverse fields including social\nscience, medicine, public health, statistics, computer science, and education,\nto tackle the unique obstacles each discipline faces in extrapolating\nexperimental findings. Our study presents three key contributions: we integrate\nongoing efforts, highlighting methodological synergies across fields; provide\nan exhaustive review of generalizability and transportability based on the\nworkshop's discourse; and identify persistent hurdles while suggesting avenues\nfor future research. By doing so, this paper aims to enhance the collective\nunderstanding of the generalizability and transportability of causal effects,\nfostering cross-disciplinary collaboration and offering valuable insights for\nresearchers working on refining and applying causal inference methods.",
        "authors": [
            "Melody Y Huang",
            "Harsh Parikh"
        ],
        "categories": "stat.ME",
        "published": "2024-02-26T21:49:44Z",
        "updated": "2024-05-25T00:05:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.16693v1",
        "title": "Fast Algorithms for Quantile Regression with Selection",
        "abstract": "This paper addresses computational challenges in estimating Quantile\nRegression with Selection (QRS). The estimation of the parameters that model\nself-selection requires the estimation of the entire quantile process several\ntimes. Moreover, closed-form expressions of the asymptotic variance are too\ncumbersome, making the bootstrap more convenient to perform inference. Taking\nadvantage of recent advancements in the estimation of quantile regression,\nalong with some specific characteristics of the QRS estimation problem, I\npropose streamlined algorithms for the QRS estimator. These algorithms\nsignificantly reduce computation time through preprocessing techniques and\nquantile grid reduction for the estimation of the copula and slope parameters.\nI show the optimization enhancements with some simulations. Lastly, I show how\npreprocessing methods can improve the precision of the estimates without\nsacrificing computational efficiency. Hence, they constitute a practical\nsolutions for estimators with non-differentiable and non-convex criterion\nfunctions such as those based on copulas.",
        "authors": [
            "Santiago Pereda-Fern\u00e1ndez"
        ],
        "categories": "econ.EM",
        "published": "2024-02-26T16:07:46Z",
        "updated": "2024-02-26T16:07:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.16580v2",
        "title": "Information-Enriched Selection of Stationary and Non-Stationary Autoregressions using the Adaptive Lasso",
        "abstract": "We propose a novel approach to elicit the weight of a potentially\nnon-stationary regressor in the consistent and oracle-efficient estimation of\nautoregressive models using the adaptive Lasso. The enhanced weight builds on a\nstatistic that exploits distinct orders in probability of the OLS estimator in\ntime series regressions when the degree of integration differs. We provide\ntheoretical results on the benefit of our approach for detecting stationarity\nwhen a tuning criterion selects the $\\ell_1$ penalty parameter. Monte Carlo\nevidence shows that our proposal is superior to using OLS-based weights, as\nsuggested by Kock [Econom. Theory, 32, 2016, 243-259]. We apply the modified\nestimator to model selection for German inflation rates after the introduction\nof the Euro. The results indicate that energy commodity price inflation and\nheadline inflation are best described by stationary autoregressions.",
        "authors": [
            "Thilo Reinschl\u00fcssel",
            "Martin C. Arnold"
        ],
        "categories": "stat.ME",
        "published": "2024-02-26T14:04:24Z",
        "updated": "2024-07-19T18:41:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.16322v1",
        "title": "Estimating Stochastic Block Models in the Presence of Covariates",
        "abstract": "In the standard stochastic block model for networks, the probability of a\nconnection between two nodes, often referred to as the edge probability,\ndepends on the unobserved communities each of these nodes belongs to. We\nconsider a flexible framework in which each edge probability, together with the\nprobability of community assignment, are also impacted by observed covariates.\nWe propose a computationally tractable two-step procedure to estimate the\nconditional edge probabilities as well as the community assignment\nprobabilities. The first step relies on a spectral clustering algorithm applied\nto a localized adjacency matrix of the network. In the second step, k-nearest\nneighbor regression estimates are computed on the extracted communities. We\nstudy the statistical properties of these estimators by providing\nnon-asymptotic bounds.",
        "authors": [
            "Yuichi Kitamura",
            "Louise Laage"
        ],
        "categories": "econ.EM",
        "published": "2024-02-26T06:05:38Z",
        "updated": "2024-02-26T06:05:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.15585v4",
        "title": "Inference for Regression with Variables Generated by AI or Machine Learning",
        "abstract": "It has become common practice for researchers to use AI-powered information\nretrieval algorithms or other machine learning methods to estimate variables of\neconomic interest, then use these estimates as covariates in a regression\nmodel. We show both theoretically and empirically that naively treating AI- and\nML-generated variables as \"data\" leads to biased estimates and invalid\ninference. We propose two methods to correct bias and perform valid inference:\n(i) an explicit bias correction with bias-corrected confidence intervals, and\n(ii) joint maximum likelihood estimation of the regression model and the\nvariables of interest. Through several applications, we demonstrate that the\ncommon approach generates substantial bias, while both corrections perform\nwell.",
        "authors": [
            "Laura Battaglia",
            "Timothy Christensen",
            "Stephen Hansen",
            "Szymon Sacher"
        ],
        "categories": "econ.EM",
        "published": "2024-02-23T19:52:09Z",
        "updated": "2024-12-10T14:08:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.14764v2",
        "title": "A Combinatorial Central Limit Theorem for Stratified Randomization",
        "abstract": "This paper establishes a combinatorial central limit theorem for stratified\nrandomization, which holds under a Lindeberg-type condition. The theorem allows\nfor an arbitrary number or sizes of strata, with the sole requirement being\nthat each stratum contains at least two units. This flexibility accommodates\nboth a growing number of large and small strata simultaneously, while imposing\nminimal conditions. We then apply this result to derive the asymptotic\ndistributions of two test statistics proposed for instrumental variables\nsettings in the presence of potentially many strata of unrestricted sizes.",
        "authors": [
            "Purevdorj Tuvaandorj"
        ],
        "categories": "math.ST",
        "published": "2024-02-22T18:23:13Z",
        "updated": "2024-04-15T15:32:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.14763v3",
        "title": "Functional Spatial Autoregressive Models",
        "abstract": "This study introduces a novel spatial autoregressive model in which the\ndependent variable is a function that may exhibit functional autocorrelation\nwith the outcome functions of nearby units. This model can be characterized as\na simultaneous integral equation system, which, in general, does not\nnecessarily have a unique solution. For this issue, we provide a simple\ncondition on the magnitude of the spatial interaction to ensure the uniqueness\nin data realization. For estimation, to account for the endogeneity caused by\nthe spatial interaction, we propose a regularized two-stage least squares\nestimator based on a basis approximation for the functional parameter. The\nasymptotic properties of the estimator including the consistency and asymptotic\nnormality are investigated under certain conditions. Additionally, we propose a\nsimple Wald-type test for detecting the presence of spatial effects. As an\nempirical illustration, we apply the proposed model and method to analyze age\ndistributions in Japanese cities.",
        "authors": [
            "Tadao Hoshino"
        ],
        "categories": "econ.EM",
        "published": "2024-02-22T18:22:35Z",
        "updated": "2024-10-01T05:23:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.14538v1",
        "title": "Interference Produces False-Positive Pricing Experiments",
        "abstract": "It is standard practice in online retail to run pricing experiments by\nrandomizing at the article-level, i.e. by changing prices of different products\nto identify treatment effects. Due to customers' cross-price substitution\nbehavior, such experiments suffer from interference bias: the observed\ndifference between treatment groups in the experiment is typically\nsignificantly larger than the global effect that could be expected after a\nroll-out decision of the tested pricing policy. We show in simulations that\nsuch bias can be as large as 100%, and report experimental data implying bias\nof similar magnitude. Finally, we discuss approaches for de-biased pricing\nexperiments, suggesting observational methods as a potentially attractive\nalternative to clustering.",
        "authors": [
            "Lars Roemheld",
            "Justin Rao"
        ],
        "categories": "stat.AP",
        "published": "2024-02-22T13:30:24Z",
        "updated": "2024-02-22T13:30:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.14506v2",
        "title": "Enhancing Rolling Horizon Production Planning Through Stochastic Optimization Evaluated by Means of Simulation",
        "abstract": "Production planning must account for uncertainty in a production system,\narising from fluctuating demand forecasts. Therefore, this article focuses on\nthe integration of updated customer demand into the rolling horizon planning\ncycle. We use scenario-based stochastic programming to solve capacitated lot\nsizing problems under stochastic demand in a rolling horizon environment. This\nenvironment is replicated using a discrete event simulation-optimization\nframework, where the optimization problem is periodically solved, leveraging\nthe latest demand information to continually adjust the production plan. We\nevaluate the stochastic optimization approach and compare its performance to\nsolving a deterministic lot sizing model, using expected demand figures as\ninput, as well as to standard Material Requirements Planning (MRP). In the\nsimulation study, we analyze three different customer behaviors related to\nforecasting, along with four levels of shop load, within a multi-item and\nmulti-stage production system. We test a range of significant parameter values\nfor the three planning methods and compute the overall costs to benchmark them.\nThe results show that the production plans obtained by MRP are outperformed by\ndeterministic and stochastic optimization. Particularly, when facing tight\nresource restrictions and rising uncertainty in customer demand, the use of\nstochastic optimization becomes preferable compared to deterministic\noptimization.",
        "authors": [
            "Manuel Schlenkrich",
            "Wolfgang Seiringer",
            "Klaus Altendorfer",
            "Sophie N. Parragh"
        ],
        "categories": "econ.EM",
        "published": "2024-02-22T12:55:24Z",
        "updated": "2024-09-26T14:10:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.14264v2",
        "title": "Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation",
        "abstract": "Average treatment effect estimation is the most central problem in causal\ninference with application to numerous disciplines. While many estimation\nstrategies have been proposed in the literature, the statistical optimality of\nthese methods has still remained an open area of investigation, especially in\nregimes where these methods do not achieve parametric rates. In this paper, we\nadopt the recently introduced structure-agnostic framework of statistical lower\nbounds, which poses no structural properties on the nuisance functions other\nthan access to black-box estimators that achieve some statistical estimation\nrate. This framework is particularly appealing when one is only willing to\nconsider estimation strategies that use non-parametric regression and\nclassification oracles as black-box sub-processes. Within this framework, we\nprove the statistical optimality of the celebrated and widely used doubly\nrobust estimators for both the Average Treatment Effect (ATE) and the Average\nTreatment Effect on the Treated (ATT), as well as weighted variants of the\nformer, which arise in policy evaluation.",
        "authors": [
            "Jikai Jin",
            "Vasilis Syrgkanis"
        ],
        "categories": "stat.ML",
        "published": "2024-02-22T04:03:32Z",
        "updated": "2024-03-02T02:00:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.14206v1",
        "title": "The impact of Facebook-Cambridge Analytica data scandal on the USA tech stock market: An event study based on clustering method",
        "abstract": "This study delves into the intra-industry effects following a firm-specific\nscandal, with a particular focus on the Facebook data leakage scandal and its\nassociated events within the U.S. tech industry and two additional relevant\ngroups. We employ various metrics including daily spread, volatility,\nvolume-weighted return, and CAPM-beta for the pre-analysis clustering, and\nsubsequently utilize CAR (Cumulative Abnormal Return) to evaluate the impact on\nfirms grouped within these clusters. From a broader industry viewpoint,\nsignificant positive CAARs are observed across U.S. sample firms over the three\ndays post-scandal announcement, indicating no adverse impact on the tech sector\noverall. Conversely, after Facebook's initial quarterly earnings report, it\nshowed a notable negative effect despite reported positive performance. The\nclustering principle should aid in identifying directly related companies and\nthus reducing the influence of randomness. This was indeed achieved for the\neffect of the key event, namely \"The Effect of Congressional Hearing on Certain\nClusters across U.S. Tech Stock Market,\" which was identified as delayed and\nsignificantly negative. Therefore, we recommend applying the clustering method\nwhen conducting such or similar event studies.",
        "authors": [
            "Vahidin Jeleskovic",
            "Yinan Wan"
        ],
        "categories": "econ.EM",
        "published": "2024-02-22T01:19:12Z",
        "updated": "2024-02-22T01:19:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.13604v2",
        "title": "Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE",
        "abstract": "This paper introduces a new tool, OccCANINE, to automatically transform\noccupational descriptions into the HISCO classification system. The manual work\ninvolved in processing and classifying occupational descriptions is\nerror-prone, tedious, and time-consuming. We finetune a preexisting language\nmodel (CANINE) to do this automatically, thereby performing in seconds and\nminutes what previously took days and weeks. The model is trained on 14 million\npairs of occupational descriptions and HISCO codes in 13 different languages\ncontributed by 22 different sources. Our approach is shown to have accuracy,\nrecall, and precision above 90 percent. Our tool breaks the metaphorical HISCO\nbarrier and makes this data readily available for analysis of occupational\nstructures with broad applicability in economics, economic history, and various\nrelated disciplines.",
        "authors": [
            "Christian M\u00f8ller Dahl",
            "Torben Johansen",
            "Christian Vedel"
        ],
        "categories": "cs.CL",
        "published": "2024-02-21T08:10:43Z",
        "updated": "2024-04-02T14:56:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.13375v2",
        "title": "Vulnerability Webs: Systemic Risk in Software Networks",
        "abstract": "Modern software development is a collaborative effort that re-uses existing\ncode to reduce development and maintenance costs. This practice exposes\nsoftware to vulnerabilities in the form of undetected bugs in direct and\nindirect dependencies, as demonstrated by the Crowdstrike and HeartBleed bugs.\nThe economic costs resulting from such vulnerabilities can be staggering. We\nstudy a directed network of 52,897 software dependencies across 16,102 Python\nrepositories, guided by a strategic model of network formation that\nincorporates both observable and unobservable heterogeneity. Using a scalable\nvariational approximation of the conditional distribution of unobserved\nheterogeneity, we show that outsourcing code to other software packages by\ncreating dependencies generates negative externalities. Modeling the\npropagation of risk in networks of software packages as an epidemiological\nprocess, we show that increasing protection of dependencies based on popular\nheuristics is ineffective at reducing systemic risk. By contrast, AI-assisted\ncoding enables developers to replace dependencies with in-house code and\nreduces systemic risk.",
        "authors": [
            "Cornelius Fritz",
            "Co-Pierre Georg",
            "Angelo Mele",
            "Michael Schweinberger"
        ],
        "categories": "econ.EM",
        "published": "2024-02-20T20:59:20Z",
        "updated": "2024-11-07T23:20:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.13023v1",
        "title": "Bridging Methodologies: Angrist and Imbens' Contributions to Causal Identification",
        "abstract": "In the 1990s, Joshua Angrist and Guido Imbens studied the causal\ninterpretation of Instrumental Variable estimates (a widespread methodology in\neconomics) through the lens of potential outcomes (a classical framework to\nformalize causality in statistics). Bridging a gap between those two strands of\nliterature, they stress the importance of treatment effect heterogeneity and\nshow that, under defendable assumptions in various applications, this method\nrecovers an average causal effect for a specific subpopulation of individuals\nwhose treatment is affected by the instrument. They were awarded the Nobel\nPrize primarily for this Local Average Treatment Effect (LATE). The first part\nof this article presents that methodological contribution in-depth: the\norigination in earlier applied articles, the different identification results\nand extensions, and related debates on the relevance of LATEs for public policy\ndecisions. The second part reviews the main contributions of the authors beyond\nthe LATE. J. Angrist has pursued the search for informative and varied\nempirical research designs in several fields, particularly in education. G.\nImbens has complemented the toolbox for treatment effect estimation in many\nways, notably through propensity score reweighting, matching, and, more\nrecently, adapting machine learning procedures.",
        "authors": [
            "Lucas Girard",
            "Yannick Guyonvarch"
        ],
        "categories": "econ.EM",
        "published": "2024-02-20T14:06:39Z",
        "updated": "2024-02-20T14:06:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.12838v2",
        "title": "Extending the Scope of Inference About Predictive Ability to Machine Learning Methods",
        "abstract": "Though out-of-sample forecast evaluation is systematically employed with\nmodern machine learning methods and there exists a well-established classic\ninference theory for predictive ability, see, e.g., West (1996, Asymptotic\nInference About Predictive Ability, Econometrica, 64, 1067-1084), such theory\nis not directly applicable to modern machine learners such as the Lasso in the\nhigh dimensional setting. We investigate under which conditions such extensions\nare possible. Two key properties for standard out-of-sample asymptotic\ninference to be valid with machine learning are (i) a zero-mean condition for\nthe score of the prediction loss function; and (ii) a fast rate of convergence\nfor the machine learner. Monte Carlo simulations confirm our theoretical\nfindings. We recommend a small out-of-sample vs in-sample size ratio for\naccurate finite sample inferences with machine learning. We illustrate the wide\napplicability of our results with a new out-of-sample test for the Martingale\nDifference Hypothesis (MDH). We obtain the asymptotic null distribution of our\ntest and use it to evaluate the MDH of some major exchange rates at daily and\nhigher frequencies.",
        "authors": [
            "Juan Carlos Escanciano",
            "Ricardo Parra"
        ],
        "categories": "econ.EM",
        "published": "2024-02-20T09:05:43Z",
        "updated": "2024-04-16T04:53:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.12607v2",
        "title": "Inference on LATEs with covariates",
        "abstract": "In theory, two-stage least squares (TSLS) identifies a weighted average of\ncovariate-specific local average treatment effects (LATEs) from a saturated\nspecification, without making parametric assumptions on how available\ncovariates enter the model. In practice, TSLS is severely biased as saturation\nleads to a large number of control dummies and an equally large number of,\narguably weak, instruments. This paper derives asymptotically valid tests and\nconfidence intervals for the weighted average of LATEs that is targeted, yet\nmissed by saturated TSLS. The proposed inference procedure is robust to\nunobserved treatment effect heterogeneity, covariates with rich support, and\nweak identification. We find LATEs statistically significantly different from\nzero in applications in criminology, finance, health, and education.",
        "authors": [
            "Tom Boot",
            "Didier Nibbering"
        ],
        "categories": "econ.EM",
        "published": "2024-02-20T00:04:40Z",
        "updated": "2024-12-01T04:00:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.12583v1",
        "title": "Non-linear Triple Changes Estimator for Targeted Policies",
        "abstract": "The renowned difference-in-differences (DiD) estimator relies on the\nassumption of 'parallel trends,' which does not hold in many practical\napplications. To address this issue, the econometrics literature has turned to\nthe triple difference estimator. Both DiD and triple difference are limited to\nassessing average effects exclusively. An alternative avenue is offered by the\nchanges-in-changes (CiC) estimator, which provides an estimate of the entire\ncounterfactual distribution at the cost of relying on (stronger) distributional\nassumptions. In this work, we extend the triple difference estimator to\naccommodate the CiC framework, presenting the `triple changes estimator' and\nits identification assumptions, thereby expanding the scope of the CiC\nparadigm. Subsequently, we empirically evaluate the proposed framework and\napply it to a study examining the impact of Medicaid expansion on children's\npreventive care.",
        "authors": [
            "Sina Akbari",
            "Negar Kiyavash"
        ],
        "categories": "stat.ME",
        "published": "2024-02-19T22:34:00Z",
        "updated": "2024-02-19T22:34:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.11659v1",
        "title": "Credible causal inference beyond toy models",
        "abstract": "Causal inference with observational data critically relies on untestable and\nextra-statistical assumptions that have (sometimes) testable implications.\nWell-known sets of assumptions that are sufficient to justify the causal\ninterpretation of certain estimators are called identification strategies.\nThese templates for causal analysis, however, do not perfectly map into\nempirical research practice. Researchers are often left in the disjunctive of\neither abstracting away from their particular setting to fit in the templates,\nrisking erroneous inferences, or avoiding situations in which the templates\ncannot be applied, missing valuable opportunities for conducting empirical\nanalysis. In this article, I show how directed acyclic graphs (DAGs) can help\nresearchers to conduct empirical research and assess the quality of evidence\nwithout excessively relying on research templates. First, I offer a concise\nintroduction to causal inference frameworks. Then I survey the arguments in the\nmethodological literature in favor of using research templates, while either\navoiding or limiting the use of causal graphical models. Third, I discuss the\nproblems with the template model, arguing for a more flexible approach to DAGs\nthat helps illuminating common problems in empirical settings and improving the\ncredibility of causal claims. I demonstrate this approach in a series of worked\nexamples, showing the gap between identification strategies as invoked by\nresearchers and their actual applications. Finally, I conclude highlighting the\nbenefits that routinely incorporating causal graphical models in our scientific\ndiscussions would have in terms of transparency, testability, and generativity.",
        "authors": [
            "Pablo Geraldo Bast\u00edas"
        ],
        "categories": "stat.ME",
        "published": "2024-02-18T17:33:14Z",
        "updated": "2024-02-18T17:33:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.11652v3",
        "title": "Doubly Robust Inference in Causal Latent Factor Models",
        "abstract": "This article introduces a new estimator of average treatment effects under\nunobserved confounding in modern data-rich environments featuring large numbers\nof units and outcomes. The proposed estimator is doubly robust, combining\noutcome imputation, inverse probability weighting, and a novel cross-fitting\nprocedure for matrix completion. We derive finite-sample and asymptotic\nguarantees, and show that the error of the new estimator converges to a\nmean-zero Gaussian distribution at a parametric rate. Simulation results\ndemonstrate the relevance of the formal properties of the estimators analyzed\nin this article.",
        "authors": [
            "Alberto Abadie",
            "Anish Agarwal",
            "Raaz Dwivedi",
            "Abhin Shah"
        ],
        "categories": "econ.EM",
        "published": "2024-02-18T17:13:46Z",
        "updated": "2024-10-29T15:26:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.11394v2",
        "title": "Maximal Inequalities for Empirical Processes under General Mixing Conditions with an Application to Strong Approximations",
        "abstract": "This paper provides a bound for the supremum of sample averages over a class\nof functions for a general class of mixing stochastic processes with arbitrary\nmixing rates. Regardless of the speed of mixing, the bound is comprised of a\nconcentration rate and a novel measure of complexity. The speed of mixing,\nhowever, affects the former quantity implying a phase transition. Fast mixing\nleads to the standard root-n concentration rate, while slow mixing leads to a\nslower concentration rate, its speed depends on the mixing structure. Our\nfindings are applied to derive strong approximation results for a general class\nof mixing processes with arbitrary mixing rates.",
        "authors": [
            "Demian Pouzo"
        ],
        "categories": "math.PR",
        "published": "2024-02-17T22:24:30Z",
        "updated": "2024-04-18T20:14:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.11134v1",
        "title": "Functional Partial Least-Squares: Optimal Rates and Adaptation",
        "abstract": "We consider the functional linear regression model with a scalar response and\na Hilbert space-valued predictor, a well-known ill-posed inverse problem. We\npropose a new formulation of the functional partial least-squares (PLS)\nestimator related to the conjugate gradient method. We shall show that the\nestimator achieves the (nearly) optimal convergence rate on a class of\nellipsoids and we introduce an early stopping rule which adapts to the unknown\ndegree of ill-posedness. Some theoretical and simulation comparison between the\nestimator and the principal component regression estimator is provided.",
        "authors": [
            "Andrii Babii",
            "Marine Carrasco",
            "Idriss Tsafack"
        ],
        "categories": "math.ST",
        "published": "2024-02-16T23:47:47Z",
        "updated": "2024-02-16T23:47:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.10836v2",
        "title": "Manipulation Test for Multidimensional RDD",
        "abstract": "The causal inference model proposed by Lee (2008) for the regression\ndiscontinuity design (RDD) relies on assumptions that imply the continuity of\nthe density of the assignment (running) variable. The test for this implication\nis commonly referred to as the manipulation test and is regularly reported in\napplied research to strengthen the design's validity. The multidimensional RDD\n(MRDD) extends the RDD to contexts where treatment assignment depends on\nseveral running variables. This paper introduces a manipulation test for the\nMRDD. First, it develops a theoretical model for causal inference with the\nMRDD, used to derive a testable implication on the conditional marginal\ndensities of the running variables. Then, it constructs the test for the\nimplication based on a quadratic form of a vector of statistics separately\ncomputed for each marginal density. Finally, the proposed test is compared with\nalternative procedures commonly employed in applied research.",
        "authors": [
            "Federico Crippa"
        ],
        "categories": "econ.EM",
        "published": "2024-02-16T17:15:52Z",
        "updated": "2024-06-26T21:37:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.10592v2",
        "title": "Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification",
        "abstract": "Practitioners conducting adaptive experiments often encounter two competing\npriorities: maximizing total welfare (or `reward') through effective treatment\nassignment and swiftly concluding experiments to implement population-wide\ntreatments. Current literature addresses these priorities separately, with\nregret minimization studies focusing on the former and best-arm identification\nresearch on the latter. This paper bridges this divide by proposing a unified\nmodel that simultaneously accounts for within-experiment performance and\npost-experiment outcomes. We provide a sharp theory of optimal performance in\nlarge populations that not only unifies canonical results in the literature but\nalso uncovers novel insights. Our theory reveals that familiar algorithms, such\nas the recently proposed top-two Thompson sampling algorithm, can optimize a\nbroad class of objectives if a single scalar parameter is appropriately\nadjusted. In addition, we demonstrate that substantial reductions in experiment\nduration can often be achieved with minimal impact on both within-experiment\nand post-experiment regret.",
        "authors": [
            "Chao Qin",
            "Daniel Russo"
        ],
        "categories": "cs.LG",
        "published": "2024-02-16T11:27:48Z",
        "updated": "2024-07-30T08:48:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.10574v2",
        "title": "Nowcasting with Mixed Frequency Data Using Gaussian Processes",
        "abstract": "We develop Bayesian machine learning methods for mixed data sampling (MIDAS)\nregressions. This involves handling frequency mismatches and specifying\nfunctional relationships between many predictors and the dependent variable. We\nuse Gaussian processes (GPs) and compress the input space with structured and\nunstructured MIDAS variants. This yields several versions of GP-MIDAS with\ndistinct properties and implications, which we evaluate in short-horizon now-\nand forecasting exercises with both simulated data and data on quarterly US\noutput growth and inflation in the GDP deflator. It turns out that our proposed\nframework leverages macroeconomic Big Data in a computationally efficient way\nand offers gains in predictive accuracy compared to other machine learning\napproaches along several dimensions.",
        "authors": [
            "Niko Hauzenberger",
            "Massimiliano Marcellino",
            "Michael Pfarrhofer",
            "Anna Stelzer"
        ],
        "categories": "econ.EM",
        "published": "2024-02-16T11:03:07Z",
        "updated": "2024-09-09T18:15:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.10982v1",
        "title": "mshw, a forecasting library to predict short-term electricity demand based on multiple seasonal Holt-Winters",
        "abstract": "Transmission system operators have a growing need for more accurate\nforecasting of electricity demand. Current electricity systems largely require\ndemand forecasting so that the electricity market establishes electricity\nprices as well as the programming of production units. The companies that are\npart of the electrical system use exclusive software to obtain predictions,\nbased on the use of time series and prediction tools, whether statistical or\nartificial intelligence. However, the most common form of prediction is based\non hybrid models that use both technologies. In any case, it is software with a\ncomplicated structure, with a large number of associated variables and that\nrequires a high computational load to make predictions. The predictions they\ncan offer are not much better than those that simple models can offer. In this\npaper we present a MATLAB toolbox created for the prediction of electrical\ndemand. The toolbox implements multiple seasonal Holt-Winters exponential\nsmoothing models and neural network models. The models used include the use of\ndiscrete interval mobile seasonalities (DIMS) to improve forecasting on special\ndays. Additionally, the results of its application in various electrical\nsystems in Europe are shown, where the results obtained can be seen. The use of\nthis library opens a new avenue of research for the use of models with discrete\nand complex seasonalities in other fields of application.",
        "authors": [
            "Oscar Trull",
            "J. Carlos Garc\u00eda-D\u00edaz",
            "Angel Peir\u00f3-Signes"
        ],
        "categories": "cs.LG",
        "published": "2024-02-15T23:08:18Z",
        "updated": "2024-02-15T23:08:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.09928v2",
        "title": "When Can We Use Two-Way Fixed-Effects (TWFE): A Comparison of TWFE and Novel Dynamic Difference-in-Differences Estimators",
        "abstract": "The conventional Two-Way Fixed-Effects (TWFE) estimator has come under\nscrutiny lately. Recent literature has revealed potential shortcomings of TWFE\nwhen the treatment effects are heterogeneous. Scholars have developed new\nadvanced dynamic Difference-in-Differences (DiD) estimators to tackle these\npotential shortcomings. However, confusion remains in applied research as to\nwhen the conventional TWFE is biased and what issues the novel estimators can\nand cannot address. In this study, we first provide an intuitive explanation of\nthe problems of TWFE and elucidate the key features of the novel alternative\nDiD estimators. We then systematically demonstrate the conditions under which\nthe conventional TWFE is inconsistent. We employ Monte Carlo simulations to\nassess the performance of dynamic DiD estimators under violations of key\nassumptions, which likely happens in applied cases. While the new dynamic DiD\nestimators offer notable advantages in capturing heterogeneous treatment\neffects, we show that the conventional TWFE performs generally well if the\nmodel specifies an event-time function. All estimators are equally sensitive to\nviolations of the parallel trends assumption, anticipation effects or\nviolations of time-varying exogeneity. Despite their advantages, the new\ndynamic DiD estimators tackle a very specific problem and they do not serve as\na universal remedy for violations of the most critical assumptions. We finally\nderive, based on our simulations, recommendations for how and when to use TWFE\nand the new DiD estimators in applied research.",
        "authors": [
            "Tobias R\u00fcttenauer",
            "Ozan Aksoy"
        ],
        "categories": "econ.EM",
        "published": "2024-02-15T13:17:43Z",
        "updated": "2024-04-10T07:24:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.09895v1",
        "title": "Spatial Data Analysis",
        "abstract": "This handbook chapter provides an essential introduction to the field of\nspatial econometrics, offering a comprehensive overview of techniques and\nmethodologies for analysing spatial data in the social sciences. Spatial\neconometrics addresses the unique challenges posed by spatially dependent\nobservations, where spatial relationships among data points can significantly\nimpact statistical analyses. The chapter begins by exploring the fundamental\nconcepts of spatial dependence and spatial autocorrelation, and highlighting\ntheir implications for traditional econometric models. It then introduces a\nrange of spatial econometric models, particularly spatial lag, spatial error,\nand spatial lag of X models, illustrating how these models accommodate spatial\nrelationships and yield accurate and insightful results about the underlying\nspatial processes. The chapter provides an intuitive understanding of these\nmodels compare to each other. A practical example on London house prices\ndemonstrates the application of spatial econometrics, emphasising its relevance\nin uncovering hidden spatial patterns, addressing endogeneity, and providing\nrobust estimates in the presence of spatial dependence.",
        "authors": [
            "Tobias R\u00fcttenauer"
        ],
        "categories": "econ.EM",
        "published": "2024-02-15T11:43:31Z",
        "updated": "2024-02-15T11:43:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.09789v1",
        "title": "Identification with Posterior-Separable Information Costs",
        "abstract": "I provide a model of rational inattention with heterogeneity and prove it is\nobservationally equivalent to a state-dependent stochastic choice model subject\nto attention costs. I demonstrate that additive separability of unobservable\nheterogeneity, together with an independence assumption, suffice for the\nempirical model to admit a representative agent. Using conditional\nprobabilities, I show how to identify: how covariates affect the desirability\nof goods, (a measure of) welfare, factual changes in welfare, and bounds on\ncounterfactual market shares.",
        "authors": [
            "Martin Bustos"
        ],
        "categories": "econ.EM",
        "published": "2024-02-15T08:40:09Z",
        "updated": "2024-02-15T08:40:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.09744v2",
        "title": "Quantile Granger Causality in the Presence of Instability",
        "abstract": "We propose a new framework for assessing Granger causality in quantiles in\nunstable environments, for a fixed quantile or over a continuum of quantile\nlevels. Our proposed test statistics are consistent against fixed alternatives,\nthey have nontrivial power against local alternatives, and they are pivotal in\ncertain important special cases. In addition, we show the validity of a\nbootstrap procedure when asymptotic distributions depend on nuisance\nparameters. Monte Carlo simulations reveal that the proposed test statistics\nhave correct empirical size and high power, even in absence of structural\nbreaks. Moreover, a procedure providing additional insight into the timing of\nGranger causal regimes based on our new tests is proposed. Finally, an\nempirical application in energy economics highlights the applicability of our\nmethod as the new tests provide stronger evidence of Granger causality.",
        "authors": [
            "Alexander Mayer",
            "Dominik Wied",
            "Victor Troster"
        ],
        "categories": "econ.EM",
        "published": "2024-02-15T06:48:14Z",
        "updated": "2024-12-06T07:59:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.09033v2",
        "title": "Cross-Temporal Forecast Reconciliation at Digital Platforms with Machine Learning",
        "abstract": "Platform businesses operate on a digital core and their decision making\nrequires high-dimensional accurate forecast streams at different levels of\ncross-sectional (e.g., geographical regions) and temporal aggregation (e.g.,\nminutes to days). It also necessitates coherent forecasts across all levels of\nthe hierarchy to ensure aligned decision making across different planning units\nsuch as pricing, product, controlling and strategy. Given that platform data\nstreams feature complex characteristics and interdependencies, we introduce a\nnon-linear hierarchical forecast reconciliation method that produces\ncross-temporal reconciled forecasts in a direct and automated way through the\nuse of popular machine learning methods. The method is sufficiently fast to\nallow forecast-based high-frequency decision making that platforms require. We\nempirically test our framework on unique, large-scale streaming datasets from a\nleading on-demand delivery platform in Europe and a bicycle sharing system in\nNew York City.",
        "authors": [
            "Jeroen Rombouts",
            "Marie Ternes",
            "Ines Wilms"
        ],
        "categories": "econ.EM",
        "published": "2024-02-14T09:16:46Z",
        "updated": "2024-05-31T14:44:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.08941v1",
        "title": "Local-Polynomial Estimation for Multivariate Regression Discontinuity Designs",
        "abstract": "We introduce a multivariate local-linear estimator for multivariate\nregression discontinuity designs in which treatment is assigned by crossing a\nboundary in the space of running variables. The dominant approach uses the\nEuclidean distance from a boundary point as the scalar running variable; hence,\nmultivariate designs are handled as uni-variate designs. However, the distance\nrunning variable is incompatible with the assumption for asymptotic validity.\nWe handle multivariate designs as multivariate. In this study, we develop a\nnovel asymptotic normality for multivariate local-polynomial estimators. Our\nestimator is asymptotically valid and can capture heterogeneous treatment\neffects over the boundary. We demonstrate the effectiveness of our estimator\nthrough numerical simulations. Our empirical illustration of a Colombian\nscholarship study reveals a richer heterogeneity (including its absence) of the\ntreatment effect that is hidden in the original estimates.",
        "authors": [
            "Masayuki Sawada",
            "Takuya Ishihara",
            "Daisuke Kurisu",
            "Yasumasa Matsuda"
        ],
        "categories": "econ.EM",
        "published": "2024-02-14T05:05:23Z",
        "updated": "2024-02-14T05:05:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.08879v1",
        "title": "Inference for an Algorithmic Fairness-Accuracy Frontier",
        "abstract": "Decision-making processes increasingly rely on the use of algorithms. Yet,\nalgorithms' predictive ability frequently exhibit systematic variation across\nsubgroups of the population. While both fairness and accuracy are desirable\nproperties of an algorithm, they often come at the cost of one another. What\nshould a fairness-minded policymaker do then, when confronted with finite data?\nIn this paper, we provide a consistent estimator for a theoretical\nfairness-accuracy frontier put forward by Liang, Lu and Mu (2023) and propose\ninference methods to test hypotheses that have received much attention in the\nfairness literature, such as (i) whether fully excluding a covariate from use\nin training the algorithm is optimal and (ii) whether there are less\ndiscriminatory alternatives to an existing algorithm. We also provide an\nestimator for the distance between a given algorithm and the fairest point on\nthe frontier, and characterize its asymptotic distribution. We leverage the\nfact that the fairness-accuracy frontier is part of the boundary of a convex\nset that can be fully represented by its support function. We show that the\nestimated support function converges to a tight Gaussian process as the sample\nsize increases, and then express policy-relevant hypotheses as restrictions on\nthe support function to construct valid test statistics.",
        "authors": [
            "Yiqi Liu",
            "Francesca Molinari"
        ],
        "categories": "econ.EM",
        "published": "2024-02-14T00:56:09Z",
        "updated": "2024-02-14T00:56:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.08575v1",
        "title": "Heterogeneity, Uncertainty and Learning: Semiparametric Identification and Estimation",
        "abstract": "We provide semiparametric identification results for a broad class of\nlearning models in which continuous outcomes depend on three types of\nunobservables: i) known heterogeneity, ii) initially unknown heterogeneity that\nmay be revealed over time, and iii) transitory uncertainty. We consider a\ncommon environment where the researcher only has access to a short panel on\nchoices and realized outcomes. We establish identification of the outcome\nequation parameters and the distribution of the three types of unobservables,\nunder the standard assumption that unknown heterogeneity and uncertainty are\nnormally distributed. We also show that, absent known heterogeneity, the model\nis identified without making any distributional assumption. We then derive the\nasymptotic properties of a sieve MLE estimator for the model parameters, and\ndevise a tractable profile likelihood based estimation procedure. Monte Carlo\nsimulation results indicate that our estimator exhibits good finite-sample\nproperties.",
        "authors": [
            "Jackson Bunting",
            "Paul Diegert",
            "Arnaud Maurel"
        ],
        "categories": "econ.EM",
        "published": "2024-02-13T16:21:47Z",
        "updated": "2024-02-13T16:21:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.08108v1",
        "title": "Finding Moving-Band Statistical Arbitrages via Convex-Concave Optimization",
        "abstract": "We propose a new method for finding statistical arbitrages that can contain\nmore assets than just the traditional pair. We formulate the problem as seeking\na portfolio with the highest volatility, subject to its price remaining in a\nband and a leverage limit. This optimization problem is not convex, but can be\napproximately solved using the convex-concave procedure, a specific sequential\nconvex programming method. We show how the method generalizes to finding\nmoving-band statistical arbitrages, where the price band midpoint varies over\ntime.",
        "authors": [
            "Kasper Johansson",
            "Thomas Schmelzer",
            "Stephen Boyd"
        ],
        "categories": "econ.EM",
        "published": "2024-02-12T22:56:16Z",
        "updated": "2024-02-12T22:56:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.08051v1",
        "title": "On Bayesian Filtering for Markov Regime Switching Models",
        "abstract": "This paper presents a framework for empirical analysis of dynamic\nmacroeconomic models using Bayesian filtering, with a specific focus on the\nstate-space formulation of Dynamic Stochastic General Equilibrium (DSGE) models\nwith multiple regimes. We outline the theoretical foundations of model\nestimation, provide the details of two families of powerful multiple-regime\nfilters, IMM and GPB, and construct corresponding multiple-regime smoothers. A\nsimulation exercise, based on a prototypical New Keynesian DSGE model, is used\nto demonstrate the computational robustness of the proposed filters and\nsmoothers and evaluate their accuracy and speed for a selection of filters from\neach family. We show that the canonical IMM filter is faster and is no less,\nand often more, accurate than its competitors within IMM and GPB families, the\nlatter including the commonly used Kim and Nelson (1999) filter. Using it with\nthe matching smoother improves the precision in recovering unobserved variables\nby about 25 percent. Furthermore, applying it to the U.S. 1947-2023\nmacroeconomic time series, we successfully identify significant past policy\nshifts including those related to the post-Covid-19 period. Our results\ndemonstrate the practical applicability and potential of the proposed routines\nin macroeconomic analysis.",
        "authors": [
            "Nigar Hashimzade",
            "Oleg Kirsanov",
            "Tatiana Kirsanova",
            "Junior Maih"
        ],
        "categories": "econ.EM",
        "published": "2024-02-12T20:33:49Z",
        "updated": "2024-02-12T20:33:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.07743v2",
        "title": "Local Projections Inference with High-Dimensional Covariates without Sparsity",
        "abstract": "This paper presents a comprehensive local projections (LP) framework for\nestimating future responses to current shocks, robust to high-dimensional\ncontrols without relying on sparsity assumptions. The approach is applicable to\nvarious settings, including impulse response analysis and\ndifference-in-differences (DiD) estimation. While methods like LASSO exist,\nthey often assume most parameters are exactly zero, limiting their\neffectiveness in dense data generation processes. I propose a novel technique\nincorporating high-dimensional covariates in local projections using the\nOrthogonal Greedy Algorithm with a high-dimensional AIC (OGA+HDAIC) model\nselection method. This approach offers robustness in both sparse and dense\nscenarios, improved interpretability, and more reliable causal inference in\nlocal projections. Simulation studies show superior performance in dense and\npersistent scenarios compared to conventional LP and LASSO-based approaches. In\nan empirical application to Acemoglu, Naidu, Restrepo, and Robinson (2019), I\ndemonstrate efficiency gains and robustness to a large set of controls.\nAdditionally, I examine the effect of subjective beliefs on economic\naggregates, demonstrating robustness to various model specifications. A novel\nstate-dependent analysis reveals that inflation behaves more in line with\nrational expectations in good states, but exhibits more subjective, pessimistic\ndynamics in bad states.",
        "authors": [
            "Jooyoung Cha"
        ],
        "categories": "econ.EM",
        "published": "2024-02-12T16:05:22Z",
        "updated": "2024-10-03T15:41:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.07521v1",
        "title": "A step towards the integration of machine learning and small area estimation",
        "abstract": "The use of machine-learning techniques has grown in numerous research areas.\nCurrently, it is also widely used in statistics, including the official\nstatistics for data collection (e.g. satellite imagery, web scraping and text\nmining, data cleaning, integration and imputation) but also for data analysis.\nHowever, the usage of these methods in survey sampling including small area\nestimation is still very limited. Therefore, we propose a predictor supported\nby these algorithms which can be used to predict any population or\nsubpopulation characteristics based on cross-sectional and longitudinal data.\nMachine learning methods have already been shown to be very powerful in\nidentifying and modelling complex and nonlinear relationships between the\nvariables, which means that they have very good properties in case of strong\ndepartures from the classic assumptions. Therefore, we analyse the performance\nof our proposal under a different set-up, in our opinion of greater importance\nin real-life surveys. We study only small departures from the assumed model, to\nshow that our proposal is a good alternative in this case as well, even in\ncomparison with optimal methods under the model. What is more, we propose the\nmethod of the accuracy estimation of machine learning predictors, giving the\npossibility of the accuracy comparison with classic methods, where the accuracy\nis measured as in survey sampling practice. The solution of this problem is\nindicated in the literature as one of the key issues in integration of these\napproaches. The simulation studies are based on a real, longitudinal dataset,\nfreely available from the Polish Local Data Bank, where the prediction problem\nof subpopulation characteristics in the last period, with \"borrowing strength\"\nfrom other subpopulations and time periods, is considered.",
        "authors": [
            "Tomasz \u017b\u0105d\u0142o",
            "Adam Chwila"
        ],
        "categories": "stat.ME",
        "published": "2024-02-12T09:43:17Z",
        "updated": "2024-02-12T09:43:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.07322v1",
        "title": "Interference Among First-Price Pacing Equilibria: A Bias and Variance Analysis",
        "abstract": "Online A/B testing is widely used in the internet industry to inform\ndecisions on new feature roll-outs. For online marketplaces (such as\nadvertising markets), standard approaches to A/B testing may lead to biased\nresults when buyers operate under a budget constraint, as budget consumption in\none arm of the experiment impacts performance of the other arm. To counteract\nthis interference, one can use a budget-split design where the budget\nconstraint operates on a per-arm basis and each arm receives an equal fraction\nof the budget, leading to ``budget-controlled A/B testing.'' Despite clear\nadvantages of budget-controlled A/B testing, performance degrades when budget\nare split too small, limiting the overall throughput of such systems. In this\npaper, we propose a parallel budget-controlled A/B testing design where we use\nmarket segmentation to identify submarkets in the larger market, and we run\nparallel experiments on each submarket.\n  Our contributions are as follows: First, we introduce and demonstrate the\neffectiveness of the parallel budget-controlled A/B test design with submarkets\nin a large online marketplace environment. Second, we formally define market\ninterference in first-price auction markets using the first price pacing\nequilibrium (FPPE) framework. Third, we propose a debiased surrogate that\neliminates the first-order bias of FPPE, drawing upon the principles of\nsensitivity analysis in mathematical programs. Fourth, we derive a plug-in\nestimator for the surrogate and establish its asymptotic normality. Fifth, we\nprovide an estimation procedure for submarket parallel budget-controlled A/B\ntests. Finally, we present numerical examples on semi-synthetic data,\nconfirming that the debiasing technique achieves the desired coverage\nproperties.",
        "authors": [
            "Luofeng Liao",
            "Christian Kroer",
            "Sergei Leonenkov",
            "Okke Schrijvers",
            "Liang Shi",
            "Nicolas Stier-Moses",
            "Congshan Zhang"
        ],
        "categories": "math.ST",
        "published": "2024-02-11T22:58:52Z",
        "updated": "2024-02-11T22:58:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.07170v1",
        "title": "Research on the multi-stage impact of digital economy on rural revitalization in Hainan Province based on GPM model",
        "abstract": "The rapid development of the digital economy has had a profound impact on the\nimplementation of the rural revitalization strategy. Based on this, this study\ntakes Hainan Province as the research object to deeply explore the impact of\ndigital economic development on rural revitalization. The study collected panel\ndata from 2003 to 2022 to construct an evaluation index system for the digital\neconomy and rural revitalization and used panel regression analysis and other\nmethods to explore the promotion effect of the digital economy on rural\nrevitalization. Research results show that the digital economy has a\nsignificant positive impact on rural revitalization, and this impact increases\nas the level of fiscal expenditure increases. The issuance of digital RMB has\nfurther exerted a regulatory effect and promoted the development of the digital\neconomy and the process of rural revitalization. At the same time, the\nestablishment of the Hainan Free Trade Port has also played a positive role in\npromoting the development of the digital economy and rural revitalization. In\nthe prediction of the optimal strategy for rural revitalization based on the\ndevelopment levels of the primary, secondary, and tertiary industries (Rate1,\nRate2, and Rate3), it was found that rate1 can encourage Hainan Province to\nimplement digital economic innovation, encourage rate3 to implement promotion\nbehaviors, and increase rate2 can At the level of sustainable development when\nrate3 promotes rate2's digital economic innovation behavior, it can standardize\nrate2's production behavior to the greatest extent, accelerate the faster\napplication of the digital economy to the rural revitalization industry, and\npromote the technological advancement of enterprises.",
        "authors": [
            "Wenbo Lyu"
        ],
        "categories": "econ.GN",
        "published": "2024-02-11T11:41:35Z",
        "updated": "2024-02-11T11:41:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.05789v1",
        "title": "High Dimensional Factor Analysis with Weak Factors",
        "abstract": "This paper studies the principal components (PC) estimator for high\ndimensional approximate factor models with weak factors in that the factor\nloading ($\\boldsymbol{\\Lambda}^0$) scales sublinearly in the number $N$ of\ncross-section units, i.e., $\\boldsymbol{\\Lambda}^{0\\top} \\boldsymbol{\\Lambda}^0\n/ N^\\alpha$ is positive definite in the limit for some $\\alpha \\in (0,1)$.\nWhile the consistency and asymptotic normality of these estimates are by now\nwell known when the factors are strong, i.e., $\\alpha=1$, the statistical\nproperties for weak factors remain less explored. Here, we show that the PC\nestimator maintains consistency and asymptotical normality for any\n$\\alpha\\in(0,1)$, provided suitable conditions regarding the dependence\nstructure in the noise are met. This complements earlier result by Onatski\n(2012) that the PC estimator is inconsistent when $\\alpha=0$, and the more\nrecent work by Bai and Ng (2023) who established the asymptotic normality of\nthe PC estimator when $\\alpha \\in (1/2,1)$. Our proof strategy integrates the\ntraditional eigendecomposition-based approach for factor models with\nleave-one-out analysis similar in spirit to those used in matrix completion and\nother settings. This combination allows us to deal with factors weaker than the\nformer and at the same time relax the incoherence and independence assumptions\noften associated with the later.",
        "authors": [
            "Jungjun Choi",
            "Ming Yuan"
        ],
        "categories": "econ.EM",
        "published": "2024-02-08T16:29:04Z",
        "updated": "2024-02-08T16:29:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.05432v1",
        "title": "Difference-in-Differences Estimators with Continuous Treatments and no Stayers",
        "abstract": "Many treatments or policy interventions are continuous in nature. Examples\ninclude prices, taxes or temperatures. Empirical researchers have usually\nrelied on two-way fixed effect regressions to estimate treatment effects in\nsuch cases. However, such estimators are not robust to heterogeneous treatment\neffects in general; they also rely on the linearity of treatment effects. We\npropose estimators for continuous treatments that do not impose those\nrestrictions, and that can be used when there are no stayers: the treatment of\nall units changes from one period to the next. We start by extending the\nnonparametric results of de Chaisemartin et al. (2023) to cases without\nstayers. We also present a parametric estimator, and use it to revisit\nDesch\\^enes and Greenstone (2012).",
        "authors": [
            "Cl\u00e9ment de Chaisemartin",
            "Xavier D'Haultf\u0153uille",
            "Gonzalo Vazquez-Bare"
        ],
        "categories": "econ.EM",
        "published": "2024-02-08T06:03:57Z",
        "updated": "2024-02-08T06:03:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.05329v1",
        "title": "Selective linear segmentation for detecting relevant parameter changes",
        "abstract": "Change-point processes are one flexible approach to model long time series.\nWe propose a method to uncover which model parameter truly vary when a\nchange-point is detected. Given a set of breakpoints, we use a penalized\nlikelihood approach to select the best set of parameters that changes over time\nand we prove that the penalty function leads to a consistent selection of the\ntrue model. Estimation is carried out via the deterministic annealing\nexpectation-maximization algorithm. Our method accounts for model selection\nuncertainty and associates a probability to all the possible time-varying\nparameter specifications. Monte Carlo simulations highlight that the method\nworks well for many time series models including heteroskedastic processes. For\na sample of 14 Hedge funds (HF) strategies, using an asset based style pricing\nmodel, we shed light on the promising ability of our method to detect the\ntime-varying dynamics of risk exposures as well as to forecast HF returns.",
        "authors": [
            "Arnaud Dufays",
            "Aristide Houndetoungan",
            "Alain Co\u00ebn"
        ],
        "categories": "econ.EM",
        "published": "2024-02-08T00:10:05Z",
        "updated": "2024-02-08T00:10:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.05030v2",
        "title": "Inference for Two-Stage Extremum Estimators",
        "abstract": "We present a simulation-based inference approach for two-stage estimators,\nfocusing on extremum estimators in the second stage. We accommodate a broad\nrange of first-stage estimators, including extremum estimators,\nhigh-dimensional estimators, and other types of estimators such as Bayesian\nestimators. The key contribution of our approach lies in its ability to\nestimate the asymptotic distribution of two-stage estimators, even when the\ndistributions of both the first- and second-stage estimators are non-normal and\nwhen the second-stage estimator's bias, scaled by the square root of the sample\nsize, does not vanish asymptotically. This enables reliable inference in\nsituations where standard methods fail. Additionally, we propose a debiased\nestimator, based on the mean of the estimated distribution function, which\nexhibits improved finite sample properties. Unlike resampling methods, our\napproach avoids the need for multiple calculations of the two-stage estimator.\nWe illustrate the effectiveness of our method in an empirical application on\npeer effects in adolescent fast-food consumption, where we address the issue of\nbiased instrumental variable estimates resulting from many weak instruments.",
        "authors": [
            "Aristide Houndetoungan",
            "Abdoul Haki Maoude"
        ],
        "categories": "econ.EM",
        "published": "2024-02-07T16:59:00Z",
        "updated": "2024-11-07T16:18:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.04828v2",
        "title": "What drives the European carbon market? Macroeconomic factors and forecasts",
        "abstract": "Putting a price on carbon -- with taxes or developing carbon markets -- is a\nwidely used policy measure to achieve the target of net-zero emissions by 2050.\nThis paper tackles the issue of producing point, direction-of-change, and\ndensity forecasts for the monthly real price of carbon within the EU Emissions\nTrading Scheme (EU ETS). We aim to uncover supply- and demand-side forces that\ncan contribute to improving the prediction accuracy of models at short- and\nmedium-term horizons. We show that a simple Bayesian Vector Autoregressive\n(BVAR) model, augmented with either one or two factors capturing a set of\npredictors affecting the price of carbon, provides substantial accuracy gains\nover a wide set of benchmark forecasts, including survey expectations and\nforecasts made available by data providers. We extend the study to verified\nemissions and demonstrate that, in this case, adding stochastic volatility can\nfurther improve the forecasting performance of a single-factor BVAR model. We\nrely on emissions and price forecasts to build market monitoring tools that\ntrack demand and price pressure in the EU ETS market. Our results are relevant\nfor policymakers and market practitioners interested in monitoring the carbon\nmarket dynamics.",
        "authors": [
            "Andrea Bastianin",
            "Elisabetta Mirto",
            "Yan Qin",
            "Luca Rossini"
        ],
        "categories": "econ.EM",
        "published": "2024-02-07T13:24:49Z",
        "updated": "2024-02-20T16:01:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.04674v1",
        "title": "Hyperparameter Tuning for Causal Inference with Double Machine Learning: A Simulation Study",
        "abstract": "Proper hyperparameter tuning is essential for achieving optimal performance\nof modern machine learning (ML) methods in predictive tasks. While there is an\nextensive literature on tuning ML learners for prediction, there is only little\nguidance available on tuning ML learners for causal machine learning and how to\nselect among different ML learners. In this paper, we empirically assess the\nrelationship between the predictive performance of ML methods and the resulting\ncausal estimation based on the Double Machine Learning (DML) approach by\nChernozhukov et al. (2018). DML relies on estimating so-called nuisance\nparameters by treating them as supervised learning problems and using them as\nplug-in estimates to solve for the (causal) parameter. We conduct an extensive\nsimulation study using data from the 2019 Atlantic Causal Inference Conference\nData Challenge. We provide empirical insights on the role of hyperparameter\ntuning and other practical decisions for causal estimation with DML. First, we\nassess the importance of data splitting schemes for tuning ML learners within\nDouble Machine Learning. Second, we investigate how the choice of ML methods\nand hyperparameters, including recent AutoML frameworks, impacts the estimation\nperformance for a causal parameter of interest. Third, we assess to what extent\nthe choice of a particular causal model, as characterized by incorporated\nparametric assumptions, can be based on predictive performance metrics.",
        "authors": [
            "Philipp Bach",
            "Oliver Schacht",
            "Victor Chernozhukov",
            "Sven Klaassen",
            "Martin Spindler"
        ],
        "categories": "econ.EM",
        "published": "2024-02-07T09:01:51Z",
        "updated": "2024-02-07T09:01:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.04433v1",
        "title": "Fast Online Changepoint Detection",
        "abstract": "We study online changepoint detection in the context of a linear regression\nmodel. We propose a class of heavily weighted statistics based on the CUSUM\nprocess of the regression residuals, which are specifically designed to ensure\ntimely detection of breaks occurring early on during the monitoring horizon. We\nsubsequently propose a class of composite statistics, constructed using\ndifferent weighing schemes; the decision rule to mark a changepoint is based on\nthe largest statistic across the various weights, thus effectively working like\na veto-based voting mechanism, which ensures fast detection irrespective of the\nlocation of the changepoint. Our theory is derived under a very general form of\nweak dependence, thus being able to apply our tests to virtually all time\nseries encountered in economics, medicine, and other applied sciences. Monte\nCarlo simulations show that our methodologies are able to control the\nprocedure-wise Type I Error, and have short detection delays in the presence of\nbreaks.",
        "authors": [
            "Fabrizio Ghezzi",
            "Eduardo Rossi",
            "Lorenzo Trapani"
        ],
        "categories": "stat.ME",
        "published": "2024-02-06T22:12:14Z",
        "updated": "2024-02-06T22:12:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.04165v1",
        "title": "Monthly GDP nowcasting with Machine Learning and Unstructured Data",
        "abstract": "In the dynamic landscape of continuous change, Machine Learning (ML)\n\"nowcasting\" models offer a distinct advantage for informed decision-making in\nboth public and private sectors. This study introduces ML-based GDP growth\nprojection models for monthly rates in Peru, integrating structured\nmacroeconomic indicators with high-frequency unstructured sentiment variables.\nAnalyzing data from January 2007 to May 2023, encompassing 91 leading economic\nindicators, the study evaluates six ML algorithms to identify optimal\npredictors. Findings highlight the superior predictive capability of ML models\nusing unstructured data, particularly Gradient Boosting Machine, LASSO, and\nElastic Net, exhibiting a 20% to 25% reduction in prediction errors compared to\ntraditional AR and Dynamic Factor Models (DFM). This enhanced performance is\nattributed to better handling of data of ML models in high-uncertainty periods,\nsuch as economic crises.",
        "authors": [
            "Juan Tenorio",
            "Wilder Perez"
        ],
        "categories": "econ.EM",
        "published": "2024-02-06T17:21:39Z",
        "updated": "2024-02-06T17:21:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.02535v2",
        "title": "Data-driven Policy Learning for Continuous Treatments",
        "abstract": "This paper studies policy learning for continuous treatments from\nobservational data. Continuous treatments present more significant challenges\nthan discrete ones because population welfare may need nonparametric\nestimation, and policy space may be infinite-dimensional and may satisfy shape\nrestrictions. We propose to approximate the policy space with a sequence of\nfinite-dimensional spaces and, for any given policy, obtain the empirical\nwelfare by applying the kernel method. We consider two cases: known and unknown\npropensity scores. In the latter case, we allow for machine learning of the\npropensity score and modify the empirical welfare to account for the effect of\nmachine learning. The learned policy maximizes the empirical welfare or the\nmodified empirical welfare over the approximating space. In both cases, we\nmodify the penalty algorithm proposed in \\cite{mbakop2021model} to\ndata-automate the tuning parameters (i.e., bandwidth and dimension of the\napproximating space) and establish an oracle inequality for the welfare regret.",
        "authors": [
            "Chunrong Ai",
            "Yue Fang",
            "Haitian Xie"
        ],
        "categories": "econ.EM",
        "published": "2024-02-04T15:30:05Z",
        "updated": "2024-11-26T06:10:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.02482v1",
        "title": "Global bank network connectedness revisited: What is common, idiosyncratic and when?",
        "abstract": "We revisit the problem of estimating high-dimensional global bank network\nconnectedness. Instead of directly regularizing the high-dimensional vector of\nrealized volatilities as in Demirer et al. (2018), we estimate a dynamic factor\nmodel with sparse VAR idiosyncratic components. This allows to disentangle: (I)\nthe part of system-wide connectedness (SWC) due to the common component shocks\n(what we call the \"banking market\"), and (II) the part due to the idiosyncratic\nshocks (the single banks). We employ both the original dataset as in Demirer et\nal. (2018) (daily data, 2003-2013), as well as a more recent vintage\n(2014-2023). For both, we compute SWC due to (I), (II), (I+II) and provide\nbootstrap confidence bands. In accordance with the literature, we find SWC to\nspike during global crises. However, our method minimizes the risk of SWC\nunderestimation in high-dimensional datasets where episodes of systemic risk\ncan be both pervasive and idiosyncratic. In fact, we are able to disentangle\nhow in normal times $\\approx$60-80% of SWC is due to idiosyncratic variation\nand only $\\approx$20-40% to market variation. However, in crises periods such\nas the 2008 financial crisis and the Covid19 outbreak in 2019, the situation is\ncompletely reversed: SWC is comparatively more driven by a market dynamic and\nless by an idiosyncratic one.",
        "authors": [
            "Jonas Krampe",
            "Luca Margaritella"
        ],
        "categories": "econ.EM",
        "published": "2024-02-04T13:20:01Z",
        "updated": "2024-02-04T13:20:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.02303v2",
        "title": "Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium",
        "abstract": "The linear Fisher market (LFM) is a basic equilibrium model from economics,\nwhich also has applications in fair and efficient resource allocation.\nFirst-price pacing equilibrium (FPPE) is a model capturing budget-management\nmechanisms in first-price auctions. In certain practical settings such as\nadvertising auctions, there is an interest in performing statistical inference\nover these models. A popular methodology for general statistical inference is\nthe bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for\nthe valid application of bootstrap procedures. In this paper, we introduce and\ndevise several statistically valid bootstrap inference procedures for LFM and\nFPPE. The most challenging part is to bootstrap general FPPE, which reduces to\nbootstrapping constrained M-estimators, a largely unexplored problem. We devise\na bootstrap procedure for FPPE under mild degeneracy conditions by using the\npowerful tool of epi-convergence theory. Experiments with synthetic and\nsemi-real data verify our theory.",
        "authors": [
            "Luofeng Liao",
            "Christian Kroer"
        ],
        "categories": "math.ST",
        "published": "2024-02-04T00:00:00Z",
        "updated": "2024-02-11T22:49:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.02272v1",
        "title": "One-inflated zero-truncated count regression models",
        "abstract": "We find that in zero-truncated count data (y=1,2,...), individuals often gain\ninformation at first observation (y=1), leading to a common but unaddressed\nphenomenon of \"one-inflation\". The current standard, the zero-truncated\nnegative binomial (ZTNB) model, is misspecified under one-inflation, causing\nbias and inconsistency. To address this, we introduce the one-inflated\nzero-truncated negative binomial (OIZTNB) regression model. The importance of\nour model is highlighted through simulation studies, and through the discovery\nof one-inflation in four datasets that have traditionally championed ZTNB. We\nrecommended OIZTNB over ZTNB for most data, and provide estimation, marginal\neffects, and testing in the accompanying R package oneinfl.",
        "authors": [
            "Ryan T. Godwin"
        ],
        "categories": "econ.EM",
        "published": "2024-02-03T21:51:09Z",
        "updated": "2024-02-03T21:51:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.01966v2",
        "title": "The general solution to an autoregressive law of motion",
        "abstract": "We provide a complete description of the set of all solutions to an\nautoregressive law of motion in a finite-dimensional complex vector space.\nEvery solution is shown to be the sum of three parts, each corresponding to a\ndirected flow of time. One part flows forward from the arbitrarily distant\npast; one flows backward from the arbitrarily distant future; and one flows\noutward from time zero. The three parts are obtained by applying three\ncomplementary spectral projections to the solution, these corresponding to a\nseparation of the eigenvalues of the autoregressive operator according to\nwhether they are inside, outside or on the unit circle. We provide a\nfinite-dimensional parametrization of the set of all solutions.",
        "authors": [
            "Brendan K. Beare",
            "Massimo Franchi",
            "Phil Howlett"
        ],
        "categories": "econ.EM",
        "published": "2024-02-03T00:21:34Z",
        "updated": "2024-09-26T22:57:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.01951v2",
        "title": "Sparse spanning portfolios and under-diversification with second-order stochastic dominance",
        "abstract": "We develop and implement methods for determining whether relaxing sparsity\nconstraints on portfolios improves the investment opportunity set for\nrisk-averse investors. We formulate a new estimation procedure for sparse\nsecond-order stochastic spanning based on a greedy algorithm and Linear\nProgramming. We show the optimal recovery of the sparse solution asymptotically\nwhether spanning holds or not. From large equity datasets, we estimate the\nexpected utility loss due to possible under-diversification, and find that\nthere is no benefit from expanding a sparse opportunity set beyond 45 assets.\nThe optimal sparse portfolio invests in 10 industry sectors and cuts tail risk\nwhen compared to a sparse mean-variance portfolio. On a rolling-window basis,\nthe number of assets shrinks to 25 assets in crisis periods, while standard\nfactor models cannot explain the performance of the sparse portfolios.",
        "authors": [
            "Stelios Arvanitis",
            "Olivier Scaillet",
            "Nikolas Topaloglou"
        ],
        "categories": "econ.EM",
        "published": "2024-02-02T23:13:59Z",
        "updated": "2024-08-29T23:50:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.01069v1",
        "title": "Data-driven model selection within the matrix completion method for causal panel data models",
        "abstract": "Matrix completion estimators are employed in causal panel data models to\nregulate the rank of the underlying factor model using nuclear norm\nminimization. This convex optimization problem enables concurrent\nregularization of a potentially high-dimensional set of covariates to shrink\nthe model size. For valid finite sample inference, we adopt a permutation-based\napproach and prove its validity for any treatment assignment mechanism.\nSimulations illustrate the consistency of the proposed estimator in parameter\nestimation and variable selection. An application to public health policies in\nGermany demonstrates the data-driven model selection feature on empirical data\nand finds no effect of travel restrictions on the containment of severe\nCovid-19 infections.",
        "authors": [
            "Sandro Heiniger"
        ],
        "categories": "econ.EM",
        "published": "2024-02-02T00:02:09Z",
        "updated": "2024-02-02T00:02:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.01785v1",
        "title": "DoubleMLDeep: Estimation of Causal Effects with Multimodal Data",
        "abstract": "This paper explores the use of unstructured, multimodal data, namely text and\nimages, in causal inference and treatment effect estimation. We propose a\nneural network architecture that is adapted to the double machine learning\n(DML) framework, specifically the partially linear model. An additional\ncontribution of our paper is a new method to generate a semi-synthetic dataset\nwhich can be used to evaluate the performance of causal effect estimation in\nthe presence of text and images as confounders. The proposed methods and\narchitectures are evaluated on the semi-synthetic dataset and compared to\nstandard approaches, highlighting the potential benefit of using text and\nimages directly in causal studies. Our findings have implications for\nresearchers and practitioners in economics, marketing, finance, medicine and\ndata science in general who are interested in estimating causal quantities\nusing non-traditional data.",
        "authors": [
            "Sven Klaassen",
            "Jan Teichert-Kluge",
            "Philipp Bach",
            "Victor Chernozhukov",
            "Martin Spindler",
            "Suhas Vijaykumar"
        ],
        "categories": "cs.LG",
        "published": "2024-02-01T21:34:34Z",
        "updated": "2024-02-01T21:34:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.01005v1",
        "title": "The prices of renewable commodities: A robust stationarity analysis",
        "abstract": "This paper addresses the problem of testing for persistence in the effects of\nthe shocks affecting the prices of renewable commodities, which have potential\nimplications on stabilization policies and economic forecasting, among other\nareas. A robust methodology is employed that enables the determination of the\npotential presence and number of instant/gradual structural changes in the\nseries, stationarity testing conditional on the number of changes detected, and\nthe detection of change points. This procedure is applied to the annual real\nprices of eighteen renewable commodities over the period of 1900-2018. Results\nindicate that most of the series display non-linear features, including\nquadratic patterns and regime transitions that often coincide with well-known\npolitical and economic episodes. The conclusions of stationarity testing\nsuggest that roughly half of the series are integrated. Stationarity fails to\nbe rejected for grains, whereas most livestock and textile commodities do\nreject stationarity. Evidence is mixed in all soft commodities and tropical\ncrops, where stationarity can be rejected in approximately half of the cases.\nThe implication would be that for these commodities, stabilization schemes\nwould not be recommended.",
        "authors": [
            "Manuel Landajo",
            "Mar\u00eda Jos\u00e9 Presno"
        ],
        "categories": "econ.EM",
        "published": "2024-02-01T20:38:25Z",
        "updated": "2024-02-01T20:38:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.00788v1",
        "title": "EU-28's progress towards the 2020 renewable energy share. A club convergence analysis",
        "abstract": "This paper assesses the convergence of the EU-28 countries towards their\ncommon goal of 20% in the renewable energy share indicator by year 2020. The\npotential presence of clubs of convergence towards different steady state\nequilibria is also analyzed from both the standpoints of global convergence to\nthe 20% goal and specific convergence to the various targets assigned to Member\nStates. Two clubs of convergence are detected in the former case, each\ncorresponding to different RES targets. A probit model is also fitted with the\naim of better understanding the determinants of club membership, that seemingly\ninclude real GDP per capita, expenditure on environmental protection, energy\ndependence, and nuclear capacity, with all of them having statistically\nsignificant effects. Finally, convergence is also analyzed separately for the\ntransport, heating and cooling, and electricity sectors.",
        "authors": [
            "Mar\u00eda Jos\u00e9 Presno",
            "Manuel Landajo"
        ],
        "categories": "econ.EM",
        "published": "2024-02-01T17:21:52Z",
        "updated": "2024-02-01T17:21:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.00584v4",
        "title": "Arellano-Bond LASSO Estimator for Dynamic Linear Panel Models",
        "abstract": "The Arellano-Bond estimator is a fundamental method for dynamic panel data\nmodels, widely used in practice. However, the estimator is severely biased when\nthe data's time series dimension $T$ is long due to the large degree of\noveridentification. We show that weak dependence along the panel's time series\ndimension naturally implies approximate sparsity of the most informative moment\nconditions, motivating the following approach to remove the bias: First, apply\nLASSO to the cross-section data at each time period to construct most\ninformative (and cross-fitted) instruments, using lagged values of suitable\ncovariates. This step relies on approximate sparsity to select the most\ninformative instruments. Second, apply a linear instrumental variable estimator\nafter first differencing the dynamic structural equation using the constructed\ninstruments. Under weak time series dependence, we show the new estimator is\nconsistent and asymptotically normal under much weaker conditions on $T$'s\ngrowth than the Arellano-Bond estimator. Our theory covers models with high\ndimensional covariates, including multiple lags of the dependent variable,\ncommon in modern applications. We illustrate our approach by applying it to\nweekly county-level panel data from the United States to study opening K-12\nschools and other mitigation policies' short and long-term effects on\nCOVID-19's spread.",
        "authors": [
            "Victor Chernozhukov",
            "Iv\u00e1n Fern\u00e1ndez-Val",
            "Chen Huang",
            "Weining Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-02-01T13:31:54Z",
        "updated": "2024-10-16T11:59:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.00567v1",
        "title": "Stochastic convergence in per capita CO$_2$ emissions. An approach from nonlinear stationarity analysis",
        "abstract": "This paper studies stochastic convergence of per capita CO$_2$ emissions in\n28 OECD countries for the 1901-2009 period. The analysis is carried out at two\naggregation levels, first for the whole set of countries (joint analysis) and\nthen separately for developed and developing states (group analysis). A\npowerful time series methodology, adapted to a nonlinear framework that allows\nfor quadratic trends with possibly smooth transitions between regimes, is\napplied. This approach provides more robust conclusions in convergence path\nanalysis, enabling (a) robust detection of the presence, and if so, the number\nof changes in the level and/or slope of the trend of the series, (b) inferences\non stationarity of relative per capita CO$_2$ emissions, conditionally on the\npresence of breaks and smooth transitions between regimes, and (c) estimation\nof change locations in the convergence paths. Finally, as stochastic\nconvergence is attained when both stationarity around a trend and\n$\\beta$-convergence hold, the linear approach proposed by Tomljanovich and\nVogelsang (2002) is extended in order to allow for more general quadratic\nmodels. Overall, joint analysis finds some evidence of stochastic convergence\nin per capita CO$_2$ emissions. Some dispersion in terms of $\\beta$-convergence\nis detected by group analysis, particularly among developed countries. This is\nin accordance with per capita GDP not being the sole determinant of convergence\nin emissions, with factors like search for more efficient technologies, fossil\nfuel substitution, innovation, and possibly outsources of industries, also\nhaving a crucial role.",
        "authors": [
            "Mar\u00eda Jos\u00e9 Presno",
            "Manuel Landajo",
            "Paula Fern\u00e1ndez Gonz\u00e1lez"
        ],
        "categories": "econ.EM",
        "published": "2024-02-01T12:55:24Z",
        "updated": "2024-02-01T12:55:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.00192v1",
        "title": "Finite- and Large-Sample Inference for Ranks using Multinomial Data with an Application to Ranking Political Parties",
        "abstract": "It is common to rank different categories by means of preferences that are\nrevealed through data on choices. A prominent example is the ranking of\npolitical candidates or parties using the estimated share of support each one\nreceives in surveys or polls about political attitudes. Since these rankings\nare computed using estimates of the share of support rather than the true share\nof support, there may be considerable uncertainty concerning the true ranking\nof the political candidates or parties. In this paper, we consider the problem\nof accounting for such uncertainty by constructing confidence sets for the rank\nof each category. We consider both the problem of constructing marginal\nconfidence sets for the rank of a particular category as well as simultaneous\nconfidence sets for the ranks of all categories. A distinguishing feature of\nour analysis is that we exploit the multinomial structure of the data to\ndevelop confidence sets that are valid in finite samples. We additionally\ndevelop confidence sets using the bootstrap that are valid only approximately\nin large samples. We use our methodology to rank political parties in Australia\nusing data from the 2019 Australian Election Survey. We find that our\nfinite-sample confidence sets are informative across the entire ranking of\npolitical parties, even in Australian territories with few survey respondents\nand/or with parties that are chosen by only a small share of the survey\nrespondents. In contrast, the bootstrap-based confidence sets may sometimes be\nconsiderably less informative. These findings motivate us to compare these\nmethods in an empirically-driven simulation study, in which we conclude that\nour finite-sample confidence sets often perform better than their large-sample,\nbootstrap-based counterparts, especially in settings that resemble our\nempirical application.",
        "authors": [
            "Sergei Bazylik",
            "Magne Mogstad",
            "Joseph Romano",
            "Azeem Shaikh",
            "Daniel Wilhelm"
        ],
        "categories": "econ.EM",
        "published": "2024-01-31T21:42:27Z",
        "updated": "2024-01-31T21:42:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.00184v1",
        "title": "The Heterogeneous Aggregate Valence Analysis (HAVAN) Model: A Flexible Approach to Modeling Unobserved Heterogeneity in Discrete Choice Analysis",
        "abstract": "This paper introduces the Heterogeneous Aggregate Valence Analysis (HAVAN)\nmodel, a novel class of discrete choice models. We adopt the term \"valence'' to\nencompass any latent quantity used to model consumer decision-making (e.g.,\nutility, regret, etc.). Diverging from traditional models that parameterize\nheterogeneous preferences across various product attributes, HAVAN models\n(pronounced \"haven\") instead directly characterize alternative-specific\nheterogeneous preferences. This innovative perspective on consumer\nheterogeneity affords unprecedented flexibility and significantly reduces\nsimulation burdens commonly associated with mixed logit models. In a simulation\nexperiment, the HAVAN model demonstrates superior predictive performance\ncompared to state-of-the-art artificial neural networks. This finding\nunderscores the potential for HAVAN models to improve discrete choice modeling\ncapabilities.",
        "authors": [
            "Connor R. Forsythe",
            "Cristian Arteaga",
            "John P. Helveston"
        ],
        "categories": "econ.EM",
        "published": "2024-01-31T21:24:27Z",
        "updated": "2024-01-31T21:24:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.00172v1",
        "title": "The Fourier-Malliavin Volatility (FMVol) MATLAB library",
        "abstract": "This paper presents the Fourier-Malliavin Volatility (FMVol) estimation\nlibrary for MATLAB. This library includes functions that implement Fourier-\nMalliavin estimators (see Malliavin and Mancino (2002, 2009)) of the volatility\nand co-volatility of continuous stochastic volatility processes and\nsecond-order quantities, like the quarticity (the squared volatility), the\nvolatility of volatility and the leverage (the covariance between changes in\nthe process and changes in its volatility). The Fourier-Malliavin method is\nfully non-parametric, does not require equally-spaced observations and is\nrobust to measurement errors, or noise, without any preliminary bias correction\nor pre-treatment of the observations. Further, in its multivariate version, it\nis intrinsically robust to irregular and asynchronous sampling. Although\noriginally introduced for a specific application in financial econometrics,\nnamely the estimation of asset volatilities, the Fourier-Malliavin method is a\ngeneral method that can be applied whenever one is interested in reconstructing\nthe latent volatility and second-order quantities of a continuous stochastic\nvolatility process from discrete observations.",
        "authors": [
            "Simona Sanfelici",
            "Giacomo Toscano"
        ],
        "categories": "stat.CO",
        "published": "2024-01-31T21:03:35Z",
        "updated": "2024-01-31T21:03:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.17909v1",
        "title": "Regularizing Discrimination in Optimal Policy Learning with Distributional Targets",
        "abstract": "A decision maker typically (i) incorporates training data to learn about the\nrelative effectiveness of the treatments, and (ii) chooses an implementation\nmechanism that implies an \"optimal\" predicted outcome distribution according to\nsome target functional. Nevertheless, a discrimination-aware decision maker may\nnot be satisfied achieving said optimality at the cost of heavily\ndiscriminating against subgroups of the population, in the sense that the\noutcome distribution in a subgroup deviates strongly from the overall optimal\noutcome distribution. We study a framework that allows the decision maker to\npenalize for such deviations, while allowing for a wide range of target\nfunctionals and discrimination measures to be employed. We establish regret and\nconsistency guarantees for empirical success policies with data-driven tuning\nparameters, and provide numerical results. Furthermore, we briefly illustrate\nthe methods in two empirical settings.",
        "authors": [
            "Anders Bredahl Kock",
            "David Preinerstorfer"
        ],
        "categories": "econ.EM",
        "published": "2024-01-31T15:15:26Z",
        "updated": "2024-01-31T15:15:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.17595v2",
        "title": "Marginal treatment effects in the absence of instrumental variables",
        "abstract": "We propose a method for defining, identifying, and estimating the marginal\ntreatment effect (MTE) without imposing the instrumental variable (IV)\nassumptions of independence, exclusion, and separability (or monotonicity).\nUnder a new definition of the MTE based on reduced-form treatment error that is\nstatistically independent of the covariates, we find that the relationship\nbetween the MTE and standard treatment parameters holds in the absence of IVs.\nWe provide a set of sufficient conditions ensuring the identification of the\ndefined MTE in an environment of essential heterogeneity. The key conditions\ninclude a linear restriction on potential outcome regression functions, a\nnonlinear restriction on the propensity score, and a conditional mean\nindependence restriction that will lead to additive separability. We prove this\nidentification using the notion of semiparametric identification based on\nfunctional form. And we provide an empirical application for the Head Start\nprogram to illustrate the usefulness of the proposed method in analyzing\nheterogenous causal effects when IVs are elusive.",
        "authors": [
            "Zhewen Pan",
            "Zhengxin Wang",
            "Junsen Zhang",
            "Yahong Zhou"
        ],
        "categories": "econ.EM",
        "published": "2024-01-31T04:38:32Z",
        "updated": "2024-08-20T08:34:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.17137v1",
        "title": "Partial Identification of Binary Choice Models with Misreported Outcomes",
        "abstract": "This paper provides partial identification of various binary choice models\nwith misreported dependent variables. We propose two distinct approaches by\nexploiting different instrumental variables respectively. In the first\napproach, the instrument is assumed to only affect the true dependent variable\nbut not misreporting probabilities. The second approach uses an instrument that\ninfluences misreporting probabilities monotonically while having no effect on\nthe true dependent variable. Moreover, we derive identification results under\nadditional restrictions on misreporting, including bounded/monotone\nmisreporting probabilities. We use simulations to demonstrate the robust\nperformance of our approaches, and apply the method to study educational\nattainment.",
        "authors": [
            "Orville Mondal",
            "Rui Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-01-30T16:16:17Z",
        "updated": "2024-01-30T16:16:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.16844v2",
        "title": "Congestion Pricing for Efficiency and Equity: Theory and Applications to the San Francisco Bay Area",
        "abstract": "Congestion pricing, while adopted by many cities to alleviate traffic\ncongestion, raises concerns about widening socioeconomic disparities due to its\ndisproportionate impact on low-income travelers. We address this concern by\nproposing a new class of congestion pricing schemes that not only minimize\ntotal travel time, but also incorporate an equity objective, reducing\ndisparities in the relative change in travel costs across populations with\ndifferent incomes, following the implementation of tolls. Our analysis builds\non a congestion game model with heterogeneous traveler populations. We present\nfour pricing schemes that account for practical considerations, such as the\nability to charge differentiated tolls to various traveler populations and the\noption to toll all or only a subset of edges in the network. We evaluate our\npricing schemes in the calibrated freeway network of the San Francisco Bay\nArea. We demonstrate that the proposed congestion pricing schemes improve both\nthe total travel time and the equity objective compared to the current pricing\nscheme.\n  Our results further show that pricing schemes charging differentiated prices\nto traveler populations with varying value-of-time lead to a more equitable\ndistribution of travel costs compared to those that charge a homogeneous price\nto all.",
        "authors": [
            "Chinmay Maheshwari",
            "Kshitij Kulkarni",
            "Druv Pai",
            "Jiarui Yang",
            "Manxi Wu",
            "Shankar Sastry"
        ],
        "categories": "cs.GT",
        "published": "2024-01-30T09:35:02Z",
        "updated": "2024-09-21T01:09:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.16275v1",
        "title": "Graph Neural Networks: Theory for Estimation with Application on Network Heterogeneity",
        "abstract": "This paper presents a novel application of graph neural networks for modeling\nand estimating network heterogeneity. Network heterogeneity is characterized by\nvariations in unit's decisions or outcomes that depend not only on its own\nattributes but also on the conditions of its surrounding neighborhood. We\ndelineate the convergence rate of the graph neural networks estimator, as well\nas its applicability in semiparametric causal inference with heterogeneous\ntreatment effects. The finite-sample performance of our estimator is evaluated\nthrough Monte Carlo simulations. In an empirical setting related to\nmicrofinance program participation, we apply the new estimator to examine the\naverage treatment effects and outcomes of counterfactual policies, and to\npropose an enhanced strategy for selecting the initial recipients of program\ninformation in social networks.",
        "authors": [
            "Yike Wang",
            "Chris Gu",
            "Taisuke Otsu"
        ],
        "categories": "econ.EM",
        "published": "2024-01-29T16:26:30Z",
        "updated": "2024-01-29T16:26:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2402.12384v1",
        "title": "Comparing MCMC algorithms in Stochastic Volatility Models using Simulation Based Calibration",
        "abstract": "Simulation Based Calibration (SBC) is applied to analyse two commonly used,\ncompeting Markov chain Monte Carlo algorithms for estimating the posterior\ndistribution of a stochastic volatility model. In particular, the bespoke\n'off-set mixture approximation' algorithm proposed by Kim, Shephard, and Chib\n(1998) is explored together with a Hamiltonian Monte Carlo algorithm\nimplemented through Stan. The SBC analysis involves a simulation study to\nassess whether each sampling algorithm has the capacity to produce valid\ninference for the correctly specified model, while also characterising\nstatistical efficiency through the effective sample size. Results show that\nStan's No-U-Turn sampler, an implementation of Hamiltonian Monte Carlo,\nproduces a well-calibrated posterior estimate while the celebrated off-set\nmixture approach is less efficient and poorly calibrated, though model\nparameterisation also plays a role. Limitations and restrictions of generality\nare discussed.",
        "authors": [
            "Benjamin Wee"
        ],
        "categories": "stat.AP",
        "published": "2024-01-28T03:42:54Z",
        "updated": "2024-01-28T03:42:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.15253v1",
        "title": "Testing the Exogeneity of Instrumental Variables and Regressors in Linear Regression Models Using Copulas",
        "abstract": "We provide a Copula-based approach to test the exogeneity of instrumental\nvariables in linear regression models. We show that the exogeneity of\ninstrumental variables is equivalent to the exogeneity of their standard normal\ntransformations with the same CDF value. Then, we establish a Wald test for the\nexogeneity of the instrumental variables. We demonstrate the performance of our\ntest using simulation studies. Our simulations show that if the instruments are\nactually endogenous, our test rejects the exogeneity hypothesis approximately\n93% of the time at the 5% significance level. Conversely, when instruments are\ntruly exogenous, it dismisses the exogeneity assumption less than 30% of the\ntime on average for data with 200 observations and less than 2% of the time for\ndata with 1,000 observations. Our results demonstrate our test's effectiveness,\noffering significant value to applied econometricians.",
        "authors": [
            "Seyed Morteza Emadi"
        ],
        "categories": "stat.ME",
        "published": "2024-01-27T00:13:16Z",
        "updated": "2024-01-27T00:13:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.15205v1",
        "title": "csranks: An R Package for Estimation and Inference Involving Ranks",
        "abstract": "This article introduces the R package csranks for estimation and inference\ninvolving ranks. First, we review methods for the construction of confidence\nsets for ranks, namely marginal and simultaneous confidence sets as well as\nconfidence sets for the identities of the tau-best. Second, we review methods\nfor estimation and inference in regressions involving ranks. Third, we describe\nthe implementation of these methods in csranks and illustrate their usefulness\nin two examples: one about the quantification of uncertainty in the PISA\nranking of countries and one about the measurement of intergenerational\nmobility using rank-rank regressions.",
        "authors": [
            "Denis Chetverikov",
            "Magne Mogstad",
            "Pawel Morgen",
            "Joseph Romano",
            "Azeem Shaikh",
            "Daniel Wilhelm"
        ],
        "categories": "econ.EM",
        "published": "2024-01-26T21:04:30Z",
        "updated": "2024-01-26T21:04:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.14582v2",
        "title": "High-dimensional forecasting with known knowns and known unknowns",
        "abstract": "Forecasts play a central role in decision making under uncertainty. After a\nbrief review of the general issues, this paper considers ways of using\nhigh-dimensional data in forecasting. We consider selecting variables from a\nknown active set, known knowns, using Lasso and OCMT, and approximating\nunobserved latent factors, known unknowns, by various means. This combines both\nsparse and dense approaches. We demonstrate the various issues involved in\nvariable selection in a high-dimensional setting with an application to\nforecasting UK inflation at different horizons over the period 2020q1-2023q1.\nThis application shows both the power of parsimonious models and the importance\nof allowing for global variables.",
        "authors": [
            "M. Hashem Pesaran",
            "Ron P. Smith"
        ],
        "categories": "econ.EM",
        "published": "2024-01-26T00:53:25Z",
        "updated": "2024-04-04T16:22:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.14545v1",
        "title": "Structural Periodic Vector Autoregressions",
        "abstract": "While seasonality inherent to raw macroeconomic data is commonly removed by\nseasonal adjustment techniques before it is used for structural inference, this\napproach might distort valuable information contained in the data. As an\nalternative method to commonly used structural vector autoregressions (SVAR)\nfor seasonally adjusted macroeconomic data, this paper offers an approach in\nwhich the periodicity of not seasonally adjusted raw data is modeled directly\nby structural periodic vector autoregressions (SPVAR) that are based on\nperiodic vector autoregressions (PVAR) as the reduced form model. In comparison\nto a VAR, the PVAR does allow not only for periodically time-varying\nintercepts, but also for periodic autoregressive parameters and innovations\nvariances, respectively. As this larger flexibility leads also to an increased\nnumber of parameters, we propose linearly constrained estimation techniques.\nOverall, SPVARs allow to capture seasonal effects and enable a direct and more\nrefined analysis of seasonal patterns in macroeconomic data, which can provide\nuseful insights into their dynamics. Moreover, based on such SPVARs, we propose\na general concept for structural impulse response analyses that takes seasonal\npatterns directly into account. We provide asymptotic theory for estimators of\nperiodic reduced form parameters and structural impulse responses under\nflexible linear restrictions. Further, for the construction of confidence\nintervals, we propose residual-based (seasonal) bootstrap methods that allow\nfor general forms of seasonalities in the data and prove its bootstrap\nconsistency. A real data application on industrial production, inflation and\nfederal funds rate is presented, showing that useful information about the data\nstructure can be lost when using common seasonal adjustment methods.",
        "authors": [
            "Daniel Dzikowski",
            "Carsten Jentsch"
        ],
        "categories": "econ.EM",
        "published": "2024-01-25T22:26:25Z",
        "updated": "2024-01-25T22:26:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.14395v1",
        "title": "Identification of Nonseparable Models with Endogenous Control Variables",
        "abstract": "We study identification of the treatment effects in a class of nonseparable\nmodels with the presence of potentially endogenous control variables. We show\nthat given the treatment variable and the controls are measurably separated,\nthe usual conditional independence condition or availability of excluded\ninstrument suffices for identification.",
        "authors": [
            "Kaicheng Chen",
            "Kyoo il Kim"
        ],
        "categories": "econ.EM",
        "published": "2024-01-25T18:55:32Z",
        "updated": "2024-01-25T18:55:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.13665v2",
        "title": "Entrywise Inference for Missing Panel Data: A Simple and Instance-Optimal Approach",
        "abstract": "Longitudinal or panel data can be represented as a matrix with rows indexed\nby units and columns indexed by time. We consider inferential questions\nassociated with the missing data version of panel data induced by staggered\nadoption. We propose a computationally efficient procedure for estimation,\ninvolving only simple matrix algebra and singular value decomposition, and\nprove non-asymptotic and high-probability bounds on its error in estimating\neach missing entry. By controlling proximity to a suitably scaled Gaussian\nvariable, we develop and analyze a data-driven procedure for constructing\nentrywise confidence intervals with pre-specified coverage. Despite its\nsimplicity, our procedure turns out to be instance-optimal: we prove that the\nwidth of our confidence intervals match a non-asymptotic instance-wise lower\nbound derived via a Bayesian Cram\\'{e}r-Rao argument. We illustrate the\nsharpness of our theoretical characterization on a variety of numerical\nexamples. Our analysis is based on a general inferential toolbox for SVD-based\nalgorithm applied to the matrix denoising model, which might be of independent\ninterest.",
        "authors": [
            "Yuling Yan",
            "Martin J. Wainwright"
        ],
        "categories": "math.ST",
        "published": "2024-01-24T18:58:18Z",
        "updated": "2024-07-01T17:10:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.13370v1",
        "title": "New accessibility measures based on unconventional big data sources",
        "abstract": "In health econometric studies we are often interested in quantifying aspects\nrelated to the accessibility to medical infrastructures. The increasing\navailability of data automatically collected through unconventional sources\n(such as webscraping, crowdsourcing or internet of things) recently opened\npreviously unconceivable opportunities to researchers interested in measuring\naccessibility and to use it as a tool for real-time monitoring, surveillance\nand health policies definition. This paper contributes to this strand of\nliterature proposing new accessibility measures that can be continuously feeded\nby automatic data collection. We present new measures of accessibility and we\nillustrate their use to study the territorial impact of supply-side shocks of\nhealth facilities. We also illustrate the potential of our proposal with a case\nstudy based on a huge set of data (related to the Emergency Departments in\nMilan, Italy) that have been webscraped for the purpose of this paper every 5\nminutes since November 2021 to March 2022, amounting to approximately 5 million\nobservations.",
        "authors": [
            "G. Arbia",
            "V. Nardelli",
            "N. Salvini",
            "I. Valentini"
        ],
        "categories": "econ.EM",
        "published": "2024-01-24T10:59:58Z",
        "updated": "2024-01-24T10:59:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.13179v2",
        "title": "Realized Stochastic Volatility Model with Skew-t Distributions for Improved Volatility and Quantile Forecasting",
        "abstract": "Forecasting volatility and quantiles of financial returns is essential for\naccurately measuring financial tail risks, such as value-at-risk and expected\nshortfall. The critical elements in these forecasts involve understanding the\ndistribution of financial returns and accurately estimating volatility. This\npaper introduces an advancement to the traditional stochastic volatility model,\ntermed the realized stochastic volatility model, which integrates realized\nvolatility as a precise estimator of volatility. To capture the well-known\ncharacteristics of return distribution, namely skewness and heavy tails, we\nincorporate three types of skew-t distributions. Among these, two distributions\ninclude the skew-normal feature, offering enhanced flexibility in modeling the\nreturn distribution. We employ a Bayesian estimation approach using the Markov\nchain Monte Carlo method and apply it to major stock indices. Our empirical\nanalysis, utilizing data from US and Japanese stock indices, indicates that the\ninclusion of both skewness and heavy tails in daily returns significantly\nimproves the accuracy of volatility and quantile forecasts.",
        "authors": [
            "Makoto Takahashi",
            "Yuta Yamauchi",
            "Toshiaki Watanabe",
            "Yasuhiro Omori"
        ],
        "categories": "econ.EM",
        "published": "2024-01-24T01:52:54Z",
        "updated": "2024-10-25T03:45:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.13057v2",
        "title": "Inference under partial identification with minimax test statistics",
        "abstract": "We provide a means of computing and estimating the asymptotic distributions\nof statistics based on an outer minimization of an inner maximization. Such\ntest statistics, which arise frequently in moment models, are of special\ninterest in providing hypothesis tests under partial identification. Under\ngeneral conditions, we provide an asymptotic characterization of such test\nstatistics using the minimax theorem, and a means of computing critical values\nusing the bootstrap. Making some light regularity assumptions, our results\naugment several asymptotic approximations that have been provided for partially\nidentified hypothesis tests, and extend them by mitigating their dependence on\nlocal linear approximations of the parameter space. These asymptotic results\nare generally simple to state and straightforward to compute (esp.\\\nadversarially).",
        "authors": [
            "Isaac Loh"
        ],
        "categories": "econ.EM",
        "published": "2024-01-23T19:29:01Z",
        "updated": "2024-04-15T22:42:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.12309v1",
        "title": "Interpreting Event-Studies from Recent Difference-in-Differences Methods",
        "abstract": "This note discusses the interpretation of event-study plots produced by\nrecent difference-in-differences methods. I show that even when specialized to\nthe case of non-staggered treatment timing, the default plots produced by\nsoftware for three of the most popular recent methods (de Chaisemartin and\nD'Haultfoeuille, 2020; Callaway and SantAnna, 2021; Borusyak, Jaravel and\nSpiess, 2024) do not match those of traditional two-way fixed effects (TWFE)\nevent-studies: the new methods may show a kink or jump at the time of treatment\neven when the TWFE event-study shows a straight line. This difference stems\nfrom the fact that the new methods construct the pre-treatment coefficients\nasymmetrically from the post-treatment coefficients. As a result, visual\nheuristics for analyzing TWFE event-study plots should not be immediately\napplied to those from these methods. I conclude with practical recommendations\nfor constructing and interpreting event-study plots when using these methods.",
        "authors": [
            "Jonathan Roth"
        ],
        "categories": "econ.EM",
        "published": "2024-01-22T19:09:10Z",
        "updated": "2024-01-22T19:09:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.12084v2",
        "title": "Temporal Aggregation for the Synthetic Control Method",
        "abstract": "The synthetic control method (SCM) is a popular approach for estimating the\nimpact of a treatment on a single unit with panel data. Two challenges arise\nwith higher frequency data (e.g., monthly versus yearly): (1) achieving\nexcellent pre-treatment fit is typically more challenging; and (2) overfitting\nto noise is more likely. Aggregating data over time can mitigate these problems\nbut can also destroy important signal. In this paper, we bound the bias for SCM\nwith disaggregated and aggregated outcomes and give conditions under which\naggregating tightens the bounds. We then propose finding weights that balance\nboth disaggregated and aggregated series.",
        "authors": [
            "Liyang Sun",
            "Eli Ben-Michael",
            "Avi Feller"
        ],
        "categories": "econ.EM",
        "published": "2024-01-22T16:23:01Z",
        "updated": "2024-04-15T09:10:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.12050v1",
        "title": "A Bracketing Relationship for Long-Term Policy Evaluation with Combined Experimental and Observational Data",
        "abstract": "Combining short-term experimental data with observational data enables\ncredible long-term policy evaluation. The literature offers two key but\nnon-nested assumptions, namely the latent unconfoundedness (LU; Athey et al.,\n2020) and equi-confounding bias (ECB; Ghassami et al., 2022) conditions, to\ncorrect observational selection. Committing to the wrong assumption leads to\nbiased estimation. To mitigate such risks, we provide a novel bracketing\nrelationship (cf. Angrist and Pischke, 2009) repurposed for the setting with\ndata combination: the LU-based estimand and the ECB-based estimand serve as the\nlower and upper bounds, respectively, with the true causal effect lying in\nbetween if either assumption holds. For researchers further seeking point\nestimates, our Lalonde-style exercise suggests the conservatively more robust\nLU-based lower bounds align closely with the hold-out experimental estimates\nfor educational policy evaluation. We investigate the economic substantives of\nthese findings through the lens of a nonparametric class of selection\nmechanisms and sensitivity analysis. We uncover as key the sub-martingale\nproperty and sufficient-statistics role (Chetty, 2009) of the potential\noutcomes of student test scores (Chetty et al., 2011, 2014).",
        "authors": [
            "Yechan Park",
            "Yuya Sasaki"
        ],
        "categories": "econ.EM",
        "published": "2024-01-22T15:42:18Z",
        "updated": "2024-01-22T15:42:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.11422v3",
        "title": "Local Identification in Instrumental Variable Multivariate Quantile Regression Models",
        "abstract": "In the instrumental variable quantile regression (IVQR) model of Chernozhukov\nand Hansen (2005), a one-dimensional unobserved rank variable monotonically\ndetermines a single potential outcome. Even when multiple outcomes are\nsimultaneously of interest, it is common to apply the IVQR model to each of\nthem separately. This practice implicitly assumes that the rank variable of\neach regression model affects only the corresponding outcome and does not\naffect the other outcomes. In reality, however, it is often the case that all\nrank variables together determine the outcomes, which leads to a systematic\ncorrelation between the outcomes. To deal with this, we propose a nonlinear IV\nmodel that allows for multivariate unobserved heterogeneity, each of which is\nconsidered as a rank variable for an observed outcome. We show that the\nstructural function of our model is locally identified under the assumption\nthat the IV and the treatment variable are sufficiently positively correlated.",
        "authors": [
            "Haruki Kono"
        ],
        "categories": "econ.EM",
        "published": "2024-01-21T08:00:05Z",
        "updated": "2024-06-13T17:43:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.11229v2",
        "title": "Estimation with Pairwise Observations",
        "abstract": "The paper introduces a new estimation method for the standard linear\nregression model. The procedure is not driven by the optimisation of any\nobjective function rather, it is a simple weighted average of slopes from\nobservation pairs. The paper shows that such estimator is consistent for\ncarefully selected weights. Other properties, such as asymptotic distributions,\nhave also been derived to facilitate valid statistical inference. Unlike\ntraditional methods, such as Least Squares and Maximum Likelihood, among\nothers, the estimated residual of this estimator is not by construction\northogonal to the explanatory variables of the model. This property allows a\nwide range of practical applications, such as the testing of endogeneity, i.e.,\nthe correlation between the explanatory variables and the disturbance terms.",
        "authors": [
            "Felix Chan",
            "Laszlo Matyas"
        ],
        "categories": "econ.EM",
        "published": "2024-01-20T13:21:46Z",
        "updated": "2024-02-26T14:02:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.11046v1",
        "title": "Information Based Inference in Models with Set-Valued Predictions and Misspecification",
        "abstract": "This paper proposes an information-based inference method for partially\nidentified parameters in incomplete models that is valid both when the model is\ncorrectly specified and when it is misspecified. Key features of the method\nare: (i) it is based on minimizing a suitably defined Kullback-Leibler\ninformation criterion that accounts for incompleteness of the model and\ndelivers a non-empty pseudo-true set; (ii) it is computationally tractable;\n(iii) its implementation is the same for both correctly and incorrectly\nspecified models; (iv) it exploits all information provided by variation in\ndiscrete and continuous covariates; (v) it relies on Rao's score statistic,\nwhich is shown to be asymptotically pivotal.",
        "authors": [
            "Hiroaki Kaido",
            "Francesca Molinari"
        ],
        "categories": "econ.EM",
        "published": "2024-01-19T22:18:31Z",
        "updated": "2024-01-19T22:18:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.11016v1",
        "title": "Bounding Consideration Probabilities in Consider-Then-Choose Ranking Models",
        "abstract": "A common theory of choice posits that individuals make choices in a two-step\nprocess, first selecting some subset of the alternatives to consider before\nmaking a selection from the resulting consideration set. However, inferring\nunobserved consideration sets (or item consideration probabilities) in this\n\"consider then choose\" setting poses significant challenges, because even\nsimple models of consideration with strong independence assumptions are not\nidentifiable, even if item utilities are known. We consider a natural extension\nof consider-then-choose models to a top-$k$ ranking setting, where we assume\nrankings are constructed according to a Plackett-Luce model after sampling a\nconsideration set. While item consideration probabilities remain non-identified\nin this setting, we prove that knowledge of item utilities allows us to infer\nbounds on the relative sizes of consideration probabilities. Additionally,\ngiven a condition on the expected consideration set size, we derive absolute\nupper and lower bounds on item consideration probabilities. We also provide\nalgorithms to tighten those bounds on consideration probabilities by\npropagating inferred constraints. Thus, we show that we can learn useful\ninformation about consideration probabilities despite not being able to\nidentify them precisely. We demonstrate our methods on a ranking dataset from a\npsychology experiment with two different ranking tasks (one with fixed\nconsideration sets and one with unknown consideration sets). This combination\nof data allows us to estimate utilities and then learn about unknown\nconsideration probabilities using our bounds.",
        "authors": [
            "Ben Aoki-Sherwood",
            "Catherine Bregou",
            "David Liben-Nowell",
            "Kiran Tomlinson",
            "Thomas Zeng"
        ],
        "categories": "cs.LG",
        "published": "2024-01-19T20:27:29Z",
        "updated": "2024-01-19T20:27:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.10054v1",
        "title": "Nowcasting economic activity in European regions using a mixed-frequency dynamic factor model",
        "abstract": "Timely information about the state of regional economies can be essential for\nplanning, implementing and evaluating locally targeted economic policies.\nHowever, European regional accounts for output are published at an annual\nfrequency and with a two-year delay. To obtain robust and more timely measures\nin a computationally efficient manner, we propose a mixed-frequency dynamic\nfactor model that accounts for national information to produce high-frequency\nestimates of the regional gross value added (GVA). We show that our model\nproduces reliable nowcasts of GVA in 162 regions across 12 European countries.",
        "authors": [
            "Luca Barbaglia",
            "Lorenzo Frattarolo",
            "Niko Hauzenberger",
            "Dominik Hirschbuehl",
            "Florian Huber",
            "Luca Onorante",
            "Michael Pfarrhofer",
            "Luca Tiozzo Pezzoli"
        ],
        "categories": "econ.EM",
        "published": "2024-01-18T15:21:42Z",
        "updated": "2024-01-18T15:21:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.09874v1",
        "title": "A Quantile Nelson-Siegel model",
        "abstract": "A widespread approach to modelling the interaction between macroeconomic\nvariables and the yield curve relies on three latent factors usually\ninterpreted as the level, slope, and curvature (Diebold et al., 2006). This\napproach is inherently focused on the conditional mean of the yields and\npostulates a dynamic linear model where the latent factors smoothly change over\ntime. However, periods of deep crisis, such as the Great Recession and the\nrecent pandemic, have highlighted the importance of statistical models that\naccount for asymmetric shocks and are able to forecast the tails of a\nvariable's distribution. A new version of the dynamic three-factor model is\nproposed to address this issue based on quantile regressions. The novel\napproach leverages the potential of quantile regression to model the entire\n(conditional) distribution of the yields instead of restricting to its mean. An\napplication to US data from the 1970s shows the significant heterogeneity of\nthe interactions between financial and macroeconomic variables across different\nquantiles. Moreover, an out-of-sample forecasting exercise showcases the\nproposed method's advantages in predicting the yield distribution tails\ncompared to the standard conditional mean model. Finally, by inspecting the\nposterior distribution of the three factors during the recent major crises, new\nevidence is found that supports the greater and longer-lasting negative impact\nof the great recession on the yields compared to the COVID-19 pandemic.",
        "authors": [
            "Matteo Iacopini",
            "Aubrey Poon",
            "Luca Rossini",
            "Dan Zhu"
        ],
        "categories": "stat.AP",
        "published": "2024-01-18T10:43:26Z",
        "updated": "2024-01-18T10:43:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.08442v1",
        "title": "Assessing the impact of forced and voluntary behavioral changes on economic-epidemiological co-dynamics: A comparative case study between Belgium and Sweden during the 2020 COVID-19 pandemic",
        "abstract": "During the COVID-19 pandemic, governments faced the challenge of managing\npopulation behavior to prevent their healthcare systems from collapsing. Sweden\nadopted a strategy centered on voluntary sanitary recommendations while Belgium\nresorted to mandatory measures. Their consequences on pandemic progression and\nassociated economic impacts remain insufficiently understood. This study\nleverages the divergent policies of Belgium and Sweden during the COVID-19\npandemic to relax the unrealistic -- but persistently used -- assumption that\nsocial contacts are not influenced by an epidemic's dynamics. We develop an\nepidemiological-economic co-simulation model where pandemic-induced behavioral\nchanges are a superposition of voluntary actions driven by fear, prosocial\nbehavior or social pressure, and compulsory compliance with government\ndirectives. Our findings emphasize the importance of early responses, which\nreduce the stringency of measures necessary to safeguard healthcare systems and\nminimize ensuing economic damage. Voluntary behavioral changes lead to a\npattern of recurring epidemics, which should be regarded as the natural\nlong-term course of pandemics. Governments should carefully consider prolonging\nlockdown longer than necessary because this leads to higher economic damage and\na potentially higher second surge when measures are released. Our model can aid\npolicymakers in the selection of an appropriate long-term strategy that\nminimizes economic damage.",
        "authors": [
            "Tijs W. Alleman",
            "Jan M. Baetens"
        ],
        "categories": "econ.EM",
        "published": "2024-01-16T15:41:26Z",
        "updated": "2024-01-16T15:41:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.08290v2",
        "title": "Causal Machine Learning for Moderation Effects",
        "abstract": "It is valuable for any decision maker to know the impact of decisions\n(treatments) on average and for subgroups. The causal machine learning\nliterature has recently provided tools for estimating group average treatment\neffects (GATE) to understand treatment heterogeneity better. This paper\naddresses the challenge of interpreting such differences in treatment effects\nbetween groups while accounting for variations in other covariates. We propose\na new parameter, the balanced group average treatment effect (BGATE), which\nmeasures a GATE with a specific distribution of a priori-determined covariates.\nBy taking the difference of two BGATEs, we can analyse heterogeneity more\nmeaningfully than by comparing two GATEs. The estimation strategy for this\nparameter is based on double/debiased machine learning for discrete treatments\nin an unconfoundedness setting, and the estimator is shown to be\n$\\sqrt{N}$-consistent and asymptotically normal under standard conditions.\nAdding additional identifying assumptions allows specific balanced differences\nin treatment effects between groups to be interpreted causally, leading to the\ncausal balanced group average treatment effect. We explore the finite sample\nproperties in a small-scale simulation study and demonstrate the usefulness of\nthese parameters in an empirical example.",
        "authors": [
            "Nora Bearth",
            "Michael Lechner"
        ],
        "categories": "econ.EM",
        "published": "2024-01-16T11:34:59Z",
        "updated": "2024-04-16T12:30:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.07176v1",
        "title": "A Note on Uncertainty Quantification for Maximum Likelihood Parameters Estimated with Heuristic Based Optimization Algorithms",
        "abstract": "Gradient-based solvers risk convergence to local optima, leading to incorrect\nresearcher inference. Heuristic-based algorithms are able to ``break free\" of\nthese local optima to eventually converge to the true global optimum. However,\ngiven that they do not provide the gradient/Hessian needed to approximate the\ncovariance matrix and that the significantly longer computational time they\nrequire for convergence likely precludes resampling procedures for inference,\nresearchers often are unable to quantify uncertainty in the estimates they\nderive with these methods. This note presents a simple and relatively fast\ntwo-step procedure to estimate the covariance matrix for parameters estimated\nwith these algorithms. This procedure relies on automatic differentiation, a\ncomputational means of calculating derivatives that is popular in machine\nlearning applications. A brief empirical example demonstrates the advantages of\nthis procedure relative to bootstrapping and shows the similarity in standard\nerror estimates between this procedure and that which would normally accompany\nmaximum likelihood estimation with a gradient-based algorithm.",
        "authors": [
            "Zachary Porreca"
        ],
        "categories": "econ.EM",
        "published": "2024-01-13T23:56:42Z",
        "updated": "2024-01-13T23:56:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.07152v2",
        "title": "Inference for Synthetic Controls via Refined Placebo Tests",
        "abstract": "The synthetic control method is often applied to problems with one treated\nunit and a small number of control units. A common inferential task in this\nsetting is to test null hypotheses regarding the average treatment effect on\nthe treated. Inference procedures that are justified asymptotically are often\nunsatisfactory due to (1) small sample sizes that render large-sample\napproximation fragile and (2) simplification of the estimation procedure that\nis implemented in practice. An alternative is permutation inference, which is\nrelated to a common diagnostic called the placebo test. It has provable Type-I\nerror guarantees in finite samples without simplification of the method, when\nthe treatment is uniformly assigned. Despite this robustness, the placebo test\nsuffers from low resolution since the null distribution is constructed from\nonly $N$ reference estimates, where $N$ is the sample size. This creates a\nbarrier for statistical inference at a common level like $\\alpha = 0.05$,\nespecially when $N$ is small. We propose a novel leave-two-out procedure that\nbypasses this issue, while still maintaining the same finite-sample Type-I\nerror guarantee under uniform assignment for a wide range of $N$. Unlike the\nplacebo test whose Type-I error always equals the theoretical upper bound, our\nprocedure often achieves a lower unconditional Type-I error than theory\nsuggests; this enables useful inference in the challenging regime when $\\alpha\n< 1/N$. Empirically, our procedure achieves a higher power when the effect size\nis reasonably large and a comparable power otherwise. We generalize our\nprocedure to non-uniform assignments and show how to conduct sensitivity\nanalysis. From a methodological perspective, our procedure can be viewed as a\nnew type of randomization inference different from permutation or rank-based\ninference, which is particularly effective in small samples.",
        "authors": [
            "Lihua Lei",
            "Timothy Sudijono"
        ],
        "categories": "stat.ME",
        "published": "2024-01-13T19:52:19Z",
        "updated": "2024-04-12T17:38:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.07038v1",
        "title": "A simple stochastic nonlinear AR model with application to bubble",
        "abstract": "Economic and financial time series can feature locally explosive behavior\nwhen a bubble is formed. The economic or financial bubble, especially its\ndynamics, is an intriguing topic that has been attracting longstanding\nattention. To illustrate the dynamics of the local explosion itself, the paper\npresents a novel, simple, yet useful time series model, called the stochastic\nnonlinear autoregressive model, which is always strictly stationary and\ngeometrically ergodic and can create long swings or persistence observed in\nmany macroeconomic variables. When a nonlinear autoregressive coefficient is\noutside of a certain range, the model has periodically explosive behaviors and\ncan then be used to portray the bubble dynamics. Further, the quasi-maximum\nlikelihood estimation (QMLE) of our model is considered, and its strong\nconsistency and asymptotic normality are established under minimal assumptions\non innovation. A new model diagnostic checking statistic is developed for model\nfitting adequacy. In addition two methods for bubble tagging are proposed, one\nfrom the residual perspective and the other from the null-state perspective.\nMonte Carlo simulation studies are conducted to assess the performances of the\nQMLE and the two bubble tagging methods in finite samples. Finally, the\nusefulness of the model is illustrated by an empirical application to the\nmonthly Hang Seng Index.",
        "authors": [
            "Xuanling Yang",
            "Dong Li",
            "Ting Zhang"
        ],
        "categories": "math.ST",
        "published": "2024-01-13T10:54:54Z",
        "updated": "2024-01-13T10:54:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.06864v1",
        "title": "Deep Learning With DAGs",
        "abstract": "Social science theories often postulate causal relationships among a set of\nvariables or events. Although directed acyclic graphs (DAGs) are increasingly\nused to represent these theories, their full potential has not yet been\nrealized in practice. As non-parametric causal models, DAGs require no\nassumptions about the functional form of the hypothesized relationships.\nNevertheless, to simplify the task of empirical evaluation, researchers tend to\ninvoke such assumptions anyway, even though they are typically arbitrary and do\nnot reflect any theoretical content or prior knowledge. Moreover, functional\nform assumptions can engender bias, whenever they fail to accurately capture\nthe complexity of the causal system under investigation. In this article, we\nintroduce causal-graphical normalizing flows (cGNFs), a novel approach to\ncausal inference that leverages deep neural networks to empirically evaluate\ntheories represented as DAGs. Unlike conventional approaches, cGNFs model the\nfull joint distribution of the data according to a DAG supplied by the analyst,\nwithout relying on stringent assumptions about functional form. In this way,\nthe method allows for flexible, semi-parametric estimation of any causal\nestimand that can be identified from the DAG, including total effects,\nconditional effects, direct and indirect effects, and path-specific effects. We\nillustrate the method with a reanalysis of Blau and Duncan's (1967) model of\nstatus attainment and Zhou's (2019) model of conditional versus controlled\nmobility. To facilitate adoption, we provide open-source software together with\na series of online tutorials for implementing cGNFs. The article concludes with\na discussion of current limitations and directions for future development.",
        "authors": [
            "Sourabh Balgi",
            "Adel Daoud",
            "Jose M. Pe\u00f1a",
            "Geoffrey T. Wodtke",
            "Jesse Zhou"
        ],
        "categories": "stat.ML",
        "published": "2024-01-12T19:35:54Z",
        "updated": "2024-01-12T19:35:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.06611v1",
        "title": "Robust Analysis of Short Panels",
        "abstract": "Many structural econometric models include latent variables on whose\nprobability distributions one may wish to place minimal restrictions. Leading\nexamples in panel data models are individual-specific variables sometimes\ntreated as \"fixed effects\" and, in dynamic models, initial conditions. This\npaper presents a generally applicable method for characterizing sharp\nidentified sets when models place no restrictions on the probability\ndistribution of certain latent variables and no restrictions on their\ncovariation with other variables. In our analysis latent variables on which\nrestrictions are undesirable are removed, leading to econometric analysis\nrobust to misspecification of restrictions on their distributions which are\ncommonplace in the applied panel data literature. Endogenous explanatory\nvariables are easily accommodated. Examples of application to some static and\ndynamic binary, ordered and multiple discrete choice and censored panel data\nmodels are presented.",
        "authors": [
            "Andrew Chesher",
            "Adam M. Rosen",
            "Yuanqi Zhang"
        ],
        "categories": "econ.EM",
        "published": "2024-01-12T14:58:07Z",
        "updated": "2024-01-12T14:58:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.06264v2",
        "title": "Exposure effects are not automatically useful for policymaking",
        "abstract": "We thank Savje (2023) for a thought-provoking article and appreciate the\nopportunity to share our perspective as social scientists. In his article,\nSavje recommends misspecified exposure effects as a way to avoid strong\nassumptions about interference when analyzing the results of an experiment. In\nthis invited discussion, we highlight a limiation of Savje's recommendation:\nexposure effects are not generally useful for evaluating social policies\nwithout the strong assumptions that Savje seeks to avoid.",
        "authors": [
            "Eric Auerbach",
            "Jonathan Auerbach",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2024-01-11T21:22:01Z",
        "updated": "2024-01-15T17:15:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.05784v2",
        "title": "Covariance Function Estimation for High-Dimensional Functional Time Series with Dual Factor Structures",
        "abstract": "We propose a flexible dual functional factor model for modelling\nhigh-dimensional functional time series. In this model, a high-dimensional\nfully functional factor parametrisation is imposed on the observed functional\nprocesses, whereas a low-dimensional version (via series approximation) is\nassumed for the latent functional factors. We extend the classic principal\ncomponent analysis technique for the estimation of a low-rank structure to the\nestimation of a large covariance matrix of random functions that satisfies a\nnotion of (approximate) functional \"low-rank plus sparse\" structure; and\ngeneralise the matrix shrinkage method to functional shrinkage in order to\nestimate the sparse structure of functional idiosyncratic components. Under\nappropriate regularity conditions, we derive the large sample theory of the\ndeveloped estimators, including the consistency of the estimated factors and\nfunctional factor loadings and the convergence rates of the estimated matrices\nof covariance functions measured by various (functional) matrix norms.\nConsistent selection of the number of factors and a data-driven rule to choose\nthe shrinkage parameter are discussed. Simulation and empirical studies are\nprovided to demonstrate the finite-sample performance of the developed model\nand estimation methodology.",
        "authors": [
            "Chenlei Leng",
            "Degui Li",
            "Hanlin Shang",
            "Yingcun Xia"
        ],
        "categories": "econ.EM",
        "published": "2024-01-11T09:38:08Z",
        "updated": "2024-01-12T06:36:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.05517v1",
        "title": "On Efficient Inference of Causal Effects with Multiple Mediators",
        "abstract": "This paper provides robust estimators and efficient inference of causal\neffects involving multiple interacting mediators. Most existing works either\nimpose a linear model assumption among the mediators or are restricted to\nhandle conditionally independent mediators given the exposure. To overcome\nthese limitations, we define causal and individual mediation effects in a\ngeneral setting, and employ a semiparametric framework to develop quadruply\nrobust estimators for these causal effects. We further establish the asymptotic\nnormality of the proposed estimators and prove their local semiparametric\nefficiencies. The proposed method is empirically validated via simulated and\nreal datasets concerning psychiatric disorders in trauma survivors.",
        "authors": [
            "Haoyu Wei",
            "Hengrui Cai",
            "Chengchun Shi",
            "Rui Song"
        ],
        "categories": "stat.ME",
        "published": "2024-01-10T19:28:01Z",
        "updated": "2024-01-10T19:28:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.04849v1",
        "title": "A Deep Learning Representation of Spatial Interaction Model for Resilient Spatial Planning of Community Business Clusters",
        "abstract": "Existing Spatial Interaction Models (SIMs) are limited in capturing the\ncomplex and context-aware interactions between business clusters and trade\nareas. To address the limitation, we propose a SIM-GAT model to predict\nspatiotemporal visitation flows between community business clusters and their\ntrade areas. The model innovatively represents the integrated system of\nbusiness clusters, trade areas, and transportation infrastructure within an\nurban region using a connected graph. Then, a graph-based deep learning model,\ni.e., Graph AttenTion network (GAT), is used to capture the complexity and\ninterdependencies of business clusters. We developed this model with data\ncollected from the Miami metropolitan area in Florida. We then demonstrated its\neffectiveness in capturing varying attractiveness of business clusters to\ndifferent residential neighborhoods and across scenarios with an eXplainable AI\napproach. We contribute a novel method supplementing conventional SIMs to\npredict and analyze the dynamics of inter-connected community business\nclusters. The analysis results can inform data-evidenced and place-specific\nplanning strategies helping community business clusters better accommodate\ntheir customers across scenarios, and hence improve the resilience of community\nbusinesses.",
        "authors": [
            "Haiyan Hao",
            "Yan Wang"
        ],
        "categories": "econ.EM",
        "published": "2024-01-09T23:42:21Z",
        "updated": "2024-01-09T23:42:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.04803v1",
        "title": "IV Estimation of Panel Data Tobit Models with Normal Errors",
        "abstract": "Amemiya (1973) proposed a ``consistent initial estimator'' for the parameters\nin a censored regression model with normal errors. This paper demonstrates that\na similar approach can be used to construct moment conditions for\nfixed--effects versions of the model considered by Amemiya. This result\nsuggests estimators for models that have not previously been considered.",
        "authors": [
            "Bo E. Honore"
        ],
        "categories": "econ.EM",
        "published": "2024-01-09T20:05:25Z",
        "updated": "2024-01-09T20:05:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.04512v3",
        "title": "Robust Bayesian Method for Refutable Models",
        "abstract": "We propose a robust Bayesian method for economic models that can be rejected\nby some data distributions. The econometrician starts with a refutable\nstructural assumption which can be written as the intersection of several\nassumptions. To avoid the assumption refutable, the econometrician first takes\na stance on which assumption $j$ will be relaxed and considers a function $m_j$\nthat measures the deviation from the assumption $j$. She then specifies a set\nof prior beliefs $\\Pi_s$ whose elements share the same marginal distribution\n$\\pi_{m_j}$ which measures the likelihood of deviations from assumption $j$.\nCompared to the standard Bayesian method that specifies a single prior, the\nrobust Bayesian method allows the econometrician to take a stance only on the\nlikeliness of violation of assumption $j$ while leaving other features of the\nmodel unspecified. We show that many frequentist approaches to relax refutable\nassumptions are equivalent to particular choices of robust Bayesian prior sets,\nand thus we give a Bayesian interpretation to the frequentist methods. We use\nthe local average treatment effect ($LATE$) in the potential outcome framework\nas the leading illustrating example.",
        "authors": [
            "Moyu Liao"
        ],
        "categories": "econ.EM",
        "published": "2024-01-09T12:11:31Z",
        "updated": "2024-09-17T00:12:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.04200v2",
        "title": "Teacher bias or measurement error?",
        "abstract": "In many countries, teachers' track recommendations are used to allocate\nstudents to secondary school tracks. Previous studies have shown that students\nfrom families with low socioeconomic status (SES) receive lower track\nrecommendations than their peers from high SES families, conditional on\nstandardized test scores. It is often argued that this indicates teacher bias.\nHowever, this claim is invalid in the presence of measurement error in test\nscores. We discuss how measurement error in test scores generates a biased\ncoefficient of the conditional SES gap, and consider three empirical strategies\nto address this bias. Using administrative data from the Netherlands, we find\nthat measurement error explains 35 to 43% of the conditional SES gap in track\nrecommendations.",
        "authors": [
            "Thomas van Huizen",
            "Madelon Jacobs",
            "Matthijs Oosterveen"
        ],
        "categories": "econ.EM",
        "published": "2024-01-08T19:26:35Z",
        "updated": "2024-02-16T12:21:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.04050v1",
        "title": "Robust Estimation in Network Vector Autoregression with Nonstationary Regressors",
        "abstract": "This article studies identification and estimation for the network vector\nautoregressive model with nonstationary regressors. In particular, network\ndependence is characterized by a nonstochastic adjacency matrix. The\ninformation set includes a stationary regressand and a node-specific vector of\nnonstationary regressors, both observed at the same equally spaced time\nfrequencies. Our proposed econometric specification correponds to the NVAR\nmodel under time series nonstationarity which relies on the local-to-unity\nparametrization for capturing the unknown form of persistence of these\nnode-specific regressors. Robust econometric estimation is achieved using an\nIVX-type estimator and the asymptotic theory analysis for the augmented vector\nof regressors is studied based on a double asymptotic regime where both the\nnetwork size and the time dimension tend to infinity.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2024-01-08T17:40:22Z",
        "updated": "2024-01-08T17:40:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.03990v4",
        "title": "Identification with possibly invalid IVs",
        "abstract": "This paper proposes a novel identification strategy relying on\nquasi-instrumental variables (quasi-IVs). A quasi-IV is a relevant but possibly\ninvalid IV because it is not exogenous or not excluded. We show that a variety\nof models with discrete or continuous endogenous treatment which are usually\nidentified with an IV - quantile models with rank invariance, additive models\nwith homogenous treatment effects, and local average treatment effect models -\ncan be identified under the joint relevance of two complementary quasi-IVs\ninstead. To achieve identification, we complement one excluded but possibly\nendogenous quasi-IV (e.g., \"relevant proxies\" such as lagged treatment choice)\nwith one exogenous (conditional on the excluded quasi-IV) but possibly included\nquasi-IV (e.g., random assignment or exogenous market shocks). Our approach\nalso holds if any of the two quasi-IVs turns out to be a valid IV. In practice,\nbeing able to address endogeneity with complementary quasi-IVs instead of IVs\nis convenient since there are many applications where quasi-IVs are more\nreadily available. Difference-in-differences is a notable example: time is an\nexogenous quasi-IV while the group assignment acts as a complementary excluded\nquasi-IV.",
        "authors": [
            "Christophe Bruneel-Zupanc",
            "Jad Beyhum"
        ],
        "categories": "econ.EM",
        "published": "2024-01-08T16:11:22Z",
        "updated": "2024-10-22T06:31:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.03756v3",
        "title": "Adaptive Experimental Design for Policy Learning",
        "abstract": "Evidence-based targeting has been a topic of growing interest among the\npractitioners of policy and business. Formulating decision-maker's policy\nlearning as a fixed-budget best arm identification (BAI) problem with\ncontextual information, we study an optimal adaptive experimental design for\npolicy learning with multiple treatment arms. In the sampling stage, the\nplanner assigns treatment arms adaptively over sequentially arriving\nexperimental units upon observing their contextual information (covariates).\nAfter the experiment, the planner recommends an individualized assignment rule\nto the population. Setting the worst-case expected regret as the performance\ncriterion of adaptive sampling and recommended policies, we derive its\nasymptotic lower bounds, and propose a strategy, Adaptive Sampling-Policy\nLearning strategy (PLAS), whose leading factor of the regret upper bound aligns\nwith the lower bound as the size of experimental units increases.",
        "authors": [
            "Masahiro Kato",
            "Kyohei Okumura",
            "Takuya Ishihara",
            "Toru Kitagawa"
        ],
        "categories": "cs.LG",
        "published": "2024-01-08T09:29:07Z",
        "updated": "2024-02-08T17:41:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.03293v1",
        "title": "Counterfactuals in factor models",
        "abstract": "We study a new model where the potential outcomes, corresponding to the\nvalues of a (possibly continuous) treatment, are linked through common factors.\nThe factors can be estimated using a panel of regressors. We propose a\nprocedure to estimate time-specific and unit-specific average marginal effects\nin this context. Our approach can be used either with high-dimensional time\nseries or with large panels. It allows for treatment effects heterogenous\nacross time and units and is straightforward to implement since it only relies\non principal components analysis and elementary computations. We derive the\nasymptotic distribution of our estimator of the average marginal effect and\nhighlight its solid finite sample performance through a simulation exercise.\nThe approach can also be used to estimate average counterfactuals or adapted to\nan instrumental variables setting and we discuss these extensions. Finally, we\nillustrate our novel methodology through an empirical application on income\ninequality.",
        "authors": [
            "Jad Beyhum"
        ],
        "categories": "econ.EM",
        "published": "2024-01-06T20:10:53Z",
        "updated": "2024-01-06T20:10:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.02819v1",
        "title": "Roughness Signature Functions",
        "abstract": "Inspired by the activity signature introduced by Todorov and Tauchen (2010),\nwhich was used to measure the activity of a semimartingale, this paper\nintroduces the roughness signature function. The paper illustrates how it can\nbe used to determine whether a discretely observed process is generated by a\ncontinuous process that is rougher than a Brownian motion, a pure-jump process,\nor a combination of the two. Further, if a continuous rough process is present,\nthe function gives an estimate of the roughness index. This is done through an\nextensive simulation study, where we find that the roughness signature function\nworks as expected on rough processes. We further derive some asymptotic\nproperties of this new signature function. The function is applied empirically\nto three different volatility measures for the S&P500 index. The three measures\nare realized volatility, the VIX, and the option-extracted volatility estimator\nof Todorov (2019). The realized volatility and option-extracted volatility show\nsigns of roughness, with the option-extracted volatility appearing smoother\nthan the realized volatility, while the VIX appears to be driven by a\ncontinuous martingale with jumps.",
        "authors": [
            "Peter Christensen"
        ],
        "categories": "econ.EM",
        "published": "2024-01-05T14:07:01Z",
        "updated": "2024-01-05T14:07:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.01804v2",
        "title": "Efficient Computation of Confidence Sets Using Classification on Equidistributed Grids",
        "abstract": "Economic models produce moment inequalities, which can be used to form tests\nof the true parameters. Confidence sets (CS) of the true parameters are derived\nby inverting these tests. However, they often lack analytical expressions,\nnecessitating a grid search to obtain the CS numerically by retaining the grid\npoints that pass the test. When the statistic is not asymptotically pivotal,\nconstructing the critical value for each grid point in the parameter space adds\nto the computational burden. In this paper, we convert the computational issue\ninto a classification problem by using a support vector machine (SVM)\nclassifier. Its decision function provides a faster and more systematic way of\ndividing the parameter space into two regions: inside vs. outside of the\nconfidence set. We label those points in the CS as 1 and those outside as -1.\nResearchers can train the SVM classifier on a grid of manageable size and use\nit to determine whether points on denser grids are in the CS or not. We\nestablish certain conditions for the grid so that there is a tuning that allows\nus to asymptotically reproduce the test in the CS. This means that in the\nlimit, a point is classified as belonging to the confidence set if and only if\nit is labeled as 1 by the SVM.",
        "authors": [
            "Lujie Zhou"
        ],
        "categories": "econ.EM",
        "published": "2024-01-03T16:04:14Z",
        "updated": "2024-11-13T04:50:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.01645v2",
        "title": "Model Averaging and Double Machine Learning",
        "abstract": "This paper discusses pairing double/debiased machine learning (DDML) with\nstacking, a model averaging method for combining multiple candidate learners,\nto estimate structural parameters. In addition to conventional stacking, we\nconsider two stacking variants available for DDML: short-stacking exploits the\ncross-fitting step of DDML to substantially reduce the computational burden and\npooled stacking enforces common stacking weights over cross-fitting folds.\nUsing calibrated simulation studies and two applications estimating gender gaps\nin citations and wages, we show that DDML with stacking is more robust to\npartially unknown functional forms than common alternative approaches based on\nsingle pre-selected learners. We provide Stata and R software implementing our\nproposals.",
        "authors": [
            "Achim Ahrens",
            "Christian B. Hansen",
            "Mark E. Schaffer",
            "Thomas Wiemann"
        ],
        "categories": "econ.EM",
        "published": "2024-01-03T09:38:13Z",
        "updated": "2024-09-25T20:56:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.01565v2",
        "title": "Classification and Treatment Learning with Constraints via Composite Heaviside Optimization: a Progressive MIP Method",
        "abstract": "This paper proposes a Heaviside composite optimization approach and presents\na progressive (mixed) integer programming (PIP) method for solving multi-class\nclassification and multi-action treatment problems with constraints. A\nHeaviside composite function is a composite of a Heaviside function (i.e., the\nindicator function of either the open $( \\, 0,\\infty )$ or closed $[ \\,\n0,\\infty \\, )$ interval) with a possibly nondifferentiable function.\nModeling-wise, we show how Heaviside composite optimization provides a unified\nformulation for learning the optimal multi-class classification and\nmulti-action treatment rules, subject to rule-dependent constraints stipulating\na variety of domain restrictions. A Heaviside composite function has an\nequivalent discrete formulation, and the resulting optimization problem can in\nprinciple be solved by integer programming (IP) methods. Nevertheless, for\nconstrained learning problems with large data sets, a straightforward\napplication of off-the-shelf IP solvers is usually ineffective in achieving\nglobal optimality. To alleviate such a computational burden, our major\ncontribution is the proposal of the PIP method by leveraging the effectiveness\nof state-of-the-art IP solvers for problems of modest sizes. We provide the\ntheoretical advantage of the PIP method with the connection to continuous\noptimization and show that the computed solution is locally optimal for a broad\nclass of Heaviside composite optimization problems. The numerical performance\nof the PIP method is demonstrated by extensive computational experimentation.",
        "authors": [
            "Yue Fang",
            "Junyi Liu",
            "Jong-Shi Pang"
        ],
        "categories": "math.OC",
        "published": "2024-01-03T06:39:18Z",
        "updated": "2024-01-05T04:04:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.01064v1",
        "title": "Robust Inference for Multiple Predictive Regressions with an Application on Bond Risk Premia",
        "abstract": "We propose a robust hypothesis testing procedure for the predictability of\nmultiple predictors that could be highly persistent. Our method improves the\npopular extended instrumental variable (IVX) testing (Phillips and Lee, 2013;\nKostakis et al., 2015) in that, besides addressing the two bias effects found\nin Hosseinkouchack and Demetrescu (2021), we find and deal with the\nvariance-enlargement effect. We show that two types of higher-order terms\ninduce these distortion effects in the test statistic, leading to significant\nover-rejection for one-sided tests and tests in multiple predictive\nregressions. Our improved IVX-based test includes three steps to tackle all the\nissues above regarding finite sample bias and variance terms. Thus, the test\nstatistics perform well in size control, while its power performance is\ncomparable with the original IVX. Monte Carlo simulations and an empirical\nstudy on the predictability of bond risk premia are provided to demonstrate the\neffectiveness of the newly proposed approach.",
        "authors": [
            "Xiaosai Liao",
            "Xinjue Li",
            "Qingliang Fan"
        ],
        "categories": "stat.ME",
        "published": "2024-01-02T06:56:10Z",
        "updated": "2024-01-02T06:56:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.00618v3",
        "title": "Changes-in-Changes for Ordered Choice Models: Too Many \"False Zeros\"?",
        "abstract": "In this paper, we develop a Difference-in-Differences model for discrete,\nordered outcomes, building upon elements from a continuous Changes-in-Changes\nmodel. We focus on outcomes derived from self-reported survey data eliciting\nsocially undesirable, illegal, or stigmatized behaviors like tax evasion or\nsubstance abuse, where too many \"false zeros\", or more broadly, underreporting\nare likely. We start by providing a characterization for parallel trends within\na general threshold-crossing model. We then propose a partial and point\nidentification framework for different distributional treatment effects when\nthe outcome is subject to underreporting. Applying our methodology, we\ninvestigate the impact of recreational marijuana legalization for adults in\nseveral U.S. states on the short-term consumption behavior of 8th-grade\nhigh-school students. The results indicate small, but significant increases in\nconsumption probabilities at each level. These effects are further amplified\nupon accounting for misreporting.",
        "authors": [
            "Daniel Gutknecht",
            "Cenchen Liu"
        ],
        "categories": "econ.EM",
        "published": "2024-01-01T00:12:56Z",
        "updated": "2024-11-23T10:24:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.10261v1",
        "title": "How industrial clusters influence the growth of the regional GDP: A spatial-approach",
        "abstract": "In this paper, we employ spatial econometric methods to analyze panel data\nfrom German NUTS 3 regions. Our goal is to gain a deeper understanding of the\nsignificance and interdependence of industry clusters in shaping the dynamics\nof GDP. To achieve a more nuanced spatial differentiation, we introduce\nindicator matrices for each industry sector which allows for extending the\nspatial Durbin model to a new version of it. This approach is essential due to\nboth the economic importance of these sectors and the potential issue of\nomitted variables. Failing to account for industry sectors can lead to omitted\nvariable bias and estimation problems. To assess the effects of the major\nindustry sectors, we incorporate eight distinct branches of industry into our\nanalysis. According to prevailing economic theory, these clusters should have a\npositive impact on the regions they are associated with. Our findings indeed\nreveal highly significant impacts, which can be either positive or negative, of\nspecific sectors on local GDP growth. Spatially, we observe that direct and\nindirect effects can exhibit opposite signs, indicative of heightened\ncompetitiveness within and between industry sectors. Therefore, we recommend\nthat industry sectors should be taken into consideration when conducting\nspatial analysis of GDP. Doing so allows for a more comprehensive understanding\nof the economic dynamics at play.",
        "authors": [
            "Vahidin Jeleskovic",
            "Steffen Loeber"
        ],
        "categories": "econ.GN",
        "published": "2023-12-31T22:29:57Z",
        "updated": "2023-12-31T22:29:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.00264v3",
        "title": "Identification of Nonlinear Dynamic Panels under Partial Stationarity",
        "abstract": "This paper studies identification for a wide range of nonlinear panel data\nmodels, including binary choice, ordered response, and other types of limited\ndependent variable models. Our approach accommodates dynamic models with any\nnumber of lagged dependent variables as well as other types of (potentially\ncontemporary) endogeneity. Our identification strategy relies on a partial\nstationarity condition, which not only allows for an unknown distribution of\nerrors but also for temporal dependencies in errors. We derive partial\nidentification results under flexible model specifications and provide\nadditional support conditions for point identification. We demonstrate the\nrobust finite-sample performance of our approach using Monte Carlo simulations,\nand apply the approach to analyze the empirical application of income\ncategories using various ordered choice models.",
        "authors": [
            "Wayne Yuan Gao",
            "Rui Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-12-30T15:33:10Z",
        "updated": "2024-05-01T12:13:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.00249v2",
        "title": "Forecasting CPI inflation under economic policy and geopolitical uncertainties",
        "abstract": "Forecasting consumer price index (CPI) inflation is of paramount importance\nfor both academics and policymakers at the central banks. This study introduces\na filtered ensemble wavelet neural network (FEWNet) to forecast CPI inflation,\nwhich is tested on BRIC countries. FEWNet breaks down inflation data into high\nand low-frequency components using wavelets and utilizes them along with other\neconomic factors (economic policy uncertainty and geopolitical risk) to produce\nforecasts. All the wavelet-transformed series and filtered exogenous variables\nare fed into downstream autoregressive neural networks to make the final\nensemble forecast. Theoretically, we show that FEWNet reduces the empirical\nrisk compared to fully connected autoregressive neural networks. FEWNet is more\naccurate than other forecasting methods and can also estimate the uncertainty\nin its predictions due to its capacity to effectively capture non-linearities\nand long-range dependencies in the data through its adaptable architecture.\nThis makes FEWNet a valuable tool for central banks to manage inflation.",
        "authors": [
            "Shovon Sengupta",
            "Tanujit Chakraborty",
            "Sunny Kumar Singh"
        ],
        "categories": "econ.EM",
        "published": "2023-12-30T14:34:22Z",
        "updated": "2024-07-02T14:46:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.17676v1",
        "title": "Robust Inference in Panel Data Models: Some Effects of Heteroskedasticity and Leveraged Data in Small Samples",
        "abstract": "With the violation of the assumption of homoskedasticity, least squares\nestimators of the variance become inefficient and statistical inference\nconducted with invalid standard errors leads to misleading rejection rates.\nDespite a vast cross-sectional literature on the downward bias of robust\nstandard errors, the problem is not extensively covered in the panel data\nframework. We investigate the consequences of the simultaneous presence of\nsmall sample size, heteroskedasticity and data points that exhibit extreme\nvalues in the covariates ('good leverage points') on the statistical inference.\nFocusing on one-way linear panel data models, we examine asymptotic and finite\nsample properties of a battery of heteroskedasticity-consistent estimators\nusing Monte Carlo simulations. We also propose a hybrid estimator of the\nvariance-covariance matrix. Results show that conventional standard errors are\nalways dominated by more conservative estimators of the variance, especially in\nsmall samples. In addition, all types of HC standard errors have excellent\nperformances in terms of size and power tests under homoskedasticity.",
        "authors": [
            "Annalivia Polselli"
        ],
        "categories": "econ.EM",
        "published": "2023-12-29T16:43:19Z",
        "updated": "2023-12-29T16:43:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.17623v2",
        "title": "Decision Theory for Treatment Choice Problems with Partial Identification",
        "abstract": "We apply classical statistical decision theory to a large class of treatment\nchoice problems with partial identification, revealing important theoretical\nand practical challenges but also interesting research opportunities. The\nchallenges are: In a general class of problems with Gaussian likelihood, all\ndecision rules are admissible; it is maximin-welfare optimal to ignore all\ndata; and, for severe enough partial identification, there are infinitely many\nminimax-regret optimal decision rules, all of which sometimes randomize the\npolicy recommendation. The opportunities are: We introduce a profiled regret\ncriterion that can reveal important differences between rules and render some\nof them inadmissible; and we uniquely characterize the minimax-regret optimal\nrule that least frequently randomizes. We apply our results to aggregation of\nexperimental estimates for policy adoption, to extrapolation of Local Average\nTreatment Effects, and to policy making in the presence of omitted variable\nbias.",
        "authors": [
            "Jos\u00e9 Luis Montiel Olea",
            "Chen Qiu",
            "J\u00f6rg Stoye"
        ],
        "categories": "econ.EM",
        "published": "2023-12-29T14:27:52Z",
        "updated": "2024-08-07T14:10:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.17061v2",
        "title": "Bayesian Analysis of High Dimensional Vector Error Correction Model",
        "abstract": "Vector Error Correction Model (VECM) is a classic method to analyse\ncointegration relationships amongst multivariate non-stationary time series. In\nthis paper, we focus on high dimensional setting and seek for\nsample-size-efficient methodology to determine the level of cointegration. Our\ninvestigation centres at a Bayesian approach to analyse the cointegration\nmatrix, henceforth determining the cointegration rank. We design two algorithms\nand implement them on simulated examples, yielding promising results\nparticularly when dealing with high number of variables and relatively low\nnumber of observations. Furthermore, we extend this methodology to empirically\ninvestigate the constituents of the S&P 500 index, where low-volatility\nportfolios can be found during both in-sample training and out-of-sample\ntesting periods.",
        "authors": [
            "Parley R Yang",
            "Alexander Y Shestopaloff"
        ],
        "categories": "stat.ME",
        "published": "2023-12-28T15:20:29Z",
        "updated": "2024-03-12T15:07:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.16927v1",
        "title": "Development of Choice Model for Brand Evaluation",
        "abstract": "Consumer choice modeling takes center stage as we delve into understanding\nhow personal preferences of decision makers (customers) for products influence\ndemand at the level of the individual. The contemporary choice theory is built\nupon the characteristics of the decision maker, alternatives available for the\nchoice of the decision maker, the attributes of the available alternatives and\ndecision rules that the decision maker uses to make a choice. The choice set in\nour research is represented by six major brands (products) of laundry\ndetergents in the Japanese market. We use the panel data of the purchases of 98\nhouseholds to which we apply the hierarchical probit model, facilitated by a\nMarkov Chain Monte Carlo simulation (MCMC) in order to evaluate the brand\nvalues of six brands. The applied model also allows us to evaluate the tangible\nand intangible brand values. These evaluated metrics help us to assess the\nbrands based on their tangible and intangible characteristics. Moreover,\nconsumer choice modeling also provides a framework for assessing the\nenvironmental performance of laundry detergent brands as the model uses the\ninformation on components (physical attributes) of laundry detergents.",
        "authors": [
            "Marina Kholod",
            "Nikita Mokrenko"
        ],
        "categories": "econ.EM",
        "published": "2023-12-28T09:51:46Z",
        "updated": "2023-12-28T09:51:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.16707v1",
        "title": "Modeling Systemic Risk: A Time-Varying Nonparametric Causal Inference Framework",
        "abstract": "We propose a nonparametric and time-varying directed information graph\n(TV-DIG) framework to estimate the evolving causal structure in time series\nnetworks, thereby addressing the limitations of traditional econometric models\nin capturing high-dimensional, nonlinear, and time-varying interconnections\namong series. This framework employs an information-theoretic measure rooted in\na generalized version of Granger-causality, which is applicable to both linear\nand nonlinear dynamics. Our framework offers advancements in measuring systemic\nrisk and establishes meaningful connections with established econometric\nmodels, including vector autoregression and switching models. We evaluate the\nefficacy of our proposed model through simulation experiments and empirical\nanalysis, reporting promising results in recovering simulated time-varying\nnetworks with nonlinear and multivariate structures. We apply this framework to\nidentify and monitor the evolution of interconnectedness and systemic risk\namong major assets and industrial sectors within the financial network. We\nfocus on cryptocurrencies' potential systemic risks to financial stability,\nincluding spillover effects on other sectors during crises like the COVID-19\npandemic and the Federal Reserve's 2020 emergency response. Our findings\nreveals significant, previously underrecognized pre-2020 influences of\ncryptocurrencies on certain financial sectors, highlighting their potential\nsystemic risks and offering a systematic approach in tracking evolving\ncross-sector interactions within financial networks.",
        "authors": [
            "Jalal Etesami",
            "Ali Habibnia",
            "Negar Kiyavash"
        ],
        "categories": "econ.EM",
        "published": "2023-12-27T20:09:57Z",
        "updated": "2023-12-27T20:09:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.16489v1",
        "title": "Best-of-Both-Worlds Linear Contextual Bandits",
        "abstract": "This study investigates the problem of $K$-armed linear contextual bandits,\nan instance of the multi-armed bandit problem, under an adversarial corruption.\nAt each round, a decision-maker observes an independent and identically\ndistributed context and then selects an arm based on the context and past\nobservations. After selecting an arm, the decision-maker incurs a loss\ncorresponding to the selected arm. The decision-maker aims to minimize the\ncumulative loss over the trial. The goal of this study is to develop a strategy\nthat is effective in both stochastic and adversarial environments, with\ntheoretical guarantees. We first formulate the problem by introducing a novel\nsetting of bandits with adversarial corruption, referred to as the contextual\nadversarial regime with a self-bounding constraint. We assume linear models for\nthe relationship between the loss and the context. Then, we propose a strategy\nthat extends the RealLinExp3 by Neu & Olkhovskaya (2020) and the\nFollow-The-Regularized-Leader (FTRL). The regret of our proposed algorithm is\nshown to be upper-bounded by $O\\left(\\min\\left\\{\\frac{(\\log(T))^3}{\\Delta_{*}}\n+ \\sqrt{\\frac{C(\\log(T))^3}{\\Delta_{*}}},\\ \\\n\\sqrt{T}(\\log(T))^2\\right\\}\\right)$, where $T \\in\\mathbb{N}$ is the number of\nrounds, $\\Delta_{*} > 0$ is the constant minimum gap between the best and\nsuboptimal arms for any context, and $C\\in[0, T] $ is an adversarial corruption\nparameter. This regret upper bound implies\n$O\\left(\\frac{(\\log(T))^3}{\\Delta_{*}}\\right)$ in a stochastic environment and\nby $O\\left( \\sqrt{T}(\\log(T))^2\\right)$ in an adversarial environment. We refer\nto our strategy as the Best-of-Both-Worlds (BoBW) RealFTRL, due to its\ntheoretical guarantees in both stochastic and adversarial regimes.",
        "authors": [
            "Masahiro Kato",
            "Shinji Ito"
        ],
        "categories": "cs.LG",
        "published": "2023-12-27T09:32:18Z",
        "updated": "2023-12-27T09:32:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.16307v2",
        "title": "Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration",
        "abstract": "We consider the setting of synthetic control methods (SCMs), a canonical\napproach used to estimate the treatment effect on the treated in a panel data\nsetting. We shed light on a frequently overlooked but ubiquitous assumption\nmade in SCMs of \"overlap\": a treated unit can be written as some combination --\ntypically, convex or linear combination -- of the units that remain under\ncontrol. We show that if units select their own interventions, and there is\nsufficiently large heterogeneity between units that prefer different\ninterventions, overlap will not hold. We address this issue by proposing a\nframework which incentivizes units with different preferences to take\ninterventions they would not normally consider. Specifically, leveraging tools\nfrom information design and online learning, we propose a SCM that incentivizes\nexploration in panel data settings by providing incentive-compatible\nintervention recommendations to units. We establish this estimator obtains\nvalid counterfactual estimates without the need for an a priori overlap\nassumption. We extend our results to the setting of synthetic interventions,\nwhere the goal is to produce counterfactual outcomes under all interventions,\nnot just control. Finally, we provide two hypothesis tests for determining\nwhether unit overlap holds for a given panel dataset.",
        "authors": [
            "Daniel Ngo",
            "Keegan Harris",
            "Anish Agarwal",
            "Vasilis Syrgkanis",
            "Zhiwei Steven Wu"
        ],
        "categories": "econ.EM",
        "published": "2023-12-26T19:25:11Z",
        "updated": "2024-02-13T22:45:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.16099v1",
        "title": "Direct Multi-Step Forecast based Comparison of Nested Models via an Encompassing Test",
        "abstract": "We introduce a novel approach for comparing out-of-sample multi-step\nforecasts obtained from a pair of nested models that is based on the forecast\nencompassing principle. Our proposed approach relies on an alternative way of\ntesting the population moment restriction implied by the forecast encompassing\nprinciple and that links the forecast errors from the two competing models in a\nparticular way. Its key advantage is that it is able to bypass the variance\ndegeneracy problem afflicting model based forecast comparisons across nested\nmodels. It results in a test statistic whose limiting distribution is standard\nnormal and which is particularly simple to construct and can accommodate both\nsingle period and longer-horizon prediction comparisons. Inferences are also\nshown to be robust to different predictor types, including stationary,\nhighly-persistent and purely deterministic processes. Finally, we illustrate\nthe use of our proposed approach through an empirical application that explores\nthe role of global inflation in enhancing individual country specific inflation\nforecasts.",
        "authors": [
            "Jean-Yves Pitarakis"
        ],
        "categories": "econ.EM",
        "published": "2023-12-26T15:55:48Z",
        "updated": "2023-12-26T15:55:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.15999v1",
        "title": "Pricing with Contextual Elasticity and Heteroscedastic Valuation",
        "abstract": "We study an online contextual dynamic pricing problem, where customers decide\nwhether to purchase a product based on its features and price. We introduce a\nnovel approach to modeling a customer's expected demand by incorporating\nfeature-based price elasticity, which can be equivalently represented as a\nvaluation with heteroscedastic noise. To solve the problem, we propose a\ncomputationally efficient algorithm called \"Pricing with Perturbation (PwP)\",\nwhich enjoys an $O(\\sqrt{dT\\log T})$ regret while allowing arbitrary\nadversarial input context sequences. We also prove a matching lower bound at\n$\\Omega(\\sqrt{dT})$ to show the optimality regarding $d$ and $T$ (up to $\\log\nT$ factors). Our results shed light on the relationship between contextual\nelasticity and heteroscedastic valuation, providing insights for effective and\npractical pricing strategies.",
        "authors": [
            "Jianyu Xu",
            "Yu-Xiang Wang"
        ],
        "categories": "cs.LG",
        "published": "2023-12-26T11:07:37Z",
        "updated": "2023-12-26T11:07:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.15624v2",
        "title": "Negative Control Falsification Tests for Instrumental Variable Designs",
        "abstract": "We develop theoretical foundations for widely used falsification tests for\ninstrumental variable (IV) designs. We characterize these tests as conditional\nindependence tests between negative control variables - proxies for potential\nthreats - and either the IV or the outcome. We find that conventional\napplications of these falsification tests would flag problems in exogenous IV\ndesigns, and propose simple solutions to avoid this. We also propose new\nfalsification tests that incorporate new types of negative control variables or\nalternative statistical tests. Finally, we illustrate that under stronger\nassumptions, negative control variables can also be used for bias correction.",
        "authors": [
            "Oren Danieli",
            "Daniel Nevo",
            "Itai Walk",
            "Bar Weinstein",
            "Dan Zeltzer"
        ],
        "categories": "econ.EM",
        "published": "2023-12-25T06:14:40Z",
        "updated": "2024-05-09T03:36:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.15595v2",
        "title": "Zero-Inflated Bandits",
        "abstract": "Many real applications of bandits have sparse non-zero rewards, leading to\nslow learning speed. Using problem-specific structures for careful distribution\nmodeling is known as critical to estimation efficiency in statistics, yet is\nunder-explored in bandits. We initiate the study of zero-inflated bandits,\nwhere the reward is modeled as a classic semi-parametric distribution called\nzero-inflated distribution. We design Upper Confidence Bound- and Thompson\nSampling-type algorithms for this specific structure. We derive the regret\nbounds under both multi-armed bandits with general reward assumptions and\ncontextual generalized linear bandit with sub-Gaussian rewards. In many\nsettings, the regret rates of our algorithms are either minimax optimal or\nstate-of-the-art. The superior empirical performance of our methods is\ndemonstrated via numerical studies.",
        "authors": [
            "Haoyu Wei",
            "Runzhe Wan",
            "Lei Shi",
            "Rui Song"
        ],
        "categories": "stat.ML",
        "published": "2023-12-25T03:13:21Z",
        "updated": "2024-10-10T19:17:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.15524v1",
        "title": "The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive potential to\nsimulate human behavior. Using a causal inference framework, we empirically and\ntheoretically analyze the challenges of conducting LLM-simulated experiments,\nand explore potential solutions. In the context of demand estimation, we show\nthat variations in the treatment included in the prompt (e.g., price of focal\nproduct) can cause variations in unspecified confounding factors (e.g., price\nof competitors, historical prices, outside temperature), introducing\nendogeneity and yielding implausibly flat demand curves. We propose a\ntheoretical framework suggesting this endogeneity issue generalizes to other\ncontexts and won't be fully resolved by merely improving the training data.\nUnlike real experiments where researchers assign pre-existing units across\nconditions, LLMs simulate units based on the entire prompt, which includes the\ndescription of the treatment. Therefore, due to associations in the training\ndata, the characteristics of individuals and environments simulated by the LLM\ncan be affected by the treatment assignment. We explore two potential\nsolutions. The first specifies all contextual variables that affect both\ntreatment and outcome, which we demonstrate to be challenging for a\ngeneral-purpose LLM. The second explicitly specifies the source of treatment\nvariation in the prompt given to the LLM (e.g., by informing the LLM that the\nstore is running an experiment). While this approach only allows the estimation\nof a conditional average treatment effect that depends on the specific\nexperimental design, it provides valuable directional results for exploratory\nanalysis.",
        "authors": [
            "George Gui",
            "Olivier Toubia"
        ],
        "categories": "cs.AI",
        "published": "2023-12-24T16:32:35Z",
        "updated": "2023-12-24T16:32:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.15494v2",
        "title": "Variable Selection in High Dimensional Linear Regressions with Parameter Instability",
        "abstract": "This paper considers the problem of variable selection allowing for parameter\ninstability. It distinguishes between signal and pseudo-signal variables that\nare correlated with the target variable, and noise variables that are not, and\ninvestigate the asymptotic properties of the One Covariate at a Time Multiple\nTesting (OCMT) method proposed by Chudik et al. (2018) under parameter\ninsatiability. It is established that OCMT continues to asymptotically select\nan approximating model that includes all the signals and none of the noise\nvariables. Properties of post selection regressions are also investigated, and\nin-sample fit of the selected regression is shown to have the oracle property.\nThe theoretical results support the use of unweighted observations at the\nselection stage of OCMT, whilst applying down-weighting of observations only at\nthe forecasting stage. Monte Carlo and empirical applications show that OCMT\nwithout down-weighting at the selection stage yields smaller mean squared\nforecast errors compared to Lasso, Adaptive Lasso, and boosting.",
        "authors": [
            "Alexander Chudik",
            "M. Hashem Pesaran",
            "Mahrad Sharifvaghefi"
        ],
        "categories": "econ.EM",
        "published": "2023-12-24T14:52:23Z",
        "updated": "2024-07-16T16:47:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.16214v4",
        "title": "Stochastic Equilibrium the Lucas Critique and Keynesian Economics",
        "abstract": "In this paper, a mathematically rigorous solution overturns existing wisdom\nregarding New Keynesian Dynamic Stochastic General Equilibrium. I develop a\nformal concept of stochastic equilibrium. I prove uniqueness and necessity,\nwhen agents are patient, with general application. Existence depends on\nappropriately specified eigenvalue conditions. Otherwise, no solution of any\nkind exists. I construct the equilibrium with Calvo pricing. I provide novel\ncomparative statics with the non-stochastic model of mathematical significance.\nI uncover a bifurcation between neighbouring stochastic systems and\napproximations taken from the Zero Inflation Non-Stochastic Steady State\n(ZINSS). The correct Phillips curve agrees with the zero limit from the trend\ninflation framework. It contains a large lagged inflation coefficient and a\nsmall response to expected inflation. Price dispersion can be first or second\norder depending how shocks are scaled. The response to the output gap is always\nmuted and is zero at standard parameters. A neutrality result is presented to\nexplain why and align Calvo with Taylor pricing. Present and lagged demand\nshocks enter the Phillips curve so there is no Divine Coincidence and the\nsystem is identified from structural shocks alone. The lagged inflation slope\nis increasing in the inflation response, embodying substantive policy\ntrade-offs. The Taylor principle is reversed, inactive settings are necessary,\npointing towards inertial policy. The observational equivalence idea of the\nLucas critique is disproven. The bifurcation results from the breakdown of the\nconstraints implied by lagged nominal rigidity, associated with cross-equation\ncancellation possible only at ZINSS. There is a dual relationship between\nrestrictions on the econometrician and constraints on repricing firms. Thus, if\nthe model is correct, goodness of fit will jump.",
        "authors": [
            "David Staines"
        ],
        "categories": "econ.TH",
        "published": "2023-12-23T22:59:33Z",
        "updated": "2024-06-03T12:53:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.15119v2",
        "title": "Functional CLTs for subordinated L\u00e9vy models in physics, finance, and econometrics",
        "abstract": "We present a simple unifying treatment of a broad class of applications from\nstatistical mechanics, econometrics, mathematical finance, and insurance\nmathematics, where (possibly subordinated) L\\'evy noise arises as a scaling\nlimit of some form of continuous-time random walk (CTRW). For each application,\nit is natural to rely on weak convergence results for stochastic integrals on\nSkorokhod space in Skorokhod's J1 or M1 topologies. As compared to earlier and\nentirely separate works, we are able to give a more streamlined account while\nalso allowing for greater generality and providing important new insights. For\neach application, we first elucidate how the fundamental conclusions for J1\nconvergent CTRWs emerge as special cases of the same general principles, and we\nthen illustrate how the specific settings give rise to different results for\nstrictly M1 convergent CTRWs.",
        "authors": [
            "Andreas S\u00f8jmark",
            "Fabrice Wunderlich"
        ],
        "categories": "math.PR",
        "published": "2023-12-22T23:39:26Z",
        "updated": "2024-01-25T10:04:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.14325v1",
        "title": "Exploring Distributions of House Prices and House Price Indices",
        "abstract": "We use house prices (HP) and house price indices (HPI) as a proxy to income\ndistribution. Specifically, we analyze sale prices in the 1970-2010 window of\nover 116,000 single-family homes in Hamilton County, Ohio, including Cincinnati\nmetro area of about 2.2 million people. We also analyze HPI, published by\nFederal Housing Finance Agency (FHFA), for nearly 18,000 US ZIP codes that\ncover a period of over 40 years starting in 1980's. If HP can be viewed as a\nfirst derivative of income, HPI can be viewed as its second derivative. We use\ngeneralized beta (GB) family of functions to fit distributions of HP and HPI\nsince GB naturally arises from the models of economic exchange described by\nstochastic differential equations. Our main finding is that HP and multi-year\nHPI exhibit a negative Dragon King (nDK) behavior, wherein power-law\ndistribution tail gives way to an abrupt decay to a finite upper limit value,\nwhich is similar to our recent findings for realized volatility of S\\&P500\nindex in the US stock market. This type of tail behavior is best fitted by a\nmodified GB (mGB) distribution. Tails of single-year HPI appear to show more\nconsistency with power-law behavior, which is better described by a GB Prime\n(GB2) distribution. We supplement full distribution fits by mGB and GB2 with\ndirect linear fits (LF) of the tails. Our numerical procedure relies on\nevaluation of confidence intervals (CI) of the fits, as well as of p-values\nthat give the likelihood that data come from the fitted distributions.",
        "authors": [
            "Jiong Liu",
            "Hamed Farahani",
            "R. A. Serota"
        ],
        "categories": "econ.EM",
        "published": "2023-12-21T22:38:24Z",
        "updated": "2023-12-21T22:38:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.14095v1",
        "title": "RetailSynth: Synthetic Data Generation for Retail AI Systems Evaluation",
        "abstract": "Significant research effort has been devoted in recent years to developing\npersonalized pricing, promotions, and product recommendation algorithms that\ncan leverage rich customer data to learn and earn. Systematic benchmarking and\nevaluation of these causal learning systems remains a critical challenge, due\nto the lack of suitable datasets and simulation environments. In this work, we\npropose a multi-stage model for simulating customer shopping behavior that\ncaptures important sources of heterogeneity, including price sensitivity and\npast experiences. We embedded this model into a working simulation environment\n-- RetailSynth. RetailSynth was carefully calibrated on publicly available\ngrocery data to create realistic synthetic shopping transactions. Multiple\npricing policies were implemented within the simulator and analyzed for impact\non revenue, category penetration, and customer retention. Applied researchers\ncan use RetailSynth to validate causal demand models for multi-category retail\nand to incorporate realistic price sensitivity into emerging benchmarking\nsuites for personalized pricing, promotions, and product recommendations.",
        "authors": [
            "Yu Xia",
            "Ali Arian",
            "Sriram Narayanamoorthy",
            "Joshua Mabry"
        ],
        "categories": "stat.AP",
        "published": "2023-12-21T18:17:16Z",
        "updated": "2023-12-21T18:17:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.13939v1",
        "title": "Binary Endogenous Treatment in Stochastic Frontier Models with an Application to Soil Conservation in El Salvador",
        "abstract": "Improving the productivity of the agricultural sector is part of one of the\nSustainable Development Goals set by the United Nations. To this end, many\ninternational organizations have funded training and technology transfer\nprograms that aim to promote productivity and income growth, fight poverty and\nenhance food security among smallholder farmers in developing countries.\nStochastic production frontier analysis can be a useful tool when evaluating\nthe effectiveness of these programs. However, accounting for treatment\nendogeneity, often intrinsic to these interventions, only recently has received\nany attention in the stochastic frontier literature. In this work, we extend\nthe classical maximum likelihood estimation of stochastic production frontier\nmodels by allowing both the production frontier and inefficiency to depend on a\npotentially endogenous binary treatment. We use instrumental variables to\ndefine an assignment mechanism for the treatment, and we explicitly model the\ndensity of the first and second-stage composite error terms. We provide\nempirical evidence of the importance of controlling for endogeneity in this\nsetting using farm-level data from a soil conservation program in El Salvador.",
        "authors": [
            "Samuele Centorrino",
            "Maria P\u00e9rez-Urdiales",
            "Boris Bravo-Ureta",
            "Alan J. Wall"
        ],
        "categories": "econ.EM",
        "published": "2023-12-21T15:32:31Z",
        "updated": "2023-12-21T15:32:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.13195v2",
        "title": "Principal Component Copulas for Capital Modelling and Systemic Risk",
        "abstract": "We introduce a class of copulas that we call Principal Component Copulas\n(PCCs). This class combines the strong points of copula-based techniques with\nprincipal component-based models, which results in flexibility when modelling\ntail dependence along the most important directions in high-dimensional data.\nWe obtain theoretical results for PCCs that are important for practical\napplications. In particular, we derive tractable expressions for the\nhigh-dimensional copula density, which can be represented in terms of\ncharacteristic functions. We also develop algorithms to perform Maximum\nLikelihood and Generalized Method of Moment estimation in high-dimensions and\nshow very good performance in simulation experiments. Finally, we apply the\ncopula to the international stock market in order to study systemic risk. We\nfind that PCCs lead to excellent performance on measures of systemic risk due\nto their ability to distinguish between parallel market movements, which\nincrease systemic risk, and orthogonal movements, which reduce systemic risk.\nAs a result, we consider the PCC promising for internal capital models, which\nfinancial institutions use to protect themselves against systemic risk.",
        "authors": [
            "K. B. Gubbels",
            "J. Y. Ypma",
            "C. W. Oosterlee"
        ],
        "categories": "stat.ME",
        "published": "2023-12-20T17:11:34Z",
        "updated": "2024-12-10T10:03:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.14191v2",
        "title": "Noisy Measurements Are Important, the Design of Census Products Is Much More Important",
        "abstract": "McCartan et al. (2023) call for \"making differential privacy work for census\ndata users.\" This commentary explains why the 2020 Census Noisy Measurement\nFiles (NMFs) are not the best focus for that plea. The August 2021 letter from\n62 prominent researchers asking for production of the direct output of the\ndifferential privacy system deployed for the 2020 Census signaled the\nengagement of the scholarly community in the design of decennial census data\nproducts. NMFs, the raw statistics produced by the 2020 Census Disclosure\nAvoidance System before any post-processing, are one component of that\ndesign-the query strategy output. The more important component is the query\nworkload output-the statistics released to the public. Optimizing the query\nworkload-the Redistricting Data (P.L. 94-171) Summary File, specifically-could\nallow the privacy-loss budget to be more effectively managed. There could be\nfewer noisy measurements, no post-processing bias, and direct estimates of the\nuncertainty from disclosure avoidance for each published statistic.",
        "authors": [
            "John M. Abowd"
        ],
        "categories": "cs.CR",
        "published": "2023-12-20T15:43:04Z",
        "updated": "2024-05-01T15:55:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.12741v2",
        "title": "Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed Gaussian Bandits with Unknown Variances",
        "abstract": "We address the problem of best arm identification (BAI) with a fixed budget\nfor two-armed Gaussian bandits. In BAI, given multiple arms, we aim to find the\nbest arm, an arm with the highest expected reward, through an adaptive\nexperiment. Kaufmann et al. (2016) develops a lower bound for the probability\nof misidentifying the best arm. They also propose a strategy, assuming that the\nvariances of rewards are known, and show that it is asymptotically optimal in\nthe sense that its probability of misidentification matches the lower bound as\nthe budget approaches infinity. However, an asymptotically optimal strategy is\nunknown when the variances are unknown. For this open issue, we propose a\nstrategy that estimates variances during an adaptive experiment and draws arms\nwith a ratio of the estimated standard deviations. We refer to this strategy as\nthe Neyman Allocation (NA)-Augmented Inverse Probability weighting (AIPW)\nstrategy. We then demonstrate that this strategy is asymptotically optimal by\nshowing that its probability of misidentification matches the lower bound when\nthe budget approaches infinity, and the gap between the expected rewards of two\narms approaches zero (small-gap regime). Our results suggest that under the\nworst-case scenario characterized by the small-gap regime, our strategy, which\nemploys estimated variance, is asymptotically optimal even when the variances\nare unknown.",
        "authors": [
            "Masahiro Kato"
        ],
        "categories": "cs.LG",
        "published": "2023-12-20T03:28:49Z",
        "updated": "2024-03-17T06:00:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.11710v1",
        "title": "Real-time monitoring with RCA models",
        "abstract": "We propose a family of weighted statistics based on the CUSUM process of the\nWLS residuals for the online detection of changepoints in a Random Coefficient\nAutoregressive model, using both the standard CUSUM and the Page-CUSUM process.\nWe derive the asymptotics under the null of no changepoint for all possible\nweighing schemes, including the case of the standardised CUSUM, for which we\nderive a Darling-Erdos-type limit theorem; our results guarantee the\nprocedure-wise size control under both an open-ended and a closed-ended\nmonitoring. In addition to considering the standard RCA model with no\ncovariates, we also extend our results to the case of exogenous regressors. Our\nresults can be applied irrespective of (and with no prior knowledge required as\nto) whether the observations are stationary or not, and irrespective of whether\nthey change into a stationary or nonstationary regime. Hence, our methodology\nis particularly suited to detect the onset, or the collapse, of a bubble or an\nepidemic. Our simulations show that our procedures, especially when\nstandardising the CUSUM process, can ensure very good size control and short\ndetection delays. We complement our theory by studying the online detection of\nbreaks in epidemiological and housing prices series.",
        "authors": [
            "Lajos Horv\u00e1th",
            "Lorenzo Trapani"
        ],
        "categories": "stat.ME",
        "published": "2023-12-18T21:10:10Z",
        "updated": "2023-12-18T21:10:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.11283v1",
        "title": "The 2010 Census Confidentiality Protections Failed, Here's How and Why",
        "abstract": "Using only 34 published tables, we reconstruct five variables (census block,\nsex, age, race, and ethnicity) in the confidential 2010 Census person records.\nUsing the 38-bin age variable tabulated at the census block level, at most\n20.1% of reconstructed records can differ from their confidential source on\neven a single value for these five variables. Using only published data, an\nattacker can verify that all records in 70% of all census blocks (97 million\npeople) are perfectly reconstructed. The tabular publications in Summary File 1\nthus have prohibited disclosure risk similar to the unreleased confidential\nmicrodata. Reidentification studies confirm that an attacker can, within blocks\nwith perfect reconstruction accuracy, correctly infer the actual census\nresponse on race and ethnicity for 3.4 million vulnerable population uniques\n(persons with nonmodal characteristics) with 95% accuracy, the same precision\nas the confidential data achieve and far greater than statistical baselines.\nThe flaw in the 2010 Census framework was the assumption that aggregation\nprevented accurate microdata reconstruction, justifying weaker disclosure\nlimitation methods than were applied to 2010 Census public microdata. The\nframework used for 2020 Census publications defends against attacks that are\nbased on reconstruction, as we also demonstrate here. Finally, we show that\nalternatives to the 2020 Census Disclosure Avoidance System with similar\naccuracy (enhanced swapping) also fail to protect confidentiality, and those\nthat partially defend against reconstruction attacks (incomplete suppression\nimplementations) destroy the primary statutory use case: data for redistricting\nall legislatures in the country in compliance with the 1965 Voting Rights Act.",
        "authors": [
            "John M. Abowd",
            "Tamara Adams",
            "Robert Ashmead",
            "David Darais",
            "Sourya Dey",
            "Simson L. Garfinkel",
            "Nathan Goldschlag",
            "Daniel Kifer",
            "Philip Leclerc",
            "Ethan Lew",
            "Scott Moore",
            "Rolando A. Rodr\u00edguez",
            "Ramy N. Tadros",
            "Lars Vilhuber"
        ],
        "categories": "stat.AP",
        "published": "2023-12-18T15:23:12Z",
        "updated": "2023-12-18T15:23:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.10984v1",
        "title": "Predicting Financial Literacy via Semi-supervised Learning",
        "abstract": "Financial literacy (FL) represents a person's ability to turn assets into\nincome, and understanding digital currencies has been added to the modern\ndefinition. FL can be predicted by exploiting unlabelled recorded data in\nfinancial networks via semi-supervised learning (SSL). Measuring and predicting\nFL has not been widely studied, resulting in limited understanding of customer\nfinancial engagement consequences. Previous studies have shown that low FL\nincreases the risk of social harm. Therefore, it is important to accurately\nestimate FL to allocate specific intervention programs to less financially\nliterate groups. This will not only increase company profitability, but will\nalso reduce government spending. Some studies considered predicting FL in\nclassification tasks, whereas others developed FL definitions and impacts. The\ncurrent paper investigated mechanisms to learn customer FL level from their\nfinancial data using sampling by synthetic minority over-sampling techniques\nfor regression with Gaussian noise (SMOGN). We propose the SMOGN-COREG model\nfor semi-supervised regression, applying SMOGN to deal with unbalanced datasets\nand a nonparametric multi-learner co-regression (COREG) algorithm for labeling.\nWe compared the SMOGN-COREG model with six well-known regressors on five\ndatasets to evaluate the proposed models effectiveness on unbalanced and\nunlabelled financial data. Experimental results confirmed that the proposed\nmethod outperformed the comparator models for unbalanced and unlabelled\nfinancial data. Therefore, SMOGN-COREG is a step towards using unlabelled data\nto estimate FL level.",
        "authors": [
            "David Hason Rudd",
            "Huan Huo",
            "Guandong Xu"
        ],
        "categories": "cs.LG",
        "published": "2023-12-18T07:12:51Z",
        "updated": "2023-12-18T07:12:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.10558v1",
        "title": "Some Finite-Sample Results on the Hausman Test",
        "abstract": "This paper shows that the endogeneity test using the control function\napproach in linear instrumental variable models is a variant of the Hausman\ntest. Moreover, we find that the test statistics used in these tests can be\nnumerically ordered, indicating their relative power properties in finite\nsamples.",
        "authors": [
            "Jinyong Hahn",
            "Zhipeng Liao",
            "Nan Liu",
            "Shuyang Sheng"
        ],
        "categories": "econ.EM",
        "published": "2023-12-16T23:14:02Z",
        "updated": "2023-12-16T23:14:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.10487v1",
        "title": "The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models",
        "abstract": "Many current approaches to shrinkage within the time-varying parameter\nframework assume that each state is equipped with only one innovation variance\nfor all time points. Sparsity is then induced by shrinking this variance\ntowards zero. We argue that this is not sufficient if the states display large\njumps or structural changes, something which is often the case in time series\nanalysis. To remedy this, we propose the dynamic triple gamma prior, a\nstochastic process that has a well-known triple gamma marginal form, while\nstill allowing for autocorrelation. Crucially, the triple gamma has many\ninteresting limiting and special cases (including the horseshoe shrinkage\nprior) which can also be chosen as the marginal distribution. Not only is the\nmarginal form well understood, we further derive many interesting properties of\nthe dynamic triple gamma, which showcase its dynamic shrinkage characteristics.\nWe develop an efficient Markov chain Monte Carlo algorithm to sample from the\nposterior and demonstrate the performance through sparse covariance modeling\nand forecasting of the returns of the components of the EURO STOXX 50 index.",
        "authors": [
            "Peter Knaus",
            "Sylvia Fr\u00fchwirth-Schnatter"
        ],
        "categories": "econ.EM",
        "published": "2023-12-16T15:46:32Z",
        "updated": "2023-12-16T15:46:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.10333v1",
        "title": "Logit-based alternatives to two-stage least squares",
        "abstract": "We propose logit-based IV and augmented logit-based IV estimators that serve\nas alternatives to the traditionally used 2SLS estimator in the model where\nboth the endogenous treatment variable and the corresponding instrument are\nbinary. Our novel estimators are as easy to compute as the 2SLS estimator but\nhave an advantage over the 2SLS estimator in terms of causal interpretability.\nIn particular, in certain cases where the probability limits of both our\nestimators and the 2SLS estimator take the form of weighted-average treatment\neffects, our estimators are guaranteed to yield non-negative weights whereas\nthe 2SLS estimator is not.",
        "authors": [
            "Denis Chetverikov",
            "Jinyong Hahn",
            "Zhipeng Liao",
            "Shuyang Sheng"
        ],
        "categories": "econ.EM",
        "published": "2023-12-16T05:47:43Z",
        "updated": "2023-12-16T05:47:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.08174v4",
        "title": "Double Machine Learning for Static Panel Models with Fixed Effects",
        "abstract": "Recent advances in causal inference have seen the development of methods\nwhich make use of the predictive power of machine learning algorithms. In this\npaper, we use these algorithms to approximate high-dimensional and non-linear\nnuisance functions of the confounders and double machine learning (DML) to make\ninferences about the effects of policy interventions from panel data. We\npropose new estimators by extending correlated random effects, within-group and\nfirst-difference estimation for linear models to an extension of Robinson\n(1988)'s partially linear regression model to static panel data models with\nindividual fixed effects and unspecified non-linear confounding effects. We\nprovide an illustrative example of DML for observational panel data showing the\nimpact of the introduction of the minimum wage on voting behaviour in the UK.",
        "authors": [
            "Paul Clarke",
            "Annalivia Polselli"
        ],
        "categories": "econ.EM",
        "published": "2023-12-13T14:34:12Z",
        "updated": "2024-09-09T12:00:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.08171v1",
        "title": "Individual Updating of Subjective Probability of Homicide Victimization: a \"Natural Experiment'' on Risk Communication",
        "abstract": "We investigate the dynamics of the update of subjective homicide\nvictimization risk after an informational shock by developing two econometric\nmodels able to accommodate both optimal decisions of changing prior\nexpectations which enable us to rationalize skeptical Bayesian agents with\ntheir disregard to new information. We apply our models to a unique household\ndata (N = 4,030) that consists of socioeconomic and victimization expectation\nvariables in Brazil, coupled with an informational ``natural experiment''\nbrought by the sample design methodology, which randomized interviewers to\ninterviewees. The higher priors about their own subjective homicide\nvictimization risk are set, the more likely individuals are to change their\ninitial perceptions. In case of an update, we find that elders and females are\nmore reluctant to change priors and choose the new response level. In addition,\neven though the respondents' level of education is not significant, the\ninterviewers' level of education has a key role in changing and updating\ndecisions. The results show that our econometric approach fits reasonable well\nthe available empirical evidence, stressing the salient role heterogeneity\nrepresented by individual characteristics of interviewees and interviewers have\non belief updating and lack of it, say, skepticism. Furthermore, we can\nrationalize skeptics through an informational quality/credibility argument.",
        "authors": [
            "Jos\u00e9 Raimundo Carvalho",
            "Diego de Maria Andr\u00e9",
            "Yuri Costa"
        ],
        "categories": "econ.EM",
        "published": "2023-12-13T14:31:27Z",
        "updated": "2023-12-13T14:31:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.07881v2",
        "title": "Efficiency of QMLE for dynamic panel data models with interactive effects",
        "abstract": "This paper derives the efficiency bound for estimating the parameters of\ndynamic panel data models in the presence of an increasing number of incidental\nparameters. We study the efficiency problem by formulating the dynamic panel as\na simultaneous equations system, and show that the quasi-maximum likelihood\nestimator (QMLE) applied to the system achieves the efficiency bound.\nComparison of QMLE with fixed effects estimators is made.",
        "authors": [
            "Jushan Bai"
        ],
        "categories": "econ.EM",
        "published": "2023-12-13T03:56:34Z",
        "updated": "2024-04-29T02:20:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.07683v2",
        "title": "On Rosenbaum's Rank-based Matching Estimator",
        "abstract": "In two influential contributions, Rosenbaum (2005, 2020) advocated for using\nthe distances between component-wise ranks, instead of the original data\nvalues, to measure covariate similarity when constructing matching estimators\nof average treatment effects. While the intuitive benefits of using covariate\nranks for matching estimation are apparent, there is no theoretical\nunderstanding of such procedures in the literature. We fill this gap by\ndemonstrating that Rosenbaum's rank-based matching estimator, when coupled with\na regression adjustment, enjoys the properties of double robustness and\nsemiparametric efficiency without the need to enforce restrictive covariate\nmoment assumptions. Our theoretical findings further emphasize the statistical\nvirtues of employing ranks for estimation and inference, more broadly aligning\nwith the insights put forth by Peter Bickel in his 2004 Rietz lecture (Bickel,\n2004).",
        "authors": [
            "Matias D. Cattaneo",
            "Fang Han",
            "Zhexiao Lin"
        ],
        "categories": "math.ST",
        "published": "2023-12-12T19:24:03Z",
        "updated": "2024-01-06T21:34:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.07520v2",
        "title": "Estimating Counterfactual Matrix Means with Short Panel Data",
        "abstract": "We develop a new, spectral approach for identifying and estimating average\ncounterfactual outcomes under a low-rank factor model with short panel data and\ngeneral outcome missingness patterns. Applications include event studies and\nstudies of outcomes of \"matches\" between agents of two types, e.g. workers and\nfirms, typically conducted under less-flexible Two-Way-Fixed-Effects (TWFE)\nmodels of outcomes. Given an infinite population of units and a finite number\nof outcomes, we show our approach identifies all counterfactual outcome means,\nincluding those not estimable by existing methods, if a particular graph\nconstructed based on overlaps in observed outcomes between subpopulations is\nconnected. Our analogous, computationally efficient estimation procedure yields\nconsistent, asymptotically normal estimates of counterfactual outcome means\nunder fixed-$T$ (number of outcomes), large-$N$ (sample size) asymptotics. In a\nsemi-synthetic simulation study based on matched employer-employee data, our\nestimator has lower bias and only slightly higher variance than a\nTWFE-model-based estimator when estimating average log-wages.",
        "authors": [
            "Lihua Lei",
            "Brad Ross"
        ],
        "categories": "econ.EM",
        "published": "2023-12-12T18:51:39Z",
        "updated": "2024-05-06T15:28:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.06402v9",
        "title": "Structural Analysis of Vector Autoregressive Models",
        "abstract": "This set of lecture notes discuss key concepts for the Structural Analysis of\nVector Autoregressive models for the teaching of a course on Applied\nMacroeconometrics with Advanced Topics.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-12-11T14:21:34Z",
        "updated": "2024-02-05T14:47:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.06379v1",
        "title": "Trends in Temperature Data: Micro-foundations of Their Nature",
        "abstract": "Determining whether Global Average Temperature (GAT) is an integrated process\nof order 1, I(1), or is a stationary process around a trend function is crucial\nfor detection, attribution, impact and forecasting studies of climate change.\nIn this paper, we investigate the nature of trends in GAT building on the\nanalysis of individual temperature grids. Our 'micro-founded' evidence suggests\nthat GAT is stationary around a non-linear deterministic trend in the form of a\nlinear function with a one-period structural break. This break can be\nattributed to a combination of individual grid breaks and the standard\naggregation method under acceleration in global warming. We illustrate our\nfindings using simulations.",
        "authors": [
            "Maria Dolores Gadea",
            "Jesus Gonzalo",
            "Andrey Ramos"
        ],
        "categories": "econ.EM",
        "published": "2023-12-11T13:37:48Z",
        "updated": "2023-12-11T13:37:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.05985v3",
        "title": "Fused Extended Two-Way Fixed Effects for Difference-in-Differences With Staggered Adoptions",
        "abstract": "To address the bias of the canonical two-way fixed effects estimator for\ndifference-in-differences under staggered adoptions, Wooldridge (2021) proposed\nthe extended two-way fixed effects estimator, which adds many parameters.\nHowever, this reduces efficiency. Restricting some of these parameters to be\nequal (for example, subsequent treatment effects within a cohort) helps, but ad\nhoc restrictions may reintroduce bias. We propose a machine learning estimator\nwith a single tuning parameter, fused extended two-way fixed effects (FETWFE),\nthat enables automatic data-driven selection of these restrictions. We prove\nthat under an appropriate sparsity assumption FETWFE identifies the correct\nrestrictions with probability tending to one, which improves efficiency. We\nalso prove the consistency, oracle property, and asymptotic normality of FETWFE\nfor several classes of heterogeneous marginal treatment effect estimators under\neither conditional or marginal parallel trends, and we prove the same results\nfor conditional average treatment effects under conditional parallel trends. We\ndemonstrate FETWFE in simulation studies and an empirical application.",
        "authors": [
            "Gregory Faletto"
        ],
        "categories": "econ.EM",
        "published": "2023-12-10T20:16:39Z",
        "updated": "2024-10-28T01:05:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.05898v1",
        "title": "Dynamic Spatiotemporal ARCH Models: Small and Large Sample Results",
        "abstract": "This paper explores the estimation of a dynamic spatiotemporal autoregressive\nconditional heteroscedasticity (ARCH) model. The log-volatility term in this\nmodel can depend on (i) the spatial lag of the log-squared outcome variable,\n(ii) the time-lag of the log-squared outcome variable, (iii) the spatiotemporal\nlag of the log-squared outcome variable, (iv) exogenous variables, and (v) the\nunobserved heterogeneity across regions and time, i.e., the regional and time\nfixed effects. We examine the small and large sample properties of two\nquasi-maximum likelihood estimators and a generalized method of moments\nestimator for this model. We first summarize the theoretical properties of\nthese estimators and then compare their finite sample properties through Monte\nCarlo simulations.",
        "authors": [
            "Philipp Otto",
            "Osman Do\u011fan",
            "S\u00fcleyman Ta\u015fp\u0131nar"
        ],
        "categories": "stat.ME",
        "published": "2023-12-10T14:19:22Z",
        "updated": "2023-12-10T14:19:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.05858v2",
        "title": "Causal inference and policy evaluation without a control group",
        "abstract": "Without a control group, the most widespread methodologies for estimating\ncausal effects cannot be applied. To fill this gap, we propose the Machine\nLearning Control Method, a new approach for causal panel analysis that\nestimates causal parameters without relying on untreated units. We formalize\nidentification within the potential outcomes framework and then provide\nestimation based on machine learning algorithms. To illustrate the practical\nrelevance of our method, we present simulation evidence, a replication study,\nand an empirical application on the impact of the COVID-19 crisis on\neducational inequality. We implement the proposed approach in the companion R\npackage MachineControl",
        "authors": [
            "Augusto Cerqua",
            "Marco Letta",
            "Fiammetta Menchetti"
        ],
        "categories": "econ.EM",
        "published": "2023-12-10T11:45:31Z",
        "updated": "2024-10-25T13:19:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.05700v1",
        "title": "Influence Analysis with Panel Data",
        "abstract": "The presence of units with extreme values in the dependent and/or independent\nvariables (i.e., vertical outliers, leveraged data) has the potential to\nseverely bias regression coefficients and/or standard errors. This is common\nwith short panel data because the researcher cannot advocate asymptotic theory.\nExample include cross-country studies, cell-group analyses, and field or\nlaboratory experimental studies, where the researcher is forced to use few\ncross-sectional observations repeated over time due to the structure of the\ndata or research design. Available diagnostic tools may fail to properly detect\nthese anomalies, because they are not designed for panel data. In this paper,\nwe formalise statistical measures for panel data models with fixed effects to\nquantify the degree of leverage and outlyingness of units, and the joint and\nconditional influences of pairs of units. We first develop a method to visually\ndetect anomalous units in a panel data set, and identify their type. Second, we\ninvestigate the effect of these units on LS estimates, and on other units'\ninfluence on the estimated parameters. To illustrate and validate the proposed\nmethod, we use a synthetic data set contaminated with different types of\nanomalous units. We also provide an empirical example.",
        "authors": [
            "Annalivia Polselli"
        ],
        "categories": "econ.EM",
        "published": "2023-12-09T22:44:09Z",
        "updated": "2023-12-09T22:44:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.05593v2",
        "title": "Economic Forecasts Using Many Noises",
        "abstract": "This paper addresses a key question in economic forecasting: does pure noise\ntruly lack predictive power? Economists typically conduct variable selection to\neliminate noises from predictors. Yet, we prove a compelling result that in\nmost economic forecasts, the inclusion of noises in predictions yields greater\nbenefits than its exclusion. Furthermore, if the total number of predictors is\nnot sufficiently large, intentionally adding more noises yields superior\nforecast performance, outperforming benchmark predictors relying on dimension\nreduction. The intuition lies in economic predictive signals being densely\ndistributed among regression coefficients, maintaining modest forecast bias\nwhile diversifying away overall variance, even when a significant proportion of\npredictors constitute pure noises. One of our empirical demonstrations shows\nthat intentionally adding 300~6,000 pure noises to the Welch and Goyal (2008)\ndataset achieves a noteworthy 10% out-of-sample R square accuracy in\nforecasting the annual U.S. equity premium. The performance surpasses the\nmajority of sophisticated machine learning models.",
        "authors": [
            "Yuan Liao",
            "Xinjie Ma",
            "Andreas Neuhierl",
            "Zhentao Shi"
        ],
        "categories": "econ.EM",
        "published": "2023-12-09T15:17:19Z",
        "updated": "2023-12-12T02:48:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.05373v1",
        "title": "GCov-Based Portmanteau Test",
        "abstract": "We examine finite sample performance of the Generalized Covariance (GCov)\nresidual-based specification test for semiparametric models with i.i.d. errors.\nThe residual-based multivariate portmanteau test statistic follows\nasymptotically a $\\chi^2$ distribution when the model is estimated by the GCov\nestimator. The test is shown to perform well in application to the univariate\nmixed causal-noncausal MAR, double autoregressive (DAR) and multivariate Vector\nAutoregressive (VAR) models. We also introduce a bootstrap procedure that\nprovides the limiting distribution of the test statistic when the specification\ntest is applied to a model estimated by the maximum likelihood, or the\napproximate or quasi-maximum likelihood under a parametric assumption on the\nerror distribution.",
        "authors": [
            "Joann Jasiak",
            "Aryan Manafi Neyazi"
        ],
        "categories": "econ.EM",
        "published": "2023-12-08T21:18:14Z",
        "updated": "2023-12-08T21:18:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.05342v1",
        "title": "Occasionally Misspecified",
        "abstract": "When fitting a particular Economic model on a sample of data, the model may\nturn out to be heavily misspecified for some observations. This can happen\nbecause of unmodelled idiosyncratic events, such as an abrupt but short-lived\nchange in policy. These outliers can significantly alter estimates and\ninferences. A robust estimation is desirable to limit their influence. For\nskewed data, this induces another bias which can also invalidate the estimation\nand inferences. This paper proposes a robust GMM estimator with a simple bias\ncorrection that does not degrade robustness significantly. The paper provides\nfinite-sample robustness bounds, and asymptotic uniform equivalence with an\noracle that discards all outliers. Consistency and asymptotic normality ensue\nfrom that result. An application to the \"Price-Puzzle,\" which finds inflation\nincreases when monetary policy tightens, illustrates the concerns and the\nmethod. The proposed estimator finds the intuitive result: tighter monetary\npolicy leads to a decline in inflation.",
        "authors": [
            "Jean-Jacques Forneron"
        ],
        "categories": "econ.EM",
        "published": "2023-12-08T20:07:45Z",
        "updated": "2023-12-08T20:07:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.04428v2",
        "title": "Probabilistic Scenario-Based Assessment of National Food Security Risks with Application to Egypt and Ethiopia",
        "abstract": "This study presents a novel approach to assessing food security risks at the\nnational level, employing a probabilistic scenario-based framework that\nintegrates both Shared Socioeconomic Pathways (SSP) and Representative\nConcentration Pathways (RCP). This innovative method allows each scenario,\nencompassing socio-economic and climate factors, to be treated as a model\ncapable of generating diverse trajectories. This approach offers a more dynamic\nunderstanding of food security risks under varying future conditions. The paper\ndetails the methodologies employed, showcasing their applicability through a\nfocused analysis of food security challenges in Egypt and Ethiopia, and\nunderscores the importance of considering a spectrum of socio-economic and\nclimatic factors in national food security assessments.",
        "authors": [
            "Phoebe Koundouri",
            "Georgios I. Papayiannis",
            "Achilleas Vassilopoulos",
            "Athanasios N. Yannacopoulos"
        ],
        "categories": "econ.EM",
        "published": "2023-12-07T16:54:46Z",
        "updated": "2023-12-14T23:13:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.03915v1",
        "title": "Alternative models for FX, arbitrage opportunities and efficient pricing of double barrier options in L\u00e9vy models",
        "abstract": "We analyze the qualitative differences between prices of double barrier\nno-touch options in the Heston model and pure jump KoBoL model calibrated to\nthe same set of the empirical data, and discuss the potential for arbitrage\nopportunities if the correct model is a pure jump model. We explain and\ndemonstrate with numerical examples that accurate and fast calculations of\nprices of double barrier options in jump models are extremely difficult using\nthe numerical methods available in the literature. We develop a new efficient\nmethod (GWR-SINH method) based of the Gaver-Wynn-Rho acceleration applied to\nthe Bromwich integral; the SINH-acceleration and simplified trapezoid rule are\nused to evaluate perpetual double barrier options for each value of the\nspectral parameter in GWR-algorithm. The program in Matlab running on a Mac\nwith moderate characteristics achieves the precision of the order of E-5 and\nbetter in several several dozen of milliseconds; the precision E-07 is\nachievable in about 0.1 sec. We outline the extension of GWR-SINH method to\nregime-switching models and models with stochastic parameters and stochastic\ninterest rates.",
        "authors": [
            "Svetlana Boyarchenko",
            "Sergei Levendorskii"
        ],
        "categories": "q-fin.CP",
        "published": "2023-12-06T21:26:58Z",
        "updated": "2023-12-06T21:26:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.03165v1",
        "title": "A Theory Guide to Using Control Functions to Instrument Hazard Models",
        "abstract": "I develop the theory around using control functions to instrument hazard\nmodels, allowing the inclusion of endogenous (e.g., mismeasured) regressors.\nSimple discrete-data hazard models can be expressed as binary choice panel data\nmodels, and the widespread Prentice and Gloeckler (1978) discrete-data\nproportional hazards model can specifically be expressed as a complementary\nlog-log model with time fixed effects. This allows me to recast it as GMM\nestimation and its instrumented version as sequential GMM estimation in a\nZ-estimation (non-classical GMM) framework; this framework can then be\nleveraged to establish asymptotic properties and sufficient conditions. Whilst\nthis paper focuses on the Prentice and Gloeckler (1978) model, the methods and\ndiscussion developed here can be applied more generally to other hazard models\nand binary choice models. I also introduce my Stata command for estimating a\ncomplementary log-log model instrumented via control functions (available as\nivcloglog on SSC), which allows practitioners to easily instrument the Prentice\nand Gloeckler (1978) model.",
        "authors": [
            "William Liu"
        ],
        "categories": "econ.EM",
        "published": "2023-12-05T22:14:14Z",
        "updated": "2023-12-05T22:14:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.02288v1",
        "title": "Almost Dominance: Inference and Application",
        "abstract": "This paper proposes a general framework for inference on three types of\nalmost dominances: Almost Lorenz dominance, almost inverse stochastic\ndominance, and almost stochastic dominance. We first generalize almost Lorenz\ndominance to almost upward and downward Lorenz dominances. We then provide a\nbootstrap inference procedure for the Lorenz dominance coefficients, which\nmeasure the degrees of almost Lorenz dominances. Furthermore, we propose almost\nupward and downward inverse stochastic dominances and provide inference on the\ninverse stochastic dominance coefficients. We also show that our results can\neasily be extended to almost stochastic dominance. Simulation studies\ndemonstrate the finite sample properties of the proposed estimators and the\nbootstrap confidence intervals. We apply our methods to the inequality growth\nin the United Kingdom and find evidence for almost upward inverse stochastic\ndominance.",
        "authors": [
            "Xiaojun Song",
            "Zhenting Sun"
        ],
        "categories": "econ.EM",
        "published": "2023-12-04T19:09:14Z",
        "updated": "2023-12-04T19:09:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.01881v1",
        "title": "Bayesian Nonlinear Regression using Sums of Simple Functions",
        "abstract": "This paper proposes a new Bayesian machine learning model that can be applied\nto large datasets arising in macroeconomics. Our framework sums over many\nsimple two-component location mixtures. The transition between components is\ndetermined by a logistic function that depends on a single threshold variable\nand two hyperparameters. Each of these individual models only accounts for a\nminor portion of the variation in the endogenous variables. But many of them\nare capable of capturing arbitrary nonlinear conditional mean relations.\nConjugate priors enable fast and efficient inference. In simulations, we show\nthat our approach produces accurate point and density forecasts. In a real-data\nexercise, we forecast US macroeconomic aggregates and consider the nonlinear\neffects of financial shocks in a large-scale nonlinear VAR.",
        "authors": [
            "Florian Huber"
        ],
        "categories": "econ.EM",
        "published": "2023-12-04T13:24:46Z",
        "updated": "2023-12-04T13:24:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.01209v2",
        "title": "A Method of Moments Approach to Asymptotically Unbiased Synthetic Controls",
        "abstract": "A common approach to constructing a Synthetic Control unit is to fit on the\noutcome variable and covariates in pre-treatment time periods, but it has been\nshown by Ferman and Pinto (2019) that this approach does not provide asymptotic\nunbiasedness when the fit is imperfect and the number of controls is fixed.\nMany related panel methods have a similar limitation when the number of units\nis fixed. I introduce and evaluate a new method in which the Synthetic Control\nis constructed using a General Method of Moments approach where units not being\nincluded in the Synthetic Control are used as instruments. I show that a\nSynthetic Control Estimator of this form will be asymptotically unbiased as the\nnumber of pre-treatment time periods goes to infinity, even when pre-treatment\nfit is imperfect and the number of units is fixed. Furthermore, if both the\nnumber of pre-treatment and post-treatment time periods go to infinity, then\naverages of treatment effects can be consistently estimated. I conduct\nsimulations and an empirical application to compare the performance of this\nmethod with existing approaches in the literature.",
        "authors": [
            "Joseph Fry"
        ],
        "categories": "econ.EM",
        "published": "2023-12-02T19:35:50Z",
        "updated": "2024-03-05T20:37:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.01162v2",
        "title": "Inference on many jumps in nonparametric panel regression models",
        "abstract": "We investigate the significance of change-point or jump effects within fully\nnonparametric regression contexts, with a particular focus on panel data\nscenarios where data generation processes vary across individual or group\nunits, and error terms may display complex dependency structures. In our\nsetting the threshold effect depends on a specific covariate, and we permit the\ntrue nonparametric regression to vary based on additional latent variables. We\npropose two uniform testing procedures: one to assess the existence of\nchange-point effects and another to evaluate the uniformity of such effects\nacross units. Even though the underlying data generation processes are neither\nindependent nor identically distributed, our approach involves deriving a\nstraightforward analytical expression to approximate the variance-covariance\nstructure of change-point effects under general dependency conditions. Notably,\nwhen Gaussian approximations are made to these test statistics, the intricate\ndependency structures within the data can be safely disregarded owing to the\nlocalized nature of the statistics. This finding bears significant implications\nfor obtaining critical values. Through extensive simulations, we demonstrate\nthat our tests exhibit excellent control over size and reasonable power\nperformance in finite samples, irrespective of strong cross-sectional and weak\nserial dependency within the data. Furthermore, applying our tests to two\ndatasets reveals the existence of significant nonsmooth effects in both cases.",
        "authors": [
            "Likai Chen",
            "Georg Keilbar",
            "Liangjun Su",
            "Weining Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-12-02T15:52:24Z",
        "updated": "2024-08-30T08:42:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.00955v1",
        "title": "Identification and Inference for Synthetic Controls with Confounding",
        "abstract": "This paper studies inference on treatment effects in panel data settings with\nunobserved confounding. We model outcome variables through a factor model with\nrandom factors and loadings. Such factors and loadings may act as unobserved\nconfounders: when the treatment is implemented depends on time-varying factors,\nand who receives the treatment depends on unit-level confounders. We study the\nidentification of treatment effects and illustrate the presence of a trade-off\nbetween time and unit-level confounding. We provide asymptotic results for\ninference for several Synthetic Control estimators and show that different\nsources of randomness should be considered for inference, depending on the\nnature of confounding. We conclude with a comparison of Synthetic Control\nestimators with alternatives for factor models.",
        "authors": [
            "Guido W. Imbens",
            "Davide Viviano"
        ],
        "categories": "econ.EM",
        "published": "2023-12-01T22:16:49Z",
        "updated": "2023-12-01T22:16:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.00590v4",
        "title": "Inference on common trends in functional time series",
        "abstract": "We study statistical inference on unit roots and cointegration for time\nseries in a Hilbert space. We develop statistical inference on the number of\ncommon stochastic trends embedded in the time series, i.e., the dimension of\nthe nonstationary subspace. We also consider tests of hypotheses on the\nnonstationary and stationary subspaces themselves. The Hilbert space can be of\nan arbitrarily large dimension, and our methods remain asymptotically valid\neven when the time series of interest takes values in a subspace of possibly\nunknown dimension. This has wide applicability in practice; for example, to the\ncase of cointegrated vector time series that are either high-dimensional or of\nfinite dimension, to high-dimensional factor model that includes a finite\nnumber of nonstationary factors, to cointegrated curve-valued (or\nfunction-valued) time series, and to nonstationary dynamic functional factor\nmodels. We include two empirical illustrations to the term structure of\ninterest rates and labor market indices, respectively.",
        "authors": [
            "Morten \u00d8rregaard Nielsen",
            "Won-Ki Seo",
            "Dakyung Seong"
        ],
        "categories": "econ.EM",
        "published": "2023-12-01T13:55:12Z",
        "updated": "2024-05-21T06:58:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.00399v2",
        "title": "GMM-lev estimation and individual heterogeneity: Monte Carlo evidence and empirical applications",
        "abstract": "We introduce a new estimator, CRE-GMM, which exploits the correlated random\neffects (CRE) approach within the generalised method of moments (GMM),\nspecifically applied to level equations, GMM-lev. It has the advantage of\nestimating the effect of measurable time-invariant covariates using all\navailable information. This is not possible with GMM-dif, applied to the\nequations of each period transformed into first differences, while GMM-sys uses\nlittle information as it adds the equation in levels for only one period. The\nGMM-lev, by implying a two-component error term containing individual\nheterogeneity and shock, exposes the explanatory variables to possible double\nendogeneity. For example, the estimation of actual persistence could suffer\nfrom bias if instruments were correlated with the unit-specific error\ncomponent. The CRE-GMM deals with double endogeneity, captures initial\nconditions and enhance inference. Monte Carlo simulations for different panel\ntypes and under different double endogeneity assumptions show the advantage of\nour approach. The empirical applications on production and R&D contribute to\nclarify the advantages of using CRE-GMM.",
        "authors": [
            "Maria Elena Bontempi",
            "Jan Ditzen"
        ],
        "categories": "econ.EM",
        "published": "2023-12-01T07:46:16Z",
        "updated": "2023-12-13T13:27:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2312.00282v1",
        "title": "Stochastic volatility models with skewness selection",
        "abstract": "This paper expands traditional stochastic volatility models by allowing for\ntime-varying skewness without imposing it. While dynamic asymmetry may capture\nthe likely direction of future asset returns, it comes at the risk of leading\nto overparameterization. Our proposed approach mitigates this concern by\nleveraging sparsity-inducing priors to automatically selects the skewness\nparameter as being dynamic, static or zero in a data-driven framework. We\nconsider two empirical applications. First, in a bond yield application,\ndynamic skewness captures interest rate cycles of monetary easing and\ntightening being partially explained by central banks' mandates. In an currency\nmodeling framework, our model indicates no skewness in the carry factor after\naccounting for stochastic volatility which supports the idea of carry crashes\nbeing the result of volatility surges instead of dynamic skewness.",
        "authors": [
            "Igor Ferreira Batista Martins",
            "Hedibert Freitas Lopes"
        ],
        "categories": "econ.EM",
        "published": "2023-12-01T01:35:41Z",
        "updated": "2023-12-01T01:35:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.18759v1",
        "title": "Bootstrap Inference on Partially Linear Binary Choice Model",
        "abstract": "The partially linear binary choice model can be used for estimating\nstructural equations where nonlinearity may appear due to diminishing marginal\nreturns, different life cycle regimes, or hectic physical phenomena. The\ninference procedure for this model based on the analytic asymptotic\napproximation could be unreliable in finite samples if the sample size is not\nsufficiently large. This paper proposes a bootstrap inference approach for the\nmodel. Monte Carlo simulations show that the proposed inference method performs\nwell in finite samples compared to the procedure based on the asymptotic\napproximation.",
        "authors": [
            "Wenzheng Gao",
            "Zhenting Sun"
        ],
        "categories": "econ.EM",
        "published": "2023-11-30T18:01:11Z",
        "updated": "2023-11-30T18:01:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.18555v1",
        "title": "Identification in Endogenous Sequential Treatment Regimes",
        "abstract": "This paper develops a novel nonparametric identification method for treatment\neffects in settings where individuals self-select into treatment sequences. I\npropose an identification strategy which relies on a dynamic version of\nstandard Instrumental Variables (IV) assumptions and builds on a dynamic\nversion of the Marginal Treatment Effects (MTE) as the fundamental building\nblock for treatment effects. The main contribution of the paper is to relax\nassumptions on the support of the observed variables and on unobservable gains\nof treatment that are present in the dynamic treatment effects literature.\nMonte Carlo simulation studies illustrate the desirable finite-sample\nperformance of a sieve estimator for MTEs and Average Treatment Effects (ATEs)\non a close-to-application simulation study.",
        "authors": [
            "Pedro Picchetti"
        ],
        "categories": "econ.EM",
        "published": "2023-11-30T13:47:49Z",
        "updated": "2023-11-30T13:47:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.18136v1",
        "title": "Extrapolating Away from the Cutoff in Regression Discontinuity Designs",
        "abstract": "Canonical RD designs yield credible local estimates of the treatment effect\nat the cutoff under mild continuity assumptions, but they fail to identify\ntreatment effects away from the cutoff without additional assumptions. The\nfundamental challenge of identifying treatment effects away from the cutoff is\nthat the counterfactual outcome under the alternative treatment status is never\nobserved. This paper aims to provide a methodological blueprint to identify\ntreatment effects away from the cutoff in various empirical settings by\noffering a non-exhaustive list of assumptions on the counterfactual outcome.\nInstead of assuming the exact evolution of the counterfactual outcome, this\npaper bounds its variation using the data and sensitivity parameters. The\nproposed assumptions are weaker than those introduced previously in the\nliterature, resulting in partially identified treatment effects that are less\nsusceptible to assumption violations. This approach accommodates both single\ncutoff and multi-cutoff designs. The specific choice of the extrapolation\nassumption depends on the institutional background of each empirical\napplication. Additionally, researchers are recommended to conduct sensitivity\nanalysis on the chosen parameter and assess resulting shifts in conclusions.\nThe paper compares the proposed identification results with results using\nprevious methods via an empirical application and simulated data. It\ndemonstrates that set identification yields a more credible conclusion about\nthe sign of the treatment effect.",
        "authors": [
            "Yiwei Sun"
        ],
        "categories": "econ.EM",
        "published": "2023-11-29T22:56:33Z",
        "updated": "2023-11-29T22:56:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.17858v1",
        "title": "On the Limits of Regression Adjustment",
        "abstract": "Regression adjustment, sometimes known as Controlled-experiment Using\nPre-Experiment Data (CUPED), is an important technique in internet\nexperimentation. It decreases the variance of effect size estimates, often\ncutting confidence interval widths in half or more while never making them\nworse. It does so by carefully regressing the goal metric against\npre-experiment features to reduce the variance. The tremendous gains of\nregression adjustment begs the question: How much better can we do by\nengineering better features from pre-experiment data, for example by using\nmachine learning techniques or synthetic controls? Could we even reduce the\nvariance in our effect sizes arbitrarily close to zero with the right\npredictors? Unfortunately, our answer is negative. A simple form of regression\nadjustment, which uses just the pre-experiment values of the goal metric,\ncaptures most of the benefit. Specifically, under a mild assumption that\nobservations closer in time are easier to predict that ones further away in\ntime, we upper bound the potential gains of more sophisticated feature\nengineering, with respect to the gains of this simple form of regression\nadjustment. The maximum reduction in variance is $50\\%$ in Theorem 1, or\nequivalently, the confidence interval width can be reduced by at most an\nadditional $29\\%$.",
        "authors": [
            "Daniel Ting",
            "Kenneth Hung"
        ],
        "categories": "stat.ME",
        "published": "2023-11-29T18:04:39Z",
        "updated": "2023-11-29T18:04:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.17575v3",
        "title": "Identifying Causal Effects of Discrete, Ordered and ContinuousTreatments using Multiple Instrumental Variables",
        "abstract": "Inferring causal relationships from observational data is often challenging\ndue to endogeneity. This paper provides new identification results for causal\neffects of discrete, ordered and continuous treatments using multiple binary\ninstruments. The key contribution is the identification of a new causal\nparameter that has a straightforward interpretation with a positive weighting\nscheme and is applicable in many settings due to a mild monotonicity\nassumption. This paper further leverages recent advances in causal machine\nlearning for both estimation and the detection of local violations of the\nunderlying monotonicity assumption. The methodology is applied to estimate the\nreturns to education and assess the impact of having an additional child on\nfemale labor market outcomes.",
        "authors": [
            "Nadja van 't Hoff"
        ],
        "categories": "econ.EM",
        "published": "2023-11-29T12:11:44Z",
        "updated": "2024-10-18T11:10:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.17021v2",
        "title": "Optimal Categorical Instrumental Variables",
        "abstract": "This paper discusses estimation with a categorical instrumental variable in\nsettings with potentially few observations per category. The proposed\ncategorical instrumental variable estimator (CIV) leverages a regularization\nassumption that implies existence of a latent categorical variable with fixed\nfinite support achieving the same first stage fit as the observed instrument.\nIn asymptotic regimes that allow the number of observations per category to\ngrow at arbitrary small polynomial rate with the sample size, I show that when\nthe cardinality of the support of the optimal instrument is known, CIV is\nroot-n asymptotically normal, achieves the same asymptotic variance as the\noracle IV estimator that presumes knowledge of the optimal instrument, and is\nsemiparametrically efficient under homoskedasticity. Under-specifying the\nnumber of support points reduces efficiency but maintains asymptotic normality.\nIn an application that leverages judge fixed effects as instruments, CIV\ncompares favorably to commonly used jackknife-based instrumental variable\nestimators.",
        "authors": [
            "Thomas Wiemann"
        ],
        "categories": "econ.EM",
        "published": "2023-11-28T18:20:05Z",
        "updated": "2024-05-23T20:56:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.16486v2",
        "title": "On the adaptation of causal forests to manifold data",
        "abstract": "Researchers often hold the belief that random forests are \"the cure to the\nworld's ills\" (Bickel, 2010). But how exactly do they achieve this? Focused on\nthe recently introduced causal forests (Athey and Imbens, 2016; Wager and\nAthey, 2018), this manuscript aims to contribute to an ongoing research trend\ntowards answering this question, proving that causal forests can adapt to the\nunknown covariate manifold structure. In particular, our analysis shows that a\ncausal forest estimator can achieve the optimal rate of convergence for\nestimating the conditional average treatment effect, with the covariate\ndimension automatically replaced by the manifold dimension. These findings\nalign with analogous observations in the realm of deep learning and resonate\nwith the insights presented in Peter Bickel's 2004 Rietz lecture.",
        "authors": [
            "Yiyi Huo",
            "Yingying Fan",
            "Fang Han"
        ],
        "categories": "math.ST",
        "published": "2023-11-28T04:32:36Z",
        "updated": "2023-12-26T17:18:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.16440v2",
        "title": "Inference for Low-rank Models without Estimating the Rank",
        "abstract": "This paper studies the inference about linear functionals of high-dimensional\nlow-rank matrices. While most existing inference methods would require\nconsistent estimation of the true rank, our procedure is robust to rank\nmisspecification, making it a promising approach in applications where rank\nestimation can be unreliable. We estimate the low-rank spaces using\npre-specified weighting matrices, known as diversified projections. A novel\nstatistical insight is that, unlike the usual statistical wisdom that\noverfitting mainly introduces additional variances, the over-estimated low-rank\nspace also gives rise to a non-negligible bias due to an implicit ridge-type\nregularization. We develop a new inference procedure and show that the central\nlimit theorem holds as long as the pre-specified rank is no smaller than the\ntrue rank. In one of our applications, we study multiple testing with\nincomplete data in the presence of confounding factors and show that our method\nremains valid as long as the number of controlled confounding factors is at\nleast as large as the true number, even when no confounding factors are\npresent.",
        "authors": [
            "Jungjun Choi",
            "Hyukjun Kwon",
            "Yuan Liao"
        ],
        "categories": "econ.EM",
        "published": "2023-11-28T02:42:53Z",
        "updated": "2024-10-17T17:04:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.16333v2",
        "title": "From Reactive to Proactive Volatility Modeling with Hemisphere Neural Networks",
        "abstract": "We reinvigorate maximum likelihood estimation (MLE) for macroeconomic density\nforecasting through a novel neural network architecture with dedicated mean and\nvariance hemispheres. Our architecture features several key ingredients making\nMLE work in this context. First, the hemispheres share a common core at the\nentrance of the network which accommodates for various forms of time variation\nin the error variance. Second, we introduce a volatility emphasis constraint\nthat breaks mean/variance indeterminacy in this class of overparametrized\nnonlinear models. Third, we conduct a blocked out-of-bag reality check to curb\noverfitting in both conditional moments. Fourth, the algorithm utilizes\nstandard deep learning software and thus handles large data sets - both\ncomputationally and statistically. Ergo, our Hemisphere Neural Network (HNN)\nprovides proactive volatility forecasts based on leading indicators when it\ncan, and reactive volatility based on the magnitude of previous prediction\nerrors when it must. We evaluate point and density forecasts with an extensive\nout-of-sample experiment and benchmark against a suite of models ranging from\nclassics to more modern machine learning-based offerings. In all cases, HNN\nfares well by consistently providing accurate mean/variance forecasts for all\ntargets and horizons. Studying the resulting volatility paths reveals its\nversatility, while probabilistic forecasting evaluation metrics showcase its\nenviable reliability. Finally, we also demonstrate how this machinery can be\nmerged with other structured deep learning models by revisiting Goulet Coulombe\n(2022)'s Neural Phillips Curve.",
        "authors": [
            "Philippe Goulet Coulombe",
            "Mikael Frenette",
            "Karin Klieber"
        ],
        "categories": "econ.EM",
        "published": "2023-11-27T21:37:50Z",
        "updated": "2024-04-23T15:53:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.16260v2",
        "title": "Using Multiple Outcomes to Improve the Synthetic Control Method",
        "abstract": "When there are multiple outcome series of interest, Synthetic Control\nanalyses typically proceed by estimating separate weights for each outcome. In\nthis paper, we instead propose estimating a common set of weights across\noutcomes, by balancing either a vector of all outcomes or an index or average\nof them. Under a low-rank factor model, we show that these approaches lead to\nlower bias bounds than separate weights, and that averaging leads to further\ngains when the number of outcomes grows. We illustrate this via a re-analysis\nof the impact of the Flint water crisis on educational outcomes.",
        "authors": [
            "Liyang Sun",
            "Eli Ben-Michael",
            "Avi Feller"
        ],
        "categories": "econ.EM",
        "published": "2023-11-27T19:07:29Z",
        "updated": "2024-11-04T23:25:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.15952v1",
        "title": "Robust Conditional Wald Inference for Over-Identified IV",
        "abstract": "For the over-identified linear instrumental variables model, researchers\ncommonly report the 2SLS estimate along with the robust standard error and seek\nto conduct inference with these quantities. If errors are homoskedastic, one\ncan control the degree of inferential distortion using the first-stage F\ncritical values from Stock and Yogo (2005), or use the robust-to-weak\ninstruments Conditional Wald critical values of Moreira (2003). If errors are\nnon-homoskedastic, these methods do not apply. We derive the generalization of\nConditional Wald critical values that is robust to non-homoskedastic errors\n(e.g., heteroskedasticity or clustered variance structures), which can also be\napplied to nonlinear weakly-identified models (e.g. weakly-identified GMM).",
        "authors": [
            "David S. Lee",
            "Justin McCrary",
            "Marcelo J. Moreira",
            "Jack Porter",
            "Luther Yap"
        ],
        "categories": "econ.EM",
        "published": "2023-11-27T15:57:58Z",
        "updated": "2023-11-27T15:57:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.15932v1",
        "title": "Valid Wald Inference with Many Weak Instruments",
        "abstract": "This paper proposes three novel test procedures that yield valid inference in\nan environment with many weak instrumental variables (MWIV). It is observed\nthat the t statistic of the jackknife instrumental variable estimator (JIVE)\nhas an asymptotic distribution that is identical to the two-stage-least squares\n(TSLS) t statistic in the just-identified environment. Consequently, test\nprocedures that were valid for TSLS t are also valid for the JIVE t. Two such\nprocedures, i.e., VtF and conditional Wald, are adapted directly. By exploiting\na feature of MWIV environments, a third, more powerful, one-sided VtF-based\ntest procedure can be obtained.",
        "authors": [
            "Luther Yap"
        ],
        "categories": "econ.EM",
        "published": "2023-11-27T15:39:32Z",
        "updated": "2023-11-27T15:39:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.15878v3",
        "title": "Policy Learning with Distributional Welfare",
        "abstract": "In this paper, we explore optimal treatment allocation policies that target\ndistributional welfare. Most literature on treatment choice has considered\nutilitarian welfare based on the conditional average treatment effect (ATE).\nWhile average welfare is intuitive, it may yield undesirable allocations\nespecially when individuals are heterogeneous (e.g., with outliers) - the very\nreason individualized treatments were introduced in the first place. This\nobservation motivates us to propose an optimal policy that allocates the\ntreatment based on the conditional quantile of individual treatment effects\n(QoTE). Depending on the choice of the quantile probability, this criterion can\naccommodate a policymaker who is either prudent or negligent. The challenge of\nidentifying the QoTE lies in its requirement for knowledge of the joint\ndistribution of the counterfactual outcomes, which is generally hard to recover\neven with experimental data. Therefore, we introduce minimax policies that are\nrobust to model uncertainty. A range of identifying assumptions can be used to\nyield more informative policies. For both stochastic and deterministic\npolicies, we establish the asymptotic bound on the regret of implementing the\nproposed policies. In simulations and two empirical applications, we compare\noptimal decisions based on the QoTE with decisions based on other criteria. The\nframework can be generalized to any setting where welfare is defined as a\nfunctional of the joint distribution of the potential outcomes.",
        "authors": [
            "Yifan Cui",
            "Sukjin Han"
        ],
        "categories": "stat.ME",
        "published": "2023-11-27T14:51:30Z",
        "updated": "2024-09-22T05:21:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.15871v1",
        "title": "On Quantile Treatment Effects, Rank Similarity, and Variation of Instrumental Variables",
        "abstract": "This paper investigates how certain relationship between observed and\ncounterfactual distributions serves as an identifying condition for treatment\neffects when the treatment is endogenous, and shows that this condition holds\nin a range of nonparametric models for treatment effects. To this end, we first\nprovide a novel characterization of the prevalent assumption restricting\ntreatment heterogeneity in the literature, namely rank similarity. Our\ncharacterization demonstrates the stringency of this assumption and allows us\nto relax it in an economically meaningful way, resulting in our identifying\ncondition. It also justifies the quest of richer exogenous variations in the\ndata (e.g., multi-valued or multiple instrumental variables) in exchange for\nweaker identifying conditions. The primary goal of this investigation is to\nprovide empirical researchers with tools that are robust and easy to implement\nbut still yield tight policy evaluations.",
        "authors": [
            "Sukjin Han",
            "Haiqing Xu"
        ],
        "categories": "econ.EM",
        "published": "2023-11-27T14:43:24Z",
        "updated": "2023-11-27T14:43:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.15829v1",
        "title": "(Frisch-Waugh-Lovell)': On the Estimation of Regression Models by Row",
        "abstract": "We demonstrate that regression models can be estimated by working\nindependently in a row-wise fashion. We document a simple procedure which\nallows for a wide class of econometric estimators to be implemented\ncumulatively, where, in the limit, estimators can be produced without ever\nstoring more than a single line of data in a computer's memory. This result is\nuseful in understanding the mechanics of many common regression models. These\nprocedures can be used to speed up the computation of estimates computed via\nOLS, IV, Ridge regression, LASSO, Elastic Net, and Non-linear models including\nprobit and logit, with all common modes of inference. This has implications for\nestimation and inference with `big data', where memory constraints may imply\nthat working with all data at once is particularly costly. We additionally show\nthat even with moderately sized datasets, this method can reduce computation\ntime compared with traditional estimation routines.",
        "authors": [
            "Damian Clarke",
            "Nicol\u00e1s Paris",
            "Benjam\u00edn Villena-Rold\u00e1n"
        ],
        "categories": "econ.EM",
        "published": "2023-11-27T13:53:19Z",
        "updated": "2023-11-27T13:53:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.15458v3",
        "title": "Causal Models for Longitudinal and Panel Data: A Survey",
        "abstract": "In this survey we discuss the recent causal panel data literature. This\nrecent literature has focused on credibly estimating causal effects of binary\ninterventions in settings with longitudinal data, emphasizing practical advice\nfor empirical researchers. It pays particular attention to heterogeneity in the\ncausal effects, often in situations where few units are treated and with\nparticular structures on the assignment pattern. The literature has extended\nearlier work on difference-in-differences or two-way-fixed-effect estimators.\nIt has more generally incorporated factor models or interactive fixed effects.\nIt has also developed novel methods using synthetic control approaches.",
        "authors": [
            "Dmitry Arkhangelsky",
            "Guido Imbens"
        ],
        "categories": "econ.EM",
        "published": "2023-11-26T23:31:41Z",
        "updated": "2024-06-25T10:56:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.14892v1",
        "title": "An Identification and Dimensionality Robust Test for Instrumental Variables Models",
        "abstract": "I propose a new identification-robust test for the structural parameter in a\nheteroskedastic linear instrumental variables model. The proposed test\nstatistic is similar in spirit to a jackknife version of the K-statistic and\nthe resulting test has exact asymptotic size so long as an auxiliary parameter\ncan be consistently estimated. This is possible under approximate sparsity even\nwhen the number of instruments is much larger than the sample size. As the\nnumber of instruments is allowed, but not required, to be large, the limiting\nbehavior of the test statistic is difficult to examine via existing central\nlimit theorems. Instead, I derive the asymptotic chi-squared distribution of\nthe test statistic using a direct Gaussian approximation technique. To improve\npower against certain alternatives, I propose a simple combination with the\nsup-score statistic of Belloni et al. (2012) based on a thresholding rule. I\ndemonstrate favorable size control and power properties in a simulation study\nand apply the new methods to revisit the effect of social spillovers in movie\nconsumption.",
        "authors": [
            "Manu Navjeevan"
        ],
        "categories": "econ.EM",
        "published": "2023-11-25T01:13:12Z",
        "updated": "2023-11-25T01:13:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.14813v1",
        "title": "A Review of Cross-Sectional Matrix Exponential Spatial Models",
        "abstract": "The matrix exponential spatial models exhibit similarities to the\nconventional spatial autoregressive model in spatial econometrics but offer\nanalytical, computational, and interpretive advantages. This paper provides a\ncomprehensive review of the literature on the estimation, inference, and model\nselection approaches for the cross-sectional matrix exponential spatial models.\nWe discuss summary measures for the marginal effects of regressors and detail\nthe matrix-vector product method for efficient estimation. Our aim is not only\nto summarize the main findings from the spatial econometric literature but also\nto make them more accessible to applied researchers. Additionally, we\ncontribute to the literature by introducing some new results. We propose an\nM-estimation approach for models with heteroskedastic error terms and\ndemonstrate that the resulting M-estimator is consistent and has an asymptotic\nnormal distribution. We also consider some new results for model selection\nexercises. In a Monte Carlo study, we examine the finite sample properties of\nvarious estimators from the literature alongside the M-estimator.",
        "authors": [
            "Ye Yang",
            "Osman Dogan",
            "Suleyman Taspinar",
            "Fei Jin"
        ],
        "categories": "econ.EM",
        "published": "2023-11-24T19:10:13Z",
        "updated": "2023-11-24T19:10:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.14204v3",
        "title": "Reproducible Aggregation of Sample-Split Statistics",
        "abstract": "Statistical inference is often simplified by sample-splitting. This\nsimplification comes at the cost of the introduction of randomness not native\nto the data. We propose a simple procedure for sequentially aggregating\nstatistics constructed with multiple splits of the same sample. The user\nspecifies a bound and a nominal error rate. If the procedure is implemented\ntwice on the same data, the nominal error rate approximates the chance that the\nresults differ by more than the bound. We illustrate the application of the\nprocedure to several widely applied econometric methods.",
        "authors": [
            "David M. Ritzwoller",
            "Joseph P. Romano"
        ],
        "categories": "econ.EM",
        "published": "2023-11-23T21:05:40Z",
        "updated": "2024-11-15T17:41:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.14032v3",
        "title": "Counterfactual Sensitivity in Quantitative Trade and Spatial Models",
        "abstract": "Counterfactuals in quantitative trade and spatial models are functions of the\ncurrent state of the world and the model parameters. Common practice treats the\ncurrent state of the world as perfectly observed, but there is good reason to\nbelieve that it is measured with error. This paper provides tools for\nquantifying uncertainty about counterfactuals when the current state of the\nworld is measured with error. I recommend an empirical Bayes approach to\nuncertainty quantification, and show that it is both practical and\ntheoretically justified. I apply the proposed method to the settings in Adao,\nCostinot, and Donaldson (2017) and Allen and Arkolakis (2022) and find\nnon-trivial uncertainty about counterfactuals.",
        "authors": [
            "Bas Sanders"
        ],
        "categories": "econ.EM",
        "published": "2023-11-23T14:36:45Z",
        "updated": "2024-05-10T13:12:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.13969v3",
        "title": "Was Javert right to be suspicious? Unpacking treatment effect heterogeneity of alternative sentences on time-to-recidivism in Brazil",
        "abstract": "This paper presents new econometric tools to unpack the treatment effect\nheterogeneity of punishing misdemeanor offenses on time-to-recidivism. We show\nhow one can identify, estimate, and make inferences on the distributional,\nquantile, and average marginal treatment effects in setups where the treatment\nselection is endogenous and the outcome of interest, usually a duration\nvariable, is potentially right-censored. We explore our proposed econometric\nmethodology to evaluate the effect of fines and community service sentences as\na form of punishment on time-to-recidivism in the State of S\\~ao Paulo, Brazil,\nbetween 2010 and 2019, leveraging the as-if random assignment of judges to\ncases. Our results highlight substantial treatment effect heterogeneity that\nother tools are not meant to capture. For instance, we find that people whom\nmost judges would punish take longer to recidivate as a consequence of the\npunishment, while people who would be punished only by strict judges recidivate\nat an earlier date than if they were not punished.",
        "authors": [
            "Santiago Acerenza",
            "Vitor Possebom",
            "Pedro H. C. Sant'Anna"
        ],
        "categories": "econ.EM",
        "published": "2023-11-23T12:32:50Z",
        "updated": "2024-05-08T17:01:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.13575v2",
        "title": "Large-Sample Properties of the Synthetic Control Method under Selection on Unobservables",
        "abstract": "We analyze the synthetic control (SC) method in panel data settings with many\nunits. We assume the treatment assignment is based on unobserved heterogeneity\nand pre-treatment information, allowing for both strictly and sequentially\nexogenous assignment processes. We show that the critical property that\ndetermines the behavior of the SC method is the ability of input features to\napproximate the unobserved heterogeneity. Our results imply that the SC method\ndelivers asymptotically normal estimators for a large class of linear panel\ndata models as long as the number of pre-treatment periods is sufficiently\nlarge, making it a natural alternative to the Difference-in-Differences.",
        "authors": [
            "Dmitry Arkhangelsky",
            "David Hirshberg"
        ],
        "categories": "econ.EM",
        "published": "2023-11-22T18:30:43Z",
        "updated": "2023-12-25T17:27:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.13327v2",
        "title": "Regressions under Adverse Conditions",
        "abstract": "We introduce a new regression method that relates the mean of an outcome\nvariable to covariates, given the \"adverse condition\" that a distress variable\nfalls in its tail. This allows to tailor classical mean regressions to adverse\neconomic scenarios, which receive increasing interest in managing macroeconomic\nand financial risks, among many others. In the terminology of the systemic risk\nliterature, our method can be interpreted as a regression for the Marginal\nExpected Shortfall. We propose a two-step procedure to estimate the new models,\nshow consistency and asymptotic normality of the estimator, and propose\nfeasible inference under weak conditions allowing for cross-sectional and time\nseries applications. The accuracy of the asymptotic approximations of the\ntwo-step estimator is verified in simulations. Two empirical applications show\nthat our regressions under adverse conditions are valuable in such diverse\nfields as the study of the relation between systemic risk and asset price\nbubbles, and dissecting macroeconomic growth vulnerabilities into individual\ncomponents.",
        "authors": [
            "Timo Dimitriadis",
            "Yannick Hoga"
        ],
        "categories": "econ.EM",
        "published": "2023-11-22T11:47:40Z",
        "updated": "2024-07-12T20:38:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.12671v1",
        "title": "Predictive Density Combination Using a Tree-Based Synthesis Function",
        "abstract": "Bayesian predictive synthesis (BPS) provides a method for combining multiple\npredictive distributions based on agent/expert opinion analysis theory and\nencompasses a range of existing density forecast pooling methods. The key\ningredient in BPS is a ``synthesis'' function. This is typically specified\nparametrically as a dynamic linear regression. In this paper, we develop a\nnonparametric treatment of the synthesis function using regression trees. We\nshow the advantages of our tree-based approach in two macroeconomic forecasting\napplications. The first uses density forecasts for GDP growth from the euro\narea's Survey of Professional Forecasters. The second combines density\nforecasts of US inflation produced by many regression models involving\ndifferent predictors. Both applications demonstrate the benefits -- in terms of\nimproved forecast accuracy and interpretability -- of modeling the synthesis\nfunction nonparametrically.",
        "authors": [
            "Tony Chernis",
            "Niko Hauzenberger",
            "Florian Huber",
            "Gary Koop",
            "James Mitchell"
        ],
        "categories": "econ.EM",
        "published": "2023-11-21T15:29:09Z",
        "updated": "2023-11-21T15:29:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.12267v2",
        "title": "Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity",
        "abstract": "We study causal representation learning, the task of recovering high-level\nlatent variables and their causal relationships in the form of a causal graph\nfrom low-level observed data (such as text and images), assuming access to\nobservations generated from multiple environments. Prior results on the\nidentifiability of causal representations typically assume access to\nsingle-node interventions which is rather unrealistic in practice, since the\nlatent variables are unknown in the first place. In this work, we provide the\nfirst identifiability results based on data that stem from general\nenvironments. We show that for linear causal models, while the causal graph can\nbe fully recovered, the latent variables are only identified up to the\nsurrounded-node ambiguity (SNA) \\citep{varici2023score}. We provide a\ncounterpart of our guarantee, showing that SNA is basically unavoidable in our\nsetting. We also propose an algorithm, \\texttt{LiNGCReL} which provably\nrecovers the ground-truth model up to SNA, and we demonstrate its effectiveness\nvia numerical experiments. Finally, we consider general non-parametric causal\nmodels and show that the same identification barrier holds when assuming access\nto groups of soft single-node interventions.",
        "authors": [
            "Jikai Jin",
            "Vasilis Syrgkanis"
        ],
        "categories": "cs.LG",
        "published": "2023-11-21T01:09:11Z",
        "updated": "2024-02-03T06:56:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.12878v2",
        "title": "Adaptive Bayesian Learning with Action and State-Dependent Signal Variance",
        "abstract": "This manuscript presents an advanced framework for Bayesian learning by\nincorporating action and state-dependent signal variances into decision-making\nmodels. This framework is pivotal in understanding complex data-feedback loops\nand decision-making processes in various economic systems. Through a series of\nexamples, we demonstrate the versatility of this approach in different\ncontexts, ranging from simple Bayesian updating in stable environments to\ncomplex models involving social learning and state-dependent uncertainties. The\npaper uniquely contributes to the understanding of the nuanced interplay\nbetween data, actions, outcomes, and the inherent uncertainty in economic\nmodels.",
        "authors": [
            "Kaiwen Hou"
        ],
        "categories": "stat.ME",
        "published": "2023-11-20T17:59:30Z",
        "updated": "2023-11-28T18:29:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.11858v2",
        "title": "Theory coherent shrinkage of Time-Varying Parameters in VARs",
        "abstract": "This paper introduces a novel theory-coherent shrinkage prior for\nTime-Varying Parameter VARs (TVP-VARs). The prior centers the time-varying\nparameters on a path implied a priori by an underlying economic theory, chosen\nto describe the dynamics of the macroeconomic variables in the system.\nLeveraging information from conventional economic theory using this prior\nsignificantly improves inference precision and forecast accuracy compared to\nthe standard TVP-VAR. In an application, I use this prior to incorporate\ninformation from a New Keynesian model that includes both the Zero Lower Bound\n(ZLB) and forward guidance into a medium-scale TVP-VAR model. This approach\nleads to more precise estimates of the impulse response functions, revealing a\ndistinct propagation of risk premium shocks inside and outside the ZLB in US\ndata.",
        "authors": [
            "Andrea Renzetti"
        ],
        "categories": "econ.EM",
        "published": "2023-11-20T15:52:22Z",
        "updated": "2024-11-04T15:11:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.11637v1",
        "title": "Modeling economies of scope in joint production: Convex regression of input distance function",
        "abstract": "Modeling of joint production has proved a vexing problem. This paper develops\na radial convex nonparametric least squares (CNLS) approach to estimate the\ninput distance function with multiple outputs. We document the correct input\ndistance function transformation and prove that the necessary orthogonality\nconditions can be satisfied in radial CNLS. A Monte Carlo study is performed to\ncompare the finite sample performance of radial CNLS and other deterministic\nand stochastic frontier approaches in terms of the input distance function\nestimation. We apply our novel approach to the Finnish electricity distribution\nnetwork regulation and empirically confirm that the input isoquants become more\ncurved. In addition, we introduce the weight restriction to radial CNLS to\nmitigate the potential overfitting and increase the out-of-sample performance\nin energy regulation.",
        "authors": [
            "Timo Kuosmanen",
            "Sheng Dai"
        ],
        "categories": "stat.ME",
        "published": "2023-11-20T09:53:08Z",
        "updated": "2023-11-20T09:53:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.10685v2",
        "title": "High-Throughput Asset Pricing",
        "abstract": "We use empirical Bayes (EB) to mine data on 140,000 long-short strategies\nconstructed from accounting ratios, past returns, and ticker symbols. This\n\"high-throughput asset pricing\" produces out-of-sample performance comparable\nto strategies in top finance journals. But unlike the published strategies, the\ndata-mined strategies are free of look-ahead bias. EB predicts that high\nreturns are concentrated in accounting strategies, small stocks, and pre-2004\nsamples, consistent with limited attention theories. The intuition is seen in\nthe cross-sectional distribution of t-stats, which is far from the null for\nequal-weighted accounting strategies. High-throughput methods provide a\nrigorous, unbiased method for documenting asset pricing facts.",
        "authors": [
            "Andrew Y. Chen",
            "Chukwuma Dim"
        ],
        "categories": "q-fin.GN",
        "published": "2023-11-17T18:13:45Z",
        "updated": "2024-03-14T20:10:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.09972v3",
        "title": "Inference in Auctions with Many Bidders Using Transaction Prices",
        "abstract": "This paper studies inference in first- and second-price sealed-bid auctions\nwith many bidders, using an asymptotic framework where the number of bidders\nincreases while the number of auctions remains fixed. Relevant applications\ninclude online, treasury, spectrum, and art auctions. Our approach enables\nasymptotically exact inference on key features such as the winner's expected\nutility, the seller's expected revenue, and the tail of the valuation\ndistribution using only transaction price data. Our simulations demonstrate the\naccuracy of the methods in finite samples. We apply our methods to Hong Kong\nvehicle license auctions, focusing on high-priced, single-letter plates.",
        "authors": [
            "Federico A. Bugni",
            "Yulong Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-11-16T15:47:30Z",
        "updated": "2024-09-30T15:43:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.09435v1",
        "title": "Estimating Functionals of the Joint Distribution of Potential Outcomes with Optimal Transport",
        "abstract": "Many causal parameters depend on a moment of the joint distribution of\npotential outcomes. Such parameters are especially relevant in policy\nevaluation settings, where noncompliance is common and accommodated through the\nmodel of Imbens & Angrist (1994). This paper shows that the sharp identified\nset for these parameters is an interval with endpoints characterized by the\nvalue of optimal transport problems. Sample analogue estimators are proposed\nbased on the dual problem of optimal transport. These estimators are root-n\nconsistent and converge in distribution under mild assumptions. Inference\nprocedures based on the bootstrap are straightforward and computationally\nconvenient. The ideas and estimators are demonstrated in an application\nrevisiting the National Supported Work Demonstration job training program. I\nfind suggestive evidence that workers who would see below average earnings\nwithout treatment tend to see above average benefits from treatment.",
        "authors": [
            "Daniel Ober-Reynolds"
        ],
        "categories": "econ.EM",
        "published": "2023-11-15T23:12:14Z",
        "updated": "2023-11-15T23:12:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.08963v1",
        "title": "Incorporating Preferences Into Treatment Assignment Problems",
        "abstract": "This study investigates the problem of individualizing treatment allocations\nusing stated preferences for treatments. If individuals know in advance how the\nassignment will be individualized based on their stated preferences, they may\nstate false preferences. We derive an individualized treatment rule (ITR) that\nmaximizes welfare when individuals strategically state their preferences. We\nalso show that the optimal ITR is strategy-proof, that is, individuals do not\nhave a strong incentive to lie even if they know the optimal ITR a priori.\nConstructing the optimal ITR requires information on the distribution of true\npreferences and the average treatment effect conditioned on true preferences.\nIn practice, the information must be identified and estimated from the data. As\ntrue preferences are hidden information, the identification is not\nstraightforward. We discuss two experimental designs that allow the\nidentification: strictly strategy-proof randomized controlled trials and doubly\nrandomized preference trials. Under the presumption that data comes from one of\nthese experiments, we develop data-dependent procedures for determining ITR,\nthat is, statistical treatment rules (STRs). The maximum regret of the proposed\nSTRs converges to zero at a rate of the square root of the sample size. An\nempirical application demonstrates our proposed STRs.",
        "authors": [
            "Daido Kido"
        ],
        "categories": "econ.EM",
        "published": "2023-11-15T13:51:42Z",
        "updated": "2023-11-15T13:51:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.08958v1",
        "title": "Locally Asymptotically Minimax Statistical Treatment Rules Under Partial Identification",
        "abstract": "Policymakers often desire a statistical treatment rule (STR) that determines\na treatment assignment rule deployed in a future population from available\ndata. With the true knowledge of the data generating process, the average\ntreatment effect (ATE) is the key quantity characterizing the optimal treatment\nrule. Unfortunately, the ATE is often not point identified but partially\nidentified. Presuming the partial identification of the ATE, this study\nconducts a local asymptotic analysis and develops the locally asymptotically\nminimax (LAM) STR. The analysis does not assume the full differentiability but\nthe directional differentiability of the boundary functions of the\nidentification region of the ATE. Accordingly, the study shows that the LAM STR\ndiffers from the plug-in STR. A simulation study also demonstrates that the LAM\nSTR outperforms the plug-in STR.",
        "authors": [
            "Daido Kido"
        ],
        "categories": "econ.EM",
        "published": "2023-11-15T13:47:24Z",
        "updated": "2023-11-15T13:47:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.08218v6",
        "title": "Estimating Conditional Value-at-Risk with Nonstationary Quantile Predictive Regression Models",
        "abstract": "This paper develops an asymptotic distribution theory for an endogenous\ninstrumentation approach in quantile predictive regressions when both generated\ncovariates and persistent predictors are used. The generated covariates are\nobtained from an auxiliary quantile predictive regression model and the\nstatistical problem of interest is the robust estimation and inference of the\nparameters that correspond to the primary quantile predictive regression in\nwhich this generated covariate is added to the set of nonstationary regressors.\nWe find that the proposed doubly IVX corrected estimator is robust to the\nabstract degree of persistence regardless of the presence of generated\nregressor obtained from the first stage procedure. The asymptotic properties of\nthe two-stage IVX estimator such as mixed Gaussianity are established while the\nasymptotic covariance matrix is adjusted to account for the first-step\nestimation error.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-11-14T14:55:44Z",
        "updated": "2024-04-21T12:48:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.07243v1",
        "title": "Optimal Estimation of Large-Dimensional Nonlinear Factor Models",
        "abstract": "This paper studies optimal estimation of large-dimensional nonlinear factor\nmodels. The key challenge is that the observed variables are possibly nonlinear\nfunctions of some latent variables where the functional forms are left\nunspecified. A local principal component analysis method is proposed to\nestimate the factor structure and recover information on latent variables and\nlatent functions, which combines $K$-nearest neighbors matching and principal\ncomponent analysis. Large-sample properties are established, including a sharp\nbound on the matching discrepancy of nearest neighbors, sup-norm error bounds\nfor estimated local factors and factor loadings, and the uniform convergence\nrate of the factor structure estimator. Under mild conditions our estimator of\nthe latent factor structure can achieve the optimal rate of uniform convergence\nfor nonparametric regression. The method is illustrated with a Monte Carlo\nexperiment and an empirical application studying the effect of tax cuts on\neconomic growth.",
        "authors": [
            "Yingjie Feng"
        ],
        "categories": "math.ST",
        "published": "2023-11-13T11:25:00Z",
        "updated": "2023-11-13T11:25:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.07067v1",
        "title": "High Dimensional Binary Choice Model with Unknown Heteroskedasticity or Instrumental Variables",
        "abstract": "This paper proposes a new method for estimating high-dimensional binary\nchoice models. The model we consider is semiparametric, placing no\ndistributional assumptions on the error term, allowing for heteroskedastic\nerrors, and permitting endogenous regressors. Our proposed approaches extend\nthe special regressor estimator originally proposed by Lewbel (2000). This\nestimator becomes impractical in high-dimensional settings due to the curse of\ndimensionality associated with high-dimensional conditional density estimation.\nTo overcome this challenge, we introduce an innovative data-driven dimension\nreduction method for nonparametric kernel estimators, which constitutes the\nmain innovation of this work. The method combines distance covariance-based\nscreening with cross-validation (CV) procedures, rendering the special\nregressor estimation feasible in high dimensions. Using the new feasible\nconditional density estimator, we address the variable and moment (instrumental\nvariable) selection problems for these models. We apply penalized least squares\n(LS) and Generalized Method of Moments (GMM) estimators with a smoothly clipped\nabsolute deviation (SCAD) penalty. A comprehensive analysis of the oracle and\nasymptotic properties of these estimators is provided. Monte Carlo simulations\nare employed to demonstrate the effectiveness of our proposed procedures in\nfinite sample scenarios.",
        "authors": [
            "Fu Ouyang",
            "Thomas Tao Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-11-13T04:15:59Z",
        "updated": "2023-11-13T04:15:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.06891v1",
        "title": "Design-based Estimation Theory for Complex Experiments",
        "abstract": "This paper considers the estimation of treatment effects in randomized\nexperiments with complex experimental designs, including cases with\ninterference between units. We develop a design-based estimation theory for\narbitrary experimental designs. Our theory facilitates the analysis of many\ndesign-estimator pairs that researchers commonly employ in practice and provide\nprocedures to consistently estimate asymptotic variance bounds. We propose new\nclasses of estimators with favorable asymptotic properties from a design-based\npoint of view. In addition, we propose a scalar measure of experimental\ncomplexity which can be linked to the design-based variance of the estimators.\nWe demonstrate the performance of our estimators using simulated datasets based\non an actual network experiment studying the effect of social networks on\ninsurance adoptions.",
        "authors": [
            "Haoge Chang"
        ],
        "categories": "econ.EM",
        "published": "2023-11-12T16:30:56Z",
        "updated": "2023-11-12T16:30:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.06831v1",
        "title": "Quasi-Bayes in Latent Variable Models",
        "abstract": "Latent variable models are widely used to account for unobserved determinants\nof economic behavior. Traditional nonparametric methods to estimate latent\nheterogeneity do not scale well into multidimensional settings. Distributional\nrestrictions alleviate tractability concerns but may impart non-trivial\nmisspecification bias. Motivated by these concerns, this paper introduces a\nquasi-Bayes approach to estimate a large class of multidimensional latent\nvariable models. Our approach to quasi-Bayes is novel in that we center it\naround relating the characteristic function of observables to the distribution\nof unobservables. We propose a computationally attractive class of priors that\nare supported on Gaussian mixtures and derive contraction rates for a variety\nof latent variable models.",
        "authors": [
            "Sid Kankanala"
        ],
        "categories": "econ.EM",
        "published": "2023-11-12T13:07:02Z",
        "updated": "2023-11-12T13:07:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2401.02428v1",
        "title": "Cu\u00e1nto es demasiada inflaci\u00f3n? Una clasificaci\u00f3n de reg\u00edmenes inflacionarios",
        "abstract": "The classifications of inflationary regimes proposed in the literature have\nmostly been based on arbitrary characterizations, subject to value judgments by\nresearchers. The objective of this study is to propose a new methodological\napproach that reduces subjectivity and improves accuracy in the construction of\nsuch regimes. The method is built upon a combination of clustering techniques\nand classification trees, which allows for an historical periodization of\nArgentina's inflationary history for the period 1943-2022. Additionally, two\nprocedures are introduced to smooth out the classification over time: a measure\nof temporal contiguity of observations and a rolling method based on the simple\nmajority rule. The obtained regimes are compared against the existing\nliterature on the inflation-relative price variability relationship, revealing\na better performance of the proposed regimes.",
        "authors": [
            "Manuel de Mier",
            "Fernando Delbianco"
        ],
        "categories": "econ.GN",
        "published": "2023-11-10T19:51:18Z",
        "updated": "2023-11-10T19:51:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.05883v4",
        "title": "Time-Varying Identification of Monetary Policy Shocks",
        "abstract": "We propose a new Bayesian heteroskedastic Markov-switching structural vector\nautoregression with data-driven time-varying identification. The model selects\nalternative exclusion restrictions over time and, as a condition for the\nsearch, allows to verify identification through heteroskedasticity within each\nregime. Based on four alternative monetary policy rules, we show that a monthly\nsix-variable system supports time variation in US monetary policy shock\nidentification. In the sample-dominating first regime, systematic monetary\npolicy follows a Taylor rule extended by the term spread, effectively curbing\ninflation. In the second regime, occurring after 2000 and gaining more\npersistence after the global financial and COVID crises, it is characterized by\na money-augmented Taylor rule. This regime's unconventional monetary policy\nprovides economic stimulus, features the liquidity effect, and is complemented\nby a pure term spread shock. Absent the specific monetary policy of the second\nregime, inflation would be over one percentage point higher on average after\n2008.",
        "authors": [
            "Annika Camehl",
            "Tomasz Wo\u017aniak"
        ],
        "categories": "econ.EM",
        "published": "2023-11-10T05:53:01Z",
        "updated": "2024-05-08T02:18:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.14698v2",
        "title": "Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash",
        "abstract": "This paper investigates an approach to both speed up business decision-making\nand lower the cost of learning through experimentation by factorizing business\npolicies and employing fractional factorial experimental designs for their\nevaluation. We illustrate how this method integrates with advances in the\nestimation of heterogeneous treatment effects, elaborating on its advantages\nand foundational assumptions. We empirically demonstrate the implementation and\nbenefits of our approach and assess its validity in evaluating consumer\npromotion policies at DoorDash, which is one of the largest delivery platforms\nin the US. Our approach discovers a policy with 5% incremental profit at 67%\nlower implementation cost.",
        "authors": [
            "Yixin Tang",
            "Yicong Lin",
            "Navdeep S. Sahni"
        ],
        "categories": "stat.ME",
        "published": "2023-11-10T00:14:14Z",
        "updated": "2023-11-29T04:23:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.04073v1",
        "title": "Debiased Fixed Effects Estimation of Binary Logit Models with Three-Dimensional Panel Data",
        "abstract": "Naive maximum likelihood estimation of binary logit models with fixed effects\nleads to unreliable inference due to the incidental parameter problem. We study\nthe case of three-dimensional panel data, where the model includes three sets\nof additive and overlapping unobserved effects. This encompasses models for\nnetwork panel data, where senders and receivers maintain bilateral\nrelationships over time, and fixed effects account for unobserved heterogeneity\nat the sender-time, receiver-time, and sender-receiver levels. In an asymptotic\nframework, where all three panel dimensions grow large at constant relative\nrates, we characterize the leading bias of the naive estimator. The inference\nproblem we identify is particularly severe, as it is not possible to balance\nthe order of the bias and the standard deviation. As a consequence, the naive\nestimator has a degenerating asymptotic distribution, which exacerbates the\ninference problem relative to other fixed effects estimators studied in the\nliterature. To resolve the inference problem, we derive explicit expressions to\ndebias the fixed effects estimator.",
        "authors": [
            "Amrei Stammann"
        ],
        "categories": "econ.EM",
        "published": "2023-11-07T15:38:08Z",
        "updated": "2023-11-07T15:38:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.03471v3",
        "title": "Optimal Estimation Methodologies for Panel Data Regression Models",
        "abstract": "This survey study discusses main aspects to optimal estimation methodologies\nfor panel data regression models. In particular, we present current\nmethodological developments for modeling stationary panel data as well as\nrobust methods for estimation and inference in nonstationary panel data\nregression models. Some applications from the network econometrics and high\ndimensional statistics literature are also discussed within a stationary time\nseries environment.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-11-06T19:15:11Z",
        "updated": "2023-11-11T15:12:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.02789v5",
        "title": "Estimation and Inference for a Class of Generalized Hierarchical Models",
        "abstract": "In this paper, we consider estimation and inference for the unknown\nparameters and function involved in a class of generalized hierarchical models.\nSuch models are of great interest in the literature of neural networks (such as\nBauer and Kohler, 2019). We propose a rectified linear unit (ReLU) based deep\nneural network (DNN) approach, and contribute to the design of DNN by i)\nproviding more transparency for practical implementation, ii) defining\ndifferent types of sparsity, iii) showing the differentiability, iv) pointing\nout the set of effective parameters, and v) offering a new variant of rectified\nlinear activation function (ReLU), etc. Asymptotic properties are established\naccordingly, and a feasible procedure for the purpose of inference is also\nproposed. We conduct extensive numerical studies to examine the finite-sample\nperformance of the estimation methods, and we also evaluate the empirical\nrelevance and applicability of the proposed models and estimation methods to\nreal data.",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Bin Peng",
            "Yayi Yan"
        ],
        "categories": "econ.EM",
        "published": "2023-11-05T23:07:17Z",
        "updated": "2024-04-02T04:07:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.02467v2",
        "title": "Individualized Policy Evaluation and Learning under Clustered Network Interference",
        "abstract": "While there now exists a large literature on policy evaluation and learning,\nmuch of prior work assumes that the treatment assignment of one unit does not\naffect the outcome of another unit. Unfortunately, ignoring interference may\nlead to biased policy evaluation and ineffective learned policies. For example,\ntreating influential individuals who have many friends can generate positive\nspillover effects, thereby improving the overall performance of an\nindividualized treatment rule (ITR). We consider the problem of evaluating and\nlearning an optimal ITR under clustered network interference (also known as\npartial interference) where clusters of units are sampled from a population and\nunits may influence one another within each cluster. Unlike previous methods\nthat impose strong restrictions on spillover effects, the proposed methodology\nonly assumes a semiparametric structural model where each unit's outcome is an\nadditive function of individual treatments within the cluster. Under this\nmodel, we propose an estimator that can be used to evaluate the empirical\nperformance of an ITR. We show that this estimator is substantially more\nefficient than the standard inverse probability weighting estimator, which does\nnot impose any assumption about spillover effects. We derive the finite-sample\nregret bound for a learned ITR, showing that the use of our efficient\nevaluation estimator leads to the improved performance of learned policies.\nFinally, we conduct simulation and empirical studies to illustrate the\nadvantages of the proposed methodology.",
        "authors": [
            "Yi Zhang",
            "Kosuke Imai"
        ],
        "categories": "stat.ME",
        "published": "2023-11-04T17:58:24Z",
        "updated": "2024-02-04T18:47:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.02299v2",
        "title": "The Fragility of Sparsity",
        "abstract": "We show, using three empirical applications, that linear regression estimates\nwhich rely on the assumption of sparsity are fragile in two ways. First, we\ndocument that different choices of the regressor matrix that do not impact\nordinary least squares (OLS) estimates, such as the choice of baseline category\nwith categorical controls, can move sparsity-based estimates two standard\nerrors or more. Second, we develop two tests of the sparsity assumption based\non comparing sparsity-based estimators with OLS. The tests tend to reject the\nsparsity assumption in all three applications. Unless the number of regressors\nis comparable to or exceeds the sample size, OLS yields more robust results at\nlittle efficiency cost.",
        "authors": [
            "Michal Koles\u00e1r",
            "Ulrich K. M\u00fcller",
            "Sebastian T. Roelsgaard"
        ],
        "categories": "econ.EM",
        "published": "2023-11-04T02:03:48Z",
        "updated": "2024-01-24T15:44:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.02196v1",
        "title": "Pooled Bewley Estimator of Long Run Relationships in Dynamic Heterogenous Panels",
        "abstract": "Using a transformation of the autoregressive distributed lag model due to\nBewley, a novel pooled Bewley (PB) estimator of long-run coefficients for\ndynamic panels with heterogeneous short-run dynamics is proposed. The PB\nestimator is directly comparable to the widely used Pooled Mean Group (PMG)\nestimator, and is shown to be consistent and asymptotically normal. Monte Carlo\nsimulations show good small sample performance of PB compared to the existing\nestimators in the literature, namely PMG, panel dynamic OLS (PDOLS), and panel\nfully-modified OLS (FMOLS). Application of two bias-correction methods and a\nbootstrapping of critical values to conduct inference robust to cross-sectional\ndependence of errors are also considered. The utility of the PB estimator is\nillustrated in an empirical application to the aggregate consumption function.",
        "authors": [
            "Alexander Chudik",
            "M. Hashem Pesaran",
            "Ron P. Smith"
        ],
        "categories": "econ.EM",
        "published": "2023-11-03T18:57:56Z",
        "updated": "2023-11-03T18:57:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.01217v4",
        "title": "The learning effects of subsidies to bundled goods: a semiparametric approach",
        "abstract": "Can temporary subsidies to bundles induce long-run changes in demand due to\nlearning about the quality of one of the constituent goods? This paper provides\ntheoretical support and empirical evidence on this mechanism. Theoretically, we\nintroduce a model where an agent learns about the quality of an innovation\nthrough repeated consumption. We then assess the predictions of our theory in a\nrandomised experiment in a ridesharing platform. The experiment subsidised car\ntrips integrating with a train or metro station, which we interpret as a\nbundle. Given the heavy-tailed nature of our data, we propose a semiparametric\nspecification for treatment effects that enables the construction of more\nefficient estimators. We then introduce an efficient estimator for our\nspecification by relying on L-moments. Our results indicate that a ten-weekday\n50\\% discount on integrated trips leads to a large contemporaneous increase in\nthe demand for integration, and, consistent with our model, persistent changes\nin the mean and dispersion of nonintegrated app rides. These effects last for\nover four months. A calibration of our theoretical model suggests that around\n40\\% of the contemporaneous increase in integrated rides may be attributable to\nincreased incentives to learning. Our results have nontrivial policy\nimplications for the design of public transit systems.",
        "authors": [
            "Luis Alvarez",
            "Ciro Biderman"
        ],
        "categories": "econ.EM",
        "published": "2023-11-02T13:18:57Z",
        "updated": "2024-06-06T17:19:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.00905v3",
        "title": "Data-driven fixed-point tuning for truncated realized variations",
        "abstract": "Many methods for estimating integrated volatility and related functionals of\nsemimartingales in the presence of jumps require specification of tuning\nparameters for their use in practice. In much of the available theory, tuning\nparameters are assumed to be deterministic and their values are specified only\nup to asymptotic constraints. However, in empirical work and in simulation\nstudies, they are typically chosen to be random and data-dependent, with\nexplicit choices often relying entirely on heuristics. In this paper, we\nconsider novel data-driven tuning procedures for the truncated realized\nvariations of a semimartingale with jumps based on a type of random fixed-point\niteration. Being effectively automated, our approach alleviates the need for\ndelicate decision-making regarding tuning parameters in practice and can be\nimplemented using information regarding sampling frequency alone. We\ndemonstrate our methods can lead to asymptotically efficient estimation of\nintegrated volatility and exhibit superior finite-sample performance compared\nto popular alternatives in the literature.",
        "authors": [
            "B. Cooper Boniece",
            "Jos\u00e9 E. Figueroa-L\u00f3pez",
            "Yuchen Han"
        ],
        "categories": "math.ST",
        "published": "2023-11-02T00:13:19Z",
        "updated": "2024-10-21T21:19:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.00662v2",
        "title": "On Gaussian Process Priors in Conditional Moment Restriction Models",
        "abstract": "This paper studies quasi Bayesian estimation and uncertainty quantification\nfor an unknown function that is identified by a nonparametric conditional\nmoment restriction. We derive contraction rates for a class of Gaussian process\npriors. Furthermore, we provide conditions under which a Bernstein von Mises\ntheorem holds for the quasi-posterior distribution. As a consequence, we show\nthat optimally weighted quasi-Bayes credible sets have exact asymptotic\nfrequentist coverage.",
        "authors": [
            "Sid Kankanala"
        ],
        "categories": "econ.EM",
        "published": "2023-11-01T17:09:54Z",
        "updated": "2023-11-07T18:41:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.00577v1",
        "title": "Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests",
        "abstract": "We consider learning personalized assignments to one of many treatment arms\nfrom a randomized controlled trial. Standard methods that estimate\nheterogeneous treatment effects separately for each arm may perform poorly in\nthis case due to excess variance. We instead propose methods that pool\ninformation across treatment arms: First, we consider a regularized\nforest-based assignment algorithm based on greedy recursive partitioning that\nshrinks effect estimates across arms. Second, we augment our algorithm by a\nclustering scheme that combines treatment arms with consistently similar\noutcomes. In a simulation study, we compare the performance of these approaches\nto predicting arm-wise outcomes separately, and document gains of directly\noptimizing the treatment assignment with regularization and clustering. In a\ntheoretical model, we illustrate how a high number of treatment arms makes\nfinding the best arm hard, while we can achieve sizable utility gains from\npersonalization by regularized optimization.",
        "authors": [
            "Rahul Ladhania",
            "Jann Spiess",
            "Lyle Ungar",
            "Wenbo Wu"
        ],
        "categories": "stat.ML",
        "published": "2023-11-01T15:18:22Z",
        "updated": "2023-11-01T15:18:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.00439v3",
        "title": "Robustify and Tighten the Lee Bounds: A Sample Selection Model under Stochastic Monotonicity and Symmetry Assumptions",
        "abstract": "In the presence of sample selection, Lee's (2009) nonparametric bounds are a\npopular tool for estimating a treatment effect. However, the Lee bounds rely on\nthe monotonicity assumption, whose empirical validity is sometimes unclear.\nFurthermore, the bounds are often regarded to be wide and less informative even\nunder monotonicity. To address these issues, this study introduces a stochastic\nversion of the monotonicity assumption alongside a nonparametric distributional\nshape constraint. The former enhances the robustness of the Lee bounds with\nrespect to monotonicity, while the latter helps tighten these bounds. The\nobtained bounds do not rely on the exclusion restriction and can be root-$n$\nconsistently estimable, making them practically viable. The potential\nusefulness of the proposed methods is illustrated by their application on\nexperimental data from the after-school instruction programme studied by\nMuralidharan, Singh, and Ganimian (2019).",
        "authors": [
            "Yuta Okamoto"
        ],
        "categories": "econ.EM",
        "published": "2023-11-01T11:04:08Z",
        "updated": "2024-03-19T07:04:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.00013v2",
        "title": "Semiparametric Discrete Choice Models for Bundles",
        "abstract": "We propose two approaches to estimate semiparametric discrete choice models\nfor bundles. Our first approach is a kernel-weighted rank estimator based on a\nmatching-based identification strategy. We establish its complete asymptotic\nproperties and prove the validity of the nonparametric bootstrap for inference.\nWe then introduce a new multi-index least absolute deviations (LAD) estimator\nas an alternative, of which the main advantage is its capacity to estimate\npreference parameters on both alternative- and agent-specific regressors. Both\nmethods can account for arbitrary correlation in disturbances across choices,\nwith the former also allowing for interpersonal heteroskedasticity. We also\ndemonstrate that the identification strategy underlying these procedures can be\nextended naturally to panel data settings, producing an analogous localized\nmaximum score estimator and a LAD estimator for estimating bundle choice models\nwith fixed effects. We derive the limiting distribution of the former and\nverify the validity of the numerical bootstrap as an inference tool. All our\nproposed methods can be applied to general multi-index models. Monte Carlo\nexperiments show that they perform well in finite samples.",
        "authors": [
            "Fu Ouyang",
            "Thomas Tao Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-10-31T14:26:58Z",
        "updated": "2023-11-07T11:30:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.19992v1",
        "title": "Robust Estimation of Realized Correlation: New Insight about Intraday Fluctuations in Market Betas",
        "abstract": "Time-varying volatility is an inherent feature of most economic time-series,\nwhich causes standard correlation estimators to be inconsistent. The quadrant\ncorrelation estimator is consistent but very inefficient. We propose a novel\nsubsampled quadrant estimator that improves efficiency while preserving\nconsistency and robustness. This estimator is particularly well-suited for\nhigh-frequency financial data and we apply it to a large panel of US stocks.\nOur empirical analysis sheds new light on intra-day fluctuations in market\nbetas by decomposing them into time-varying correlations and relative\nvolatility changes. Our results show that intraday variation in betas is\nprimarily driven by intraday variation in correlations.",
        "authors": [
            "Peter Reinhard Hansen",
            "Yiyao Luo"
        ],
        "categories": "econ.EM",
        "published": "2023-10-30T20:21:11Z",
        "updated": "2023-10-30T20:21:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.19788v3",
        "title": "Worst-Case Optimal Multi-Armed Gaussian Best Arm Identification with a Fixed Budget",
        "abstract": "This study investigates the experimental design problem for identifying the\narm with the highest expected outcome, referred to as best arm identification\n(BAI). In our experiments, the number of treatment-allocation rounds is fixed.\nDuring each round, a decision-maker allocates an arm and observes a\ncorresponding outcome, which follows a Gaussian distribution with variances\nthat can differ among the arms. At the end of the experiment, the\ndecision-maker recommends one of the arms as an estimate of the best arm. To\ndesign an experiment, we first discuss lower bounds for the probability of\nmisidentification. Our analysis highlights that the available information on\nthe outcome distribution, such as means (expected outcomes), variances, and the\nchoice of the best arm, significantly influences the lower bounds. Because\navailable information is limited in actual experiments, we develop a lower\nbound that is valid under the unknown means and the unknown choice of the best\narm, which are referred to as the worst-case lower bound. We demonstrate that\nthe worst-case lower bound depends solely on the variances of the outcomes.\nThen, under the assumption that the variances are known, we propose the\nGeneralized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, an\nextension of the Neyman allocation proposed by Neyman (1934). We show that the\nGNA-EBA strategy is asymptotically optimal in the sense that its probability of\nmisidentification aligns with the lower bounds as the sample size increases\ninfinitely and the differences between the expected outcomes of the best and\nother suboptimal arms converge to the same values across arms. We refer to such\nstrategies as asymptotically worst-case optimal.",
        "authors": [
            "Masahiro Kato"
        ],
        "categories": "math.ST",
        "published": "2023-10-30T17:52:46Z",
        "updated": "2024-03-11T00:56:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.19747v2",
        "title": "Characteristics of price related fluctuations in Non-Fungible Token (NFT) market",
        "abstract": "A non-fungible token (NFT) market is a new trading invention based on the\nblockchain technology which parallels the cryptocurrency market. In the present\nwork we study capitalization, floor price, the number of transactions, the\ninter-transaction times, and the transaction volume value of a few selected\npopular token collections. The results show that the fluctuations of all these\nquantities are characterized by heavy-tailed probability distribution\nfunctions, in most cases well described by the stretched exponentials, with a\ntrace of power-law scaling at times, long-range memory, and in several cases\neven the fractal organization of fluctuations, mostly restricted to the larger\nfluctuations, however. We conclude that the NFT market - even though young and\ngoverned by a somewhat different mechanisms of trading - shares several\nstatistical properties with the regular financial markets. However, some\ndifferences are visible in the specific quantitative indicators.",
        "authors": [
            "Pawe\u0142 Szyd\u0142o",
            "Marcin W\u0105torek",
            "Jaros\u0142aw Kwapie\u0144",
            "Stanis\u0142aw Dro\u017cd\u017c"
        ],
        "categories": "q-fin.CP",
        "published": "2023-10-30T17:15:18Z",
        "updated": "2024-01-16T14:14:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.19557v1",
        "title": "A Bayesian Markov-switching SAR model for time-varying cross-price spillovers",
        "abstract": "The spatial autoregressive (SAR) model is extended by introducing a Markov\nswitching dynamics for the weight matrix and spatial autoregressive parameter.\nThe framework enables the identification of regime-specific connectivity\npatterns and strengths and the study of the spatiotemporal propagation of\nshocks in a system with a time-varying spatial multiplier matrix. The proposed\nmodel is applied to disaggregated CPI data from 15 EU countries to examine\ncross-price dependencies. The analysis identifies distinct connectivity\nstructures and spatial weights across the states, which capture shifts in\nconsumer behaviour, with marked cross-country differences in the spillover from\none price category to another.",
        "authors": [
            "Christian Glocker",
            "Matteo Iacopini",
            "Tam\u00e1s Krisztin",
            "Philipp Piribauer"
        ],
        "categories": "stat.AP",
        "published": "2023-10-30T14:12:05Z",
        "updated": "2023-10-30T14:12:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.19543v1",
        "title": "Spectral identification and estimation of mixed causal-noncausal invertible-noninvertible models",
        "abstract": "This paper introduces new techniques for estimating, identifying and\nsimulating mixed causal-noncausal invertible-noninvertible models. We propose a\nframework that integrates high-order cumulants, merging both the spectrum and\nbispectrum into a single estimation function. The model that most adequately\nrepresents the data under the assumption that the error term is i.i.d. is\nselected. Our Monte Carlo study reveals unbiased parameter estimates and a high\nfrequency with which correct models are identified. We illustrate our strategy\nthrough an empirical analysis of returns from 24 Fama-French emerging market\nstock portfolios. The findings suggest that each portfolio displays noncausal\ndynamics, producing white noise residuals devoid of conditional heteroscedastic\neffects.",
        "authors": [
            "Alain Hecq",
            "Daniel Velasquez-Gaviria"
        ],
        "categories": "econ.EM",
        "published": "2023-10-30T13:49:10Z",
        "updated": "2023-10-30T13:49:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.19200v1",
        "title": "Popularity, face and voice: Predicting and interpreting livestreamers' retail performance using machine learning techniques",
        "abstract": "Livestreaming commerce, a hybrid of e-commerce and self-media, has expanded\nthe broad spectrum of traditional sales performance determinants. To\ninvestigate the factors that contribute to the success of livestreaming\ncommerce, we construct a longitudinal firm-level database with 19,175\nobservations, covering an entire livestreaming subsector. By comparing the\nforecasting accuracy of eight machine learning models, we identify a random\nforest model that provides the best prediction of gross merchandise volume\n(GMV). Furthermore, we utilize explainable artificial intelligence to open the\nblack-box of machine learning model, discovering four new facts: 1) variables\nrepresenting the popularity of livestreaming events are crucial features in\npredicting GMV. And voice attributes are more important than appearance; 2)\npopularity is a major determinant of sales for female hosts, while vocal\naesthetics is more decisive for their male counterparts; 3) merits and\ndrawbacks of the voice are not equally valued in the livestreaming market; 4)\nbased on changes of comments, page views and likes, sales growth can be divided\ninto three stages. Finally, we innovatively propose a 3D-SHAP diagram that\ndemonstrates the relationship between predicting feature importance, target\nvariable, and its predictors. This diagram identifies bottlenecks for both\nbeginner and top livestreamers, providing insights into ways to optimize their\nsales performance.",
        "authors": [
            "Xiong Xiong",
            "Fan Yang",
            "Li Su"
        ],
        "categories": "econ.EM",
        "published": "2023-10-29T23:48:34Z",
        "updated": "2023-10-29T23:48:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.18836v3",
        "title": "Cluster-Randomized Trials with Cross-Cluster Interference",
        "abstract": "The literature on cluster-randomized trials typically assumes no interference\nacross clusters. This may be implausible when units are irregularly distributed\nin space without well-separated communities, in which case clusters may not\nrepresent significant geographic, social, or economic divisions. In this paper,\nwe develop methods for reducing bias due to cross-cluster interference. First,\nwe propose an estimation strategy that excludes units not surrounded by\nclusters assigned to the same treatment arm. We show that this substantially\nreduces asymptotic bias relative to conventional difference-in-means estimators\nwithout substantial cost to variance. Second, we formally establish a\nbias-variance trade-off in the choice of clusters: constructing fewer, larger\nclusters reduces bias due to interference but increases variance. We provide a\nrule for choosing the number of clusters to balance the asymptotic orders of\nthe bias and variance of our estimator. Finally, we consider unsupervised\nlearning for cluster construction and provide theoretical guarantees for\n$k$-medoids.",
        "authors": [
            "Michael P. Leung"
        ],
        "categories": "stat.ME",
        "published": "2023-10-28T22:36:37Z",
        "updated": "2024-11-03T23:26:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.18563v1",
        "title": "Covariate Balancing and the Equivalence of Weighting and Doubly Robust Estimators of Average Treatment Effects",
        "abstract": "We show that when the propensity score is estimated using a suitable\ncovariate balancing procedure, the commonly used inverse probability weighting\n(IPW) estimator, augmented inverse probability weighting (AIPW) with linear\nconditional mean, and inverse probability weighted regression adjustment\n(IPWRA) with linear conditional mean are all numerically the same for\nestimating the average treatment effect (ATE) or the average treatment effect\non the treated (ATT). Further, suitably chosen covariate balancing weights are\nautomatically normalized, which means that normalized and unnormalized versions\nof IPW and AIPW are identical. For estimating the ATE, the weights that achieve\nthe algebraic equivalence of IPW, AIPW, and IPWRA are based on propensity\nscores estimated using the inverse probability tilting (IPT) method of Graham,\nPinto and Egel (2012). For the ATT, the weights are obtained using the\ncovariate balancing propensity score (CBPS) method developed in Imai and\nRatkovic (2014). These equivalences also make covariate balancing methods\nattractive when the treatment is confounded and one is interested in the local\naverage treatment effect.",
        "authors": [
            "Tymon S\u0142oczy\u0144ski",
            "S. Derya Uysal",
            "Jeffrey M. Wooldridge"
        ],
        "categories": "econ.EM",
        "published": "2023-10-28T02:25:14Z",
        "updated": "2023-10-28T02:25:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.18504v3",
        "title": "Doubly Robust Identification of Causal Effects of a Continuous Treatment using Discrete Instruments",
        "abstract": "Many empirical applications estimate causal effects of a continuous\nendogenous variable (treatment) using a binary instrument. Estimation is\ntypically done through linear 2SLS. This approach requires a mean treatment\nchange and causal interpretation requires the LATE-type monotonicity in the\nfirst stage. An alternative approach is to explore distributional changes in\nthe treatment, where the first-stage restriction is treatment rank similarity.\nWe propose causal estimands that are doubly robust in that they are valid under\neither of these two restrictions. We apply the doubly robust estimation to\nestimate the impacts of sleep on well-being. Our new estimates corroborate the\nusual 2SLS estimates.",
        "authors": [
            "Yingying Dong",
            "Ying-Ying Lee"
        ],
        "categories": "econ.EM",
        "published": "2023-10-27T21:43:11Z",
        "updated": "2024-02-26T20:47:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.17571v3",
        "title": "Inside the black box: Neural network-based real-time prediction of US recessions",
        "abstract": "Long short-term memory (LSTM) and gated recurrent unit (GRU) are used to\nmodel US recessions from 1967 to 2021. Their predictive performances are\ncompared to those of the traditional linear models. The out-of-sample\nperformance suggests the application of LSTM and GRU in recession forecasting,\nespecially for longer-term forecasts. The Shapley additive explanations (SHAP)\nmethod is applied to both groups of models. The SHAP-based different weight\nassignments imply the capability of these types of neural networks to capture\nthe business cycle asymmetries and nonlinearities. The SHAP method delivers key\nrecession indicators, such as the S&P 500 index for short-term forecasting up\nto 3 months and the term spread for longer-term forecasting up to 12 months.\nThese findings are robust against other interpretation methods, such as the\nlocal interpretable model-agnostic explanations (LIME) and the marginal\neffects.",
        "authors": [
            "Seulki Chung"
        ],
        "categories": "econ.EM",
        "published": "2023-10-26T16:58:16Z",
        "updated": "2024-05-23T15:51:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.17496v5",
        "title": "Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach",
        "abstract": "In modern recommendation systems, the standard pipeline involves training\nmachine learning models on historical data to predict user behaviors and\nimprove recommendations continuously. However, these data training loops can\nintroduce interference in A/B tests, where data generated by control and\ntreatment algorithms, potentially with different distributions, are combined.\nTo address these challenges, we introduce a novel approach called weighted\ntraining. This approach entails training a model to predict the probability of\neach data point appearing in either the treatment or control data and\nsubsequently applying weighted losses during model training. We demonstrate\nthat this approach achieves the least variance among all estimators that do not\ncause shifts in the training distributions. Through simulation studies, we\ndemonstrate the lower bias and variance of our approach compared to other\nmethods.",
        "authors": [
            "Nian Si"
        ],
        "categories": "stat.ME",
        "published": "2023-10-26T15:52:34Z",
        "updated": "2024-04-05T00:40:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.17473v1",
        "title": "Bayesian SAR model with stochastic volatility and multiple time-varying weights",
        "abstract": "A novel spatial autoregressive model for panel data is introduced, which\nincorporates multilayer networks and accounts for time-varying relationships.\nMoreover, the proposed approach allows the structural variance to evolve\nsmoothly over time and enables the analysis of shock propagation in terms of\ntime-varying spillover effects. The framework is applied to analyse the\ndynamics of international relationships among the G7 economies and their impact\non stock market returns and volatilities. The findings underscore the\nsubstantial impact of cooperative interactions and highlight discernible\ndisparities in network exposure across G7 nations, along with nuanced patterns\nin direct and indirect spillover effects.",
        "authors": [
            "Michele Costola",
            "Matteo Iacopini",
            "Casper Wichers"
        ],
        "categories": "stat.AP",
        "published": "2023-10-26T15:24:11Z",
        "updated": "2023-10-26T15:24:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.17278v2",
        "title": "Dynamic Factor Models: a Genealogy",
        "abstract": "Dynamic factor models have been developed out of the need of analyzing and\nforecasting time series in increasingly high dimensions. While mathematical\nstatisticians faced with inference problems in high-dimensional observation\nspaces were focusing on the so-called spiked-model-asymptotics, econometricians\nadopted an entirely and considerably more effective asymptotic approach, rooted\nin the factor models originally considered in psychometrics. The so-called\ndynamic factor model methods, in two decades, has grown into a wide and\nsuccessful body of techniques that are widely used in central banks, financial\ninstitutions, economic and statistical institutes. The objective of this\nchapter is not an extensive survey of the topic but a sketch of its historical\ngrowth, with emphasis on the various assumptions and interpretations, and a\nfamily tree of its main variants.",
        "authors": [
            "Matteo Barigozzi",
            "Marc Hallin"
        ],
        "categories": "econ.EM",
        "published": "2023-10-26T09:59:27Z",
        "updated": "2024-01-02T10:02:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.16945v4",
        "title": "Causal Q-Aggregation for CATE Model Selection",
        "abstract": "Accurate estimation of conditional average treatment effects (CATE) is at the\ncore of personalized decision making. While there is a plethora of models for\nCATE estimation, model selection is a nontrivial task, due to the fundamental\nproblem of causal inference. Recent empirical work provides evidence in favor\nof proxy loss metrics with double robust properties and in favor of model\nensembling. However, theoretical understanding is lacking. Direct application\nof prior theoretical work leads to suboptimal oracle model selection rates due\nto the non-convexity of the model selection problem. We provide regret rates\nfor the major existing CATE ensembling approaches and propose a new CATE model\nensembling approach based on Q-aggregation using the doubly robust loss. Our\nmain result shows that causal Q-aggregation achieves statistically optimal\noracle model selection regret rates of $\\frac{\\log(M)}{n}$ (with $M$ models and\n$n$ samples), with the addition of higher-order estimation error terms related\nto products of errors in the nuisance functions. Crucially, our regret rate\ndoes not require that any of the candidate CATE models be close to the truth.\nWe validate our new method on many semi-synthetic datasets and also provide\nextensions of our work to CATE model selection with instrumental variables and\nunobserved confounding.",
        "authors": [
            "Hui Lan",
            "Vasilis Syrgkanis"
        ],
        "categories": "stat.ML",
        "published": "2023-10-25T19:27:05Z",
        "updated": "2023-11-11T02:24:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.16819v1",
        "title": "CATE Lasso: Conditional Average Treatment Effect Estimation with High-Dimensional Linear Regression",
        "abstract": "In causal inference about two treatments, Conditional Average Treatment\nEffects (CATEs) play an important role as a quantity representing an\nindividualized causal effect, defined as a difference between the expected\noutcomes of the two treatments conditioned on covariates. This study assumes\ntwo linear regression models between a potential outcome and covariates of the\ntwo treatments and defines CATEs as a difference between the linear regression\nmodels. Then, we propose a method for consistently estimating CATEs even under\nhigh-dimensional and non-sparse parameters. In our study, we demonstrate that\ndesirable theoretical properties, such as consistency, remain attainable even\nwithout assuming sparsity explicitly if we assume a weaker assumption called\nimplicit sparsity originating from the definition of CATEs. In this assumption,\nwe suppose that parameters of linear models in potential outcomes can be\ndivided into treatment-specific and common parameters, where the\ntreatment-specific parameters take difference values between each linear\nregression model, while the common parameters remain identical. Thus, in a\ndifference between two linear regression models, the common parameters\ndisappear, leaving only differences in the treatment-specific parameters.\nConsequently, the non-zero parameters in CATEs correspond to the differences in\nthe treatment-specific parameters. Leveraging this assumption, we develop a\nLasso regression method specialized for CATE estimation and present that the\nestimator is consistent. Finally, we confirm the soundness of the proposed\nmethod by simulation studies.",
        "authors": [
            "Masahiro Kato",
            "Masaaki Imaizumi"
        ],
        "categories": "econ.EM",
        "published": "2023-10-25T17:51:07Z",
        "updated": "2023-10-25T17:51:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.16638v3",
        "title": "Double Debiased Covariate Shift Adaptation Robust to Density-Ratio Estimation",
        "abstract": "Consider a scenario where we have access to train data with both covariates\nand outcomes while test data only contains covariates. In this scenario, our\nprimary aim is to predict the missing outcomes of the test data. With this\nobjective in mind, we train parametric regression models under a covariate\nshift, where covariate distributions are different between the train and test\ndata. For this problem, existing studies have proposed covariate shift\nadaptation via importance weighting using the density ratio. This approach\naverages the train data losses, each weighted by an estimated ratio of the\ncovariate densities between the train and test data, to approximate the\ntest-data risk. Although it allows us to obtain a test-data risk minimizer, its\nperformance heavily relies on the accuracy of the density ratio estimation.\nMoreover, even if the density ratio can be consistently estimated, the\nestimation errors of the density ratio also yield bias in the estimators of the\nregression model's parameters of interest. To mitigate these challenges, we\nintroduce a doubly robust estimator for covariate shift adaptation via\nimportance weighting, which incorporates an additional estimator for the\nregression function. Leveraging double machine learning techniques, our\nestimator reduces the bias arising from the density ratio estimation errors. We\ndemonstrate the asymptotic distribution of the regression parameter estimator.\nNotably, our estimator remains consistent if either the density ratio estimator\nor the regression function is consistent, showcasing its robustness against\npotential errors in density ratio estimation. Finally, we confirm the soundness\nof our proposed method via simulation studies.",
        "authors": [
            "Masahiro Kato",
            "Kota Matsui",
            "Ryo Inokuchi"
        ],
        "categories": "stat.ME",
        "published": "2023-10-25T13:38:29Z",
        "updated": "2024-10-26T19:19:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.16290v1",
        "title": "Fair Adaptive Experiments",
        "abstract": "Randomized experiments have been the gold standard for assessing the\neffectiveness of a treatment or policy. The classical complete randomization\napproach assigns treatments based on a prespecified probability and may lead to\ninefficient use of data. Adaptive experiments improve upon complete\nrandomization by sequentially learning and updating treatment assignment\nprobabilities. However, their application can also raise fairness and equity\nconcerns, as assignment probabilities may vary drastically across groups of\nparticipants. Furthermore, when treatment is expected to be extremely\nbeneficial to certain groups of participants, it is more appropriate to expose\nmany of these participants to favorable treatment. In response to these\nchallenges, we propose a fair adaptive experiment strategy that simultaneously\nenhances data use efficiency, achieves an envy-free treatment assignment\nguarantee, and improves the overall welfare of participants. An important\nfeature of our proposed strategy is that we do not impose parametric modeling\nassumptions on the outcome variables, making it more versatile and applicable\nto a wider array of applications. Through our theoretical investigation, we\ncharacterize the convergence rate of the estimated treatment effects and the\nassociated standard deviations at the group level and further prove that our\nadaptive treatment assignment algorithm, despite not having a closed-form\nexpression, approaches the optimal allocation rule asymptotically. Our proof\nstrategy takes into account the fact that the allocation decisions in our\ndesign depend on sequentially accumulated data, which poses a significant\nchallenge in characterizing the properties and conducting statistical inference\nof our method. We further provide simulation evidence to showcase the\nperformance of our fair adaptive experiment strategy.",
        "authors": [
            "Waverly Wei",
            "Xinwei Ma",
            "Jingshen Wang"
        ],
        "categories": "stat.ME",
        "published": "2023-10-25T01:52:41Z",
        "updated": "2023-10-25T01:52:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.16281v4",
        "title": "Improving Robust Decisions with Data",
        "abstract": "A decision-maker faces uncertainty governed by a data-generating process\n(DGP), which is only known to belong to a set of sequences of independent but\npossibly non-identical distributions. A robust decision maximizes the expected\npayoff against the worst possible DGP in this set. This paper characterizes\nwhen and how such robust decisions can be improved with data, measured by the\nexpected payoff under the true DGP, no matter which possible DGP is the truth.\nIt further develops novel and simple inference methods to achieve it, as common\nmethods (e.g., maximum likelihood) may fail to deliver such an improvement.",
        "authors": [
            "Xiaoyu Cheng"
        ],
        "categories": "econ.TH",
        "published": "2023-10-25T01:27:24Z",
        "updated": "2024-07-27T23:58:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.15796v1",
        "title": "Testing for equivalence of pre-trends in Difference-in-Differences estimation",
        "abstract": "The plausibility of the ``parallel trends assumption'' in\nDifference-in-Differences estimation is usually assessed by a test of the null\nhypothesis that the difference between the average outcomes of both groups is\nconstant over time before the treatment. However, failure to reject the null\nhypothesis does not imply the absence of differences in time trends between\nboth groups. We provide equivalence tests that allow researchers to find\nevidence in favor of the parallel trends assumption and thus increase the\ncredibility of their treatment effect estimates. While we motivate our tests in\nthe standard two-way fixed effects model, we discuss simple extensions to\nsettings in which treatment adoption is staggered over time.",
        "authors": [
            "Holger Dette",
            "Martin Schumann"
        ],
        "categories": "econ.EM",
        "published": "2023-10-24T12:47:25Z",
        "updated": "2023-10-24T12:47:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.16850v1",
        "title": "The impact of the Russia-Ukraine conflict on the extreme risk spillovers between agricultural futures and spots",
        "abstract": "The ongoing Russia-Ukraine conflict between two major agricultural powers has\nposed significant threats and challenges to the global food system and world\nfood security. Focusing on the impact of the conflict on the global\nagricultural market, we propose a new analytical framework for tail dependence,\nand combine the Copula-CoVaR method with the ARMA-GARCH-skewed Student-t model\nto examine the tail dependence structure and extreme risk spillover between\nagricultural futures and spots over the pre- and post-outbreak periods. Our\nresults indicate that the tail dependence structures in the futures-spot\nmarkets of soybean, maize, wheat, and rice have all reacted to the\nRussia-Ukraine conflict. Furthermore, the outbreak of the conflict has\nintensified risks of the four agricultural markets in varying degrees, with the\nwheat market being affected the most. Additionally, all the agricultural\nfutures markets exhibit significant downside and upside risk spillovers to\ntheir corresponding spot markets before and after the outbreak of the conflict,\nwhereas the strengths of these extreme risk spillover effects demonstrate\nsignificant asymmetries at the directional (downside versus upside) and\ntemporal (pre-outbreak versus post-outbreak) levels.",
        "authors": [
            "Wei-Xing Zhou",
            "Yun-Shi Dai",
            "Kiet Tuan Duong",
            "Peng-Fei Dai"
        ],
        "categories": "q-fin.ST",
        "published": "2023-10-24T08:12:12Z",
        "updated": "2023-10-24T08:12:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.16849v1",
        "title": "Correlation structure analysis of the global agricultural futures market",
        "abstract": "This paper adopts the random matrix theory (RMT) to analyze the correlation\nstructure of the global agricultural futures market from 2000 to 2020. It is\nfound that the distribution of correlation coefficients is asymmetric and right\nskewed, and many eigenvalues of the correlation matrix deviate from the RMT\nprediction. The largest eigenvalue reflects a collective market effect common\nto all agricultural futures, the other largest deviating eigenvalues can be\nimplemented to identify futures groups, and there are modular structures based\non regional properties or agricultural commodities among the significant\nparticipants of their corresponding eigenvectors. Except for the smallest\neigenvalue, other smallest deviating eigenvalues represent the agricultural\nfutures pairs with highest correlations. This paper can be of reference and\nsignificance for using agricultural futures to manage risk and optimize asset\nallocation.",
        "authors": [
            "Yun-Shi Dai",
            "Ngoc Quang Anh Huynh",
            "Qing-Huan Zheng",
            "Wei-Xing Zhou"
        ],
        "categories": "q-fin.ST",
        "published": "2023-10-24T07:21:31Z",
        "updated": "2023-10-24T07:21:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.15512v3",
        "title": "Inference for Rank-Rank Regressions",
        "abstract": "The slope coefficient in a rank-rank regression is a popular measure of\nintergenerational mobility. In this article, we first show that commonly used\ninference methods for this slope parameter are invalid. Second, when the\nunderlying distribution is not continuous, the OLS estimator and its asymptotic\ndistribution may be highly sensitive to how ties in the ranks are handled.\nMotivated by these findings we develop a new asymptotic theory for the OLS\nestimator in a general class of rank-rank regression specifications without\nimposing any assumptions about the continuity of the underlying distribution.\nWe then extend the asymptotic theory to other regressions involving ranks that\nhave been used in empirical work. Finally, we apply our new inference methods\nto two empirical studies on intergenerational mobility, highlighting the\npractical implications of our theoretical findings.",
        "authors": [
            "Denis Chetverikov",
            "Daniel Wilhelm"
        ],
        "categories": "econ.EM",
        "published": "2023-10-24T04:41:37Z",
        "updated": "2024-07-02T13:11:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.14983v2",
        "title": "Causal clustering: design of cluster experiments under network interference",
        "abstract": "This paper studies the design of cluster experiments to estimate the global\ntreatment effect in the presence of network spillovers. We provide a framework\nto choose the clustering that minimizes the worst-case mean-squared error of\nthe estimated global effect. We show that optimal clustering solves a novel\npenalized min-cut optimization problem computed via off-the-shelf semi-definite\nprogramming algorithms. Our analysis also characterizes simple conditions to\nchoose between any two cluster designs, including choosing between a cluster or\nindividual-level randomization. We illustrate the method's properties using\nunique network data from the universe of Facebook's users and existing data\nfrom a field experiment.",
        "authors": [
            "Davide Viviano",
            "Lihua Lei",
            "Guido Imbens",
            "Brian Karrer",
            "Okke Schrijvers",
            "Liang Shi"
        ],
        "categories": "econ.EM",
        "published": "2023-10-23T14:30:46Z",
        "updated": "2024-01-13T15:30:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.14438v1",
        "title": "BVARs and Stochastic Volatility",
        "abstract": "Bayesian vector autoregressions (BVARs) are the workhorse in macroeconomic\nforecasting. Research in the last decade has established the importance of\nallowing time-varying volatility to capture both secular and cyclical\nvariations in macroeconomic uncertainty. This recognition, together with the\ngrowing availability of large datasets, has propelled a surge in recent\nresearch in building stochastic volatility models suitable for large BVARs.\nSome of these new models are also equipped with additional features that are\nespecially desirable for large systems, such as order invariance -- i.e.,\nestimates are not dependent on how the variables are ordered in the BVAR -- and\nrobustness against COVID-19 outliers. Estimation of these large, flexible\nmodels is made possible by the recently developed equation-by-equation approach\nthat drastically reduces the computational cost of estimating large systems.\nDespite these recent advances, there remains much ongoing work, such as the\ndevelopment of parsimonious approaches for time-varying coefficients and other\ntypes of nonlinearities in large BVARs.",
        "authors": [
            "Joshua Chan"
        ],
        "categories": "econ.EM",
        "published": "2023-10-22T22:48:17Z",
        "updated": "2023-10-22T22:48:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.14142v2",
        "title": "On propensity score matching with a diverging number of matches",
        "abstract": "This paper reexamines Abadie and Imbens (2016)'s work on propensity score\nmatching for average treatment effect estimation. We explore the asymptotic\nbehavior of these estimators when the number of nearest neighbors, $M$, grows\nwith the sample size. It is shown, hardly surprising but technically\nnontrivial, that the modified estimators can improve upon the original\nfixed-$M$ estimators in terms of efficiency. Additionally, we demonstrate the\npotential to attain the semiparametric efficiency lower bound when the\npropensity score achieves \"sufficient\" dimension reduction, echoing Hahn\n(1998)'s insight about the role of dimension reduction in propensity\nscore-based causal inference.",
        "authors": [
            "Yihui He",
            "Fang Han"
        ],
        "categories": "math.ST",
        "published": "2023-10-22T00:38:52Z",
        "updated": "2023-11-14T21:35:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.14068v2",
        "title": "Unobserved Grouped Heteroskedasticity and Fixed Effects",
        "abstract": "This paper extends the linear grouped fixed effects (GFE) panel model to\nallow for heteroskedasticity from a discrete latent group variable. Key\nfeatures of GFE are preserved, such as individuals belonging to one of a finite\nnumber of groups and group membership is unrestricted and estimated. Ignoring\ngroup heteroskedasticity may lead to poor classification, which is detrimental\nto finite sample bias and standard errors of estimators. I introduce the\n\"weighted grouped fixed effects\" (WGFE) estimator that minimizes a weighted\naverage of group sum of squared residuals. I establish $\\sqrt{NT}$-consistency\nand normality under a concept of group separation based on second moments. A\ntest of group homoskedasticity is discussed. A fast computation procedure is\nprovided. Simulations show that WGFE outperforms alternatives that exclude\nsecond moment information. I demonstrate this approach by considering the link\nbetween income and democracy and the effect of unionization on earnings.",
        "authors": [
            "Jorge A. Rivero"
        ],
        "categories": "econ.EM",
        "published": "2023-10-21T17:17:41Z",
        "updated": "2023-10-26T02:17:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.13785v2",
        "title": "Bayesian Estimation of Panel Models under Potentially Sparse Heterogeneity",
        "abstract": "We incorporate a version of a spike and slab prior, comprising a pointmass at\nzero (\"spike\") and a Normal distribution around zero (\"slab\") into a dynamic\npanel data framework to model coefficient heterogeneity. In addition to\nhomogeneity and full heterogeneity, our specification can also capture sparse\nheterogeneity, that is, there is a core group of units that share common\nparameters and a set of deviators with idiosyncratic parameters. We fit a model\nwith unobserved components to income data from the Panel Study of Income\nDynamics. We find evidence for sparse heterogeneity for balanced panels\ncomposed of individuals with long employment histories.",
        "authors": [
            "Hyungsik Roger Moon",
            "Frank Schorfheide",
            "Boyuan Zhang"
        ],
        "categories": "econ.EM",
        "published": "2023-10-20T19:31:47Z",
        "updated": "2024-02-06T01:02:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.13240v2",
        "title": "Transparency challenges in policy evaluation with causal machine learning -- improving usability and accountability",
        "abstract": "Causal machine learning tools are beginning to see use in real-world policy\nevaluation tasks to flexibly estimate treatment effects. One issue with these\nmethods is that the machine learning models used are generally black boxes,\ni.e., there is no globally interpretable way to understand how a model makes\nestimates. This is a clear problem in policy evaluation applications,\nparticularly in government, because it is difficult to understand whether such\nmodels are functioning in ways that are fair, based on the correct\ninterpretation of evidence and transparent enough to allow for accountability\nif things go wrong. However, there has been little discussion of transparency\nproblems in the causal machine learning literature and how these might be\novercome. This paper explores why transparency issues are a problem for causal\nmachine learning in public policy evaluation applications and considers ways\nthese problems might be addressed through explainable AI tools and by\nsimplifying models in line with interpretable AI principles. It then applies\nthese ideas to a case-study using a causal forest model to estimate conditional\naverage treatment effects for a hypothetical change in the school leaving age\nin Australia. It shows that existing tools for understanding black-box\npredictive models are poorly suited to causal machine learning and that\nsimplifying the model to make it interpretable leads to an unacceptable\nincrease in error (in this application). It concludes that new tools are needed\nto properly understand causal machine learning models and the algorithms that\nfit them.",
        "authors": [
            "Patrick Rehill",
            "Nicholas Biddle"
        ],
        "categories": "cs.LG",
        "published": "2023-10-20T02:48:29Z",
        "updated": "2024-03-29T09:49:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.12863v3",
        "title": "A remark on moment-dependent phase transitions in high-dimensional Gaussian approximations",
        "abstract": "In this article, we study the critical growth rates of dimension below which\nGaussian critical values can be used for hypothesis testing but beyond which\nthey cannot. We are particularly interested in how these growth rates depend on\nthe number of moments that the observations possess.",
        "authors": [
            "Anders Bredahl Kock",
            "David Preinerstorfer"
        ],
        "categories": "math.ST",
        "published": "2023-10-19T16:12:13Z",
        "updated": "2024-02-12T14:10:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.12825v1",
        "title": "Nonparametric Regression with Dyadic Data",
        "abstract": "This paper studies the identification and estimation of a nonparametric\nnonseparable dyadic model where the structural function and the distribution of\nthe unobservable random terms are assumed to be unknown. The identification and\nthe estimation of the distribution of the unobservable random term are also\nproposed. I assume that the structural function is continuous and strictly\nincreasing in the unobservable heterogeneity. I propose suitable normalization\nfor the identification by allowing the structural function to have some\ndesirable properties such as homogeneity of degree one in the unobservable\nrandom term and some of its observables. The consistency and the asymptotic\ndistribution of the estimators are proposed. The finite sample properties of\nthe proposed estimators in a Monte-Carlo simulation are assessed.",
        "authors": [
            "Brice Romuald Gueyap Kounga"
        ],
        "categories": "econ.EM",
        "published": "2023-10-19T15:22:12Z",
        "updated": "2023-10-19T15:22:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.11969v2",
        "title": "Survey calibration for causal inference: a simple method to balance covariate distributions",
        "abstract": "This paper proposes a~simple, yet powerful, method for balancing\ndistributions of covariates for causal inference based on observational\nstudies. The method makes it possible to balance an arbitrary number of\nquantiles (e.g., medians, quartiles, or deciles) together with means if\nnecessary. The proposed approach is based on the theory of calibration\nestimators (Deville and S\\\"arndal 1992), in particular, calibration estimators\nfor quantiles, proposed by Harms and Duchesne (2006). The method does not\nrequire numerical integration, kernel density estimation or assumptions about\nthe distributions. Valid estimates can be obtained by drawing on existing\nasymptotic theory. An~illustrative example of the proposed approach is\npresented for the entropy balancing method and the covariate balancing\npropensity score method. Results of a~simulation study indicate that the method\nefficiently estimates average treatment effects on the treated (ATT), the\naverage treatment effect (ATE), the quantile treatment effect on the treated\n(QTT) and the quantile treatment effect (QTE), especially in the presence of\nnon-linearity and mis-specification of the models. The proposed approach can be\nfurther generalized to other designs (e.g. multi-category, continuous) or\nmethods (e.g. synthetic control method). An open source software implementing\nproposed methods is available.",
        "authors": [
            "Maciej Ber\u0119sewicz"
        ],
        "categories": "stat.ME",
        "published": "2023-10-18T13:50:32Z",
        "updated": "2024-03-12T20:48:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.11962v1",
        "title": "Machine Learning for Staggered Difference-in-Differences and Dynamic Treatment Effect Heterogeneity",
        "abstract": "We combine two recently proposed nonparametric difference-in-differences\nmethods, extending them to enable the examination of treatment effect\nheterogeneity in the staggered adoption setting using machine learning. The\nproposed method, machine learning difference-in-differences (MLDID), allows for\nestimation of time-varying conditional average treatment effects on the\ntreated, which can be used to conduct detailed inference on drivers of\ntreatment effect heterogeneity. We perform simulations to evaluate the\nperformance of MLDID and find that it accurately identifies the true predictors\nof treatment effect heterogeneity. We then use MLDID to evaluate the\nheterogeneous impacts of Brazil's Family Health Program on infant mortality,\nand find those in poverty and urban locations experienced the impact of the\npolicy more quickly than other subgroups.",
        "authors": [
            "Julia Hatamyar",
            "Noemi Kreif",
            "Rudi Rocha",
            "Martin Huber"
        ],
        "categories": "econ.EM",
        "published": "2023-10-18T13:41:41Z",
        "updated": "2023-10-18T13:41:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.11680v2",
        "title": "Trimmed Mean Group Estimation of Average Effects in Ultra Short T Panels under Correlated Heterogeneity",
        "abstract": "The commonly used two-way fixed effects estimator is biased under correlated\nheterogeneity and can lead to misleading inference. This paper proposes a new\ntrimmed mean group (TMG) estimator which is consistent at the irregular rate of\nn^{1/3} even if the time dimension of the panel is as small as the number of\nits regressors. Extensions to panels with time effects are provided, and a\nHausman test of correlated heterogeneity is proposed. Small sample properties\nof the TMG estimator (with and without time effects) are investigated by Monte\nCarlo experiments and shown to be satisfactory and perform better than other\ntrimmed estimators proposed in the literature. The proposed test of correlated\nheterogeneity is also shown to have the correct size and satisfactory power.\nThe utility of the TMG approach is illustrated with an empirical application.",
        "authors": [
            "M. Hashem Pesaran",
            "Liying Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-10-18T03:04:59Z",
        "updated": "2024-07-05T14:07:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.09597v2",
        "title": "Adaptive maximization of social welfare",
        "abstract": "We consider the problem of repeatedly choosing policies to maximize social\nwelfare. Welfare is a weighted sum of private utility and public revenue.\nEarlier outcomes inform later policies. Utility is not observed, but indirectly\ninferred. Response functions are learned through experimentation. We derive a\nlower bound on regret, and a matching adversarial upper bound for a variant of\nthe Exp3 algorithm. Cumulative regret grows at a rate of $T^{2/3}$. This\nimplies that (i) welfare maximization is harder than the multi-armed bandit\nproblem (with a rate of $T^{1/2}$ for finite policy sets), and (ii) our\nalgorithm achieves the optimal rate. For the stochastic setting, if social\nwelfare is concave, we can achieve a rate of $T^{1/2}$ (for continuous policy\nsets), using a dyadic search algorithm. We analyze an extension to nonlinear\nincome taxation, and sketch an extension to commodity taxation. We compare our\nsetting to monopoly pricing (which is easier), and price setting for bilateral\ntrade (which is harder).",
        "authors": [
            "Nicolo Cesa-Bianchi",
            "Roberto Colomboni",
            "Maximilian Kasy"
        ],
        "categories": "econ.EM",
        "published": "2023-10-14T15:09:56Z",
        "updated": "2024-07-29T07:54:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.09545v1",
        "title": "A Semiparametric Instrumented Difference-in-Differences Approach to Policy Learning",
        "abstract": "Recently, there has been a surge in methodological development for the\ndifference-in-differences (DiD) approach to evaluate causal effects. Standard\nmethods in the literature rely on the parallel trends assumption to identify\nthe average treatment effect on the treated. However, the parallel trends\nassumption may be violated in the presence of unmeasured confounding, and the\naverage treatment effect on the treated may not be useful in learning a\ntreatment assignment policy for the entire population. In this article, we\npropose a general instrumented DiD approach for learning the optimal treatment\npolicy. Specifically, we establish identification results using a binary\ninstrumental variable (IV) when the parallel trends assumption fails to hold.\nAdditionally, we construct a Wald estimator, novel inverse probability\nweighting (IPW) estimators, and a class of semiparametric efficient and\nmultiply robust estimators, with theoretical guarantees on consistency and\nasymptotic normality, even when relying on flexible machine learning algorithms\nfor nuisance parameters estimation. Furthermore, we extend the instrumented DiD\nto the panel data setting. We evaluate our methods in extensive simulations and\na real data application.",
        "authors": [
            "Pan Zhao",
            "Yifan Cui"
        ],
        "categories": "stat.ME",
        "published": "2023-10-14T09:38:32Z",
        "updated": "2023-10-14T09:38:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.09398v1",
        "title": "An In-Depth Examination of Requirements for Disclosure Risk Assessment",
        "abstract": "The use of formal privacy to protect the confidentiality of responses in the\n2020 Decennial Census of Population and Housing has triggered renewed interest\nand debate over how to measure the disclosure risks and societal benefits of\nthe published data products. Following long-established precedent in economics\nand statistics, we argue that any proposal for quantifying disclosure risk\nshould be based on pre-specified, objective criteria. Such criteria should be\nused to compare methodologies to identify those with the most desirable\nproperties. We illustrate this approach, using simple desiderata, to evaluate\nthe absolute disclosure risk framework, the counterfactual framework underlying\ndifferential privacy, and prior-to-posterior comparisons. We conclude that\nsatisfying all the desiderata is impossible, but counterfactual comparisons\nsatisfy the most while absolute disclosure risk satisfies the fewest.\nFurthermore, we explain that many of the criticisms levied against differential\nprivacy would be levied against any technology that is not equivalent to\ndirect, unrestricted access to confidential data. Thus, more research is\nneeded, but in the near-term, the counterfactual approach appears best-suited\nfor privacy-utility analysis.",
        "authors": [
            "Ron S. Jarmin",
            "John M. Abowd",
            "Robert Ashmead",
            "Ryan Cumings-Menon",
            "Nathan Goldschlag",
            "Michael B. Hawes",
            "Sallie Ann Keller",
            "Daniel Kifer",
            "Philip Leclerc",
            "Jerome P. Reiter",
            "Rolando A. Rodr\u00edguez",
            "Ian Schmutte",
            "Victoria A. Velkoff",
            "Pavel Zhuravlev"
        ],
        "categories": "cs.CR",
        "published": "2023-10-13T20:36:29Z",
        "updated": "2023-10-13T20:36:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.09105v3",
        "title": "Estimating Individual Responses when Tomorrow Matters",
        "abstract": "We propose a regression-based approach to estimate how individuals'\nexpectations influence their responses to a counterfactual change. We provide\nconditions under which average partial effects based on regression estimates\nrecover structural effects. We propose a practical three-step estimation method\nthat relies on panel data on subjective expectations. We illustrate our\napproach in a model of consumption and saving, focusing on the impact of an\nincome tax that not only changes current income but also affects beliefs about\nfuture income. Applying our approach to Italian survey data, we find that\nindividuals' beliefs matter for evaluating the impact of tax policies on\nconsumption decisions.",
        "authors": [
            "Stephane Bonhomme",
            "Angela Denis"
        ],
        "categories": "econ.EM",
        "published": "2023-10-13T13:51:51Z",
        "updated": "2024-05-15T16:50:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.09013v1",
        "title": "Smoothed instrumental variables quantile regression",
        "abstract": "In this article, I introduce the sivqr command, which estimates the\ncoefficients of the instrumental variables (IV) quantile regression model\nintroduced by Chernozhukov and Hansen (2005). The sivqr command offers several\nadvantages over the existing ivqreg and ivqreg2 commands for estimating this IV\nquantile regression model, which complements the alternative \"triangular model\"\nbehind cqiv and the \"local quantile treatment effect\" model of ivqte.\nComputationally, sivqr implements the smoothed estimator of Kaplan and Sun\n(2017), who show that smoothing improves both computation time and statistical\naccuracy. Standard errors are computed analytically or by Bayesian bootstrap;\nfor non-iid sampling, sivqr is compatible with bootstrap. I discuss syntax and\nthe underlying methodology, and I compare sivqr with other commands in an\nexample.",
        "authors": [
            "David M. Kaplan"
        ],
        "categories": "econ.EM",
        "published": "2023-10-13T11:20:25Z",
        "updated": "2023-10-13T11:20:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.08672v2",
        "title": "Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal",
        "abstract": "In many settings, interventions may be more effective for some individuals\nthan others, so that targeting interventions may be beneficial. We analyze the\nvalue of targeting in the context of a large-scale field experiment with over\n53,000 college students, where the goal was to use \"nudges\" to encourage\nstudents to renew their financial-aid applications before a non-binding\ndeadline. We begin with baseline approaches to targeting. First, we target\nbased on a causal forest that estimates heterogeneous treatment effects and\nthen assigns students to treatment according to those estimated to have the\nhighest treatment effects. Next, we evaluate two alternative targeting\npolicies, one targeting students with low predicted probability of renewing\nfinancial aid in the absence of the treatment, the other targeting those with\nhigh probability. The predicted baseline outcome is not the ideal criterion for\ntargeting, nor is it a priori clear whether to prioritize low, high, or\nintermediate predicted probability. Nonetheless, targeting on low baseline\noutcomes is common in practice, for example because the relationship between\nindividual characteristics and treatment effects is often difficult or\nimpossible to estimate with historical data. We propose hybrid approaches that\nincorporate the strengths of both predictive approaches (accurate estimation)\nand causal approaches (correct criterion); we show that targeting intermediate\nbaseline outcomes is most effective in our specific application, while\ntargeting based on low baseline outcomes is detrimental. In one year of the\nexperiment, nudging all students improved early filing by an average of 6.4\npercentage points over a baseline average of 37% filing, and we estimate that\ntargeting half of the students using our preferred policy attains around 75% of\nthis benefit.",
        "authors": [
            "Susan Athey",
            "Niall Keleher",
            "Jann Spiess"
        ],
        "categories": "econ.EM",
        "published": "2023-10-12T19:08:45Z",
        "updated": "2024-05-31T23:59:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.08536v5",
        "title": "Real-time Prediction of the Great Recession and the Covid-19 Recession",
        "abstract": "This paper uses standard and penalized logistic regression models to predict\nthe Great Recession and the Covid-19 recession in the US in real time. It\nexamines the predictability of various macroeconomic and financial indicators\nwith respect to the NBER recession indicator. The findings strongly support the\nuse of penalized logistic regression models in recession forecasting. These\nmodels, particularly the ridge logistic regression model, outperform the\nstandard logistic regression model in predicting the Great Recession in the US\nacross different forecast horizons. The study also confirms the traditional\nsignificance of the term spread as an important recession indicator. However,\nit acknowledges that the Covid-19 recession remains unpredictable due to the\nunprecedented nature of the pandemic. The results are validated by creating a\nrecession indicator through principal component analysis (PCA) on selected\nvariables, which strongly correlates with the NBER recession indicator and is\nless affected by publication lags.",
        "authors": [
            "Seulki Chung"
        ],
        "categories": "econ.EM",
        "published": "2023-10-12T17:24:18Z",
        "updated": "2024-05-24T11:02:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.08173v1",
        "title": "Structural Vector Autoregressions and Higher Moments: Challenges and Solutions in Small Samples",
        "abstract": "Generalized method of moments estimators based on higher-order moment\nconditions derived from independent shocks can be used to identify and estimate\nthe simultaneous interaction in structural vector autoregressions. This study\nhighlights two problems that arise when using these estimators in small\nsamples. First, imprecise estimates of the asymptotically efficient weighting\nmatrix and the asymptotic variance lead to volatile estimates and inaccurate\ninference. Second, many moment conditions lead to a small sample scaling bias\ntowards innovations with a variance smaller than the normalizing unit variance\nassumption. To address the first problem, I propose utilizing the assumption of\nindependent structural shocks to estimate the efficient weighting matrix and\nthe variance of the estimator. For the second issue, I propose incorporating a\ncontinuously updated scaling term into the weighting matrix, eliminating the\nscaling bias. To demonstrate the effectiveness of these measures, I conducted a\nMonte Carlo simulation which shows a significant improvement in the performance\nof the estimator.",
        "authors": [
            "Sascha A. Keweloh"
        ],
        "categories": "econ.EM",
        "published": "2023-10-12T09:57:03Z",
        "updated": "2023-10-12T09:57:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.08115v2",
        "title": "Model-Agnostic Covariate-Assisted Inference on Partially Identified Causal Effects",
        "abstract": "Many causal estimands are only partially identifiable since they depend on\nthe unobservable joint distribution between potential outcomes. Stratification\non pretreatment covariates can yield sharper bounds; however, unless the\ncovariates are discrete with relatively small support, this approach typically\nrequires binning covariates or estimating the conditional distributions of the\npotential outcomes given the covariates. Binning can result in substantial\nefficiency loss and become challenging to implement, even with a moderate\nnumber of covariates. Estimating conditional distributions, on the other hand,\nmay yield invalid inference if the distributions are inaccurately estimated,\nsuch as when a misspecified model is used or when the covariates are\nhigh-dimensional. In this paper, we propose a unified and model-agnostic\ninferential approach for a wide class of partially identified estimands. Our\nmethod, based on duality theory for optimal transport problems, has four key\nproperties. First, in randomized experiments, our approach can wrap around any\nestimates of the conditional distributions and provide uniformly valid\ninference, even if the initial estimates are arbitrarily inaccurate. A simple\nextension of our method to observational studies is doubly robust in the usual\nsense. Second, if nuisance parameters are estimated at semiparametric rates,\nour estimator is asymptotically unbiased for the sharp partial identification\nbound. Third, we can apply the multiplier bootstrap to select covariates and\nmodels without sacrificing validity, even if the true model is not selected.\nFinally, our method is computationally efficient. Overall, in three empirical\napplications, our method consistently reduces the width of estimated identified\nsets and confidence intervals without making additional structural assumptions.",
        "authors": [
            "Wenlong Ji",
            "Lihua Lei",
            "Asher Spector"
        ],
        "categories": "econ.EM",
        "published": "2023-10-12T08:17:30Z",
        "updated": "2024-11-17T05:39:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.08063v3",
        "title": "Inference for Nonlinear Endogenous Treatment Effects Accounting for High-Dimensional Covariate Complexity",
        "abstract": "Nonlinearity and endogeneity are prevalent challenges in causal analysis\nusing observational data. This paper proposes an inference procedure for a\nnonlinear and endogenous marginal effect function, defined as the derivative of\nthe nonparametric treatment function, with a primary focus on an additive model\nthat includes high-dimensional covariates. Using the control function approach\nfor identification, we implement a regularized nonparametric estimation to\nobtain an initial estimator of the model. Such an initial estimator suffers\nfrom two biases: the bias in estimating the control function and the\nregularization bias for the high-dimensional outcome model. Our key innovation\nis to devise the double bias correction procedure that corrects these two\nbiases simultaneously. Building on this debiased estimator, we further provide\na confidence band of the marginal effect function. Simulations and an empirical\nstudy of air pollution and migration demonstrate the validity of our\nprocedures.",
        "authors": [
            "Qingliang Fan",
            "Zijian Guo",
            "Ziwei Mei",
            "Cun-Hui Zhang"
        ],
        "categories": "econ.EM",
        "published": "2023-10-12T06:24:52Z",
        "updated": "2024-06-18T16:05:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.07839v1",
        "title": "Marital Sorting, Household Inequality and Selection",
        "abstract": "Using CPS data for 1976 to 2022 we explore how wage inequality has evolved\nfor married couples with both spouses working full time full year, and its\nimpact on household income inequality. We also investigate how marriage sorting\npatterns have changed over this period. To determine the factors driving income\ninequality we estimate a model explaining the joint distribution of wages which\naccounts for the spouses' employment decisions. We find that income inequality\nhas increased for these households and increased assortative matching of wages\nhas exacerbated the inequality resulting from individual wage growth. We find\nthat positive sorting partially reflects the correlation across unobservables\ninfluencing both members' of the marriage wages. We decompose the changes in\nsorting patterns over the 47 years comprising our sample into structural,\ncomposition and selection effects and find that the increase in positive\nsorting primarily reflects the increased skill premia for both observed and\nunobserved characteristics.",
        "authors": [
            "Iv\u00e1n Fern\u00e1ndez-Val",
            "Aico van Vuuren",
            "Francis Vella"
        ],
        "categories": "econ.EM",
        "published": "2023-10-11T19:32:45Z",
        "updated": "2023-10-11T19:32:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.07790v1",
        "title": "Integration or fragmentation? A closer look at euro area financial markets",
        "abstract": "This paper examines the degree of integration at euro area financial markets.\nTo that end, we estimate overall and country-specific integration indices based\non a panel vector-autoregression with factor stochastic volatility. Our results\nindicate a more heterogeneous bond market compared to the market for lending\nrates. At both markets, the global financial crisis and the sovereign debt\ncrisis led to a severe decline in financial integration, which fully recovered\nsince then. We furthermore identify countries that deviate from their peers\neither by responding differently to crisis events or by taking on different\nroles in the spillover network. The latter analysis reveals two set of\ncountries, namely a main body of countries that receives and transmits\nspillovers and a second, smaller group of spillover absorbing economies.\nFinally, we demonstrate by estimating an augmented Taylor rule that euro area\nshort-term interest rates are positively linked to the level of integration on\nthe bond market.",
        "authors": [
            "Martin Feldkircher",
            "Karin Klieber"
        ],
        "categories": "econ.EM",
        "published": "2023-10-11T18:23:31Z",
        "updated": "2023-10-11T18:23:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.07558v2",
        "title": "Smoothness-Adaptive Dynamic Pricing with Nonparametric Demand Learning",
        "abstract": "We study the dynamic pricing problem where the demand function is\nnonparametric and H\\\"older smooth, and we focus on adaptivity to the unknown\nH\\\"older smoothness parameter $\\beta$ of the demand function. Traditionally the\noptimal dynamic pricing algorithm heavily relies on the knowledge of $\\beta$ to\nachieve a minimax optimal regret of\n$\\widetilde{O}(T^{\\frac{\\beta+1}{2\\beta+1}})$. However, we highlight the\nchallenge of adaptivity in this dynamic pricing problem by proving that no\npricing policy can adaptively achieve this minimax optimal regret without\nknowledge of $\\beta$. Motivated by the impossibility result, we propose a\nself-similarity condition to enable adaptivity. Importantly, we show that the\nself-similarity condition does not compromise the problem's inherent complexity\nsince it preserves the regret lower bound\n$\\Omega(T^{\\frac{\\beta+1}{2\\beta+1}})$. Furthermore, we develop a\nsmoothness-adaptive dynamic pricing algorithm and theoretically prove that the\nalgorithm achieves this minimax optimal regret bound without the prior\nknowledge $\\beta$.",
        "authors": [
            "Zeqi Ye",
            "Hansheng Jiang"
        ],
        "categories": "stat.ML",
        "published": "2023-10-11T15:02:13Z",
        "updated": "2023-11-01T00:56:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.07151v2",
        "title": "Identification and Estimation of a Semiparametric Logit Model using Network Data",
        "abstract": "This paper studies the identification and estimation of a semiparametric\nbinary network model in which the unobserved social characteristic is\nendogenous, that is, the unobserved individual characteristic influences both\nthe binary outcome of interest and how links are formed within the network. The\nexact functional form of the latent social characteristic is not known. The\nproposed estimators are obtained based on matching pairs of agents whose\nnetwork formation distributions are the same. The consistency and the\nasymptotic distribution of the estimators are proposed. The finite sample\nproperties of the proposed estimators in a Monte-Carlo simulation are assessed.\nWe conclude this study with an empirical application.",
        "authors": [
            "Brice Romuald Gueyap Kounga"
        ],
        "categories": "econ.EM",
        "published": "2023-10-11T02:54:31Z",
        "updated": "2024-06-02T02:30:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.06242v1",
        "title": "Treatment Choice, Mean Square Regret and Partial Identification",
        "abstract": "We consider a decision maker who faces a binary treatment choice when their\nwelfare is only partially identified from data. We contribute to the literature\nby anchoring our finite-sample analysis on mean square regret, a decision\ncriterion advocated by Kitagawa, Lee, and Qiu (2022). We find that optimal\nrules are always fractional, irrespective of the width of the identified set\nand precision of its estimate. The optimal treatment fraction is a simple\nlogistic transformation of the commonly used t-statistic multiplied by a factor\ncalculated by a simple constrained optimization. This treatment fraction gets\ncloser to 0.5 as the width of the identified set becomes wider, implying the\ndecision maker becomes more cautious against the adversarial Nature.",
        "authors": [
            "Toru Kitagawa",
            "Sokbae Lee",
            "Chen Qiu"
        ],
        "categories": "econ.EM",
        "published": "2023-10-10T01:36:38Z",
        "updated": "2023-10-10T01:36:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.05761v1",
        "title": "Robust Minimum Distance Inference in Structural Models",
        "abstract": "This paper proposes minimum distance inference for a structural parameter of\ninterest, which is robust to the lack of identification of other structural\nnuisance parameters. Some choices of the weighting matrix lead to asymptotic\nchi-squared distributions with degrees of freedom that can be consistently\nestimated from the data, even under partial identification. In any case,\nknowledge of the level of under-identification is not required. We study the\npower of our robust test. Several examples show the wide applicability of the\nprocedure and a Monte Carlo investigates its finite sample performance. Our\nidentification-robust inference method can be applied to make inferences on\nboth calibrated (fixed) parameters and any other structural parameter of\ninterest. We illustrate the method's usefulness by applying it to a structural\nmodel on the non-neutrality of monetary policy, as in \\cite{nakamura2018high},\nwhere we empirically evaluate the validity of the calibrated parameters and we\ncarry out robust inference on the slope of the Phillips curve and the\ninformation effect.",
        "authors": [
            "Joan Alegre",
            "Juan Carlos Escanciano"
        ],
        "categories": "econ.EM",
        "published": "2023-10-09T14:41:53Z",
        "updated": "2023-10-09T14:41:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.05311v1",
        "title": "Identification and Estimation in a Class of Potential Outcomes Models",
        "abstract": "This paper develops a class of potential outcomes models characterized by\nthree main features: (i) Unobserved heterogeneity can be represented by a\nvector of potential outcomes and a type describing the manner in which an\ninstrument determines the choice of treatment; (ii) The availability of an\ninstrumental variable that is conditionally independent of unobserved\nheterogeneity; and (iii) The imposition of convex restrictions on the\ndistribution of unobserved heterogeneity. The proposed class of models\nencompasses multiple classical and novel research designs, yet possesses a\ncommon structure that permits a unifying analysis of identification and\nestimation. In particular, we establish that these models share a common\nnecessary and sufficient condition for identifying certain causal parameters.\nOur identification results are constructive in that they yield estimating\nmoment conditions for the parameters of interest. Focusing on a leading special\ncase of our framework, we further show how these estimating moment conditions\nmay be modified to be doubly robust. The corresponding double robust estimators\nare shown to be asymptotically normally distributed, bootstrap based inference\nis shown to be asymptotically valid, and the semi-parametric efficiency bound\nis derived for those parameters that are root-n estimable. We illustrate the\nusefulness of our results for developing, identifying, and estimating causal\nmodels through an empirical evaluation of the role of mental health as a\nmediating variable in the Moving To Opportunity experiment.",
        "authors": [
            "Manu Navjeevan",
            "Rodrigo Pinto",
            "Andres Santos"
        ],
        "categories": "econ.EM",
        "published": "2023-10-08T23:47:47Z",
        "updated": "2023-10-08T23:47:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.04853v1",
        "title": "On changepoint detection in functional data using empirical energy distance",
        "abstract": "We propose a novel family of test statistics to detect the presence of\nchangepoints in a sequence of dependent, possibly multivariate,\nfunctional-valued observations. Our approach allows to test for a very general\nclass of changepoints, including the \"classical\" case of changes in the mean,\nand even changes in the whole distribution. Our statistics are based on a\ngeneralisation of the empirical energy distance; we propose weighted\nfunctionals of the energy distance process, which are designed in order to\nenhance the ability to detect breaks occurring at sample endpoints. The\nlimiting distribution of the maximally selected version of our statistics\nrequires only the computation of the eigenvalues of the covariance function,\nthus being readily implementable in the most commonly employed packages, e.g.\nR. We show that, under the alternative, our statistics are able to detect\nchangepoints occurring even very close to the beginning/end of the sample. In\nthe presence of multiple changepoints, we propose a binary segmentation\nalgorithm to estimate the number of breaks and the locations thereof.\nSimulations show that our procedures work very well in finite samples. We\ncomplement our theory with applications to financial and temperature data.",
        "authors": [
            "B. Cooper Boniece",
            "Lajos Horv\u00e1th",
            "Lorenzo Trapani"
        ],
        "categories": "stat.ME",
        "published": "2023-10-07T15:28:42Z",
        "updated": "2023-10-07T15:28:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.04576v4",
        "title": "Challenges in Statistically Rejecting the Perfect Competition Hypothesis Using Imperfect Competition Data",
        "abstract": "We theoretically prove why statistically rejecting the null hypothesis of\nperfect competition is challenging, known as a common problem in the\nliterature. We also assess the finite sample performance of the conduct\nparameter test in homogeneous goods markets, showing that statistical power\nincreases with the number of markets, a larger conduct parameter, and a\nstronger demand rotation instrument. However, even with a moderate number of\nmarkets and five firms, rejecting the null hypothesis of perfect competition\nremains difficult, irrespective of instrument strength or the use of optimal\ninstruments. Our findings suggest that empirical results failing to reject\nperfect competition are due to the limited number of markets rather than\nmethodological shortcomings.",
        "authors": [
            "Yuri Matsumura",
            "Suguru Otani"
        ],
        "categories": "econ.EM",
        "published": "2023-10-06T20:38:32Z",
        "updated": "2024-08-28T09:06:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.03521v2",
        "title": "Cutting Feedback in Misspecified Copula Models",
        "abstract": "In copula models the marginal distributions and copula function are specified\nseparately. We treat these as two modules in a modular Bayesian inference\nframework, and propose conducting modified Bayesian inference by \"cutting\nfeedback\". Cutting feedback limits the influence of potentially misspecified\nmodules in posterior inference. We consider two types of cuts. The first limits\nthe influence of a misspecified copula on inference for the marginals, which is\na Bayesian analogue of the popular Inference for Margins (IFM) estimator. The\nsecond limits the influence of misspecified marginals on inference for the\ncopula parameters by using a pseudo likelihood of the ranks to define the cut\nmodel. We establish that if only one of the modules is misspecified, then the\nappropriate cut posterior gives accurate uncertainty quantification\nasymptotically for the parameters in the other module. Computation of the cut\nposteriors is difficult, and new variational inference methods to do so are\nproposed. The efficacy of the new methodology is demonstrated using both\nsimulated data and a substantive multivariate time series application from\nmacroeconomic forecasting. In the latter, cutting feedback from misspecified\nmarginals to a 1096 dimension copula improves posterior inference and\npredictive accuracy greatly, compared to conventional Bayesian inference.",
        "authors": [
            "Michael Stanley Smith",
            "Weichang Yu",
            "David J. Nott",
            "David Frazier"
        ],
        "categories": "stat.ME",
        "published": "2023-10-05T13:10:53Z",
        "updated": "2024-06-27T05:33:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.03435v1",
        "title": "Variational Inference for GARCH-family Models",
        "abstract": "The Bayesian estimation of GARCH-family models has been typically addressed\nthrough Monte Carlo sampling. Variational Inference is gaining popularity and\nattention as a robust approach for Bayesian inference in complex machine\nlearning models; however, its adoption in econometrics and finance is limited.\nThis paper discusses the extent to which Variational Inference constitutes a\nreliable and feasible alternative to Monte Carlo sampling for Bayesian\ninference in GARCH-like models. Through a large-scale experiment involving the\nconstituents of the S&P 500 index, several Variational Inference optimizers, a\nvariety of volatility models, and a case study, we show that Variational\nInference is an attractive, remarkably well-calibrated, and competitive method\nfor Bayesian learning.",
        "authors": [
            "Martin Magris",
            "Alexandros Iosifidis"
        ],
        "categories": "stat.ML",
        "published": "2023-10-05T10:21:31Z",
        "updated": "2023-10-05T10:21:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.02773v1",
        "title": "Moran's I Lasso for models with spatially correlated data",
        "abstract": "This paper proposes a Lasso-based estimator which uses information embedded\nin the Moran statistic to develop a selection procedure called Moran's I Lasso\n(Mi-Lasso) to solve the Eigenvector Spatial Filtering (ESF) eigenvector\nselection problem. ESF uses a subset of eigenvectors from a spatial weights\nmatrix to efficiently account for any omitted cross-sectional correlation terms\nin a classical linear regression framework, thus does not require the\nresearcher to explicitly specify the spatial part of the underlying structural\nmodel. We derive performance bounds and show the necessary conditions for\nconsistent eigenvector selection. The key advantages of the proposed estimator\nare that it is intuitive, theoretically grounded, and substantially faster than\nLasso based on cross-validation or any proposed forward stepwise procedure. Our\nmain simulation results show the proposed selection procedure performs well in\nfinite samples. Compared to existing selection procedures, we find Mi-Lasso has\none of the smallest biases and mean squared errors across a range of sample\nsizes and levels of spatial correlation. An application on house prices further\ndemonstrates Mi-Lasso performs well compared to existing procedures.",
        "authors": [
            "Sylvain Barde",
            "Rowan Cherodian",
            "Guy Tchuente"
        ],
        "categories": "econ.EM",
        "published": "2023-10-04T12:43:04Z",
        "updated": "2023-10-04T12:43:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.02414v4",
        "title": "Sharp and Robust Estimation of Partially Identified Discrete Response Models",
        "abstract": "Semiparametric discrete choice models are widely used in a variety of\npractical applications. While these models are point identified in the presence\nof continuous covariates, they can become partially identified when covariates\nare discrete. In this paper we find that classical estimators, including the\nmaximum score estimator, (Manski (1975)), loose their attractive statistical\nproperties without point identification. First of all, they are not sharp with\nthe estimator converging to an outer region of the identified set, (Komarova\n(2013)), and in many discrete designs it weakly converges to a random set.\nSecond, they are not robust, with their distribution limit discontinuously\nchanging with respect to the parameters of the model. We propose a novel class\nof estimators based on the concept of a quantile of a random set, which we show\nto be both sharp and robust. We demonstrate that our approach extends from\ncross-sectional settings to classical static and dynamic discrete panel data\nmodels.",
        "authors": [
            "Shakeeb Khan",
            "Tatiana Komarova",
            "Denis Nekipelov"
        ],
        "categories": "econ.EM",
        "published": "2023-10-03T20:15:44Z",
        "updated": "2024-05-28T11:54:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.02008v2",
        "title": "fmeffects: An R Package for Forward Marginal Effects",
        "abstract": "Forward marginal effects have recently been introduced as a versatile and\neffective model-agnostic interpretation method particularly suited for\nnon-linear and non-parametric prediction models. They provide comprehensible\nmodel explanations of the form: if we change feature values by a pre-specified\nstep size, what is the change in the predicted outcome? We present the R\npackage fmeffects, the first software implementation of the theory surrounding\nforward marginal effects. The relevant theoretical background, package\nfunctionality and handling, as well as the software design and options for\nfuture extensions are discussed in this paper.",
        "authors": [
            "Holger L\u00f6we",
            "Christian A. Scholbeck",
            "Christian Heumann",
            "Bernd Bischl",
            "Giuseppe Casalicchio"
        ],
        "categories": "cs.LG",
        "published": "2023-10-03T12:24:51Z",
        "updated": "2024-09-12T05:10:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.01950v1",
        "title": "Specification testing with grouped fixed effects",
        "abstract": "We propose a bootstrap generalized Hausman test for the correct specification\nof unobserved heterogeneity in both linear and nonlinear fixed-effects panel\ndata models. We consider as null hypotheses two scenarios in which the\nunobserved heterogeneity is either time-invariant or specified as additive\nindividual and time effects. We contrast the standard fixed-effects estimators\nwith the recently developed two-way grouped fixed-effects estimator, that is\nconsistent in the presence of time-varying heterogeneity under minimal\nspecification and distributional assumptions for the unobserved effects. The\nHausman test exploits the general formulation for the variance of the vector of\ncontrasts and critical values are computed via parametric percentile bootstrap,\nso as to account for the non-centrality of the asymptotic chi-square\ndistribution arising from the incidental parameters and approximation biases.\nMonte Carlo evidence shows that the test has correct size and good power\nproperties. We provide two empirical applications to illustrate the proposed\ntest: the first one is based on a linear model for the determinants of the wage\nof working women and the second analyzes the trade extensive margin.",
        "authors": [
            "Claudia Pigini",
            "Alessandro Pionati",
            "Francesco Valentini"
        ],
        "categories": "econ.EM",
        "published": "2023-10-03T10:52:07Z",
        "updated": "2023-10-03T10:52:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.01123v2",
        "title": "Impact of Economic Uncertainty, Geopolitical Risk, Pandemic, Financial & Macroeconomic Factors on Crude Oil Returns -- An Empirical Investigation",
        "abstract": "This study aims to use simultaneous quantile regression (SQR) to examine the\nimpact of macroeconomic and financial uncertainty including global pandemic,\ngeopolitical risk on the futures returns of crude oil (ROC). The data for this\nstudy is sourced from the FRED (Federal Reserve Economic Database) economic\ndataset; the importance of the factors have been validated by using variation\ninflation factor (VIF) and principal component analysis (PCA). To fully\nunderstand the combined effect of these factors on WTI, study includes\ninteraction terms in the multi-factor model. Empirical results suggest that\nchanges in ROC can have varying impacts depending on the specific period and\nmarket conditions. The results can be used for informed investment decisions\nand to construct portfolios that are well-balanced in terms of risk and return.\nStructural breaks, such as changes in global economic conditions or shifts in\ndemand for crude oil, can cause return on crude oil to be sensitive to changes\nin different time periods. The unique aspect ness of this study also lies in\nits inclusion of explanatory factors related to the pandemic, geopolitical\nrisk, and inflation.",
        "authors": [
            "Sarit Maitra"
        ],
        "categories": "econ.EM",
        "published": "2023-10-02T11:55:01Z",
        "updated": "2023-10-03T12:57:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.01104v2",
        "title": "Multi-period static hedging of European options",
        "abstract": "We consider the hedging of European options when the price of the underlying\nasset follows a single-factor Markovian framework. By working in such a\nsetting, Carr and Wu \\cite{carr2014static} derived a spanning relation between\na given option and a continuum of shorter-term options written on the same\nasset. In this paper, we have extended their approach to simultaneously include\noptions over multiple short maturities. We then show a practical implementation\nof this with a finite set of shorter-term options to determine the hedging\nerror using a Gaussian Quadrature method. We perform a wide range of\nexperiments for both the \\textit{Black-Scholes} and \\textit{Merton Jump\nDiffusion} models, illustrating the comparative performance of the two methods.",
        "authors": [
            "Purba Banerjee",
            "Srikanth Iyer",
            "Shashi Jain"
        ],
        "categories": "q-fin.MF",
        "published": "2023-10-02T11:24:36Z",
        "updated": "2023-10-18T09:02:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.00786v2",
        "title": "Semidiscrete optimal transport with unknown costs",
        "abstract": "Semidiscrete optimal transport is a challenging generalization of the\nclassical transportation problem in linear programming. The goal is to design a\njoint distribution for two random variables (one continuous, one discrete) with\nfixed marginals, in a way that minimizes expected cost. We formulate a novel\nvariant of this problem in which the cost functions are unknown, but can be\nlearned through noisy observations; however, only one function can be sampled\nat a time. We develop a semi-myopic algorithm that couples online learning with\nstochastic approximation, and prove that it achieves optimal convergence rates,\ndespite the non-smoothness of the stochastic gradient and the lack of strong\nconcavity in the objective function.",
        "authors": [
            "Yinchu Zhu",
            "Ilya O. Ryzhov"
        ],
        "categories": "econ.EM",
        "published": "2023-10-01T20:49:38Z",
        "updated": "2023-11-14T21:12:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.00561v1",
        "title": "CausalGPS: An R Package for Causal Inference With Continuous Exposures",
        "abstract": "Quantifying the causal effects of continuous exposures on outcomes of\ninterest is critical for social, economic, health, and medical research.\nHowever, most existing software packages focus on binary exposures. We develop\nthe CausalGPS R package that implements a collection of algorithms to provide\nalgorithmic solutions for causal inference with continuous exposures. CausalGPS\nimplements a causal inference workflow, with algorithms based on generalized\npropensity scores (GPS) as the core, extending propensity scores (the\nprobability of a unit being exposed given pre-exposure covariates) from binary\nto continuous exposures. As the first step, the package implements efficient\nand flexible estimations of the GPS, allowing multiple user-specified modeling\noptions. As the second step, the package provides two ways to adjust for\nconfounding: weighting and matching, generating weighted and matched data sets,\nrespectively. Lastly, the package provides built-in functions to fit flexible\nparametric, semi-parametric, or non-parametric regression models on the\nweighted or matched data to estimate the exposure-response function relating\nthe outcome with the exposures. The computationally intensive tasks are\nimplemented in C++, and efficient shared-memory parallelization is achieved by\nOpenMP API. This paper outlines the main components of the CausalGPS R package\nand demonstrates its application to assess the effect of long-term exposure to\nPM2.5 on educational attainment using zip code-level data from the contiguous\nUnited States from 2000-2016.",
        "authors": [
            "Naeem Khoshnevis",
            "Xiao Wu",
            "Danielle Braun"
        ],
        "categories": "stat.CO",
        "published": "2023-10-01T03:31:01Z",
        "updated": "2023-10-01T03:31:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.00260v1",
        "title": "On Sinkhorn's Algorithm and Choice Modeling",
        "abstract": "For a broad class of choice and ranking models based on Luce's choice axiom,\nincluding the Bradley--Terry--Luce and Plackett--Luce models, we show that the\nassociated maximum likelihood estimation problems are equivalent to a classic\nmatrix balancing problem with target row and column sums. This perspective\nopens doors between two seemingly unrelated research areas, and allows us to\nunify existing algorithms in the choice modeling literature as special\ninstances or analogs of Sinkhorn's celebrated algorithm for matrix balancing.\nWe draw inspirations from these connections and resolve important open problems\non the study of Sinkhorn's algorithm. We first prove the global linear\nconvergence of Sinkhorn's algorithm for non-negative matrices whenever finite\nsolutions to the matrix balancing problem exist. We characterize this global\nrate of convergence in terms of the algebraic connectivity of the bipartite\ngraph constructed from data. Next, we also derive the sharp asymptotic rate of\nlinear convergence, which generalizes a classic result of Knight (2008), but\nwith a more explicit analysis that exploits an intrinsic orthogonality\nstructure. To our knowledge, these are the first quantitative linear\nconvergence results for Sinkhorn's algorithm for general non-negative matrices\nand positive marginals. The connections we establish in this paper between\nmatrix balancing and choice modeling could help motivate further transmission\nof ideas and interesting results in both directions.",
        "authors": [
            "Zhaonan Qu",
            "Alfred Galichon",
            "Johan Ugander"
        ],
        "categories": "math.OC",
        "published": "2023-09-30T05:20:23Z",
        "updated": "2023-09-30T05:20:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2310.00197v1",
        "title": "Identification, Impacts, and Opportunities of Three Common Measurement Considerations when using Digital Trace Data",
        "abstract": "Cataloguing specific URLs, posts, and applications with digital traces is the\nnew best practice for measuring media use and content consumption. Despite the\napparent accuracy that comes with greater granularity, however, digital traces\nmay introduce additional ambiguity and new errors into the measurement of media\nuse. In this note, we identify three new measurement challenges when using\nDigital Trace Data that were recently uncovered using a new measurement\nframework - Screenomics - that records media use at the granularity of\nindividual screenshots obtained every few seconds as people interact with\nmobile devices. We label the considerations as follows: (1) entangling - the\ncommon measurement error introduced by proxying exposure to content by exposure\nto format; (2) flattening - aggregating unique segments of media interaction\nwithout incorporating temporal information, most commonly intraindividually and\n(3) bundling - summation of the durations of segments of media interaction,\nindiscriminate with respect to variations across media segments.",
        "authors": [
            "Daniel Muise",
            "Nilam Ram",
            "Thomas Robinson",
            "Byron Reeves"
        ],
        "categories": "cs.HC",
        "published": "2023-09-30T00:28:19Z",
        "updated": "2023-09-30T00:28:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.16348v1",
        "title": "Smoothing the Nonsmoothness",
        "abstract": "To tackle difficulties for theoretical studies in situations involving\nnonsmooth functions, we propose a sequence of infinitely differentiable\nfunctions to approximate the nonsmooth function under consideration. A rate of\napproximation is established and an illustration of its application is then\nprovided.",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Bin Peng",
            "Yundong Tu"
        ],
        "categories": "econ.EM",
        "published": "2023-09-28T11:14:55Z",
        "updated": "2023-09-28T11:14:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.15983v4",
        "title": "Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study",
        "abstract": "Two-way fixed effects (TWFE) models are widely used in political science to\nestablish causality, but recent methodological discussions highlight their\nlimitations under heterogeneous treatment effects (HTE) and violations of the\nparallel trends (PT) assumption. This growing literature has introduced new\nestimators and diagnostics, causing confusion among researchers about the\nreliability of existing results and best practices. To address these concerns,\nwe replicated and reanalyzed 49 articles from leading journals using TWFE\nmodels for observational panel data with binary treatments. Using six\nHTE-robust estimators, diagnostic tests, and sensitivity analyses, we find: (i)\nHTE-robust estimators yield qualitatively similar but highly variable results;\n(ii) while a few studies show clear signs of PT violations, many lack evidence\nto support this assumption; and (iii) many studies are underpowered when\naccounting for HTE and potential PT violations. We emphasize the importance of\nstrong research designs and rigorous validation of key identifying assumptions.",
        "authors": [
            "Albert Chiu",
            "Xingchen Lan",
            "Ziyi Liu",
            "Yiqing Xu"
        ],
        "categories": "stat.ME",
        "published": "2023-09-27T20:03:33Z",
        "updated": "2024-11-29T22:37:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.15705v1",
        "title": "Sluggish news reactions: A combinatorial approach for synchronizing stock jumps",
        "abstract": "Stock prices often react sluggishly to news, producing gradual jumps and jump\ndelays. Econometricians typically treat these sluggish reactions as\nmicrostructure effects and settle for a coarse sampling grid to guard against\nthem. Synchronizing mistimed stock returns on a fine sampling grid allows us to\nautomatically detect noisy jumps and better approximate the true common jumps\nin related stock prices.",
        "authors": [
            "Nabil Bouamara",
            "Kris Boudt",
            "S\u00e9bastien Laurent",
            "Christopher J. Neely"
        ],
        "categories": "econ.EM",
        "published": "2023-09-27T14:47:10Z",
        "updated": "2023-09-27T14:47:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.15297v1",
        "title": "Double machine learning and design in batch adaptive experiments",
        "abstract": "We consider an experiment with at least two stages or batches and $O(N)$\nsubjects per batch. First, we propose a semiparametric treatment effect\nestimator that efficiently pools information across the batches, and show it\nasymptotically dominates alternatives that aggregate single batch estimates.\nThen, we consider the design problem of learning propensity scores for\nassigning treatment in the later batches of the experiment to maximize the\nasymptotic precision of this estimator. For two common causal estimands, we\nestimate this precision using observations from previous batches, and then\nsolve a finite-dimensional concave maximization problem to adaptively learn\nflexible propensity scores that converge to suitably defined optima in each\nbatch at rate $O_p(N^{-1/4})$. By extending the framework of double machine\nlearning, we show this rate suffices for our pooled estimator to attain the\ntargeted precision after each batch, as long as nuisance function estimates\nconverge at rate $o_p(N^{-1/4})$. These relatively weak rate requirements\nenable the investigator to avoid the common practice of discretizing the\ncovariate space for design and estimation in batch adaptive experiments while\nmaintaining the advantages of pooling. Our numerical study shows that such\ndiscretization often leads to substantial asymptotic and finite sample\nprecision losses outweighing any gains from design.",
        "authors": [
            "Harrison H. Li",
            "Art B. Owen"
        ],
        "categories": "stat.ME",
        "published": "2023-09-26T22:21:42Z",
        "updated": "2023-09-26T22:21:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.14630v2",
        "title": "Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns",
        "abstract": "Discontinuities in regression functions can reveal important insights. In\nmany contexts, like geographic settings, such discontinuities are multivariate\nand unknown a priori. We propose a non-parametric regression method that\nestimates the location and size of discontinuities by segmenting the regression\nsurface. This estimator is based on a convex relaxation of the Mumford-Shah\nfunctional, for which we establish identification and convergence. We use it to\nshow that an internet shutdown in India resulted in a reduction of economic\nactivity by 25--35%, greatly surpassing previous estimates and shedding new\nlight on the true cost of such shutdowns for digital economies globally.",
        "authors": [
            "Florian Gunsilius",
            "David Van Dijcke"
        ],
        "categories": "econ.EM",
        "published": "2023-09-26T02:49:30Z",
        "updated": "2024-01-28T17:05:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.14581v1",
        "title": "Assessing Utility of Differential Privacy for RCTs",
        "abstract": "Randomized control trials, RCTs, have become a powerful tool for assessing\nthe impact of interventions and policies in many contexts. They are considered\nthe gold-standard for inference in the biomedical fields and in many social\nsciences. Researchers have published an increasing number of studies that rely\non RCTs for at least part of the inference, and these studies typically include\nthe response data collected, de-identified and sometimes protected through\ntraditional disclosure limitation methods. In this paper, we empirically assess\nthe impact of strong privacy-preservation methodology (with \\ac{DP}\nguarantees), on published analyses from RCTs, leveraging the availability of\nreplication packages (research compendia) in economics and policy analysis. We\nprovide simulations studies and demonstrate how we can replicate the analysis\nin a published economics article on privacy-protected data under various\nparametrizations. We find that relatively straightforward DP-based methods\nallow for inference-valid protection of the published data, though\ncomputational issues may limit more complex analyses from using these methods.\nThe results have applicability to researchers wishing to share RCT data,\nespecially in the context of low- and middle-income countries, with strong\nprivacy protection.",
        "authors": [
            "Soumya Mukherjee",
            "Aratrika Mustafi",
            "Aleksandra Slavkovi\u0107",
            "Lars Vilhuber"
        ],
        "categories": "stat.AP",
        "published": "2023-09-26T00:10:32Z",
        "updated": "2023-09-26T00:10:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.14160v2",
        "title": "Unified Inference for Dynamic Quantile Predictive Regression",
        "abstract": "This paper develops unified asymptotic distribution theory for dynamic\nquantile predictive regressions which is useful when examining quantile\npredictability in stock returns under possible presence of nonstationarity.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-09-25T14:10:33Z",
        "updated": "2023-11-10T09:41:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.13251v3",
        "title": "Nonparametric estimation of conditional densities by generalized random forests",
        "abstract": "Considering a continuous random variable $Y$ together with a continuous\nrandom vector $X$, I propose a nonparametric estimator $\\hat{f}(\\cdot|x)$ for\nthe conditional density of $Y$ given $X=x$. This estimator takes the form of an\nexponential series whose coefficients $\\hat{\\theta}_{x}=(\\hat{\\theta}_{x,1},\n\\dots,\\hat{\\theta}_{x,J})$ are the solution of a system of nonlinear equations\nthat depends on an estimator of the conditional expectation $E[\\phi (Y)|X=x]$,\nwhere $\\phi$ is a $J$-dimensional vector of basis functions. The distinguishing\nfeature of the proposed estimator is that $E[\\phi(Y)|X=x]$ is estimated by\ngeneralized random forest (Athey, Tibshirani, and Wager, Annals of Statistics,\n2019), targeting the heterogeneity of $\\hat{\\theta}_{x}$ across $x$. I show\nthat $\\hat{f}(\\cdot|x)$ is uniformly consistent and asymptotically normal,\nallowing $J \\rightarrow \\infty$. I also provide a standard error formula to\nconstruct asymptotically valid confidence intervals. Results from Monte Carlo\nexperiments and an empirical illustration are provided.",
        "authors": [
            "Federico Zincenko"
        ],
        "categories": "econ.EM",
        "published": "2023-09-23T04:20:13Z",
        "updated": "2024-05-31T16:16:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.13159v2",
        "title": "Estimating a k-modal nonparametric mixed logit model with market-level data",
        "abstract": "We propose a group-level agent-based mixed (GLAM) logit model that is\nestimated using market-level choice share data. The model non-parametrically\nrepresents taste heterogeneity through market-specific parameters by solving a\nmultiagent inverse utility maximization problem, addressing the limitations of\nexisting market-level choice models with parametric taste heterogeneity. A case\nstudy of mode choice in New York State is conducted using synthetic population\ndata of 53.55 million trips made by 19.53 million residents in 2019. These\ntrips are aggregated based on population segments and census block group-level\norigin-destination (OD) pairs, resulting in 120,740 markets/agents. We\nbenchmark in-sample and out-of-sample predictive performance of the GLAM logit\nmodel against multinomial logit, nested logit, inverse product differentiation\nlogit, and random coefficient logit (RCL) models. The results show that GLAM\nlogit outperforms benchmark models, improving the overall in-sample predictive\naccuracy from 78.7% to 96.71% and out-of-sample accuracy from 65.30% to 81.78%.\nThe price elasticities and diversion ratios retrieved from GLAM logit and\nbenchmark models exhibit similar substitution patterns among the six travel\nmodes. GLAM logit is scalable and computationally efficient, taking less than\none-tenth of the time taken to estimate the RCL model. The agent-specific\nparameters in GLAM logit provide additional insights such as value-of-time\n(VOT) across segments and regions, which has been further utilized to\ndemonstrate its application in analyzing NYS travelers' mode choice response to\nthe congestion pricing. The agent-specific parameters in GLAM logit facilitate\ntheir seamless integration into supply-side optimization models for revenue\nmanagement and system design.",
        "authors": [
            "Xiyuan Ren",
            "Joseph Y. J. Chow",
            "Prateek Bansal"
        ],
        "categories": "econ.EM",
        "published": "2023-09-22T19:50:55Z",
        "updated": "2024-08-22T15:14:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.12162v1",
        "title": "Optimal Conditional Inference in Adaptive Experiments",
        "abstract": "We study batched bandit experiments and consider the problem of inference\nconditional on the realized stopping time, assignment probabilities, and target\nparameter, where all of these may be chosen adaptively using information up to\nthe last batch of the experiment. Absent further restrictions on the\nexperiment, we show that inference using only the results of the last batch is\noptimal. When the adaptive aspects of the experiment are known to be\nlocation-invariant, in the sense that they are unchanged when we shift all\nbatch-arm means by a constant, we show that there is additional information in\nthe data, captured by one additional linear function of the batch-arm means. In\nthe more restrictive case where the stopping time, assignment probabilities,\nand target parameter are known to depend on the data only through a collection\nof polyhedral events, we derive computationally tractable and optimal\nconditional inference procedures.",
        "authors": [
            "Jiafeng Chen",
            "Isaiah Andrews"
        ],
        "categories": "stat.ME",
        "published": "2023-09-21T15:17:38Z",
        "updated": "2023-09-21T15:17:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.12034v1",
        "title": "A detection analysis for temporal memory patterns at different time-scales",
        "abstract": "This paper introduces a novel methodology that utilizes latency to unveil\ntime-series dependence patterns. A customized statistical test detects memory\ndependence in event sequences by analyzing their inter-event time\ndistributions. Synthetic experiments based on the renewal-aging property assess\nthe impact of observer latency on the renewal property. Our test uncovers\nmemory patterns across diverse time scales, emphasizing the event sequence's\nprobability structure beyond correlations. The time series analysis produces a\nstatistical test and graphical plots which helps to detect dependence patterns\namong events at different time-scales if any. Furthermore, the test evaluates\nthe renewal assumption through aging experiments, offering valuable\napplications in time-series analysis within economics.",
        "authors": [
            "Fabio Vanni",
            "David Lambert"
        ],
        "categories": "econ.EM",
        "published": "2023-09-21T12:56:16Z",
        "updated": "2023-09-21T12:56:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.11400v1",
        "title": "Transformers versus LSTMs for electronic trading",
        "abstract": "With the rapid development of artificial intelligence, long short term memory\n(LSTM), one kind of recurrent neural network (RNN), has been widely applied in\ntime series prediction.\n  Like RNN, Transformer is designed to handle the sequential data. As\nTransformer achieved great success in Natural Language Processing (NLP),\nresearchers got interested in Transformer's performance on time series\nprediction, and plenty of Transformer-based solutions on long time series\nforecasting have come out recently. However, when it comes to financial time\nseries prediction, LSTM is still a dominant architecture. Therefore, the\nquestion this study wants to answer is: whether the Transformer-based model can\nbe applied in financial time series prediction and beat LSTM.\n  To answer this question, various LSTM-based and Transformer-based models are\ncompared on multiple financial prediction tasks based on high-frequency limit\norder book data. A new LSTM-based model called DLSTM is built and new\narchitecture for the Transformer-based model is designed to adapt for financial\nprediction. The experiment result reflects that the Transformer-based model\nonly has the limited advantage in absolute price sequence prediction. The\nLSTM-based models show better and more robust performance on difference\nsequence prediction, such as price difference and price movement.",
        "authors": [
            "Paul Bilokon",
            "Yitao Qiu"
        ],
        "categories": "q-fin.TR",
        "published": "2023-09-20T15:25:43Z",
        "updated": "2023-09-20T15:25:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.11387v2",
        "title": "Identifying Causal Effects in Information Provision Experiments",
        "abstract": "Information provision experiments are a popular way to study causal effects\nof beliefs on behavior. Researchers estimate these effects using TSLS. I show\nthat existing TSLS specifications do not estimate the average partial effect;\nthey have weights proportional to belief updating in the first-stage. If people\nwhose decisions depend on their beliefs gather information before the\nexperiment, the information treatment may shift beliefs more for people with\nweak belief effects. This attenuates TSLS estimates. I propose researchers use\na local-least-squares (LLS) estimator that I show consistently estimates the\naverage partial effect (APE) under Bayesian updating, and apply it to Settele\n(2022).",
        "authors": [
            "Dylan Balla-Elliott"
        ],
        "categories": "econ.EM",
        "published": "2023-09-20T15:14:19Z",
        "updated": "2023-11-08T16:50:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.11058v2",
        "title": "require: Package dependencies for reproducible research",
        "abstract": "The ability to conduct reproducible research in Stata is often limited by the\nlack of version control for community-contributed packages. This article\nintroduces the require command, a tool designed to ensure Stata package\ndependencies are compatible across users and computer systems. Given a list of\nStata packages, require verifies that each package is installed, checks for a\nminimum or exact version or package release date, and optionally installs the\npackage if prompted by the researcher.",
        "authors": [
            "Sergio Correia",
            "Matthew P. Seay"
        ],
        "categories": "econ.EM",
        "published": "2023-09-20T04:35:07Z",
        "updated": "2024-04-10T20:36:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.10642v4",
        "title": "Correcting Selection Bias in Standardized Test Scores Comparisons",
        "abstract": "This paper addresses the issue of sample selection bias when comparing\ncountries using International assessments like PISA (Program for International\nStudent Assessment). Despite its widespread use, PISA rankings may be biased\ndue to different attrition patterns in different countries, leading to\ninaccurate comparisons. This study proposes a methodology to correct for sample\nselection bias using a quantile selection model. Applying the method to PISA\n2018 data, I find that correcting for selection bias significantly changes the\nrankings (based on the mean) of countries' educational performances. My results\nhighlight the importance of accounting for sample selection bias in\ninternational educational comparisons.",
        "authors": [
            "Onil Boussim"
        ],
        "categories": "econ.EM",
        "published": "2023-09-19T14:22:26Z",
        "updated": "2024-06-28T19:03:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.10481v1",
        "title": "Regressing on distributions: The nonlinear effect of temperature on regional economic growth",
        "abstract": "A nonlinear regression framework is proposed for time series and panel data\nfor the situation where certain explanatory variables are available at a higher\ntemporal resolution than the dependent variable. The main idea is to use the\nmoments of the empirical distribution of these variables to construct\nregressors with the correct resolution. As the moments are likely to display\nnonlinear marginal and interaction effects, an artificial neural network\nregression function is proposed. The corresponding model operates within the\ntraditional stochastic nonlinear least squares framework. In particular, a\nnumerical Hessian is employed to calculate confidence intervals. The practical\nusefulness is demonstrated by analyzing the influence of daily temperatures in\n260 European NUTS2 regions on the yearly growth of gross value added in these\nregions in the time period 2000 to 2021. In the particular example, the model\nallows for an appropriate assessment of regional economic impacts resulting\nfrom (future) changes in the regional temperature distribution (mean AND\nvariance).",
        "authors": [
            "Malte Jahn"
        ],
        "categories": "stat.ME",
        "published": "2023-09-19T09:51:15Z",
        "updated": "2023-09-19T09:51:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.09299v3",
        "title": "Bounds on Average Effects in Discrete Choice Panel Data Models",
        "abstract": "In discrete choice panel data, the estimation of average effects is crucial\nfor quantifying the effect of covariates, and for policy evaluation and\ncounterfactual analysis. This task is challenging in short panels with\nindividual-specific effects due to partial identification and the incidental\nparameter problem. In particular, estimation of the sharp identified set is\npractically infeasible at realistic sample sizes whenever the number of support\npoints of the observed covariates is large, such as when the covariates are\ncontinuous. In this paper, we therefore propose estimating outer bounds on the\nidentified set of average effects. Our bounds are easy to construct, converge\nat the parametric rate, and are computationally simple to obtain even in\nmoderately large samples, independent of whether the covariates are discrete or\ncontinuous. We also provide asymptotically valid confidence intervals on the\nidentified set.",
        "authors": [
            "Cavit Pakel",
            "Martin Weidner"
        ],
        "categories": "econ.EM",
        "published": "2023-09-17T15:25:14Z",
        "updated": "2024-05-01T12:06:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.09103v1",
        "title": "Optimal Estimation under a Semiparametric Density Ratio Model",
        "abstract": "In many statistical and econometric applications, we gather individual\nsamples from various interconnected populations that undeniably exhibit common\nlatent structures. Utilizing a model that incorporates these latent structures\nfor such data enhances the efficiency of inferences. Recently, many researchers\nhave been adopting the semiparametric density ratio model (DRM) to address the\npresence of latent structures. The DRM enables estimation of each population\ndistribution using pooled data, resulting in statistically more efficient\nestimations in contrast to nonparametric methods that analyze each sample in\nisolation. In this article, we investigate the limit of the efficiency\nimprovement attainable through the DRM. We focus on situations where one\npopulation's sample size significantly exceeds those of the other populations.\nIn such scenarios, we demonstrate that the DRM-based inferences for populations\nwith smaller sample sizes achieve the highest attainable asymptotic efficiency\nas if a parametric model is assumed. The estimands we consider include the\nmodel parameters, distribution functions, and quantiles. We use simulation\nexperiments to support the theoretical findings with a specific focus on\nquantile estimation. Additionally, we provide an analysis of real revenue data\nfrom U.S. collegiate sports to illustrate the efficacy of our contribution.",
        "authors": [
            "Archer Gong Zhang",
            "Jiahua Chen"
        ],
        "categories": "stat.ME",
        "published": "2023-09-16T22:06:22Z",
        "updated": "2023-09-16T22:06:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.08982v3",
        "title": "Least squares estimation in nonstationary nonlinear cohort panels with learning from experience",
        "abstract": "We discuss techniques of estimation and inference for nonstationary nonlinear\ncohort panels with learning from experience, showing, inter alia, the\nconsistency and asymptotic normality of the nonlinear least squares estimator\nused in empirical practice. Potential pitfalls for hypothesis testing are\nidentified and solutions proposed. Monte Carlo simulations verify the\nproperties of the estimator and corresponding test statistics in finite\nsamples, while an application to a panel of survey expectations demonstrates\nthe usefulness of the theory developed.",
        "authors": [
            "Alexander Mayer",
            "Michael Massmann"
        ],
        "categories": "econ.EM",
        "published": "2023-09-16T12:46:44Z",
        "updated": "2024-03-03T09:21:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.08910v2",
        "title": "Total-effect Test May Erroneously Reject So-called \"Full\" or \"Complete\" Mediation",
        "abstract": "The procedure for establishing mediation, i.e., determining that an\nindependent variable X affects a dependent variable Y through some mediator M,\nhas been under debate. The classic causal steps require that a \"total effect\"\nbe significant, now also known as statistically acknowledged. It has been shown\nthat the total-effect test can erroneously reject competitive mediation and is\nsuperfluous for establishing complementary mediation. Little is known about the\nlast type, indirect-only mediation, aka \"full\" or \"complete\" mediation, in\nwhich the indirect (ab) path passes the statistical partition test while the\ndirect-and-remainder (d) path fails. This study 1) provides proof that the\ntotal-effect test can erroneously reject indirect-only mediation, including\nboth sub-types, assuming least square estimation (LSE) F-test or Sobel test; 2)\nprovides a simulation to duplicate the mathematical proofs and extend the\nconclusion to LAD-Z test; 3) provides two real-data examples, one for each\nsub-type, to illustrate the mathematical conclusion; 4) in view of the\nmathematical findings, proposes to revisit concepts, theories, and techniques\nof mediation analysis and other causal dissection analyses, and showcase a more\ncomprehensive alternative, process-and-product analysis (PAPA).",
        "authors": [
            "Tingxuan Han",
            "Luxi Zhang",
            "Xinshu Zhao",
            "Ke Deng"
        ],
        "categories": "econ.EM",
        "published": "2023-09-16T07:26:34Z",
        "updated": "2023-09-25T13:32:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.08808v2",
        "title": "Adaptive Neyman Allocation",
        "abstract": "In experimental design, Neyman allocation refers to the practice of\nallocating subjects into treated and control groups, potentially in unequal\nnumbers proportional to their respective standard deviations, with the\nobjective of minimizing the variance of the treatment effect estimator. This\nwidely recognized approach increases statistical power in scenarios where the\ntreated and control groups have different standard deviations, as is often the\ncase in social experiments, clinical trials, marketing research, and online A/B\ntesting. However, Neyman allocation cannot be implemented unless the standard\ndeviations are known in advance. Fortunately, the multi-stage nature of the\naforementioned applications allows the use of earlier stage observations to\nestimate the standard deviations, which further guide allocation decisions in\nlater stages. In this paper, we introduce a competitive analysis framework to\nstudy this multi-stage experimental design problem. We propose a simple\nadaptive Neyman allocation algorithm, which almost matches the\ninformation-theoretic limit of conducting experiments. Using online A/B testing\ndata from a social media site, we demonstrate the effectiveness of our adaptive\nNeyman allocation algorithm, highlighting its practicality especially when\napplied with only a limited number of stages.",
        "authors": [
            "Jinglong Zhao"
        ],
        "categories": "stat.ME",
        "published": "2023-09-15T23:23:31Z",
        "updated": "2023-09-22T01:06:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.08755v1",
        "title": "Ordered Correlation Forest",
        "abstract": "Empirical studies in various social sciences often involve categorical\noutcomes with inherent ordering, such as self-evaluations of subjective\nwell-being and self-assessments in health domains. While ordered choice models,\nsuch as the ordered logit and ordered probit, are popular tools for analyzing\nthese outcomes, they may impose restrictive parametric and distributional\nassumptions. This paper introduces a novel estimator, the ordered correlation\nforest, that can naturally handle non-linearities in the data and does not\nassume a specific error term distribution. The proposed estimator modifies a\nstandard random forest splitting criterion to build a collection of forests,\neach estimating the conditional probability of a single class. Under an\n\"honesty\" condition, predictions are consistent and asymptotically normal. The\nweights induced by each forest are used to obtain standard errors for the\npredicted probabilities and the covariates' marginal effects. Evidence from\nsynthetic data shows that the proposed estimator features a superior prediction\nperformance than alternative forest-based estimators and demonstrates its\nability to construct valid confidence intervals for the covariates' marginal\neffects.",
        "authors": [
            "Riccardo Di Francesco"
        ],
        "categories": "econ.EM",
        "published": "2023-09-15T20:43:55Z",
        "updated": "2023-09-15T20:43:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.08707v4",
        "title": "Fixed-b Asymptotics for Panel Models with Two-Way Clustering",
        "abstract": "This paper studies a cluster robust variance estimator proposed by Chiang,\nHansen and Sasaki (2024) for linear panels. First, we show algebraically that\nthis variance estimator (CHS estimator, hereafter) is a linear combination of\nthree common variance estimators: the one-way unit cluster estimator, the \"HAC\nof averages\" estimator, and the \"average of HACs\" estimator. Based on this\nfinding, we obtain a fixed-$b$ asymptotic result for the CHS estimator and\ncorresponding test statistics as the cross-section and time sample sizes\njointly go to infinity. Furthermore, we propose two simple bias-corrected\nversions of the variance estimator and derive the fixed-$b$ limits. In a\nsimulation study, we find that the two bias-corrected variance estimators along\nwith fixed-$b$ critical values provide improvements in finite sample coverage\nprobabilities. We illustrate the impact of bias-correction and use of the\nfixed-$b$ critical values on inference in an empirical example on the\nrelationship between industry profitability and market concentration.",
        "authors": [
            "Kaicheng Chen",
            "Timothy J. Vogelsang"
        ],
        "categories": "econ.EM",
        "published": "2023-09-15T18:58:08Z",
        "updated": "2024-08-23T00:31:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.07476v2",
        "title": "Causal inference in network experiments: regression-based analysis and design-based properties",
        "abstract": "Investigating interference or spillover effects among units is a central task\nin many social science problems. Network experiments are powerful tools for\nthis task, which avoids endogeneity by randomly assigning treatments to units\nover networks. However, it is non-trivial to analyze network experiments\nproperly without imposing strong modeling assumptions. Previously, many\nresearchers have proposed sophisticated point estimators and standard errors\nfor causal effects under network experiments. We further show that\nregression-based point estimators and standard errors can have strong\ntheoretical guarantees if the regression functions and robust standard errors\nare carefully specified to accommodate the interference patterns under network\nexperiments. We first recall a well-known result that the Hajek estimator is\nnumerically identical to the coefficient from the weighted-least-squares fit\nbased on the inverse probability of the exposure mapping. Moreover, we\ndemonstrate that the regression-based approach offers three notable advantages:\nits ease of implementation, the ability to derive standard errors through the\nsame weighted-least-squares fit, and the capacity to integrate covariates into\nthe analysis, thereby enhancing estimation efficiency. Furthermore, we analyze\nthe asymptotic bias of the regression-based network-robust standard errors.\nRecognizing that the covariance estimator can be anti-conservative, we propose\nan adjusted covariance estimator to improve the empirical coverage rates.\nAlthough we focus on regression-based point estimators and standard errors, our\ntheory holds under the design-based framework, which assumes that the\nrandomness comes solely from the design of network experiments and allows for\narbitrary misspecification of the regression models.",
        "authors": [
            "Mengsi Gao",
            "Peng Ding"
        ],
        "categories": "econ.EM",
        "published": "2023-09-14T07:29:49Z",
        "updated": "2023-11-20T07:12:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2311.06256v1",
        "title": "From Deep Filtering to Deep Econometrics",
        "abstract": "Calculating true volatility is an essential task for option pricing and risk\nmanagement. However, it is made difficult by market microstructure noise.\nParticle filtering has been proposed to solve this problem as it favorable\nstatistical properties, but relies on assumptions about underlying market\ndynamics. Machine learning methods have also been proposed but lack\ninterpretability, and often lag in performance. In this paper we implement the\nSV-PF-RNN: a hybrid neural network and particle filter architecture. Our\nSV-PF-RNN is designed specifically with stochastic volatility estimation in\nmind. We then show that it can improve on the performance of a basic particle\nfilter.",
        "authors": [
            "Robert Stok",
            "Paul Bilokon"
        ],
        "categories": "q-fin.ST",
        "published": "2023-09-13T19:57:13Z",
        "updated": "2023-09-13T19:57:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.06693v2",
        "title": "Stochastic Learning of Semiparametric Monotone Index Models with Large Sample Size",
        "abstract": "I study the estimation of semiparametric monotone index models in the\nscenario where the number of observation points $n$ is extremely large and\nconventional approaches fail to work due to heavy computational burdens.\nMotivated by the mini-batch gradient descent algorithm (MBGD) that is widely\nused as a stochastic optimization tool in the machine learning field, I\nproposes a novel subsample- and iteration-based estimation procedure. In\nparticular, starting from any initial guess of the true parameter, I\nprogressively update the parameter using a sequence of subsamples randomly\ndrawn from the data set whose sample size is much smaller than $n$. The update\nis based on the gradient of some well-chosen loss function, where the\nnonparametric component is replaced with its Nadaraya-Watson kernel estimator\nbased on subsamples. My proposed algorithm essentially generalizes MBGD\nalgorithm to the semiparametric setup. Compared with full-sample-based method,\nthe new method reduces the computational time by roughly $n$ times if the\nsubsample size and the kernel function are chosen properly, so can be easily\napplied when the sample size $n$ is large. Moreover, I show that if I further\nconduct averages across the estimators produced during iterations, the\ndifference between the average estimator and full-sample-based estimator will\nbe $1/\\sqrt{n}$-trivial. Consequently, the average estimator is\n$1/\\sqrt{n}$-consistent and asymptotically normally distributed. In other\nwords, the new estimator substantially improves the computational speed, while\nat the same time maintains the estimation accuracy.",
        "authors": [
            "Qingsong Yao"
        ],
        "categories": "econ.EM",
        "published": "2023-09-13T03:28:31Z",
        "updated": "2023-10-28T00:09:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.06305v3",
        "title": "Sensitivity Analysis for Linear Estimators",
        "abstract": "We propose a novel sensitivity analysis framework for linear estimators with\nidentification failures that can be viewed as seeing the wrong outcome\ndistribution. Our approach measures the degree of identification failure\nthrough the change in measure between the observed distribution and a\nhypothetical target distribution that would identify the causal parameter of\ninterest. The framework yields a sensitivity analysis that generalizes existing\nbounds for Average Potential Outcome (APO), Regression Discontinuity (RD), and\ninstrumental variables (IV) exclusion failure designs. Our partial\nidentification results extend results from the APO context to allow even\nunbounded likelihood ratios. Our proposed sensitivity analysis consistently\nestimates sharp bounds under plausible conditions and estimates valid bounds\nunder mild conditions. We find that our method performs well in simulations\neven when targeting a discontinuous and nearly infinite bound.",
        "authors": [
            "Jacob Dorn",
            "Luther Yap"
        ],
        "categories": "econ.EM",
        "published": "2023-09-12T15:16:23Z",
        "updated": "2024-04-29T15:43:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.05639v3",
        "title": "Forecasted Treatment Effects",
        "abstract": "We consider estimation and inference of the effects of a policy in the\nabsence of a control group. We obtain unbiased estimators of individual\n(heterogeneous) treatment effects and a consistent and asymptotically normal\nestimator of the average treatment effect. Our estimator averages over unbiased\nforecasts of individual counterfactuals, based on a (short) time series of\npre-treatment data. The paper emphasizes the importance of focusing on forecast\nunbiasedness rather than accuracy when the end goal is estimation of average\ntreatment effects. We show that simple basis function regressions ensure\nforecast unbiasedness for a broad class of data-generating processes for the\ncounterfactuals, even in short panels. In contrast, model-based forecasting\nrequires stronger assumptions and is prone to misspecification and estimation\nbias. We show that our method can replicate the findings of some previous\nempirical studies, but without using a control group.",
        "authors": [
            "Irene Botosaru",
            "Raffaella Giacomini",
            "Martin Weidner"
        ],
        "categories": "econ.EM",
        "published": "2023-09-11T17:30:34Z",
        "updated": "2024-01-15T09:32:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.05107v1",
        "title": "Nonlinear Granger Causality using Kernel Ridge Regression",
        "abstract": "I introduce a novel algorithm and accompanying Python library, named\nmlcausality, designed for the identification of nonlinear Granger causal\nrelationships. This novel algorithm uses a flexible plug-in architecture that\nenables researchers to employ any nonlinear regressor as the base prediction\nmodel. Subsequently, I conduct a comprehensive performance analysis of\nmlcausality when the prediction regressor is the kernel ridge regressor with\nthe radial basis function kernel. The results demonstrate that mlcausality\nemploying kernel ridge regression achieves competitive AUC scores across a\ndiverse set of simulated data. Furthermore, mlcausality with kernel ridge\nregression yields more finely calibrated $p$-values in comparison to rival\nalgorithms. This enhancement enables mlcausality to attain superior accuracy\nscores when using intuitive $p$-value-based thresholding criteria. Finally,\nmlcausality with the kernel ridge regression exhibits significantly reduced\ncomputation times compared to existing nonlinear Granger causality algorithms.\nIn fact, in numerous instances, this innovative approach achieves superior\nsolutions within computational timeframes that are an order of magnitude\nshorter than those required by competing algorithms.",
        "authors": [
            "Wojciech \"Victor\" Fulmyk"
        ],
        "categories": "stat.ML",
        "published": "2023-09-10T18:28:48Z",
        "updated": "2023-09-10T18:28:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.04926v5",
        "title": "Testing for Stationary or Persistent Coefficient Randomness in Predictive Regressions",
        "abstract": "This study considers tests for coefficient randomness in predictive\nregressions. Our focus is on how tests for coefficient randomness are\ninfluenced by the persistence of random coefficient. We show that when the\nrandom coefficient is stationary, or I(0), Nyblom's (1989) LM test loses its\noptimality (in terms of power), which is established against the alternative of\nintegrated, or I(1), random coefficient. We demonstrate this by constructing a\ntest that is more powerful than the LM test when the random coefficient is\nstationary, although the test is dominated in terms of power by the LM test\nwhen the random coefficient is integrated. The power comparison is made under\nthe sequence of local alternatives that approaches the null hypothesis at\ndifferent rates depending on the persistence of the random coefficient and\nwhich test is considered. We revisit an earlier empirical research and apply\nthe tests considered in this study to the U.S. stock returns data. The result\nmostly reverses the earlier finding.",
        "authors": [
            "Mikihito Nishi"
        ],
        "categories": "econ.EM",
        "published": "2023-09-10T03:18:44Z",
        "updated": "2024-06-01T00:31:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.08619v1",
        "title": "Structural Econometric Estimation of the Basic Reproduction Number for Covid-19 Across U.S. States and Selected Countries",
        "abstract": "This paper proposes a structural econometric approach to estimating the basic\nreproduction number ($\\mathcal{R}_{0}$) of Covid-19. This approach identifies\n$\\mathcal{R}_{0}$ in a panel regression model by filtering out the effects of\nmitigating factors on disease diffusion and is easy to implement. We apply the\nmethod to data from 48 contiguous U.S. states and a diverse set of countries.\nOur results reveal a notable concentration of $\\mathcal{R}_{0}$ estimates with\nan average value of 4.5. Through a counterfactual analysis, we highlight a\nsignificant underestimation of the $\\mathcal{R}_{0}$ when mitigating factors\nare not appropriately accounted for.",
        "authors": [
            "Ida Johnsson",
            "M. Hashem Pesaran",
            "Cynthia Fan Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-09-09T17:05:51Z",
        "updated": "2023-09-09T17:05:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.04821v1",
        "title": "Non-linear dimension reduction in factor-augmented vector autoregressions",
        "abstract": "This paper introduces non-linear dimension reduction in factor-augmented\nvector autoregressions to analyze the effects of different economic shocks. I\nargue that controlling for non-linearities between a large-dimensional dataset\nand the latent factors is particularly useful during turbulent times of the\nbusiness cycle. In simulations, I show that non-linear dimension reduction\ntechniques yield good forecasting performance, especially when data is highly\nvolatile. In an empirical application, I identify a monetary policy as well as\nan uncertainty shock excluding and including observations of the COVID-19\npandemic. Those two applications suggest that the non-linear FAVAR approaches\nare capable of dealing with the large outliers caused by the COVID-19 pandemic\nand yield reliable results in both scenarios.",
        "authors": [
            "Karin Klieber"
        ],
        "categories": "econ.EM",
        "published": "2023-09-09T15:22:30Z",
        "updated": "2023-09-09T15:22:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.04793v4",
        "title": "Interpreting TSLS Estimators in Information Provision Experiments",
        "abstract": "To estimate the causal effects of beliefs on actions, researchers often run\ninformation provision experiments. We consider the causal interpretation of\ntwo-stage least squares (TSLS) estimators in these experiments. We characterize\ncommon TSLS estimators as weighted averages of causal effects, and interpret\nthese weights under general belief updating conditions that nest parametric\nmodels from the literature. Our framework accommodates TSLS estimators for both\npassive and active control designs. Notably, we find that some passive control\nestimators allow for negative weights, which compromises their causal\ninterpretation. We give practical guidance on such issues, and illustrate our\nresults in two empirical applications.",
        "authors": [
            "Vod Vilfort",
            "Whitney Zhang"
        ],
        "categories": "econ.EM",
        "published": "2023-09-09T13:36:44Z",
        "updated": "2024-06-21T17:22:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.03740v1",
        "title": "Identifying spatial interdependence in panel data with large N and small T",
        "abstract": "This paper develops a simple two-stage variational Bayesian algorithm to\nestimate panel spatial autoregressive models, where N, the number of\ncross-sectional units, is much larger than T, the number of time periods\nwithout restricting the spatial effects using a predetermined weighting matrix.\nWe use Dirichlet-Laplace priors for variable selection and parameter shrinkage.\nWithout imposing any a priori structures on the spatial linkages between\nvariables, we let the data speak for themselves. Extensive Monte Carlo studies\nshow that our method is super-fast and our estimated spatial weights matrices\nstrongly resemble the true spatial weights matrices. As an illustration, we\ninvestigate the spatial interdependence of European Union regional gross value\nadded growth rates. In addition to a clear pattern of predominant country\nclusters, we have uncovered a number of important between-country spatial\nlinkages which are yet to be documented in the literature. This new procedure\nfor estimating spatial effects is of particular relevance for researchers and\npolicy makers alike.",
        "authors": [
            "Deborah Gefang",
            "Stephen G. Hall",
            "George S. Tavlas"
        ],
        "categories": "econ.EM",
        "published": "2023-09-07T14:29:28Z",
        "updated": "2023-09-07T14:29:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.03730v1",
        "title": "A Causal Perspective on Loan Pricing: Investigating the Impacts of Selection Bias on Identifying Bid-Response Functions",
        "abstract": "In lending, where prices are specific to both customers and products, having\na well-functioning personalized pricing policy in place is essential to\neffective business making. Typically, such a policy must be derived from\nobservational data, which introduces several challenges. While the problem of\n``endogeneity'' is prominently studied in the established pricing literature,\nthe problem of selection bias (or, more precisely, bid selection bias) is not.\nWe take a step towards understanding the effects of selection bias by posing\npricing as a problem of causal inference. Specifically, we consider the\nreaction of a customer to price a treatment effect. In our experiments, we\nsimulate varying levels of selection bias on a semi-synthetic dataset on\nmortgage loan applications in Belgium. We investigate the potential of\nparametric and nonparametric methods for the identification of individual\nbid-response functions. Our results illustrate how conventional methods such as\nlogistic regression and neural networks suffer adversely from selection bias.\nIn contrast, we implement state-of-the-art methods from causal machine learning\nand show their capability to overcome selection bias in pricing data.",
        "authors": [
            "Christopher Bockel-Rickermann",
            "Sam Verboven",
            "Tim Verdonck",
            "Wouter Verbeke"
        ],
        "categories": "cs.LG",
        "published": "2023-09-07T14:14:30Z",
        "updated": "2023-09-07T14:14:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.02183v1",
        "title": "Instrumental variable estimation of the proportional hazards model by presmoothing",
        "abstract": "We consider instrumental variable estimation of the proportional hazards\nmodel of Cox (1972). The instrument and the endogenous variable are discrete\nbut there can be (possibly continuous) exogenous covariables. By making a rank\ninvariance assumption, we can reformulate the proportional hazards model into a\nsemiparametric version of the instrumental variable quantile regression model\nof Chernozhukov and Hansen (2005). A na\\\"ive estimation approach based on\nconditional moment conditions generated by the model would lead to a highly\nnonconvex and nonsmooth objective function. To overcome this problem, we\npropose a new presmoothing methodology. First, we estimate the model\nnonparametrically - and show that this nonparametric estimator has a\nclosed-form solution in the leading case of interest of randomized experiments\nwith one-sided noncompliance. Second, we use the nonparametric estimator to\ngenerate ``proxy'' observations for which exogeneity holds. Third, we apply the\nusual partial likelihood estimator to the ``proxy'' data. While the paper\nfocuses on the proportional hazards model, our presmoothing approach could be\napplied to estimate other semiparametric formulations of the instrumental\nvariable quantile regression model. Our estimation procedure allows for random\nright-censoring. We show asymptotic normality of the resulting estimator. The\napproach is illustrated via simulation studies and an empirical application to\nthe Illinois",
        "authors": [
            "Lorenzo Tedesco",
            "Jad Beyhum",
            "Ingrid Van Keilegom"
        ],
        "categories": "econ.EM",
        "published": "2023-09-05T12:39:59Z",
        "updated": "2023-09-05T12:39:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.02089v1",
        "title": "On the use of U-statistics for linear dyadic interaction models",
        "abstract": "Even though dyadic regressions are widely used in empirical applications, the\n(asymptotic) properties of estimation methods only began to be studied recently\nin the literature. This paper aims to provide in a step-by-step manner how\nU-statistics tools can be applied to obtain the asymptotic properties of\npairwise differences estimators for a two-way fixed effects model of dyadic\ninteractions. More specifically, we first propose an estimator for the model\nthat relies on pairwise differencing such that the fixed effects are\ndifferenced out. As a result, the summands of the influence function will not\nbe independent anymore, showing dependence on the individual level and\ntranslating to the fact that the usual law of large numbers and central limit\ntheorems do not straightforwardly apply. To overcome such obstacles, we show\nhow to generalize tools of U-statistics for single-index variables to the\ndouble-indices context of dyadic datasets. A key result is that there can be\ndifferent ways of defining the Hajek projection for a directed dyadic\nstructure, which will lead to distinct, but equivalent, consistent estimators\nfor the asymptotic variances. The results presented in this paper are easily\nextended to non-linear models.",
        "authors": [
            "G. M. Szini"
        ],
        "categories": "econ.EM",
        "published": "2023-09-05T09:51:45Z",
        "updated": "2023-09-05T09:51:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.02072v5",
        "title": "Data Scaling Effect of Deep Learning in Financial Time Series Forecasting",
        "abstract": "For years, researchers investigated the applications of deep learning in\nforecasting financial time series. However, they continued to rely on the\nconventional econometric approach for model training that optimizes the deep\nlearning models on individual assets. This study highlights the importance of\nglobal training, where the deep learning model is optimized across a wide\nspectrum of stocks. Focusing on stock volatility forecasting as an exemplar, we\nshow that global training is not only beneficial but also necessary for deep\nlearning-based financial time series forecasting. We further demonstrate that,\ngiven a sufficient amount of training data, a globally trained deep learning\nmodel is capable of delivering accurate zero-shot forecasts for any stocks.",
        "authors": [
            "Chen Liu",
            "Minh-Ngoc Tran",
            "Chao Wang",
            "Richard Gerlach",
            "Robert Kohn"
        ],
        "categories": "econ.EM",
        "published": "2023-09-05T09:18:45Z",
        "updated": "2024-06-01T03:40:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.01889v3",
        "title": "The Local Projection Residual Bootstrap for AR(1) Models",
        "abstract": "This paper proposes a local projection residual bootstrap method to construct\nconfidence intervals for impulse response coefficients of AR(1) models. Our\nbootstrap method is based on the local projection (LP) approach and involves a\nresidual bootstrap procedure applied to AR(1) models. We present theoretical\nresults for our bootstrap method and proposed confidence intervals. First, we\nprove the uniform consistency of the LP-residual bootstrap over a large class\nof AR(1) models that allow for a unit root. Then, we prove the asymptotic\nvalidity of our confidence intervals over the same class of AR(1) models.\nFinally, we show that the LP-residual bootstrap provides asymptotic refinements\nfor confidence intervals on a restricted class of AR(1) models relative to\nthose required for the uniform consistency of our bootstrap.",
        "authors": [
            "Amilcar Velez"
        ],
        "categories": "econ.EM",
        "published": "2023-09-05T01:45:32Z",
        "updated": "2024-02-15T23:56:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.01791v2",
        "title": "Non-Transitivity of the Win Ratio and the Area Under the Receiver Operating Characteristics Curve (AUC): a case for evaluating the strength of stochastic comparisons",
        "abstract": "The win ratio (WR) is a novel statistic used in randomized controlled trials\nthat can account for hierarchies within event outcomes. In this paper we report\nand study the long-run non-transitive behavior of the win ratio and the closely\nrelated Area Under the Receiver Operating Characteristics Curve (AUC) and argue\nthat their transitivity cannot be taken for granted. Crucially, traditional\nwithin-group statistics (i.e., comparison of means) are always transitive,\nwhile the WR can detect non-transitivity. Non-transitivity provides valuable\ninformation on the stochastic relationship between two treatment groups, which\nshould be tested and reported. We specify the necessary conditions for\ntransitivity, the sufficient conditions for non-transitivity, and demonstrate\nnon-transitivity in a real-life large randomized controlled trial for the WR of\ntime-to-death. Our results can be used to rule out or evaluate the possibility\nof non-transitivity and show the importance of studying the strength of\nstochastic relationships.",
        "authors": [
            "Olga V. Demler",
            "Ilona A. Demler"
        ],
        "categories": "stat.ME",
        "published": "2023-09-04T20:08:28Z",
        "updated": "2023-09-11T16:09:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.01764v1",
        "title": "Generalized Information Criteria for Structured Sparse Models",
        "abstract": "Regularized m-estimators are widely used due to their ability of recovering a\nlow-dimensional model in high-dimensional scenarios. Some recent efforts on\nthis subject focused on creating a unified framework for establishing oracle\nbounds, and deriving conditions for support recovery. Under this same\nframework, we propose a new Generalized Information Criteria (GIC) that takes\ninto consideration the sparsity pattern one wishes to recover. We obtain\nnon-asymptotic model selection bounds and sufficient conditions for model\nselection consistency of the GIC. Furthermore, we show that the GIC can also be\nused for selecting the regularization parameter within a regularized\n$m$-estimation framework, which allows practical use of the GIC for model\nselection in high-dimensional scenarios. We provide examples of group LASSO in\nthe context of generalized linear regression and low rank matrix regression.",
        "authors": [
            "Eduardo F. Mendes",
            "Gabriel J. P. Pinto"
        ],
        "categories": "stat.ME",
        "published": "2023-09-04T18:50:13Z",
        "updated": "2023-09-04T18:50:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.01658v1",
        "title": "Design-Based Multi-Way Clustering",
        "abstract": "This paper extends the design-based framework to settings with multi-way\ncluster dependence, and shows how multi-way clustering can be justified when\nclustered assignment and clustered sampling occurs on different dimensions, or\nwhen either sampling or assignment is multi-way clustered. Unlike one-way\nclustering, the plug-in variance estimator in multi-way clustering is no longer\nconservative, so valid inference either requires an assumption on the\ncorrelation of treatment effects or a more conservative variance estimator.\nSimulations suggest that the plug-in variance estimator is usually robust, and\nthe conservative variance estimator is often too conservative.",
        "authors": [
            "Luther Yap"
        ],
        "categories": "econ.EM",
        "published": "2023-09-04T15:21:20Z",
        "updated": "2023-09-04T15:21:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.01637v2",
        "title": "The Robust F-Statistic as a Test for Weak Instruments",
        "abstract": "Montiel Olea and Pflueger (2013) proposed the effective F-statistic as a test\nfor weak instruments in terms of the Nagar bias of the two-stage least squares\n(2SLS) estimator relative to a benchmark worst-case bias. We show that their\nmethodology applies to a class of linear generalized method of moments (GMM)\nestimators with an associated class of generalized effective F-statistics. The\nstandard nonhomoskedasticity robust F-statistic is a member of this class. The\nassociated GMMf estimator, with the extension f for first-stage, is a novel and\nunusual estimator as the weight matrix is based on the first-stage residuals.\nAs the robust F-statistic can also be used as a test for underidentification,\nexpressions for the calculation of the weak-instruments critical values in\nterms of the Nagar bias of the GMMf estimator relative to the benchmark\nsimplify and no simulation methods or Patnaik (1949) distributional\napproximations are needed. In the grouped-data IV designs of Andrews (2018),\nwhere the robust F-statistic is large but the effective F-statistic is small,\nthe GMMf estimator is shown to behave much better in terms of bias than the\n2SLS estimator, as expected by the weak-instruments test results.",
        "authors": [
            "Frank Windmeijer"
        ],
        "categories": "econ.EM",
        "published": "2023-09-04T14:42:21Z",
        "updated": "2024-12-02T15:39:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.01489v1",
        "title": "Moment-Based Estimation of Diffusion and Adoption Parameters in Networks",
        "abstract": "According to standard econometric theory, Maximum Likelihood estimation (MLE)\nis the efficient estimation choice, however, it is not always a feasible one.\nIn network diffusion models with unobserved signal propagation, MLE requires\nintegrating out a large number of latent variables, which quickly becomes\ncomputationally infeasible even for moderate network sizes and time horizons.\nLimiting the model time horizon on the other hand entails loss of important\ninformation while approximation techniques entail a (small) error that.\nSearching for a viable alternative is thus potentially highly beneficial. This\npaper proposes two estimators specifically tailored to the network diffusion\nmodel of partially observed adoption and unobserved network diffusion.",
        "authors": [
            "L. S. Sanna Stephan"
        ],
        "categories": "econ.EM",
        "published": "2023-09-04T09:51:55Z",
        "updated": "2023-09-04T09:51:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.01471v1",
        "title": "A Trimming Estimator for the Latent-Diffusion-Observed-Adoption Model",
        "abstract": "Network diffusion models are applicable to many socioeconomic interactions,\nyet network interaction is hard to observe or measure. Whenever the diffusion\nprocess is unobserved, the number of possible realizations of the latent matrix\nthat captures agents' diffusion statuses grows exponentially with the size of\nnetwork. Due to interdependencies, the log likelihood function can not be\nfactorized in individual components. As a consequence, exact estimation of\nlatent diffusion models with more than one round of interaction is\ncomputationally infeasible. In the present paper, I propose a trimming\nestimator that enables me to establish and maximize an approximate log\nlikelihood function that almost exactly identifies the peak of the true log\nlikelihood function whenever no more than one third of eligible agents are\nsubject to trimming.",
        "authors": [
            "L. S. Sanna Stephan"
        ],
        "categories": "econ.EM",
        "published": "2023-09-04T09:29:25Z",
        "updated": "2023-09-04T09:29:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.00943v2",
        "title": "iCOS: Option-Implied COS Method",
        "abstract": "This paper proposes the option-implied Fourier-cosine method, iCOS, for\nnon-parametric estimation of risk-neutral densities, option prices, and option\nsensitivities. The iCOS method leverages the Fourier-based COS technique,\nproposed by Fang and Oosterlee (2008), by utilizing the option-implied cosine\nseries coefficients. Notably, this procedure does not rely on any model\nassumptions about the underlying asset price dynamics, it is fully\nnon-parametric, and it does not involve any numerical optimization. These\nfeatures make it rather general and computationally appealing. Furthermore, we\nderive the asymptotic properties of the proposed non-parametric estimators and\nstudy their finite-sample behavior in Monte Carlo simulations. Our empirical\nanalysis using S&P 500 index options and Amazon equity options illustrates the\neffectiveness of the iCOS method in extracting valuable information from option\nprices under different market conditions. Additionally, we apply our\nmethodology to dissect and quantify observation and discretization errors in\nthe VIX index.",
        "authors": [
            "Evgenii Vladimirov"
        ],
        "categories": "q-fin.ST",
        "published": "2023-09-02T13:39:57Z",
        "updated": "2024-02-11T13:26:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.00805v1",
        "title": "Fairness Implications of Heterogeneous Treatment Effect Estimation with Machine Learning Methods in Policy-making",
        "abstract": "Causal machine learning methods which flexibly generate heterogeneous\ntreatment effect estimates could be very useful tools for governments trying to\nmake and implement policy. However, as the critical artificial intelligence\nliterature has shown, governments must be very careful of unintended\nconsequences when using machine learning models. One way to try and protect\nagainst unintended bad outcomes is with AI Fairness methods which seek to\ncreate machine learning models where sensitive variables like race or gender do\nnot influence outcomes. In this paper we argue that standard AI Fairness\napproaches developed for predictive machine learning are not suitable for all\ncausal machine learning applications because causal machine learning generally\n(at least so far) uses modelling to inform a human who is the ultimate\ndecision-maker while AI Fairness approaches assume a model that is making\ndecisions directly. We define these scenarios as indirect and direct\ndecision-making respectively and suggest that policy-making is best seen as a\njoint decision where the causal machine learning model usually only has\nindirect power. We lay out a definition of fairness for this scenario - a model\nthat provides the information a decision-maker needs to accurately make a value\njudgement about just policy outcomes - and argue that the complexity of causal\nmachine learning models can make this difficult to achieve. The solution here\nis not traditional AI Fairness adjustments, but careful modelling and awareness\nof some of the decision-making biases that these methods might encourage which\nwe describe.",
        "authors": [
            "Patrick Rehill",
            "Nicholas Biddle"
        ],
        "categories": "econ.EM",
        "published": "2023-09-02T03:06:14Z",
        "updated": "2023-09-02T03:06:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2309.00025v1",
        "title": "New general dependence measures: construction, estimation and application to high-frequency stock returns",
        "abstract": "We propose a set of dependence measures that are non-linear, local, invariant\nto a wide range of transformations on the marginals, can show tail and risk\nasymmetries, are always well-defined, are easy to estimate and can be used on\nany dataset. We propose a nonparametric estimator and prove its consistency and\nasymptotic normality. Thereby we significantly improve on existing (extreme)\ndependence measures used in asset pricing and statistics. To show practical\nutility, we use these measures on high-frequency stock return data around\nmarket distress events such as the 2010 Flash Crash and during the GFC.\nContrary to ubiquitously used correlations we find that our measures clearly\nshow tail asymmetry, non-linearity, lack of diversification and endogenous\nbuildup of risks present during these distress events. Additionally, our\nmeasures anticipate large (joint) losses during the Flash Crash while also\nanticipating the bounce back and flagging the subsequent market fragility. Our\nfindings have implications for risk management, portfolio construction and\nhedging at any frequency.",
        "authors": [
            "Aleksy Leeuwenkamp",
            "Wentao Hu"
        ],
        "categories": "q-fin.ST",
        "published": "2023-08-31T12:19:19Z",
        "updated": "2023-08-31T12:19:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.15627v1",
        "title": "Target PCA: Transfer Learning Large Dimensional Panel Data",
        "abstract": "This paper develops a novel method to estimate a latent factor model for a\nlarge target panel with missing observations by optimally using the information\nfrom auxiliary panel data sets. We refer to our estimator as target-PCA.\nTransfer learning from auxiliary panel data allows us to deal with a large\nfraction of missing observations and weak signals in the target panel. We show\nthat our estimator is more efficient and can consistently estimate weak\nfactors, which are not identifiable with conventional methods. We provide the\nasymptotic inferential theory for target-PCA under very general assumptions on\nthe approximate factor model and missing patterns. In an empirical study of\nimputing data in a mixed-frequency macroeconomic panel, we demonstrate that\ntarget-PCA significantly outperforms all benchmark methods.",
        "authors": [
            "Junting Duan",
            "Markus Pelger",
            "Ruoxuan Xiong"
        ],
        "categories": "econ.EM",
        "published": "2023-08-29T20:53:06Z",
        "updated": "2023-08-29T20:53:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.15445v1",
        "title": "Mixed-Effects Methods for Search and Matching Research",
        "abstract": "We study mixed-effects methods for estimating equations containing person and\nfirm effects. In economics such models are usually estimated using\nfixed-effects methods. Recent enhancements to those fixed-effects methods\ninclude corrections to the bias in estimating the covariance matrix of the\nperson and firm effects, which we also consider.",
        "authors": [
            "John M. Abowd",
            "Kevin L. McKinney"
        ],
        "categories": "econ.EM",
        "published": "2023-08-29T17:13:59Z",
        "updated": "2023-08-29T17:13:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.15443v1",
        "title": "Combining predictive distributions of electricity prices: Does minimizing the CRPS lead to optimal decisions in day-ahead bidding?",
        "abstract": "Probabilistic price forecasting has recently gained attention in power\ntrading because decisions based on such predictions can yield significantly\nhigher profits than those made with point forecasts alone. At the same time,\nmethods are being developed to combine predictive distributions, since no model\nis perfect and averaging generally improves forecasting performance. In this\narticle we address the question of whether using CRPS learning, a novel\nweighting technique minimizing the continuous ranked probability score (CRPS),\nleads to optimal decisions in day-ahead bidding. To this end, we conduct an\nempirical study using hourly day-ahead electricity prices from the German EPEX\nmarket. We find that increasing the diversity of an ensemble can have a\npositive impact on accuracy. At the same time, the higher computational cost of\nusing CRPS learning compared to an equal-weighted aggregation of distributions\nis not offset by higher profits, despite significantly more accurate\npredictions.",
        "authors": [
            "Weronika Nitka",
            "Rafa\u0142 Weron"
        ],
        "categories": "q-fin.ST",
        "published": "2023-08-29T17:10:38Z",
        "updated": "2023-08-29T17:10:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.15338v3",
        "title": "Another Look at the Linear Probability Model and Nonlinear Index Models",
        "abstract": "We reassess the use of linear models to approximate response probabilities of\nbinary outcomes, focusing on average partial effects (APE). We confirm that\nlinear projection parameters coincide with APEs in certain scenarios. Through\nsimulations, we identify other cases where OLS does or does not approximate\nAPEs and find that having large fraction of fitted values in [0, 1] is neither\nnecessary nor sufficient. We also show nonlinear least squares estimation of\nthe ramp model is consistent and asymptotically normal and is equivalent to\nusing OLS on an iteratively trimmed sample to reduce bias. Our findings offer\npractical guidance for empirical research.",
        "authors": [
            "Kaicheng Chen",
            "Robert S. Martin",
            "Jeffrey M. Wooldridge"
        ],
        "categories": "econ.EM",
        "published": "2023-08-29T14:32:22Z",
        "updated": "2023-10-17T19:58:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.15062v3",
        "title": "Forecasting with Feedback",
        "abstract": "Systematically biased forecasts are typically interpreted as evidence of\nforecasters' irrationality and/or asymmetric loss. In this paper we propose an\nalternative explanation: when forecasts inform economic policy decisions, and\nthe resulting actions affect the realization of the forecast target itself,\nforecasts may be optimally biased even under quadratic loss. The result arises\nin environments in which the forecaster is uncertain about the decision maker's\nreaction to the forecast, which is presumably the case in most applications. We\nillustrate the empirical relevance of our theory by reviewing some stylized\nproperties of Green Book inflation forecasts and relating them to the\npredictions from our model. Our results point out that the presence of policy\nfeedback poses a challenge to traditional tests of forecast rationality.",
        "authors": [
            "Robert P. Lieli",
            "Augusto Nieto-Barthaburu"
        ],
        "categories": "econ.TH",
        "published": "2023-08-29T06:50:13Z",
        "updated": "2024-08-27T08:06:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.14952v1",
        "title": "Stochastic Variational Inference for GARCH Models",
        "abstract": "Stochastic variational inference algorithms are derived for fitting various\nheteroskedastic time series models. We examine Gaussian, t, and skew-t response\nGARCH models and fit these using Gaussian variational approximating densities.\nWe implement efficient stochastic gradient ascent procedures based on the use\nof control variates or the reparameterization trick and demonstrate that the\nproposed implementations provide a fast and accurate alternative to Markov\nchain Monte Carlo sampling. Additionally, we present sequential updating\nversions of our variational algorithms, which are suitable for efficient\nportfolio construction and dynamic asset allocation.",
        "authors": [
            "Hanwen Xuan",
            "Luca Maestrini",
            "Feng Chen",
            "Clara Grazian"
        ],
        "categories": "stat.CO",
        "published": "2023-08-29T00:49:47Z",
        "updated": "2023-08-29T00:49:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.14464v1",
        "title": "Donut Regression Discontinuity Designs",
        "abstract": "We study the econometric properties of so-called donut regression\ndiscontinuity (RD) designs, a robustness exercise which involves repeating\nestimation and inference without the data points in some area around the\ntreatment threshold. This approach is often motivated by concerns that possible\nsystematic sorting of units, or similar data issues, in some neighborhood of\nthe treatment threshold might distort estimation and inference of RD treatment\neffects. We show that donut RD estimators can have substantially larger bias\nand variance than contentional RD estimators, and that the corresponding\nconfidence intervals can be substantially longer. We also provide a formal\ntesting framework for comparing donut and conventional RD estimation results.",
        "authors": [
            "Cladia Noack",
            "Chistoph Rothe"
        ],
        "categories": "econ.EM",
        "published": "2023-08-28T10:02:38Z",
        "updated": "2023-08-28T10:02:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.14375v2",
        "title": "Bandwidth Selection for Treatment Choice with Binary Outcomes",
        "abstract": "This study considers the treatment choice problem when outcome variables are\nbinary. We focus on statistical treatment rules that plug in fitted values\nbased on nonparametric kernel regression and show that optimizing two\nparameters enables the calculation of the maximum regret. Using this result, we\npropose a novel bandwidth selection method based on the minimax regret\ncriterion. Finally, we perform a numerical analysis to compare the optimal\nbandwidth choices for the binary and normally distributed outcomes.",
        "authors": [
            "Takuya Ishihara"
        ],
        "categories": "econ.EM",
        "published": "2023-08-28T07:46:05Z",
        "updated": "2023-09-16T11:15:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.16200v1",
        "title": "Can Machine Learning Catch Economic Recessions Using Economic and Market Sentiments?",
        "abstract": "Quantitative models are an important decision-making factor for policy makers\nand investors. Predicting an economic recession with high accuracy and\nreliability would be very beneficial for the society. This paper assesses\nmachine learning technics to predict economic recessions in United States using\nmarket sentiment and economic indicators (seventy-five explanatory variables)\nfrom Jan 1986 - June 2022 on a monthly basis frequency. In order to solve the\nissue of missing time-series data points, Autoregressive Integrated Moving\nAverage (ARIMA) method used to backcast explanatory variables. Analysis started\nwith reduction in high dimensional dataset to only most important characters\nusing Boruta algorithm, correlation matrix and solving multicollinearity issue.\nAfterwards, built various cross-validated models, both probability regression\nmethods and machine learning technics, to predict recession binary outcome. The\nmethods considered are Probit, Logit, Elastic Net, Random Forest, Gradient\nBoosting, and Neural Network. Lastly, discussed different models performance\nbased on confusion matrix, accuracy and F1 score with potential reasons for\ntheir weakness and robustness.",
        "authors": [
            "Kian Tehranian"
        ],
        "categories": "econ.EM",
        "published": "2023-08-28T01:35:40Z",
        "updated": "2023-08-28T01:35:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.14196v1",
        "title": "Identification and Estimation of Demand Models with Endogenous Product Entry and Exit",
        "abstract": "This paper deals with the endogeneity of firms' entry and exit decisions in\ndemand estimation. Product entry decisions lack a single crossing property in\nterms of demand unobservables, which causes the inconsistency of conventional\nmethods dealing with selection. We present a novel and straightforward two-step\napproach to estimate demand while addressing endogenous product entry. In the\nfirst step, our method estimates a finite mixture model of product entry\naccommodating latent market types. In the second step, it estimates demand\ncontrolling for the propensity scores of all latent market types. We apply this\napproach to data from the airline industry.",
        "authors": [
            "Victor Aguirregabiria",
            "Alessandro Iaria",
            "Senay Sokullu"
        ],
        "categories": "econ.EM",
        "published": "2023-08-27T20:13:44Z",
        "updated": "2023-08-27T20:13:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.16192v1",
        "title": "High Dimensional Time Series Regression Models: Applications to Statistical Learning Methods",
        "abstract": "These lecture notes provide an overview of existing methodologies and recent\ndevelopments for estimation and inference with high dimensional time series\nregression models. First, we present main limit theory results for high\ndimensional dependent data which is relevant to covariance matrix structures as\nwell as to dependent time series sequences. Second, we present main aspects of\nthe asymptotic theory related to time series regression models with many\ncovariates. Third, we discuss various applications of statistical learning\nmethodologies for time series analysis purposes.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-08-27T15:53:31Z",
        "updated": "2023-08-27T15:53:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.13915v1",
        "title": "Break-Point Date Estimation for Nonstationary Autoregressive and Predictive Regression Models",
        "abstract": "In this article, we study the statistical and asymptotic properties of\nbreak-point estimators in nonstationary autoregressive and predictive\nregression models for testing the presence of a single structural break at an\nunknown location in the full sample. Moreover, we investigate aspects such as\nhow the persistence properties of covariates and the location of the\nbreak-point affects the limiting distribution of the proposed break-point\nestimators.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-08-26T16:31:37Z",
        "updated": "2023-08-26T16:31:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.13688v1",
        "title": "Splash! Robustifying Donor Pools for Policy Studies",
        "abstract": "Policy researchers using synthetic control methods typically choose a donor\npool in part by using policy domain expertise so the untreated units are most\nlike the treated unit in the pre intervention period. This potentially leaves\nestimation open to biases, especially when researchers have many potential\ndonors. We compare how functional principal component analysis synthetic\ncontrol, forward-selection, and the original synthetic control method select\ndonors. To do this, we use Gaussian Process simulations as well as policy case\nstudies from West German Reunification, a hotel moratorium in Barcelona, and a\nsugar-sweetened beverage tax in San Francisco. We then summarize the\nimplications for policy research and provide avenues for future work.",
        "authors": [
            "Jared Amani Greathouse",
            "Mani Bayani",
            "Jason Coupet"
        ],
        "categories": "econ.EM",
        "published": "2023-08-25T22:04:20Z",
        "updated": "2023-08-25T22:04:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.13346v3",
        "title": "GARCHX-NoVaS: A Model-free Approach to Incorporate Exogenous Variables",
        "abstract": "In this work, we explore the forecasting ability of a recently proposed\nnormalizing and variance-stabilizing (NoVaS) transformation with the possible\ninclusion of exogenous variables. From an applied point-of-view, extra\nknowledge such as fundamentals- and sentiments-based information could be\nbeneficial to improve the prediction accuracy of market volatility if they are\nincorporated into the forecasting process. In the classical approach, these\nmodels including exogenous variables are typically termed GARCHX-type models.\nBeing a Model-free prediction method, NoVaS has generally shown more accurate,\nstable and robust (to misspecifications) performance than that compared to\nclassical GARCH-type methods. This motivates us to extend this framework to the\nGARCHX forecasting as well. We derive the NoVaS transformation needed to\ninclude exogenous covariates and then construct the corresponding prediction\nprocedure. We show through extensive simulation studies that bolster our claim\nthat the NoVaS method outperforms traditional ones, especially for long-term\ntime aggregated predictions. We also provide an interesting data analysis to\nexhibit how our method could possibly shed light on the role of geopolitical\nrisks in forecasting volatility in national stock market indices for three\ndifferent countries in Europe.",
        "authors": [
            "Kejin Wu",
            "Sayar Karmakar",
            "Rangan Gupta"
        ],
        "categories": "econ.EM",
        "published": "2023-08-25T12:35:34Z",
        "updated": "2024-09-29T21:54:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.13564v2",
        "title": "SGMM: Stochastic Approximation to Generalized Method of Moments",
        "abstract": "We introduce a new class of algorithms, Stochastic Generalized Method of\nMoments (SGMM), for estimation and inference on (overidentified) moment\nrestriction models. Our SGMM is a novel stochastic approximation alternative to\nthe popular Hansen (1982) (offline) GMM, and offers fast and scalable\nimplementation with the ability to handle streaming datasets in real time. We\nestablish the almost sure convergence, and the (functional) central limit\ntheorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we\npropose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that\ncan be seamlessly integrated within the SGMM framework. Extensive Monte Carlo\nsimulations show that as the sample size increases, the SGMM matches the\nstandard (offline) GMM in terms of estimation accuracy and gains over\ncomputational efficiency, indicating its practical value for both large-scale\nand online datasets. We demonstrate the efficacy of our approach by a proof of\nconcept using two well known empirical examples with large sample sizes.",
        "authors": [
            "Xiaohong Chen",
            "Sokbae Lee",
            "Yuan Liao",
            "Myung Hwan Seo",
            "Youngki Shin",
            "Myunghyun Song"
        ],
        "categories": "econ.EM",
        "published": "2023-08-25T00:22:45Z",
        "updated": "2023-10-30T22:13:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.13061v1",
        "title": "Spatial and Spatiotemporal Volatility Models: A Review",
        "abstract": "Spatial and spatiotemporal volatility models are a class of models designed\nto capture spatial dependence in the volatility of spatial and spatiotemporal\ndata. Spatial dependence in the volatility may arise due to spatial spillovers\namong locations; that is, if two locations are in close proximity, they can\nexhibit similar volatilities. In this paper, we aim to provide a comprehensive\nreview of the recent literature on spatial and spatiotemporal volatility\nmodels. We first briefly review time series volatility models and their\nmultivariate extensions to motivate their spatial and spatiotemporal\ncounterparts. We then review various spatial and spatiotemporal volatility\nspecifications proposed in the literature along with their underlying\nmotivations and estimation strategies. Through this analysis, we effectively\ncompare all models and provide practical recommendations for their appropriate\nusage. We highlight possible extensions and conclude by outlining directions\nfor future research.",
        "authors": [
            "Philipp Otto",
            "Osman Do\u011fan",
            "S\u00fcleyman Ta\u015fp\u0131nar",
            "Wolfgang Schmid",
            "Anil K. Bera"
        ],
        "categories": "econ.EM",
        "published": "2023-08-24T20:01:08Z",
        "updated": "2023-08-24T20:01:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.12485v2",
        "title": "Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models",
        "abstract": "Shrinkage methods are frequently used to estimate fixed effects to reduce the\nnoisiness of the least squares estimators. However, widely used shrinkage\nestimators guarantee such noise reduction only under strong distributional\nassumptions. I develop an estimator for the fixed effects that obtains the best\npossible mean squared error within a class of shrinkage estimators. This class\nincludes conventional shrinkage estimators and the optimality does not require\ndistributional assumptions. The estimator has an intuitive form and is easy to\nimplement. Moreover, the fixed effects are allowed to vary with time and to be\nserially correlated, and the shrinkage optimally incorporates the underlying\ncorrelation structure in this case. In such a context, I also provide a method\nto forecast fixed effects one period ahead.",
        "authors": [
            "Soonwoo Kwon"
        ],
        "categories": "econ.EM",
        "published": "2023-08-24T00:59:42Z",
        "updated": "2023-10-26T10:37:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.12470v3",
        "title": "Scalable Estimation of Multinomial Response Models with Random Consideration Sets",
        "abstract": "A common assumption in the fitting of unordered multinomial response models\nfor $J$ mutually exclusive categories is that the responses arise from the same\nset of $J$ categories across subjects. However, when responses measure a choice\nmade by the subject, it is more appropriate to condition the distribution of\nmultinomial responses on a subject-specific consideration set, drawn from the\npower set of $\\{1,2,\\ldots,J\\}$. This leads to a mixture of multinomial\nresponse models governed by a probability distribution over the $J^{\\ast} = 2^J\n-1$ consideration sets. We introduce a novel method for estimating such\ngeneralized multinomial response models based on the fundamental result that\nany mass distribution over $J^{\\ast}$ consideration sets can be represented as\na mixture of products of $J$ component-specific inclusion-exclusion\nprobabilities. Moreover, under time-invariant consideration sets, the\nconditional posterior distribution of consideration sets is sparse. These\nfeatures enable a scalable MCMC algorithm for sampling the posterior\ndistribution of parameters, random effects, and consideration sets. Under\nregularity conditions, the posterior distributions of the marginal response\nprobabilities and the model parameters satisfy consistency. The methodology is\ndemonstrated in a longitudinal data set on weekly cereal purchases that cover\n$J = 101$ brands, a dimension substantially beyond the reach of existing\nmethods.",
        "authors": [
            "Siddhartha Chib",
            "Kenichi Shimizu"
        ],
        "categories": "stat.ME",
        "published": "2023-08-23T23:48:47Z",
        "updated": "2024-09-01T00:47:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.11173v1",
        "title": "Forecasting inflation using disaggregates and machine learning",
        "abstract": "This paper examines the effectiveness of several forecasting methods for\npredicting inflation, focusing on aggregating disaggregated forecasts - also\nknown in the literature as the bottom-up approach. Taking the Brazilian case as\nan application, we consider different disaggregation levels for inflation and\nemploy a range of traditional time series techniques as well as linear and\nnonlinear machine learning (ML) models to deal with a larger number of\npredictors. For many forecast horizons, the aggregation of disaggregated\nforecasts performs just as well survey-based expectations and models that\ngenerate forecasts using the aggregate directly. Overall, ML methods outperform\ntraditional time series models in predictive accuracy, with outstanding\nperformance in forecasting disaggregates. Our results reinforce the benefits of\nusing models in a data-rich environment for inflation forecasting, including\naggregating disaggregated forecasts from ML techniques, mainly during volatile\nperiods. Starting from the COVID-19 pandemic, the random forest model based on\nboth aggregate and disaggregated inflation achieves remarkable predictive\nperformance at intermediate and longer horizons.",
        "authors": [
            "Gilberto Boaretto",
            "Marcelo C. Medeiros"
        ],
        "categories": "econ.EM",
        "published": "2023-08-22T04:01:40Z",
        "updated": "2023-08-22T04:01:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.10993v1",
        "title": "Econometrics of Machine Learning Methods in Economic Forecasting",
        "abstract": "This paper surveys the recent advances in machine learning method for\neconomic forecasting. The survey covers the following topics: nowcasting,\ntextual data, panel and tensor data, high-dimensional Granger causality tests,\ntime series cross-validation, classification with economic losses.",
        "authors": [
            "Andrii Babii",
            "Eric Ghysels",
            "Jonas Striaukas"
        ],
        "categories": "econ.EM",
        "published": "2023-08-21T19:19:34Z",
        "updated": "2023-08-21T19:19:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.10823v1",
        "title": "Simulation Experiments as a Causal Problem",
        "abstract": "Simulation methods are among the most ubiquitous methodological tools in\nstatistical science. In particular, statisticians often is simulation to\nexplore properties of statistical functionals in models for which developed\nstatistical theory is insufficient or to assess finite sample properties of\ntheoretical results. We show that the design of simulation experiments can be\nviewed from the perspective of causal intervention on a data generating\nmechanism. We then demonstrate the use of causal tools and frameworks in this\ncontext. Our perspective is agnostic to the particular domain of the simulation\nexperiment which increases the potential impact of our proposed approach. In\nthis paper, we consider two illustrative examples. First, we re-examine a\npredictive machine learning example from a popular textbook designed to assess\nthe relationship between mean function complexity and the mean-squared error.\nSecond, we discuss a traditional causal inference method problem, simulating\nthe effect of unmeasured confounding on estimation, specifically to illustrate\nbias amplification. In both cases, applying causal principles and using\ngraphical models with parameters and distributions as nodes in the spirit of\ninfluence diagrams can 1) make precise which estimand the simulation targets ,\n2) suggest modifications to better attain the simulation goals, and 3) provide\nscaffolding to discuss performance criteria for a particular simulation design.",
        "authors": [
            "Tyrel Stokes",
            "Ian Shrier",
            "Russell Steele"
        ],
        "categories": "stat.ME",
        "published": "2023-08-21T16:14:16Z",
        "updated": "2023-08-21T16:14:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.10138v5",
        "title": "On the Inconsistency of Cluster-Robust Inference and How Subsampling Can Fix It",
        "abstract": "Conventional methods of cluster-robust inference are inconsistent in the\npresence of unignorably large clusters. We formalize this claim by establishing\na necessary and sufficient condition for the consistency of the conventional\nmethods. We find that this condition for the consistency is rejected for a\nmajority of empirical research papers. In this light, we propose a novel score\nsubsampling method that achieves uniform size control over a broad class of\ndata generating processes, covering that fails the conventional method.\nSimulation studies support these claims. With real data used by an empirical\npaper, we showcase that the conventional methods conclude significance while\nour proposed method concludes insignificance.",
        "authors": [
            "Harold D. Chiang",
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-08-20T02:35:52Z",
        "updated": "2024-03-23T14:31:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.09535v2",
        "title": "Weak Identification with Many Instruments",
        "abstract": "Linear instrumental variable regressions are widely used to estimate causal\neffects. Many instruments arise from the use of ``technical'' instruments and\nmore recently from the empirical strategy of ``judge design''. This paper\nsurveys and summarizes ideas from recent literature on estimation and\nstatistical inferences with many instruments for a single endogenous regressor.\nWe discuss how to assess the strength of the instruments and how to conduct\nweak identification-robust inference under heteroskedasticity. We establish new\nresults for a jack-knifed version of the Lagrange Multiplier (LM) test\nstatistic. Furthermore, we extend the weak-identification-robust tests to\nsettings with both many exogenous regressors and many instruments. We propose a\ntest that properly partials out many exogenous regressors while preserving the\nre-centering property of the jack-knife. The proposed tests have correct size\nand good power properties.",
        "authors": [
            "Anna Mikusheva",
            "Liyang Sun"
        ],
        "categories": "econ.EM",
        "published": "2023-08-18T13:13:41Z",
        "updated": "2024-01-25T01:48:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.09009v1",
        "title": "Closed-form approximations of moments and densities of continuous-time Markov models",
        "abstract": "This paper develops power series expansions of a general class of moment\nfunctions, including transition densities and option prices, of continuous-time\nMarkov processes, including jump--diffusions. The proposed expansions extend\nthe ones in Kristensen and Mele (2011) to cover general Markov processes. We\ndemonstrate that the class of expansions nests the transition density and\noption price expansions developed in Yang, Chen, and Wan (2019) and Wan and\nYang (2021) as special cases, thereby connecting seemingly different ideas in a\nunified framework. We show how the general expansion can be implemented for\nfully general jump--diffusion models. We provide a new theory for the validity\nof the expansions which shows that series expansions are not guaranteed to\nconverge as more terms are added in general. Thus, these methods should be used\nwith caution. At the same time, the numerical studies in this paper demonstrate\ngood performance of the proposed implementation in practice when a small number\nof terms are included.",
        "authors": [
            "Dennis Kristensen",
            "Young Jun Lee",
            "Antonio Mele"
        ],
        "categories": "econ.EM",
        "published": "2023-08-17T14:25:39Z",
        "updated": "2023-08-17T14:25:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.08958v2",
        "title": "Linear Regression with Weak Exogeneity",
        "abstract": "This paper studies linear time series regressions with many regressors. Weak\nexogeneity is the most used identifying assumption in time series. Weak\nexogeneity requires the structural error to have zero conditional expectation\ngiven the present and past regressor values, allowing errors to correlate with\nfuture regressor realizations. We show that weak exogeneity in time series\nregressions with many controls may produce substantial biases and even render\nthe least squares (OLS) estimator inconsistent. The bias arises in settings\nwith many regressors because the normalized OLS design matrix remains\nasymptotically random and correlates with the regression error when only weak\n(but not strict) exogeneity holds. This bias's magnitude increases with the\nnumber of regressors and their average autocorrelation. To address this issue,\nwe propose an innovative approach to bias correction that yields a new\nestimator with improved properties relative to OLS. We establish consistency\nand conditional asymptotic Gaussianity of this new estimator and provide a\nmethod for inference.",
        "authors": [
            "Anna Mikusheva",
            "Mikkel S\u00f8lvsten"
        ],
        "categories": "econ.EM",
        "published": "2023-08-17T12:57:51Z",
        "updated": "2024-01-17T15:10:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.08390v2",
        "title": "Testing Partial Instrument Monotonicity",
        "abstract": "When multi-dimensional instruments are used to identify and estimate causal\neffects, the monotonicity condition may not hold due to heterogeneity in the\npopulation. Under a partial monotonicity condition, which only requires the\nmonotonicity to hold for each instrument separately holding all the other\ninstruments fixed, the 2SLS estimand can still be a positively weighted average\nof LATEs. In this paper, we provide a simple nonparametric test for partial\ninstrument monotonicity. We demonstrate the good finite sample properties of\nthe test through Monte Carlo simulations. We then apply the test to monetary\nincentives and distance from results centers as instruments for the knowledge\nof HIV status.",
        "authors": [
            "Hongyi Jiang",
            "Zhenting Sun"
        ],
        "categories": "econ.EM",
        "published": "2023-08-16T14:22:51Z",
        "updated": "2023-08-24T22:12:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.08276v1",
        "title": "Computer vision-enriched discrete choice models, with an application to residential location choice",
        "abstract": "Visual imagery is indispensable to many multi-attribute decision situations.\nExamples of such decision situations in travel behaviour research include\nresidential location choices, vehicle choices, tourist destination choices, and\nvarious safety-related choices. However, current discrete choice models cannot\nhandle image data and thus cannot incorporate information embedded in images\ninto their representations of choice behaviour. This gap between discrete\nchoice models' capabilities and the real-world behaviour it seeks to model\nleads to incomplete and, possibly, misleading outcomes. To solve this gap, this\nstudy proposes \"Computer Vision-enriched Discrete Choice Models\" (CV-DCMs).\nCV-DCMs can handle choice tasks involving numeric attributes and images by\nintegrating computer vision and traditional discrete choice models. Moreover,\nbecause CV-DCMs are grounded in random utility maximisation principles, they\nmaintain the solid behavioural foundation of traditional discrete choice\nmodels. We demonstrate the proposed CV-DCM by applying it to data obtained\nthrough a novel stated choice experiment involving residential location\nchoices. In this experiment, respondents faced choice tasks with trade-offs\nbetween commute time, monthly housing cost and street-level conditions,\npresented using images. As such, this research contributes to the growing body\nof literature in the travel behaviour field that seeks to integrate discrete\nchoice modelling and machine learning.",
        "authors": [
            "Sander van Cranenburgh",
            "Francisco Garrido-Valenzuela"
        ],
        "categories": "cs.CV",
        "published": "2023-08-16T10:33:24Z",
        "updated": "2023-08-16T10:33:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.08152v2",
        "title": "Estimating Effects of Long-Term Treatments",
        "abstract": "Estimating the effects of long-term treatments through A/B testing is\nchallenging. Treatments, such as updates to product functionalities, user\ninterface designs, and recommendation algorithms, are intended to persist\nwithin the system for a long duration of time after their initial launches.\nHowever, due to the constraints of conducting long-term experiments,\npractitioners often rely on short-term experimental results to make product\nlaunch decisions. It remains open how to accurately estimate the effects of\nlong-term treatments using short-term experimental data. To address this\nquestion, we introduce a longitudinal surrogate framework that decomposes the\nlong-term effects into functions based on user attributes, short-term metrics,\nand treatment assignments. We outline identification assumptions, estimation\nstrategies, inferential techniques, and validation methods under this\nframework. Empirically, we demonstrate that our approach outperforms existing\nsolutions by using data from two real-world experiments, each involving more\nthan a million users on WeChat, one of the world's largest social networking\nplatforms.",
        "authors": [
            "Shan Huang",
            "Chen Wang",
            "Yuan Yuan",
            "Jinglong Zhao",
            "Brocco",
            "Zhang"
        ],
        "categories": "econ.EM",
        "published": "2023-08-16T05:42:58Z",
        "updated": "2024-12-07T03:23:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.07979v2",
        "title": "Emerging Frontiers: Exploring the Impact of Generative AI Platforms on University Quantitative Finance Examinations",
        "abstract": "This study evaluated three Artificial Intelligence (AI) large language model\n(LLM) enabled platforms - ChatGPT, BARD, and Bing AI - to answer an\nundergraduate finance exam with 20 quantitative questions across various\ndifficulty levels. ChatGPT scored 30 percent, outperforming Bing AI, which\nscored 20 percent, while Bard lagged behind with a score of 15 percent. These\nmodels faced common challenges, such as inaccurate computations and formula\nselection. While they are currently insufficient for helping students pass the\nfinance exam, they serve as valuable tools for dedicated learners. Future\nadvancements are expected to overcome these limitations, allowing for improved\nformula selection and accurate computations and potentially enabling students\nto score 90 percent or higher.",
        "authors": [
            "Rama K. Malladi"
        ],
        "categories": "econ.EM",
        "published": "2023-08-15T18:27:39Z",
        "updated": "2023-08-20T04:42:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.07830v1",
        "title": "Optimizing B2B Product Offers with Machine Learning, Mixed Logit, and Nonlinear Programming",
        "abstract": "In B2B markets, value-based pricing and selling has become an important\nalternative to discounting. This study outlines a modeling method that uses\ncustomer data (product offers made to each current or potential customer,\nfeatures, discounts, and customer purchase decisions) to estimate a mixed logit\nchoice model. The model is estimated via hierarchical Bayes and machine\nlearning, delivering customer-level parameter estimates. Customer-level\nestimates are input into a nonlinear programming next-offer maximization\nproblem to select optimal features and discount level for customer segments,\nwhere segments are based on loyalty and discount elasticity. The mixed logit\nmodel is integrated with economic theory (the random utility model), and it\npredicts both customer perceived value for and response to alternative future\nsales offers. The methodology can be implemented to support value-based pricing\nand selling efforts.\n  Contributions to the literature include: (a) the use of customer-level\nparameter estimates from a mixed logit model, delivered via a hierarchical\nBayes estimation procedure, to support value-based pricing decisions; (b)\nvalidation that mixed logit customer-level modeling can deliver strong\npredictive accuracy, not as high as random forest but comparing favorably; and\n(c) a nonlinear programming problem that uses customer-level mixed logit\nestimates to select optimal features and discounts.",
        "authors": [
            "John V. Colias",
            "Stella Park",
            "Elizabeth Horn"
        ],
        "categories": "econ.EM",
        "published": "2023-08-15T15:17:09Z",
        "updated": "2023-08-15T15:17:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.07519v1",
        "title": "Serendipity in Science",
        "abstract": "Serendipity plays an important role in scientific discovery. Indeed, many of\nthe most important breakthroughs, ranging from penicillin to the electric\nbattery, have been made by scientists who were stimulated by a chance exposure\nto unsought but useful information. However, not all scientists are equally\nlikely to benefit from such serendipitous exposure. Although scholars generally\nagree that scientists with a prepared mind are most likely to benefit from\nserendipitous encounters, there is much less consensus over what precisely\nconstitutes a prepared mind, with some research suggesting the importance of\nopenness and others emphasizing the need for deep prior experience in a\nparticular domain. In this paper, we empirically investigate the role of\nserendipity in science by leveraging a policy change that exogenously shifted\nthe shelving location of journals in university libraries and subsequently\nexposed scientists to unsought scientific information. Using large-scale data\non 2.4 million papers published in 9,750 journals by 520,000 scientists at 115\nNorth American research universities, we find that scientists with greater\nopenness are more likely to benefit from serendipitous encounters. Following\nthe policy change, these scientists tended to cite less familiar and newer\nwork, and ultimately published papers that were more innovative. By contrast,\nwe find little effect on innovativeness for scientists with greater depth of\nexperience, who, in our sample, tended to cite more familiar and older work\nfollowing the policy change.",
        "authors": [
            "Pyung Nahm",
            "Raviv Murciano-Goroff",
            "Michael Park",
            "Russell J. Funk"
        ],
        "categories": "econ.EM",
        "published": "2023-08-15T01:22:07Z",
        "updated": "2023-08-15T01:22:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.06617v3",
        "title": "Quantile Time Series Regression Models Revisited",
        "abstract": "This article discusses recent developments in the literature of quantile time\nseries models in the cases of stationary and nonstationary underline stochastic\nprocesses.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-08-12T17:21:54Z",
        "updated": "2023-08-22T07:50:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.06426v1",
        "title": "Driver Heterogeneity in Willingness to Give Control to Conditional Automation",
        "abstract": "The driver's willingness to give (WTG) control in conditionally automated\ndriving is assessed in a virtual reality based driving-rig, through their\nchoice to give away driving control and through the extent to which automated\ndriving is adopted in a mixed-traffic environment. Within- and across-class\nunobserved heterogeneity and locus of control variations are taken into\naccount. The choice of giving away control is modelled using the mixed logit\n(MIXL) and mixed latent class (LCML) model. The significant latent segments of\nthe locus of control are developed into internalizers and externalizers by the\nlatent class model (LCM) based on the taste heterogeneity identified from the\nMIXL model. Results suggest that drivers choose to \"giveAway\" control of the\nvehicle when greater concentration/attentiveness is required (e.g., in the\nnighttime) or when they are interested in performing a non-driving-related task\n(NDRT). In addition, it is observed that internalizers demonstrate more\nheterogeneity compared to externalizers in terms of WTG.",
        "authors": [
            "Muhammad Sajjad Ansar",
            "Nael Alsaleh",
            "Bilal Farooq"
        ],
        "categories": "cs.HC",
        "published": "2023-08-12T00:53:04Z",
        "updated": "2023-08-12T00:53:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.05895v1",
        "title": "Characterizing Correlation Matrices that Admit a Clustered Factor Representation",
        "abstract": "The Clustered Factor (CF) model induces a block structure on the correlation\nmatrix and is commonly used to parameterize correlation matrices. Our results\nreveal that the CF model imposes superfluous restrictions on the correlation\nmatrix. This can be avoided by a different parametrization, involving the\nlogarithmic transformation of the block correlation matrix.",
        "authors": [
            "Chen Tong",
            "Peter Reinhard Hansen"
        ],
        "categories": "econ.EM",
        "published": "2023-08-11T01:07:39Z",
        "updated": "2023-08-11T01:07:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.05564v4",
        "title": "Large Skew-t Copula Models and Asymmetric Dependence in Intraday Equity Returns",
        "abstract": "Skew-t copula models are attractive for the modeling of financial data\nbecause they allow for asymmetric and extreme tail dependence. We show that the\ncopula implicit in the skew-t distribution of Azzalini and Capitanio (2003)\nallows for a higher level of pairwise asymmetric dependence than two popular\nalternative skew-t copulas. Estimation of this copula in high dimensions is\nchallenging, and we propose a fast and accurate Bayesian variational inference\n(VI) approach to do so. The method uses a generative representation of the\nskew-t distribution to define an augmented posterior that can be approximated\naccurately. A stochastic gradient ascent algorithm is used to solve the\nvariational optimization. The methodology is used to estimate skew-t factor\ncopula models with up to 15 factors for intraday returns from 2017 to 2021 on\n93 U.S. equities. The copula captures substantial heterogeneity in asymmetric\ndependence over equity pairs, in addition to the variability in pairwise\ncorrelations. In a moving window study we show that the asymmetric dependencies\nalso vary over time, and that intraday predictive densities from the skew-t\ncopula are more accurate than those from benchmark copula models. Portfolio\nselection strategies based on the estimated pairwise asymmetric dependencies\nimprove performance relative to the index.",
        "authors": [
            "Lin Deng",
            "Michael Stanley Smith",
            "Worapree Maneesoonthorn"
        ],
        "categories": "econ.EM",
        "published": "2023-08-10T13:24:45Z",
        "updated": "2024-07-02T07:27:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.05486v3",
        "title": "Money Growth and Inflation: A Quantile Sensitivity Approach",
        "abstract": "An innovative method is proposed to construct a quantile dependence system\nfor inflation and money growth. By considering all quantiles and leveraging a\nnovel notion of quantile sensitivity, the method allows the assessment of\nchanges in the entire distribution of a variable of interest in response to a\nperturbation in another variable's quantile. The construction of this\nrelationship is demonstrated through a system of linear quantile regressions.\nThen, the proposed framework is exploited to examine the distributional effects\nof money growth on the distributions of inflation and its disaggregate measures\nin the United States and the Euro area. The empirical analysis uncovers\nsignificant impacts of the upper quantile of the money growth distribution on\nthe distribution of inflation and its disaggregate measures. Conversely, the\nlower and median quantiles of the money growth distribution are found to have a\nnegligible influence. Finally, this distributional impact exhibits variation\nover time in both the United States and the Euro area.",
        "authors": [
            "Matteo Iacopini",
            "Aubrey Poon",
            "Luca Rossini",
            "Dan Zhu"
        ],
        "categories": "econ.EM",
        "published": "2023-08-10T10:26:10Z",
        "updated": "2023-11-17T09:19:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.05263v1",
        "title": "Solving the Forecast Combination Puzzle",
        "abstract": "We demonstrate that the forecasting combination puzzle is a consequence of\nthe methodology commonly used to produce forecast combinations. By the\ncombination puzzle, we refer to the empirical finding that predictions formed\nby combining multiple forecasts in ways that seek to optimize forecast\nperformance often do not out-perform more naive, e.g. equally-weighted,\napproaches. In particular, we demonstrate that, due to the manner in which such\nforecasts are typically produced, tests that aim to discriminate between the\npredictive accuracy of competing combination strategies can have low power, and\ncan lack size control, leading to an outcome that favours the naive approach.\nWe show that this poor performance is due to the behavior of the corresponding\ntest statistic, which has a non-standard asymptotic distribution under the null\nhypothesis of no inferior predictive accuracy, rather than the {standard normal\ndistribution that is} {typically adopted}. In addition, we demonstrate that the\nlow power of such predictive accuracy tests in the forecast combination setting\ncan be completely avoided if more efficient estimation strategies are used in\nthe production of the combinations, when feasible. We illustrate these findings\nboth in the context of forecasting a functional of interest and in terms of\npredictive densities. A short empirical example {using daily financial returns}\nexemplifies how researchers can avoid the puzzle in practical settings.",
        "authors": [
            "David T. Frazier",
            "Ryan Covey",
            "Gael M. Martin",
            "Donald Poskitt"
        ],
        "categories": "econ.EM",
        "published": "2023-08-10T00:16:31Z",
        "updated": "2023-08-10T00:16:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.05183v1",
        "title": "Interpolation of numerical series by the Fermat-Torricelli point construction method on the example of the numerical series of inflation in the Czech Republic in 2011-2021",
        "abstract": "The use of regression analysis for processing experimental data is fraught\nwith certain difficulties, which, when models are constructed, are associated\nwith assumptions, and there is a normal law of error distribution and variables\nare statistically independent. In practice , these conditions do not always\ntake place . This may cause the constructed economic and mathematical model to\nhave no practical value. As an alternative approach to the study of numerical\nseries, according to the author, smoothing of numerical series using\nFermat-Torricelli points with subsequent interpolation of these points by\nseries of exponents could be used. The use of exponential series for\ninterpolating numerical series makes it possible to achieve the accuracy of\nmodel construction no worse than regression analysis . At the same time, the\ninterpolation by series of exponents does not require the statistical material\nthat the errors of the numerical series obey the normal distribution law, and\nstatistical independence of variables is also not required. Interpolation of\nnumerical series by exponential series represents a \"black box\" type model,\nthat is, only input parameters and output parameters matter.",
        "authors": [
            "Yekimov Sergey"
        ],
        "categories": "econ.EM",
        "published": "2023-08-09T18:40:04Z",
        "updated": "2023-08-09T18:40:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.05171v1",
        "title": "Statistical Decision Theory Respecting Stochastic Dominance",
        "abstract": "The statistical decision theory pioneered by Wald (1950) has used\nstate-dependent mean loss (risk) to measure the performance of statistical\ndecision functions across potential samples. We think it evident that\nevaluation of performance should respect stochastic dominance, but we do not\nsee a compelling reason to focus exclusively on mean loss. We think it\ninstructive to also measure performance by other functionals that respect\nstochastic dominance, such as quantiles of the distribution of loss. This paper\ndevelops general principles and illustrative applications for statistical\ndecision theory respecting stochastic dominance. We modify the Wald definition\nof admissibility to an analogous concept of stochastic dominance (SD)\nadmissibility, which uses stochastic dominance rather than mean sampling\nperformance to compare alternative decision rules. We study SD admissibility in\ntwo relatively simple classes of decision problems that arise in treatment\nchoice. We reevaluate the relationship between the MLE, James-Stein, and\nJames-Stein positive part estimators from the perspective of SD admissibility.\nWe consider alternative criteria for choice among SD-admissible rules. We\njuxtapose traditional criteria based on risk, regret, or Bayes risk with\nanalogous ones based on quantiles of state-dependent sampling distributions or\nthe Bayes distribution of loss.",
        "authors": [
            "Charles F. Manski",
            "Aleksey Tetenov"
        ],
        "categories": "econ.EM",
        "published": "2023-08-09T18:15:37Z",
        "updated": "2023-08-09T18:15:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.04963v1",
        "title": "A Guide to Impact Evaluation under Sample Selection and Missing Data: Teacher's Aides and Adolescent Mental Health",
        "abstract": "This paper is concerned with identification, estimation, and specification\ntesting in causal evaluation problems when data is selective and/or missing. We\nleverage recent advances in the literature on graphical methods to provide a\nunifying framework for guiding empirical practice. The approach integrates and\nconnects to prominent identification and testing strategies in the literature\non missing data, causal machine learning, panel data analysis, and more. We\ndemonstrate its utility in the context of identification and specification\ntesting in sample selection models and field experiments with attrition. We\nprovide a novel analysis of a large-scale cluster-randomized controlled\nteacher's aide trial in Danish schools at grade 6. Even with detailed\nadministrative data, the handling of missing data crucially affects broader\nconclusions about effects on mental health. Results suggest that teaching\nassistants provide an effective way of improving internalizing behavior for\nlarge parts of the student population.",
        "authors": [
            "Simon Calmar Andersen",
            "Louise Beuchert",
            "Phillip Heiler",
            "Helena Skyt Nielsen"
        ],
        "categories": "econ.EM",
        "published": "2023-08-09T13:53:41Z",
        "updated": "2023-08-09T13:53:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.04276v2",
        "title": "Causal Interpretation of Linear Social Interaction Models with Endogenous Networks",
        "abstract": "This study investigates the causal interpretation of linear social\ninteraction models in the presence of endogeneity in network formation under a\nheterogeneous treatment effects framework. We consider an experimental setting\nin which individuals are randomly assigned to treatments while no interventions\nare made for the network structure. We show that running a linear regression\nignoring network endogeneity is not problematic for estimating the average\ndirect treatment effect. However, it leads to sample selection bias and\nnegative-weights problem for the estimation of the average spillover effect. To\novercome these problems, we propose using potential peer treatment as an\ninstrumental variable (IV), which is automatically a valid IV for actual\nspillover exposure. Using this IV, we examine two IV-based estimands and\ndemonstrate that they have a local average treatment-effect-type causal\ninterpretation for the spillover effect.",
        "authors": [
            "Tadao Hoshino"
        ],
        "categories": "econ.EM",
        "published": "2023-08-08T14:18:11Z",
        "updated": "2023-10-20T07:50:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.04057v1",
        "title": "Threshold Regression in Heterogeneous Panel Data with Interactive Fixed Effects",
        "abstract": "This paper introduces unit-specific heterogeneity in panel data threshold\nregression. Both slope coefficients and threshold parameters are allowed to\nvary by unit. The heterogeneous threshold parameters manifest via a\nunit-specific empirical quantile transformation of a common underlying\nthreshold parameter which is estimated efficiently from the whole panel. In the\nerrors, the unobserved heterogeneity of the panel takes the general form of\ninteractive fixed effects. The newly introduced parameter heterogeneity has\nimplications for model identification, estimation, interpretation, and\nasymptotic inference. The assumption of a shrinking threshold magnitude now\nimplies shrinking heterogeneity and leads to faster estimator rates of\nconvergence than previously encountered. The asymptotic theory for the proposed\nestimators is derived and Monte Carlo simulations demonstrate its usefulness in\nsmall samples. The new model is employed to examine the Feldstein-Horioka\npuzzle and it is found that the trade liberalization policies of the 80's\nsignificantly impacted cross-country capital mobility.",
        "authors": [
            "Marco Barassi",
            "Yiannis Karavias",
            "Chongxian Zhu"
        ],
        "categories": "econ.EM",
        "published": "2023-08-08T05:37:14Z",
        "updated": "2023-08-08T05:37:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.03708v1",
        "title": "Measuring income inequality via percentile relativities",
        "abstract": "\"The rich are getting richer\" implies that the population income\ndistributions are getting more right skewed and heavily tailed. For such\ndistributions, the mean is not the best measure of the center, but the\nclassical indices of income inequality, including the celebrated Gini index,\nare all mean-based. In view of this, Professor Gastwirth sounded an alarm back\nin 2014 by suggesting to incorporate the median into the definition of the Gini\nindex, although noted a few shortcomings of his proposed index. In the present\npaper we make a further step in the modification of classical indices and, to\nacknowledge the possibility of differing viewpoints, arrive at three\nmedian-based indices of inequality. They avoid the shortcomings of the previous\nindices and can be used even when populations are ultra heavily tailed, that\nis, when their first moments are infinite. The new indices are illustrated both\nanalytically and numerically using parametric families of income distributions,\nand further illustrated using capital incomes coming from 2001 and 2018 surveys\nof fifteen European countries. We also discuss the performance of the indices\nfrom the perspective of income transfers.",
        "authors": [
            "Vytaras Brazauskas",
            "Francesca Greselin",
            "Ricardas Zitikis"
        ],
        "categories": "stat.ME",
        "published": "2023-08-07T16:25:12Z",
        "updated": "2023-08-07T16:25:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.02899v1",
        "title": "Treatment Effects in Staggered Adoption Designs with Non-Parallel Trends",
        "abstract": "This paper considers identifying and estimating causal effect parameters in a\nstaggered treatment adoption setting -- that is, where a researcher has access\nto panel data and treatment timing varies across units. We consider the case\nwhere untreated potential outcomes may follow non-parallel trends over time\nacross groups. This implies that the identifying assumptions of leading\napproaches such as difference-in-differences do not hold. We mainly focus on\nthe case where untreated potential outcomes are generated by an interactive\nfixed effects model and show that variation in treatment timing provides\nadditional moment conditions that can be used to recover a large class of\ntarget causal effect parameters. Our approach exploits the variation in\ntreatment timing without requiring either (i) a large number of time periods or\n(ii) requiring any extra exclusion restrictions. This is in contrast to\nessentially all of the literature on interactive fixed effects models which\nrequires at least one of these extra conditions. Rather, our approach directly\napplies in settings where there is variation in treatment timing. Although our\nmain focus is on a model with interactive fixed effects, our idea of using\nvariation in treatment timing to recover causal effect parameters is quite\ngeneral and could be adapted to other settings with non-parallel trends across\ngroups such as dynamic panel data models.",
        "authors": [
            "Brantly Callaway",
            "Emmanuel Selorm Tsyawo"
        ],
        "categories": "econ.EM",
        "published": "2023-08-05T15:17:39Z",
        "updated": "2023-08-05T15:17:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.02450v2",
        "title": "Composite Quantile Factor Model",
        "abstract": "This paper introduces the method of composite quantile factor model for\nfactor analysis in high-dimensional panel data. We propose to estimate the\nfactors and factor loadings across multiple quantiles of the data, allowing the\nestimates to better adapt to features of the data at different quantiles while\nstill modeling the mean of the data. We develop the limiting distribution of\nthe estimated factors and factor loadings, and an information criterion for\nconsistent factor number selection is also discussed. Simulations show that the\nproposed estimator and the information criterion have good finite sample\nproperties for several non-normal distributions under consideration. We also\nconsider an empirical study on the factor analysis for 246 quarterly\nmacroeconomic variables. A companion R package cqrfactor is developed.",
        "authors": [
            "Xiao Huang"
        ],
        "categories": "econ.EM",
        "published": "2023-08-04T16:32:11Z",
        "updated": "2024-11-30T23:30:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.02364v1",
        "title": "Matrix Completion When Missing Is Not at Random and Its Applications in Causal Panel Data Models",
        "abstract": "This paper develops an inferential framework for matrix completion when\nmissing is not at random and without the requirement of strong signals. Our\ndevelopment is based on the observation that if the number of missing entries\nis small enough compared to the panel size, then they can be estimated well\neven when missing is not at random. Taking advantage of this fact, we divide\nthe missing entries into smaller groups and estimate each group via nuclear\nnorm regularization. In addition, we show that with appropriate debiasing, our\nproposed estimate is asymptotically normal even for fairly weak signals. Our\nwork is motivated by recent research on the Tick Size Pilot Program, an\nexperiment conducted by the Security and Exchange Commission (SEC) to evaluate\nthe impact of widening the tick size on the market quality of stocks from 2016\nto 2018. While previous studies were based on traditional regression or\ndifference-in-difference methods by assuming that the treatment effect is\ninvariant with respect to time and unit, our analyses suggest significant\nheterogeneity across units and intriguing dynamics over time during the pilot\nprogram.",
        "authors": [
            "Jungjun Choi",
            "Ming Yuan"
        ],
        "categories": "stat.ME",
        "published": "2023-08-04T14:54:29Z",
        "updated": "2023-08-04T14:54:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.05753v1",
        "title": "Amortized neural networks for agent-based model forecasting",
        "abstract": "In this paper, we propose a new procedure for unconditional and conditional\nforecasting in agent-based models. The proposed algorithm is based on the\napplication of amortized neural networks and consists of two steps. The first\nstep simulates artificial datasets from the model. In the second step, a neural\nnetwork is trained to predict the future values of the variables using the\nhistory of observations. The main advantage of the proposed algorithm is its\nspeed. This is due to the fact that, after the training procedure, it can be\nused to yield predictions for almost any data without additional simulations or\nthe re-estimation of the neural network",
        "authors": [
            "Denis Koshelev",
            "Alexey Ponomarenko",
            "Sergei Seleznev"
        ],
        "categories": "econ.EM",
        "published": "2023-08-03T11:24:34Z",
        "updated": "2023-08-03T11:24:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.01596v1",
        "title": "A Robust Method for Microforecasting and Estimation of Random Effects",
        "abstract": "We propose a method for forecasting individual outcomes and estimating random\neffects in linear panel data models and value-added models when the panel has a\nshort time dimension. The method is robust, trivial to implement and requires\nminimal assumptions. The idea is to take a weighted average of time series- and\npooled forecasts/estimators, with individual weights that are based on time\nseries information. We show the forecast optimality of individual weights, both\nin terms of minimax-regret and of mean squared forecast error. We then provide\nfeasible weights that ensure good performance under weaker assumptions than\nthose required by existing approaches. Unlike existing shrinkage methods, our\napproach borrows the strength - but avoids the tyranny - of the majority, by\ntargeting individual (instead of group) accuracy and letting the data decide\nhow much strength each individual should borrow. Unlike existing empirical\nBayesian methods, our frequentist approach requires no distributional\nassumptions, and, in fact, it is particularly advantageous in the presence of\nfeatures such as heavy tails that would make a fully nonparametric procedure\nproblematic.",
        "authors": [
            "Raffaella Giacomini",
            "Sokbae Lee",
            "Silvia Sarpietro"
        ],
        "categories": "econ.EM",
        "published": "2023-08-03T08:01:07Z",
        "updated": "2023-08-03T08:01:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.01418v4",
        "title": "Limit Theory under Network Dependence and Nonstationarity",
        "abstract": "These lecture notes represent supplementary material for a short course on\ntime series econometrics and network econometrics. We give emphasis on limit\ntheory for time series regression models as well as the use of the\nlocal-to-unity parametrization when modeling time series nonstationarity.\nMoreover, we present various non-asymptotic theory results for moderate\ndeviation principles when considering the eigenvalues of covariance matrices as\nwell as asymptotics for unit root moderate deviations in nonstationary\nautoregressive processes. Although not all applications from the literature are\ncovered we also discuss some open problems in the time series and network\neconometrics literature.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-08-02T20:32:19Z",
        "updated": "2023-08-14T11:31:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.01198v3",
        "title": "Analyzing the Reporting Error of Public Transport Trips in the Danish National Travel Survey Using Smart Card Data",
        "abstract": "Household travel surveys have been used for decades to collect individuals\nand households' travel behavior. However, self-reported surveys are subject to\nrecall bias, as respondents might struggle to recall and report their\nactivities accurately. This study examines the time reporting error of public\ntransit users in a nationwide household travel survey by matching, at the\nindividual level, five consecutive years of data from two sources, namely the\nDanish National Travel Survey (TU) and the Danish Smart Card system\n(Rejsekort). Survey respondents are matched with travel cards from the\nRejsekort data solely based on the respondents' declared spatiotemporal travel\nbehavior. Approximately, 70% of the respondents were successfully matched with\nRejsekort travel cards. The findings reveal a median time reporting error of\n11.34 minutes, with an Interquartile Range of 28.14 minutes. Furthermore, a\nstatistical analysis was performed to explore the relationships between the\nsurvey respondents' reporting error and their socio-economic and demographic\ncharacteristics. The results indicate that females and respondents with a fixed\nschedule are in general more accurate than males and respondents with a\nflexible schedule in reporting their times of travel. Moreover, trips reported\nduring weekdays or via the internet displayed higher accuracies compared to\ntrips reported during weekends and holidays or via telephone interviews. This\ndisaggregated analysis provides valuable insights that could help in improving\nthe design and analysis of travel surveys, as well accounting for reporting\nerrors/biases in travel survey-based applications. Furthermore, it offers\nvaluable insights underlying the psychology of travel recall by survey\nrespondents.",
        "authors": [
            "Georges Sfeir",
            "Filipe Rodrigues",
            "Maya Abou Zeid",
            "Francisco Camara Pereira"
        ],
        "categories": "stat.AP",
        "published": "2023-08-02T15:07:25Z",
        "updated": "2024-07-01T15:47:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.00913v2",
        "title": "The Bayesian Context Trees State Space Model for time series modelling and forecasting",
        "abstract": "A hierarchical Bayesian framework is introduced for developing rich mixture\nmodels for real-valued time series, partly motivated by important applications\nin financial time series analysis. At the top level, meaningful discrete states\nare identified as appropriately quantised values of some of the most recent\nsamples. These observable states are described as a discrete context-tree\nmodel. At the bottom level, a different, arbitrary model for real-valued time\nseries -- a base model -- is associated with each state. This defines a very\ngeneral framework that can be used in conjunction with any existing model class\nto build flexible and interpretable mixture models. We call this the Bayesian\nContext Trees State Space Model, or the BCT-X framework. Efficient algorithms\nare introduced that allow for effective, exact Bayesian inference and learning\nin this setting; in particular, the maximum a posteriori probability (MAP)\ncontext-tree model can be identified. These algorithms can be updated\nsequentially, facilitating efficient online forecasting. The utility of the\ngeneral framework is illustrated in two particular instances: When\nautoregressive (AR) models are used as base models, resulting in a nonlinear AR\nmixture model, and when conditional heteroscedastic (ARCH) models are used,\nresulting in a mixture model that offers a powerful and systematic way of\nmodelling the well-known volatility asymmetries in financial data. In\nforecasting, the BCT-X methods are found to outperform state-of-the-art\ntechniques on simulated and real-world data, both in terms of accuracy and\ncomputational requirements. In modelling, the BCT-X structure finds natural\nstructure present in the data. In particular, the BCT-ARCH model reveals a\nnovel, important feature of stock market index data, in the form of an enhanced\nleverage effect.",
        "authors": [
            "Ioannis Papageorgiou",
            "Ioannis Kontoyiannis"
        ],
        "categories": "stat.ME",
        "published": "2023-08-02T02:40:42Z",
        "updated": "2023-10-10T23:46:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.00444v1",
        "title": "Testing for Threshold Effects in Presence of Heteroskedasticity and Measurement Error with an application to Italian Strikes",
        "abstract": "Many macroeconomic time series are characterised by nonlinearity both in the\nconditional mean and in the conditional variance and, in practice, it is\nimportant to investigate separately these two aspects. Here we address the\nissue of testing for threshold nonlinearity in the conditional mean, in the\npresence of conditional heteroskedasticity. We propose a supremum Lagrange\nMultiplier approach to test a linear ARMA-GARCH model against the alternative\nof a TARMA-GARCH model. We derive the asymptotic null distribution of the test\nstatistic and this requires novel results since the difficulties of working\nwith nuisance parameters, absent under the null hypothesis, are amplified by\nthe non-linear moving average, combined with GARCH-type innovations. We show\nthat tests that do not account for heteroskedasticity fail to achieve the\ncorrect size even for large sample sizes. Moreover, we show that the TARMA\nspecification naturally accounts for the ubiquitous presence of measurement\nerror that affects macroeconomic data. We apply the results to analyse the time\nseries of Italian strikes and we show that the TARMA-GARCH specification is\nconsistent with the relevant macroeconomic theory while capturing the main\nfeatures of the Italian strikes dynamics, such as asymmetric cycles and\nregime-switching.",
        "authors": [
            "Francesco Angelini",
            "Massimiliano Castellani",
            "Simone Giannerini",
            "Greta Goracci"
        ],
        "categories": "econ.EM",
        "published": "2023-08-01T10:43:28Z",
        "updated": "2023-08-01T10:43:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.00202v2",
        "title": "Randomization Inference of Heterogeneous Treatment Effects under Network Interference",
        "abstract": "We design randomization tests of heterogeneous treatment effects when units\ninteract on a single connected network. Our modeling strategy allows network\ninterference into the potential outcomes framework using the concept of\nexposure mapping. We consider several null hypotheses representing different\nnotions of homogeneous treatment effects. However, these hypotheses are not\nsharp due to nuisance parameters and multiple potential outcomes. To address\nthe issue of multiple potential outcomes, we propose a conditional\nrandomization method that expands on existing procedures. Our conditioning\napproach permits the use of treatment assignment as a conditioning variable,\nwidening the range of application of the randomization method of inference. In\naddition, we propose techniques that overcome the nuisance parameter issue. We\nshow that our resulting testing methods based on the conditioning procedure and\nthe strategies for handling nuisance parameters are asymptotically valid. We\ndemonstrate the testing methods using a network data set and also present the\nfindings of a Monte Carlo study.",
        "authors": [
            "Julius Owusu"
        ],
        "categories": "econ.EM",
        "published": "2023-07-31T23:48:36Z",
        "updated": "2024-01-12T15:57:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.00167v3",
        "title": "What's Logs Got to do With it: On the Perils of log Dependent Variables and Difference-in-Differences",
        "abstract": "The log transformation of the dependent variable is not innocuous when using\na difference-in-differences (DD) model. With a dependent variable in logs, the\nDD term captures an approximation of the proportional difference in growth\nrates across groups. As I show with both simulations and two empirical\nexamples, if the baseline outcome distributions are sufficiently different\nacross groups, the DD parameter for a log-specification can be different in\nsign to that of a levels-specification. I provide a condition, based on (i) the\naggregate time effect, and (ii) the difference in relative baseline outcome\nmeans, for when the sign-switch will occur.",
        "authors": [
            "Brendon McConnell"
        ],
        "categories": "econ.EM",
        "published": "2023-07-31T21:51:04Z",
        "updated": "2023-08-07T12:23:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2308.00014v3",
        "title": "A new mapping of technological interdependence",
        "abstract": "How does technological interdependence affect innovation? We address this\nquestion by examining the influence of neighbors' innovativeness and the\nstructure of the innovators' network on a sector's capacity to develop new\ntechnologies. We study these two dimensions of technological interdependence by\napplying novel methods of text mining and network analysis to the documents of\n6.5 million patents granted by the United States Patent and Trademark Office\n(USPTO) between 1976 and 2021. We find that, in the long run, the influence of\nnetwork linkages is as important as that of neighbor innovativeness. In the\nshort run, however, positive shocks to neighbor innovativeness yield relatively\nrapid effects, while the impact of shocks strengthening network linkages\nmanifests with delay, even though lasts longer. Our analysis also highlights\nthat patent text contains a wealth of information often not captured by\ntraditional innovation metrics, such as patent citations.",
        "authors": [
            "A. Fronzetti Colladon",
            "B. Guardabascio",
            "F. Venturini"
        ],
        "categories": "econ.EM",
        "published": "2023-07-31T07:37:37Z",
        "updated": "2024-09-16T18:20:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.16427v1",
        "title": "Causal Inference for Banking Finance and Insurance A Survey",
        "abstract": "Causal Inference plays an significant role in explaining the decisions taken\nby statistical models and artificial intelligence models. Of late, this field\nstarted attracting the attention of researchers and practitioners alike. This\npaper presents a comprehensive survey of 37 papers published during 1992-2023\nand concerning the application of causal inference to banking, finance, and\ninsurance. The papers are categorized according to the following families of\ndomains: (i) Banking, (ii) Finance and its subdomains such as corporate\nfinance, governance finance including financial risk and financial policy,\nfinancial economics, and Behavioral finance, and (iii) Insurance. Further, the\npaper covers the primary ingredients of causal inference namely, statistical\nmethods such as Bayesian Causal Network, Granger Causality and jargon used\nthereof such as counterfactuals. The review also recommends some important\ndirections for future research. In conclusion, we observed that the application\nof causal inference in the banking and insurance sectors is still in its\ninfancy, and thus more research is possible to turn it into a viable method.",
        "authors": [
            "Satyam Kumar",
            "Yelleti Vivek",
            "Vadlamani Ravi",
            "Indranil Bose"
        ],
        "categories": "cs.AI",
        "published": "2023-07-31T06:22:34Z",
        "updated": "2023-07-31T06:22:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.16370v1",
        "title": "Inference for Low-rank Completion without Sample Splitting with Application to Treatment Effect Estimation",
        "abstract": "This paper studies the inferential theory for estimating low-rank matrices.\nIt also provides an inference method for the average treatment effect as an\napplication. We show that the least square estimation of eigenvectors following\nthe nuclear norm penalization attains the asymptotic normality. The key\ncontribution of our method is that it does not require sample splitting. In\naddition, this paper allows dependent observation patterns and heterogeneous\nobservation probabilities. Empirically, we apply the proposed procedure to\nestimating the impact of the presidential vote on allocating the U.S. federal\nbudget to the states.",
        "authors": [
            "Jungjun Choi",
            "Hyukjun Kwon",
            "Yuan Liao"
        ],
        "categories": "econ.EM",
        "published": "2023-07-31T02:26:52Z",
        "updated": "2023-07-31T02:26:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.16315v1",
        "title": "Towards Practical Robustness Auditing for Linear Regression",
        "abstract": "We investigate practical algorithms to find or disprove the existence of\nsmall subsets of a dataset which, when removed, reverse the sign of a\ncoefficient in an ordinary least squares regression involving that dataset. We\nempirically study the performance of well-established algorithmic techniques\nfor this task -- mixed integer quadratically constrained optimization for\ngeneral linear regression problems and exact greedy methods for special cases.\nWe show that these methods largely outperform the state of the art and provide\na useful robustness check for regression problems in a few dimensions. However,\nsignificant computational bottlenecks remain, especially for the important task\nof disproving the existence of such small sets of influential samples for\nregression problems of dimension $3$ or greater. We make some headway on this\nchallenge via a spectral algorithm using ideas drawn from recent innovations in\nalgorithmic robust statistics. We summarize the limitations of known techniques\nin several challenge datasets to encourage further algorithmic innovation.",
        "authors": [
            "Daniel Freund",
            "Samuel B. Hopkins"
        ],
        "categories": "stat.ME",
        "published": "2023-07-30T20:47:36Z",
        "updated": "2023-07-30T20:47:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.15863v1",
        "title": "Panel Data Models with Time-Varying Latent Group Structures",
        "abstract": "This paper considers a linear panel model with interactive fixed effects and\nunobserved individual and time heterogeneities that are captured by some latent\ngroup structures and an unknown structural break, respectively. To enhance\nrealism the model may have different numbers of groups and/or different group\nmemberships before and after the break. With the preliminary\nnuclear-norm-regularized estimation followed by row- and column-wise linear\nregressions, we estimate the break point based on the idea of binary\nsegmentation and the latent group structures together with the number of groups\nbefore and after the break by sequential testing K-means algorithm\nsimultaneously. It is shown that the break point, the number of groups and the\ngroup memberships can each be estimated correctly with probability approaching\none. Asymptotic distributions of the estimators of the slope coefficients are\nestablished. Monte Carlo simulations demonstrate excellent finite sample\nperformance for the proposed estimation algorithm. An empirical application to\nreal house price data across 377 Metropolitan Statistical Areas in the US from\n1975 to 2014 suggests the presence both of structural breaks and of changes in\ngroup membership.",
        "authors": [
            "Yiren Wang",
            "Peter C B Phillips",
            "Liangjun Su"
        ],
        "categories": "econ.EM",
        "published": "2023-07-29T01:58:59Z",
        "updated": "2023-07-29T01:58:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.15313v1",
        "title": "Group-Heterogeneous Changes-in-Changes and Distributional Synthetic Controls",
        "abstract": "We develop new methods for changes-in-changes and distributional synthetic\ncontrols when there exists group level heterogeneity. For changes-in-changes,\nwe allow individuals to belong to a large number of heterogeneous groups. The\nnew method extends the changes-in-changes method in Athey and Imbens (2006) by\nfinding appropriate subgroups within the control groups which share similar\ngroup level unobserved characteristics to the treatment groups. For\ndistributional synthetic control, we show that the appropriate synthetic\ncontrol needs to be constructed using units in potentially different time\nperiods in which they have comparable group level heterogeneity to the\ntreatment group, instead of units that are only in the same time period as in\nGunsilius (2023). Implementation and data requirements for these new methods\nare briefly discussed.",
        "authors": [
            "Songnian Chen",
            "Junlong Feng"
        ],
        "categories": "econ.EM",
        "published": "2023-07-28T05:23:43Z",
        "updated": "2023-07-28T05:23:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.15181v4",
        "title": "On the Efficiency of Finely Stratified Experiments",
        "abstract": "This paper studies the use of finely stratified designs for the efficient\nestimation of a large class of treatment effect parameters that arise in the\nanalysis of experiments. By a \"finely stratified\" design, we mean experiments\nin which units are divided into groups of a fixed size and a proportion within\neach group is assigned to a binary treatment uniformly at random. The class of\nparameters considered are those that can be expressed as the solution to a set\nof moment conditions constructed using a known function of the observed data.\nThey include, among other things, average treatment effects, quantile treatment\neffects, and local average treatment effects as well as the counterparts to\nthese quantities in experiments in which the unit is itself a cluster. In this\nsetting, we establish two results. First, we show that under a finely\nstratified design, the na\\\"ive method of moments estimator achieves the same\nasymptotic variance as what could typically be attained under alternative\ntreatment assignment schemes only through ex post covariate adjustment. Second,\nwe argue that in fact the na\\\"ive method of moments estimator under a finely\nstratified design is asymptotically efficient by deriving a lower bound on the\nasymptotic variance of \"regular\" estimators of the parameter of interest in the\nform of a convolution theorem. This result accommodates a large class of\npossible treatment assignment schemes that are used routinely throughout the\nsciences, such as stratified block randomization and matched pairs. In this\nsense, \"finely stratified\" experiments are attractive because they lead to\nefficient estimators of treatment effect parameters \"by design\" rather than\nthrough ex post covariate adjustment and thereby remain \"hands above the\ntable.\"",
        "authors": [
            "Yuehao Bai",
            "Jizhou Liu",
            "Azeem M. Shaikh",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2023-07-27T20:20:09Z",
        "updated": "2024-08-23T18:51:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.15151v1",
        "title": "Predictability Tests Robust against Parameter Instability",
        "abstract": "We consider Wald type statistics designed for joint predictability and\nstructural break testing based on the instrumentation method of Phillips and\nMagdalinos (2009). We show that under the assumption of nonstationary\npredictors: (i) the tests based on the OLS estimators converge to a nonstandard\nlimiting distribution which depends on the nuisance coefficient of persistence;\nand (ii) the tests based on the IVX estimators can filter out the persistence\nunder certain parameter restrictions due to the supremum functional. These\nresults contribute to the literature of joint predictability and parameter\ninstability testing by providing analytical tractable asymptotic theory when\ntaking into account nonstationary regressors. We compare the finite-sample size\nand power performance of the Wald tests under both estimators via extensive\nMonte Carlo experiments. Critical values are computed using standard bootstrap\ninference methodologies. We illustrate the usefulness of the proposed framework\nto test for predictability under the presence of parameter instability by\nexamining the stock market predictability puzzle for the US equity premium.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-07-27T18:56:53Z",
        "updated": "2023-07-27T18:56:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.14867v4",
        "title": "One-step smoothing splines instrumental regression",
        "abstract": "We extend nonparametric regression smoothing splines to a context where there\nis endogeneity and instrumental variables are available. Unlike popular\nexisting estimators, the resulting estimator is one-step and relies on a unique\nregularization parameter. We derive rates of the convergence for the estimator\nand its first derivative, which are uniform in the support of the endogenous\nvariable. We also address the issue of imposing monotonicity in estimation and\nextend the approach to a partly linear model. Simulations confirm the good\nperformances of our estimator compared to two-step procedures. Our method\nyields economically sensible results when used to estimate Engel curves.",
        "authors": [
            "Jad Beyhum",
            "Elia Lapenta",
            "Pascal Lavergne"
        ],
        "categories": "econ.EM",
        "published": "2023-07-27T13:53:34Z",
        "updated": "2024-12-09T17:48:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.14499v1",
        "title": "Weak (Proxy) Factors Robust Hansen-Jagannathan Distance For Linear Asset Pricing Models",
        "abstract": "The Hansen-Jagannathan (HJ) distance statistic is one of the most dominant\nmeasures of model misspecification. However, the conventional HJ specification\ntest procedure has poor finite sample performance, and we show that it can be\nsize distorted even in large samples when (proxy) factors exhibit small\ncorrelations with asset returns. In other words, applied researchers are likely\nto falsely reject a model even when it is correctly specified. We provide two\nalternatives for the HJ statistic and two corresponding novel procedures for\nmodel specification tests, which are robust against the presence of weak\n(proxy) factors, and we also offer a novel robust risk premia estimator.\nSimulation exercises support our theory. Our empirical application documents\nthe non-reliability of the traditional HJ test since it may produce\ncounter-intuitive results when comparing nested models by rejecting a\nfour-factor model but not the reduced three-factor model. At the same time, our\nproposed methods are practically more appealing and show support for a\nfour-factor model for Fama French portfolios.",
        "authors": [
            "Lingwei Kong"
        ],
        "categories": "econ.EM",
        "published": "2023-07-26T20:58:12Z",
        "updated": "2023-07-26T20:58:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.14463v1",
        "title": "Bootstrapping Nonstationary Autoregressive Processes with Predictive Regression Models",
        "abstract": "We establish the asymptotic validity of the bootstrap-based IVX estimator\nproposed by Phillips and Magdalinos (2009) for the predictive regression model\nparameter based on a local-to-unity specification of the autoregressive\ncoefficient which covers both nearly nonstationary and nearly stationary\nprocesses. A mixed Gaussian limit distribution is obtained for the\nbootstrap-based IVX estimator. The statistical validity of the theoretical\nresults are illustrated by Monte Carlo experiments for various statistical\ninference problems.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-07-26T19:10:12Z",
        "updated": "2023-07-26T19:10:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.14282v2",
        "title": "Causal Effects in Matching Mechanisms with Strategically Reported Preferences",
        "abstract": "A growing number of central authorities use assignment mechanisms to allocate\nstudents to schools in a way that reflects student preferences and school\npriorities. However, most real-world mechanisms incentivize students to\nstrategically misreport their preferences. In this paper, we provide an\napproach for identifying the causal effects of school assignment on future\noutcomes that accounts for strategic misreporting. Misreporting may invalidate\nexisting point-identification approaches, and we derive sharp bounds for causal\neffects that are robust to strategic behavior. Our approach applies to any\nmechanism as long as there exist placement scores and cutoffs that characterize\nthat mechanism's allocation rule. We use data from a deferred acceptance\nmechanism that assigns students to more than 1,000 university-major\ncombinations in Chile. Matching theory predicts that students' behavior in\nChile should be strategic because they can list only up to eight options, and\nwe find empirical evidence consistent with such behavior. Our bounds are\ninformative enough to reveal significant heterogeneity in graduation success\nwith respect to preferences and school assignment.",
        "authors": [
            "Marinho Bertanha",
            "Margaux Luflade",
            "Ismael Mourifi\u00e9"
        ],
        "categories": "econ.EM",
        "published": "2023-07-26T16:35:42Z",
        "updated": "2024-05-02T22:16:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.14203v1",
        "title": "Dynamic Regression Discontinuity: A Within-Design Approach",
        "abstract": "I propose a novel argument to point identify economically interpretable\nintertemporal treatment effects in dynamic regression discontinuity designs\n(RDDs). Specifically, I develop a dynamic potential outcomes model and\nspecialize two assumptions of the difference-in-differences literature, the no\nanticipation and common trends restrictions, to point identify cutoff-specific\nimpulse responses. The estimand associated with each target parameter can be\nexpressed as the sum of two static RDD outcome contrasts, thereby allowing for\nestimation via standard local polynomial tools. I leverage a limited path\nindependence assumption to reduce the dimensionality of the problem.",
        "authors": [
            "Francesco Ruggieri"
        ],
        "categories": "econ.EM",
        "published": "2023-07-26T14:02:24Z",
        "updated": "2023-07-26T14:02:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.13966v1",
        "title": "Using Probabilistic Stated Preference Analyses to Understand Actual Choices",
        "abstract": "Can stated preferences help in counterfactual analyses of actual choice? This\nresearch proposes a novel approach to researchers who have access to both\nstated choices in hypothetical scenarios and actual choices. The key idea is to\nuse probabilistic stated choices to identify the distribution of individual\nunobserved heterogeneity, even in the presence of measurement error. If this\nunobserved heterogeneity is the source of endogeneity, the researcher can\ncorrect for its influence in a demand function estimation using actual choices,\nand recover causal effects. Estimation is possible with an off-the-shelf Group\nFixed Effects estimator.",
        "authors": [
            "Romuald Meango"
        ],
        "categories": "econ.EM",
        "published": "2023-07-26T05:56:54Z",
        "updated": "2023-07-26T05:56:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.13849v1",
        "title": "The Core of Bayesian Persuasion",
        "abstract": "An analyst observes the frequency with which an agent takes actions, but not\nthe frequency with which she takes actions conditional on a payoff relevant\nstate. In this setting, we ask when the analyst can rationalize the agent's\nchoices as the outcome of the agent learning something about the state before\ntaking action. Our characterization marries the obedience approach in\ninformation design (Bergemann and Morris, 2016) and the belief approach in\nBayesian persuasion (Kamenica and Gentzkow, 2011) relying on a theorem by\nStrassen (1965) and Hall's marriage theorem. We apply our results to\nring-network games and to identify conditions under which a data set is\nconsistent with a public information structure in first-order Bayesian\npersuasion games.",
        "authors": [
            "Laura Doval",
            "Ran Eilat"
        ],
        "categories": "econ.TH",
        "published": "2023-07-25T22:48:55Z",
        "updated": "2023-07-25T22:48:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.13793v1",
        "title": "Source Condition Double Robust Inference on Functionals of Inverse Problems",
        "abstract": "We consider estimation of parameters defined as linear functionals of\nsolutions to linear inverse problems. Any such parameter admits a doubly robust\nrepresentation that depends on the solution to a dual linear inverse problem,\nwhere the dual solution can be thought as a generalization of the inverse\npropensity function. We provide the first source condition double robust\ninference method that ensures asymptotic normality around the parameter of\ninterest as long as either the primal or the dual inverse problem is\nsufficiently well-posed, without knowledge of which inverse problem is the more\nwell-posed one. Our result is enabled by novel guarantees for iterated Tikhonov\nregularized adversarial estimators for linear inverse problems, over general\nhypothesis spaces, which are developments of independent interest.",
        "authors": [
            "Andrew Bennett",
            "Nathan Kallus",
            "Xiaojie Mao",
            "Whitney Newey",
            "Vasilis Syrgkanis",
            "Masatoshi Uehara"
        ],
        "categories": "stat.ME",
        "published": "2023-07-25T19:54:46Z",
        "updated": "2023-07-25T19:54:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.13686v3",
        "title": "Characteristics and Predictive Modeling of Short-term Impacts of Hurricanes on the US Employment",
        "abstract": "The physical and economic damages of hurricanes can acutely affect employment\nand the well-being of employees. However, a comprehensive understanding of\nthese impacts remains elusive as many studies focused on narrow subsets of\nregions or hurricanes. Here we present an open-source dataset that serves\ninterdisciplinary research on hurricane impacts on US employment. Compared to\npast domain-specific efforts, this dataset has greater spatial-temporal\ngranularity and variable coverage. To demonstrate potential applications of\nthis dataset, we focus on the short-term employment disruptions related to\nhurricanes during 1990-2020. The observed county-level employment changes in\nthe initial month are small on average, though large employment losses (>30%)\ncan occur after extreme storms. The overall small changes partly result from\ncompensation among different employment sectors, which may obscure large,\nconcentrated employment losses after hurricanes. Additional econometric\nanalyses concur on the post-storm employment losses in hospitality and leisure\nbut disagree on employment changes in the other industries. The dataset also\nenables data-driven analyses that highlight vulnerabilities such as pronounced\nemployment losses related to Puerto Rico and rainy hurricanes. Furthermore,\npredictive modeling of short-term employment changes shows promising\nperformance for service-providing industries and high-impact storms. In the\nexamined cases, the nonlinear Random Forests model greatly outperforms the\nmultiple linear regression model. The nonlinear model also suggests that more\nsevere hurricane hazards projected by physical models may cause more extreme\nlosses in US service-providing employment. Finally, we share our dataset and\nanalytical code to facilitate the study and modeling of hurricane impacts in a\nchanging climate.",
        "authors": [
            "Gan Zhang",
            "Wenjun Zhu"
        ],
        "categories": "econ.EM",
        "published": "2023-07-25T17:46:57Z",
        "updated": "2024-05-25T03:45:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.14378v1",
        "title": "Smoothing of numerical series by the triangle method on the example of hungarian gdp data 1992-2022 based on approximation by series of exponents",
        "abstract": "In practice , quite often there is a need to describe the values set by means\nof a table in the form of some functional dependence . The observed values ,\ndue to certain circumstances , have an error . For approximation, it is\nadvisable to use a functional dependence that would allow smoothing out the\nerrors of the observation results. Approximation allows you to determine\nintermediate values of functions that are not listed among the data in the\nobservation table. The use of exponential series for data approximation allows\nyou to get a result no worse than from approximation by polynomials In the\neconomic scientific literature, approximation in the form of power functions,\nfor example, the Cobb-Douglas function, has become widespread. The advantage of\nthis type of approximation can be called a simple type of approximating\nfunction , and the disadvantage is that in nature not all processes can be\ndescribed by power functions with a given accuracy. An example is the GDP\nindicator for several decades . For this case , it is difficult to find a power\nfunction approximating a numerical series . But in this case, as shown in this\narticle, you can use exponential series to approximate the data. In this paper,\nthe time series of Hungary's GDP in the period from 1992 to 2022 was\napproximated by a series of thirty exponents of a complex variable. The use of\ndata smoothing by the method of triangles allows you to average the data and\nincrease the accuracy of approximation . This is of practical importance if the\nobserved random variable contains outliers that need to be smoothed out.",
        "authors": [
            "Yekimov Sergey"
        ],
        "categories": "econ.EM",
        "published": "2023-07-25T17:35:21Z",
        "updated": "2023-07-25T17:35:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.13475v1",
        "title": "Large sample properties of GMM estimators under second-order identification",
        "abstract": "Dovonon and Hall (Journal of Econometrics, 2018) proposed a limiting\ndistribution theory for GMM estimators for a p - dimensional globally\nidentified parameter vector {\\phi} when local identification conditions fail at\nfirst-order but hold at second-order. They assumed that the first-order\nunderidentification is due to the expected Jacobian having rank p-1 at the true\nvalue {\\phi}_{0}, i.e., having a rank deficiency of one. After reparametrizing\nthe model such that the last column of the Jacobian vanishes, they showed that\nthe GMM estimator of the first p-1 parameters converges at rate T^{-1/2} and\nthe GMM estimator of the remaining parameter, {\\phi}_{p}, converges at rate\nT^{-1/4}. They also provided a limiting distribution of\nT^{1/4}({\\phi}_{p}-{\\phi}_{0,p}) subject to a (non-transparent) condition which\nthey claimed to be not restrictive in general. However, as we show in this\npaper, their condition is in fact only satisfied when {\\phi} is overidentified\nand the limiting distribution of T^{1/4}({\\phi}_{p}-{\\phi}_{0,p}), which is\nnon-standard, depends on whether {\\phi} is exactly identified or\noveridentified. In particular, the limiting distributions of the sign of\nT^{1/4}({\\phi}_{p}-{\\phi}_{0,p}) for the cases of exact and overidentification,\nrespectively, are different and are obtained by using expansions of the GMM\nobjective function of different orders. Unsurprisingly, we find that the\nlimiting distribution theories of Dovonon and Hall (2018) for Indirect\nInference (II) estimation under two different scenarios with second-order\nidentification where the target function is a GMM estimator of the auxiliary\nparameter vector, are incomplete for similar reasons. We discuss how our\nresults for GMM estimation can be used to complete both theories and how they\ncan be used to obtain the limiting distributions of the II estimators in the\ncase of exact identification under either scenario.",
        "authors": [
            "Hugo Kruiniger"
        ],
        "categories": "econ.EM",
        "published": "2023-07-25T13:08:32Z",
        "updated": "2023-07-25T13:08:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.13364v4",
        "title": "Testing for sparse idiosyncratic components in factor-augmented regression models",
        "abstract": "We propose a novel bootstrap test of a dense model, namely factor regression,\nagainst a sparse plus dense alternative augmenting model with sparse\nidiosyncratic components. The asymptotic properties of the test are established\nunder time series dependence and polynomial tails. We outline a data-driven\nrule to select the tuning parameter and prove its theoretical validity. In\nsimulation experiments, our procedure exhibits high power against sparse\nalternatives and low power against dense deviations from the null. Moreover, we\napply our test to various datasets in macroeconomics and finance and often\nreject the null. This suggests the presence of sparsity -- on top of a dense\nmodel -- in commonly studied economic applications. The R package FAS\nimplements our approach.",
        "authors": [
            "Jad Beyhum",
            "Jonas Striaukas"
        ],
        "categories": "econ.EM",
        "published": "2023-07-25T09:33:36Z",
        "updated": "2024-07-10T14:35:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.13094v2",
        "title": "Inference in Experiments with Matched Pairs and Imperfect Compliance",
        "abstract": "This paper studies inference for the local average treatment effect in\nrandomized controlled trials with imperfect compliance where treatment status\nis determined according to \"matched pairs.\" By \"matched pairs,\" we mean that\nunits are sampled i.i.d. from the population of interest, paired according to\nobserved, baseline covariates and finally, within each pair, one unit is\nselected at random for treatment. Under weak assumptions governing the quality\nof the pairings, we first derive the limit distribution of the usual Wald\n(i.e., two-stage least squares) estimator of the local average treatment\neffect. We show further that conventional heteroskedasticity-robust estimators\nof the Wald estimator's limiting variance are generally conservative, in that\ntheir probability limits are (typically strictly) larger than the limiting\nvariance. We therefore provide an alternative estimator of the limiting\nvariance that is consistent. Finally, we consider the use of additional\nobserved, baseline covariates not used in pairing units to increase the\nprecision with which we can estimate the local average treatment effect. To\nthis end, we derive the limiting behavior of a two-stage least squares\nestimator of the local average treatment effect which includes both the\nadditional covariates in addition to pair fixed effects, and show that its\nlimiting variance is always less than or equal to that of the Wald estimator.\nTo complete our analysis, we provide a consistent estimator of this limiting\nvariance. A simulation study confirms the practical relevance of our\ntheoretical results. Finally, we apply our results to revisit a prominent\nexperiment studying the effect of macroinsurance on microenterprise in Egypt.",
        "authors": [
            "Yuehao Bai",
            "Hongchang Guo",
            "Azeem M. Shaikh",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2023-07-24T19:28:31Z",
        "updated": "2024-06-26T19:43:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.12628v1",
        "title": "Identification Robust Inference for the Risk Premium in Term Structure Models",
        "abstract": "We propose identification robust statistics for testing hypotheses on the\nrisk premia in dynamic affine term structure models. We do so using the moment\nequation specification proposed for these models in Adrian et al. (2013). We\nextend the subset (factor) Anderson-Rubin test from Guggenberger et al. (2012)\nto models with multiple dynamic factors and time-varying risk prices. Unlike\nprojection-based tests, it provides a computationally tractable manner to\nconduct identification robust tests on a larger number of parameters. We\nanalyze the potential identification issues arising in empirical studies.\nStatistical inference based on the three-stage estimator from Adrian et al.\n(2013) requires knowledge of the factors' quality and is misleading without\nfull-rank beta's or with sampling errors of comparable size as the loadings.\nEmpirical applications show that some factors, though potentially weak, may\ndrive the time variation of risk prices, and weak identification issues are\nmore prominent in multi-factor models.",
        "authors": [
            "Frank Kleibergen",
            "Lingwei Kong"
        ],
        "categories": "econ.EM",
        "published": "2023-07-24T09:00:21Z",
        "updated": "2023-07-24T09:00:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.11857v1",
        "title": "Scenario Sampling for Large Supermodular Games",
        "abstract": "This paper introduces a simulation algorithm for evaluating the\nlog-likelihood function of a large supermodular binary-action game. Covered\nexamples include (certain types of) peer effect, technology adoption, strategic\nnetwork formation, and multi-market entry games. More generally, the algorithm\nfacilitates simulated maximum likelihood (SML) estimation of games with large\nnumbers of players, $T$, and/or many binary actions per player, $M$ (e.g.,\ngames with tens of thousands of strategic actions, $TM=O(10^4)$). In such cases\nthe likelihood of the observed pure strategy combination is typically (i) very\nsmall and (ii) a $TM$-fold integral who region of integration has a complicated\ngeometry. Direct numerical integration, as well as accept-reject Monte Carlo\nintegration, are computationally impractical in such settings. In contrast, we\nintroduce a novel importance sampling algorithm which allows for accurate\nlikelihood simulation with modest numbers of simulation draws.",
        "authors": [
            "Bryan S. Graham",
            "Andrin Pelican"
        ],
        "categories": "econ.EM",
        "published": "2023-07-21T18:51:32Z",
        "updated": "2023-07-21T18:51:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.11484v1",
        "title": "Functional Differencing in Networks",
        "abstract": "Economic interactions often occur in networks where heterogeneous agents\n(such as workers or firms) sort and produce. However, most existing estimation\napproaches either require the network to be dense, which is at odds with many\nempirical networks, or they require restricting the form of heterogeneity and\nthe network formation process. We show how the functional differencing approach\nintroduced by Bonhomme (2012) in the context of panel data, can be applied in\nnetwork settings to derive moment restrictions on model parameters and average\neffects. Those restrictions are valid irrespective of the form of\nheterogeneity, and they hold in both dense and sparse networks. We illustrate\nthe analysis with linear and nonlinear models of matched employer-employee\ndata, in the spirit of the model introduced by Abowd, Kramarz, and Margolis\n(1999).",
        "authors": [
            "St\u00e9phane Bonhomme",
            "Kevin Dano"
        ],
        "categories": "econ.EM",
        "published": "2023-07-21T10:37:49Z",
        "updated": "2023-07-21T10:37:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.11127v3",
        "title": "Asymptotically Unbiased Synthetic Control Methods by Distribution Matching",
        "abstract": "Synthetic Control Methods (SCMs) have become an essential tool for\ncomparative case studies. The fundamental idea of SCMs is to estimate the\ncounterfactual outcomes of a treated unit using a weighted sum of the observed\noutcomes of untreated units. The accuracy of the synthetic control (SC) is\ncritical for evaluating the treatment effect of a policy intervention;\ntherefore, the estimation of SC weights has been the focus of extensive\nresearch. In this study, we first point out that existing SCMs suffer from an\nendogeneity problem, the correlation between the outcomes of untreated units\nand the error term of the synthetic control, which yields a bias in the\ntreatment effect estimator. We then propose a novel SCM based on density\nmatching, assuming that the density of outcomes of the treated unit can be\napproximated by a weighted average of the joint density of untreated units\n(i.e., a mixture model). Based on this assumption, we estimate SC weights by\nmatching the moments of treated outcomes with the weighted sum of moments of\nuntreated outcomes. Our proposed method has three advantages over existing\nmethods: first, our estimator is asymptotically unbiased under the assumption\nof the mixture model; second, due to the asymptotic unbiasedness, we can reduce\nthe mean squared error in counterfactual predictions; third, our method\ngenerates full densities of the treatment effect, not merely expected values,\nwhich broadens the applicability of SCMs. We provide experimental results to\ndemonstrate the effectiveness of our proposed method.",
        "authors": [
            "Masahiro Kato",
            "Akari Ohda",
            "Masaaki Imaizumi"
        ],
        "categories": "econ.EM",
        "published": "2023-07-20T15:52:22Z",
        "updated": "2024-05-15T04:05:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.10872v1",
        "title": "Real-Time Detection of Local No-Arbitrage Violations",
        "abstract": "This paper focuses on the task of detecting local episodes involving\nviolation of the standard It\\^o semimartingale assumption for financial asset\nprices in real time that might induce arbitrage opportunities. Our proposed\ndetectors, defined as stopping rules, are applied sequentially to continually\nincoming high-frequency data. We show that they are asymptotically\nexponentially distributed in the absence of Ito semimartingale violations. On\nthe other hand, when a violation occurs, we can achieve immediate detection\nunder infill asymptotics. A Monte Carlo study demonstrates that the asymptotic\nresults provide a good approximation to the finite-sample behavior of the\nsequential detectors. An empirical application to S&P 500 index futures data\ncorroborates the effectiveness of our detectors in swiftly identifying the\nemergence of an extreme return persistence episode in real time.",
        "authors": [
            "Torben G. Andersen",
            "Viktor Todorov",
            "Bo Zhou"
        ],
        "categories": "econ.EM",
        "published": "2023-07-20T13:42:52Z",
        "updated": "2023-07-20T13:42:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.10694v2",
        "title": "PySDTest: a Python/Stata Package for Stochastic Dominance Tests",
        "abstract": "We introduce PySDTest, a Python/Stata package for statistical tests of\nstochastic dominance. PySDTest implements various testing procedures such as\nBarrett and Donald (2003), Linton et al. (2005), Linton et al. (2010), and\nDonald and Hsu (2016), along with their extensions. Users can flexibly combine\nseveral resampling methods and test statistics, including the numerical delta\nmethod (D\\\"umbgen, 1993; Hong and Li, 2018; Fang and Santos, 2019). The package\nallows for testing advanced hypotheses on stochastic dominance relations, such\nas stochastic maximality among multiple prospects. We first provide an overview\nof the concepts of stochastic dominance and testing methods. Then, we offer\npractical guidance for using the package and the Stata command pysdtest. We\napply PySDTest to investigate the portfolio choice problem between the daily\nreturns of Bitcoin and the S&P 500 index as an empirical illustration. Our\nfindings indicate that the S&P 500 index returns second-order stochastically\ndominate the Bitcoin returns.",
        "authors": [
            "Kyungho Lee",
            "Yoon-Jae Whang"
        ],
        "categories": "econ.EM",
        "published": "2023-07-20T08:37:20Z",
        "updated": "2024-08-05T21:46:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.10454v2",
        "title": "Latent Gaussian dynamic factor modeling and forecasting for multivariate count time series",
        "abstract": "This work considers estimation and forecasting in a multivariate, possibly\nhigh-dimensional count time series model constructed from a transformation of a\nlatent Gaussian dynamic factor series. The estimation of the latent model\nparameters is based on second-order properties of the count and underlying\nGaussian time series, yielding estimators of the underlying covariance matrices\nfor which standard principal component analysis applies. Theoretical\nconsistency results are established for the proposed estimation, building on\ncertain concentration results for the models of the type considered. They also\ninvolve the memory of the latent Gaussian process, quantified through a\nspectral gap, shown to be suitably bounded as the model dimension increases,\nwhich is of independent interest. In addition, novel cross-validation schemes\nare suggested for model selection. The forecasting is carried out through a\nparticle-based sequential Monte Carlo, leveraging Kalman filtering techniques.\nA simulation study and an application are also considered.",
        "authors": [
            "Younghoon Kim",
            "Marie-Christine D\u00fcker",
            "Zachary F. Fisher",
            "Vladas Pipiras"
        ],
        "categories": "stat.ME",
        "published": "2023-07-19T20:48:44Z",
        "updated": "2024-07-18T04:09:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.09864v5",
        "title": "Asymptotic equivalence of Principal Components and Quasi Maximum Likelihood estimators in Large Approximate Factor Models",
        "abstract": "This paper investigates the properties of Quasi Maximum Likelihood estimation\nof an approximate factor model for an $n$-dimensional vector of stationary time\nseries. We prove that the factor loadings estimated by Quasi Maximum Likelihood\nare asymptotically equivalent, as $n\\to\\infty$, to those estimated via\nPrincipal Components. Both estimators are, in turn, also asymptotically\nequivalent, as $n\\to\\infty$, to the unfeasible Ordinary Least Squares estimator\nwe would have if the factors were observed. We also show that the usual\nsandwich form of the asymptotic covariance matrix of the Quasi Maximum\nLikelihood estimator is asymptotically equivalent to the simpler asymptotic\ncovariance matrix of the unfeasible Ordinary Least Squares. All these results\nhold in the general case in which the idiosyncratic components are\ncross-sectionally heteroskedastic, as well as serially and cross-sectionally\nweakly correlated. The intuition behind these results is that as $n\\to\\infty$\nthe factors can be considered as observed, thus showing that factor models\nenjoy a blessing of dimensionality.",
        "authors": [
            "Matteo Barigozzi"
        ],
        "categories": "econ.EM",
        "published": "2023-07-19T09:50:14Z",
        "updated": "2024-06-27T08:12:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.09411v1",
        "title": "Risk Preference Types, Limited Consideration, and Welfare",
        "abstract": "We provide sufficient conditions for semi-nonparametric point identification\nof a mixture model of decision making under risk, when agents make choices in\nmultiple lines of insurance coverage (contexts) by purchasing a bundle. As a\nfirst departure from the related literature, the model allows for two\npreference types. In the first one, agents behave according to standard\nexpected utility theory with CARA Bernoulli utility function, with an\nagent-specific coefficient of absolute risk aversion whose distribution is left\ncompletely unspecified. In the other, agents behave according to the dual\ntheory of choice under risk(Yaari, 1987) combined with a one-parameter family\ndistortion function, where the parameter is agent-specific and is drawn from a\ndistribution that is left completely unspecified. Within each preference type,\nthe model allows for unobserved heterogeneity in consideration sets, where the\nlatter form at the bundle level -- a second departure from the related\nliterature. Our point identification result rests on observing sufficient\nvariation in covariates across contexts, without requiring any independent\nvariation across alternatives within a single context. We estimate the model on\ndata on households' deductible choices in two lines of property insurance, and\nuse the results to assess the welfare implications of a hypothetical market\nintervention where the two lines of insurance are combined into a single one.\nWe study the role of limited consideration in mediating the welfare effects of\nsuch intervention.",
        "authors": [
            "Levon Barseghyan",
            "Francesca Molinari"
        ],
        "categories": "econ.EM",
        "published": "2023-07-18T16:31:13Z",
        "updated": "2023-07-18T16:31:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.08853v1",
        "title": "Comparative Analysis of Machine Learning, Hybrid, and Deep Learning Forecasting Models Evidence from European Financial Markets and Bitcoins",
        "abstract": "This study analyzes the transmission of market uncertainty on key European\nfinancial markets and the cryptocurrency market over an extended period,\nencompassing the pre, during, and post-pandemic periods. Daily financial market\nindices and price observations are used to assess the forecasting models. We\ncompare statistical, machine learning, and deep learning forecasting models to\nevaluate the financial markets, such as the ARIMA, hybrid ETS-ANN, and kNN\npredictive models. The study results indicate that predicting financial market\nfluctuations is challenging, and the accuracy levels are generally low in\nseveral instances. ARIMA and hybrid ETS-ANN models perform better over extended\nperiods compared to the kNN model, with ARIMA being the best-performing model\nin 2018-2021 and the hybrid ETS-ANN model being the best-performing model in\nmost of the other subperiods. Still, the kNN model outperforms the others in\nseveral periods, depending on the observed accuracy measure. Researchers have\nadvocated using parametric and non-parametric modeling combinations to generate\nbetter results. In this study, the results suggest that the hybrid ETS-ANN\nmodel is the best-performing model despite its moderate level of accuracy.\nThus, the hybrid ETS-ANN model is a promising financial time series forecasting\napproach. The findings offer financial analysts an additional source that can\nprovide valuable insights for investment decisions.",
        "authors": [
            "Apostolos Ampountolas"
        ],
        "categories": "q-fin.ST",
        "published": "2023-07-17T21:25:00Z",
        "updated": "2023-07-17T21:25:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.07689v1",
        "title": "Supervised Dynamic PCA: Linear Dynamic Forecasting with Many Predictors",
        "abstract": "This paper proposes a novel dynamic forecasting method using a new supervised\nPrincipal Component Analysis (PCA) when a large number of predictors are\navailable. The new supervised PCA provides an effective way to bridge the gap\nbetween predictors and the target variable of interest by scaling and combining\nthe predictors and their lagged values, resulting in an effective dynamic\nforecasting. Unlike the traditional diffusion-index approach, which does not\nlearn the relationships between the predictors and the target variable before\nconducting PCA, we first re-scale each predictor according to their\nsignificance in forecasting the targeted variable in a dynamic fashion, and a\nPCA is then applied to a re-scaled and additive panel, which establishes a\nconnection between the predictability of the PCA factors and the target\nvariable. Furthermore, we also propose to use penalized methods such as the\nLASSO approach to select the significant factors that have superior predictive\npower over the others. Theoretically, we show that our estimators are\nconsistent and outperform the traditional methods in prediction under some mild\nconditions. We conduct extensive simulations to verify that the proposed method\nproduces satisfactory forecasting results and outperforms most of the existing\nmethods using the traditional PCA. A real example of predicting U.S.\nmacroeconomic variables using a large number of predictors showcases that our\nmethod fares better than most of the existing ones in applications. The\nproposed method thus provides a comprehensive and effective approach for\ndynamic forecasting in high-dimensional data analysis.",
        "authors": [
            "Zhaoxing Gao",
            "Ruey S. Tsay"
        ],
        "categories": "econ.EM",
        "published": "2023-07-15T03:01:49Z",
        "updated": "2023-07-15T03:01:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.07574v1",
        "title": "Sparsified Simultaneous Confidence Intervals for High-Dimensional Linear Models",
        "abstract": "Statistical inference of the high-dimensional regression coefficients is\nchallenging because the uncertainty introduced by the model selection procedure\nis hard to account for. A critical question remains unsettled; that is, is it\npossible and how to embed the inference of the model into the simultaneous\ninference of the coefficients? To this end, we propose a notion of simultaneous\nconfidence intervals called the sparsified simultaneous confidence intervals.\nOur intervals are sparse in the sense that some of the intervals' upper and\nlower bounds are shrunken to zero (i.e., $[0,0]$), indicating the unimportance\nof the corresponding covariates. These covariates should be excluded from the\nfinal model. The rest of the intervals, either containing zero (e.g., $[-1,1]$\nor $[0,1]$) or not containing zero (e.g., $[2,3]$), indicate the plausible and\nsignificant covariates, respectively. The proposed method can be coupled with\nvarious selection procedures, making it ideal for comparing their uncertainty.\nFor the proposed method, we establish desirable asymptotic properties, develop\nintuitive graphical tools for visualization, and justify its superior\nperformance through simulation and real data analysis.",
        "authors": [
            "Xiaorui Zhu",
            "Yichen Qin",
            "Peng Wang"
        ],
        "categories": "stat.ME",
        "published": "2023-07-14T18:37:57Z",
        "updated": "2023-07-14T18:37:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.08646v1",
        "title": "Global path preference and local response: A reward decomposition approach for network path choice analysis in the presence of locally perceived attributes",
        "abstract": "This study performs an attribute-level analysis of the global and local path\npreferences of network travelers. To this end, a reward decomposition approach\nis proposed and integrated into a link-based recursive (Markovian) path choice\nmodel. The approach decomposes the instantaneous reward function associated\nwith each state-action pair into the global utility, a function of attributes\nglobally perceived from anywhere in the network, and the local utility, a\nfunction of attributes that are only locally perceived from the current state.\nOnly the global utility then enters the value function of each state,\nrepresenting the future expected utility toward the destination. This\nglobal-local path choice model with decomposed reward functions allows us to\nanalyze to what extent and which attributes affect the global and local path\nchoices of agents. Moreover, unlike most adaptive path choice models, the\nproposed model can be estimated based on revealed path observations (without\nthe information of plans) and as efficiently as deterministic recursive path\nchoice models. The model was applied to the real pedestrian path choice\nobservations in an urban street network where the green view index was\nextracted as a visual street quality from Google Street View images. The result\nrevealed that pedestrians locally perceive and react to the visual street\nquality, rather than they have the pre-trip global perception on it.\nFurthermore, the simulation results using the estimated models suggested the\nimportance of location selection of interventions when policy-related\nattributes are only locally perceived by travelers.",
        "authors": [
            "Yuki Oyama"
        ],
        "categories": "physics.soc-ph",
        "published": "2023-07-14T04:54:28Z",
        "updated": "2023-07-14T04:54:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.07090v2",
        "title": "Choice Models and Permutation Invariance: Demand Estimation in Differentiated Products Markets",
        "abstract": "Choice modeling is at the core of understanding how changes to the\ncompetitive landscape affect consumer choices and reshape market equilibria. In\nthis paper, we propose a fundamental characterization of choice functions that\nencompasses a wide variety of extant choice models. We demonstrate how\nnon-parametric estimators like neural nets can easily approximate such\nfunctionals and overcome the curse of dimensionality that is inherent in the\nnon-parametric estimation of choice functions. We demonstrate through extensive\nsimulations that our proposed functionals can flexibly capture underlying\nconsumer behavior in a completely data-driven fashion and outperform\ntraditional parametric models. As demand settings often exhibit endogenous\nfeatures, we extend our framework to incorporate estimation under endogenous\nfeatures. Further, we also describe a formal inference procedure to construct\nvalid confidence intervals on objects of interest like price elasticity.\nFinally, to assess the practical applicability of our estimator, we utilize a\nreal-world dataset from S. Berry, Levinsohn, and Pakes (1995). Our empirical\nanalysis confirms that the estimator generates realistic and comparable own-\nand cross-price elasticities that are consistent with the observations reported\nin the existing literature.",
        "authors": [
            "Amandeep Singh",
            "Ye Liu",
            "Hema Yoganarasimhan"
        ],
        "categories": "econ.EM",
        "published": "2023-07-13T23:24:05Z",
        "updated": "2024-02-20T14:17:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.10067v2",
        "title": "Weak Factors are Everywhere",
        "abstract": "Factor Sequences are stochastic double sequences indexed in time and\ncross-section which have a so called factor structure. The term was coined by\nForni and Lippi (2001) who introduced dynamic factor sequences. We introduce\nthe distinction between dynamic- and static factor sequences which has been\noverlooked in the literature. Static factor sequences, where the static factors\nare modeled by a dynamic system, are the most common model of macro-econometric\nfactor analysis, building on Chamberlain and Rothschild (1983a); Stock and\nWatson (2002a); Bai and Ng (2002).\n  We show that there exist two types of common components - a dynamic and a\nstatic common component. The difference between those consists of the weak\ncommon component, which is spanned by (potentially infinitely many) weak\nfactors. We also show that the dynamic common component of a dynamic factor\nsequence is causally subordinated to the output under suitable conditions. As a\nconsequence only the dynamic common component can be interpreted as the\nprojection on the infinite past of the common innovations of the economy, i.e.\nthe part which is dynamically common. On the other hand the static common\ncomponent captures only the contemporaneous co-movement.",
        "authors": [
            "Philipp Gersing",
            "Christoph Rust",
            "Manfred Deistler"
        ],
        "categories": "econ.EM",
        "published": "2023-07-13T23:08:49Z",
        "updated": "2024-01-22T15:49:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.12731v2",
        "title": "The Yule-Frisch-Waugh-Lovell Theorem for Linear Instrumental Variables Estimation",
        "abstract": "In this paper, I discuss three aspects of the Frisch-Waugh-Lovell theorem.\nFirst, I show that the theorem holds for linear instrumental variables\nestimation of a multiple regression model that is either exactly or\noveridentified. I show that with linear instrumental variables estimation: (a)\ncoefficients on endogenous variables are identical in full and partial (or\nresidualized) regressions; (b) residual vectors are identical for full and\npartial regressions; and (c) estimated covariance matrices of the coefficient\nvectors from full and partial regressions are equal (up to a degree of freedom\ncorrection) if the estimator of the error vector is a function only of the\nresidual vectors and does not use any information about the covariate matrix\nother than its dimensions. While estimation of the full model uses the full set\nof instrumental variables, estimation of the partial model uses the\nresidualized version of the same set of instrumental variables, with\nresidualization carried out with respect to the set of exogenous variables.\nSecond, I show that: (a) the theorem applies in large samples to the K-class of\nestimators, including the limited information maximum likelihood (LIML)\nestimator, and (b) the theorem does not apply in general to linear GMM\nestimators, but it does apply to the two step optimal linear GMM estimator.\nThird, I trace the historical and analytical development of the theorem and\nsuggest that it be renamed as the Yule-Frisch-Waugh-Lovell (YFWL) theorem to\nrecognize the pioneering contribution of the statistician G. Udny Yule in its\ndevelopment.",
        "authors": [
            "Deepankar Basu"
        ],
        "categories": "econ.EM",
        "published": "2023-07-12T19:18:22Z",
        "updated": "2023-08-14T14:17:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.06190v1",
        "title": "Stationarity with Occasionally Binding Constraints",
        "abstract": "This paper studies a class of multivariate threshold autoregressive models,\nknown as censored and kinked structural vector autoregressions (CKSVAR), which\nare notably able to accommodate series that are subject to occasionally binding\nconstraints. We develop a set of sufficient conditions for the processes\ngenerated by a CKSVAR to be stationary, ergodic, and weakly dependent. Our\nconditions relate directly to the stability of the deterministic part of the\nmodel, and are therefore less conservative than those typically available for\ngeneral vector threshold autoregressive (VTAR) models. Though our criteria\nrefer to quantities, such as refinements of the joint spectral radius, that\ncannot feasibly be computed exactly, they can be approximated numerically to a\nhigh degree of precision. Our results also permit us to provide a treatment of\nunit roots and cointegration in the CKSVAR, for the case where the model is\nconfigured so as to generate linear cointegration.",
        "authors": [
            "James A. Duffy",
            "Sophocles Mavroeidis",
            "Sam Wycherley"
        ],
        "categories": "econ.EM",
        "published": "2023-07-12T14:28:34Z",
        "updated": "2023-07-12T14:28:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.06174v1",
        "title": "Identification in Multiple Treatment Models under Discrete Variation",
        "abstract": "We develop a method to learn about treatment effects in multiple treatment\nmodels with discrete-valued instruments. We allow selection into treatment to\nbe governed by a general class of threshold crossing models that permits\nmultidimensional unobserved heterogeneity. Under a semi-parametric restriction\non the distribution of unobserved heterogeneity, we show how a sequence of\nlinear programs can be used to compute sharp bounds for a number of treatment\neffect parameters when the marginal treatment response functions underlying\nthem remain nonparametric or are additionally parameterized.",
        "authors": [
            "Vishal Kamat",
            "Samuel Norris",
            "Matthew Pecenco"
        ],
        "categories": "econ.EM",
        "published": "2023-07-12T13:59:35Z",
        "updated": "2023-07-12T13:59:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.06145v1",
        "title": "Robust Impulse Responses using External Instruments: the Role of Information",
        "abstract": "External-instrument identification leads to biased responses when the shock\nis not invertible and the measurement error is present. We propose to use this\nidentification strategy in a structural Dynamic Factor Model, which we call\nProxy DFM. In a simulation analysis, we show that the Proxy DFM always\nsuccessfully retrieves the true impulse responses, while the Proxy SVAR\nsystematically fails to do so when the model is either misspecified, does not\ninclude all relevant information, or the measurement error is present. In an\napplication to US monetary policy, the Proxy DFM shows that a tightening shock\nis unequivocally contractionary, with deteriorations in domestic demand, labor,\ncredit, housing, exchange, and financial markets. This holds true for all raw\ninstruments available in the literature. The variance decomposition analysis\nhighlights the importance of monetary policy shocks in explaining economic\nfluctuations, albeit at different horizons.",
        "authors": [
            "Davide Brignone",
            "Alessandro Franconi",
            "Marco Mazzali"
        ],
        "categories": "econ.EM",
        "published": "2023-07-12T13:00:00Z",
        "updated": "2023-07-12T13:00:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.05818v2",
        "title": "What Does it Take to Control Global Temperatures? A toolbox for testing and estimating the impact of economic policies on climate",
        "abstract": "This paper tests the feasibility and estimates the cost of climate control\nthrough economic policies. It provides a toolbox for a statistical historical\nassessment of a Stochastic Integrated Model of Climate and the Economy, and its\nuse in (possibly counterfactual) policy analysis. Recognizing that\nstabilization requires supressing a trend, we use an integrated-cointegrated\nVector Autoregressive Model estimated using a newly compiled dataset ranging\nbetween years A.D. 1000-2008, extending previous results on Control Theory in\nnonstationary systems. We test statistically whether, and quantify to what\nextent, carbon abatement policies can effectively stabilize or reduce global\ntemperatures. Our formal test of policy feasibility shows that carbon abatement\ncan have a significant long run impact and policies can render temperatures\nstationary around a chosen long run mean. In a counterfactual empirical\nillustration of the possibilities of our modeling strategy, we study a\nretrospective policy aiming to keep global temperatures close to their 1900\nhistorical level. Achieving this via carbon abatement may cost about 75% of the\nobserved 2008 level of world GDP, a cost equivalent to reverting to levels of\noutput historically observed in the mid 1960s. By contrast, investment in\ncarbon neutral technology could achieve the policy objective and be\nself-sustainable as long as it costs less than 50% of 2008 global GDP and 75%\nof consumption.",
        "authors": [
            "Guillaume Chevillon",
            "Takamitsu Kurita"
        ],
        "categories": "econ.EM",
        "published": "2023-07-11T21:54:08Z",
        "updated": "2024-07-09T16:50:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.05122v1",
        "title": "Synthetic Decomposition for Counterfactual Predictions",
        "abstract": "Counterfactual predictions are challenging when the policy variable goes\nbeyond its pre-policy support. However, in many cases, information about the\npolicy of interest is available from different (\"source\") regions where a\nsimilar policy has already been implemented. In this paper, we propose a novel\nmethod of using such data from source regions to predict a new policy in a\ntarget region. Instead of relying on extrapolation of a structural relationship\nusing a parametric specification, we formulate a transferability condition and\nconstruct a synthetic outcome-policy relationship such that it is as close as\npossible to meeting the condition. The synthetic relationship weighs both the\nsimilarity in distributions of observables and in structural relationships. We\ndevelop a general procedure to construct asymptotic confidence intervals for\ncounterfactual predictions and prove its asymptotic validity. We then apply our\nproposal to predict average teenage employment in Texas following a\ncounterfactual increase in the minimum wage.",
        "authors": [
            "Nathan Canen",
            "Kyungchul Song"
        ],
        "categories": "econ.EM",
        "published": "2023-07-11T09:00:19Z",
        "updated": "2023-07-11T09:00:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.05562v1",
        "title": "Decentralized Decision-Making in Retail Chains: Evidence from Inventory Management",
        "abstract": "This paper investigates the impact of decentralizing inventory\ndecision-making in multi-establishment firms using data from a large retail\nchain. Analyzing two years of daily data, we find significant heterogeneity\namong the inventory decisions made by 634 store managers. By estimating a\ndynamic structural model, we reveal substantial heterogeneity in managers'\nperceived costs. Moreover, we observe a correlation between the variance of\nthese perceptions and managers' education and experience. Counterfactual\nexperiments show that centralized inventory management reduces costs by\neliminating the impact of managers' skill heterogeneity. However, these\nbenefits are offset by the negative impact of delayed demand information.",
        "authors": [
            "Victor Aguirregabiria",
            "Francis Guiton"
        ],
        "categories": "econ.EM",
        "published": "2023-07-09T22:06:37Z",
        "updated": "2023-07-09T22:06:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.03693v1",
        "title": "Are there Dragon Kings in the Stock Market?",
        "abstract": "We undertake a systematic study of historic market volatility spanning\nroughly five preceding decades. We focus specifically on the time series of\nrealized volatility (RV) of the S&P500 index and its distribution function. As\nexpected, the largest values of RV coincide with the largest economic upheavals\nof the period: Savings and Loan Crisis, Tech Bubble, Financial Crisis and Covid\nPandemic. We address the question of whether these values belong to one of the\nthree categories: Black Swans (BS), that is they lie on scale-free, power-law\ntails of the distribution; Dragon Kings (DK), defined as statistically\nsignificant upward deviations from BS; or Negative Dragons Kings (nDK), defined\nas statistically significant downward deviations from BS. In analyzing the\ntails of the distribution with RV > 40, we observe the appearance of\n\"potential\" DK which eventually terminate in an abrupt plunge to nDK. This\nphenomenon becomes more pronounced with the increase of the number of days over\nwhich the average RV is calculated -- here from daily, n=1, to \"monthly,\" n=21.\nWe fit the entire distribution with a modified Generalized Beta (mGB)\ndistribution function, which terminates at a finite value of the variable but\nexhibits a long power-law stretch prior to that, as well as Generalized Beta\nPrime (GB2) distribution function, which has a power-law tail. We also fit the\ntails directly with a straight line on a log-log scale. In order to ascertain\nBS, DK or nDK behavior, all fits include their confidence intervals and\np-values are evaluated for the data points to check if they can come from the\nrespective distributions.",
        "authors": [
            "Jiong Liu",
            "M. Dashti Moghaddam",
            "R. A. Serota"
        ],
        "categories": "q-fin.ST",
        "published": "2023-07-07T15:59:45Z",
        "updated": "2023-07-07T15:59:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.03594v2",
        "title": "Generalised Covariances and Correlations",
        "abstract": "The covariance of two random variables measures the average joint deviations\nfrom their respective means. We generalise this well-known measure by replacing\nthe means with other statistical functionals such as quantiles, expectiles, or\nthresholds. Deviations from these functionals are defined via generalised\nerrors, often induced by identification or moment functions. As a normalised\nmeasure of dependence, a generalised correlation is constructed. Replacing the\ncommon Cauchy-Schwarz normalisation by a novel Fr\\'echet-Hoeffding\nnormalisation, we obtain attainability of the entire interval $[-1, 1]$ for any\ngiven marginals. We uncover favourable properties of these new dependence\nmeasures. The families of quantile and threshold correlations give rise to\nfunction-valued distributional correlations, exhibiting the entire dependence\nstructure. They lead to tail correlations, which should arguably supersede the\ncoefficients of tail dependence. Finally, we construct summary covariances\n(correlations), which arise as (normalised) weighted averages of distributional\ncovariances. We retrieve Pearson covariance and Spearman correlation as special\ncases. The applicability and usefulness of our new dependence measures is\nillustrated on demographic data from the Panel Study of Income Dynamics.",
        "authors": [
            "Tobias Fissler",
            "Marc-Oliver Pohle"
        ],
        "categories": "stat.ME",
        "published": "2023-07-07T13:38:04Z",
        "updated": "2023-09-21T08:06:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.03552v1",
        "title": "Climate Models Underestimate the Sensitivity of Arctic Sea Ice to Carbon Emissions",
        "abstract": "Arctic sea ice has steadily diminished as atmospheric greenhouse gas\nconcentrations have increased. Using observed data from 1979 to 2019, we\nestimate a close contemporaneous linear relationship between Arctic sea ice\narea and cumulative carbon dioxide emissions. For comparison, we provide\nanalogous regression estimates using simulated data from global climate models\n(drawn from the CMIP5 and CMIP6 model comparison exercises). The carbon\nsensitivity of Arctic sea ice area is considerably stronger in the observed\ndata than in the climate models. Thus, for a given future emissions path, an\nice-free Arctic is likely to occur much earlier than the climate models\nproject. Furthermore, little progress has been made in recent global climate\nmodeling (from CMIP5 to CMIP6) to more accurately match the observed\ncarbon-climate response of Arctic sea ice.",
        "authors": [
            "Francis X. Diebold",
            "Glenn D. Rudebusch"
        ],
        "categories": "econ.EM",
        "published": "2023-07-07T12:36:23Z",
        "updated": "2023-07-07T12:36:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.02673v1",
        "title": "Panel Data Nowcasting: The Case of Price-Earnings Ratios",
        "abstract": "The paper uses structured machine learning regressions for nowcasting with\npanel data consisting of series sampled at different frequencies. Motivated by\nthe problem of predicting corporate earnings for a large cross-section of firms\nwith macroeconomic, financial, and news time series sampled at different\nfrequencies, we focus on the sparse-group LASSO regularization which can take\nadvantage of the mixed frequency time series panel data structures. Our\nempirical results show the superior performance of our machine learning panel\ndata regression models over analysts' predictions, forecast combinations,\nfirm-specific time series regression models, and standard machine learning\nmethods.",
        "authors": [
            "Andrii Babii",
            "Ryan T. Ball",
            "Eric Ghysels",
            "Jonas Striaukas"
        ],
        "categories": "econ.EM",
        "published": "2023-07-05T22:04:46Z",
        "updated": "2023-07-05T22:04:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.02375v2",
        "title": "Online Learning of Order Flow and Market Impact with Bayesian Change-Point Detection Methods",
        "abstract": "Financial order flow exhibits a remarkable level of persistence, wherein buy\n(sell) trades are often followed by subsequent buy (sell) trades over extended\nperiods. This persistence can be attributed to the division and gradual\nexecution of large orders. Consequently, distinct order flow regimes might\nemerge, which can be identified through suitable time series models applied to\nmarket data. In this paper, we propose the use of Bayesian online change-point\ndetection (BOCPD) methods to identify regime shifts in real-time and enable\nonline predictions of order flow and market impact. To enhance the\neffectiveness of our approach, we have developed a novel BOCPD method using a\nscore-driven approach. This method accommodates temporal correlations and\ntime-varying parameters within each regime. Through empirical application to\nNASDAQ data, we have found that: (i) Our newly proposed model demonstrates\nsuperior out-of-sample predictive performance compared to existing models that\nassume i.i.d. behavior within each regime; (ii) When examining the residuals,\nour model demonstrates good specification in terms of both distributional\nassumptions and temporal correlations; (iii) Within a given regime, the price\ndynamics exhibit a concave relationship with respect to time and volume,\nmirroring the characteristics of actual large orders; (iv) By incorporating\nregime information, our model produces more accurate online predictions of\norder flow and market impact compared to models that do not consider regimes.",
        "authors": [
            "Ioanna-Yvonni Tsaknaki",
            "Fabrizio Lillo",
            "Piero Mazzarisi"
        ],
        "categories": "q-fin.TR",
        "published": "2023-07-05T15:42:06Z",
        "updated": "2024-05-02T21:08:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.10808v3",
        "title": "Claim Reserving via Inverse Probability Weighting: A Micro-Level Chain-Ladder Method",
        "abstract": "Claim reserving primarily relies on macro-level models, with the Chain-Ladder\nmethod being the most widely adopted. These methods were heuristically\ndeveloped without minimal statistical foundations, relying on oversimplified\ndata assumptions and neglecting policyholder heterogeneity, often resulting in\nconservative reserve predictions. Micro-level reserving, utilizing stochastic\nmodeling with granular information, can improve predictions but tends to\ninvolve less attractive and complex models for practitioners. This paper aims\nto strike a practical balance between aggregate and individual models by\nintroducing a methodology that enables the Chain-Ladder method to incorporate\nindividual information. We achieve this by proposing a novel framework,\nformulating the claim reserving problem within a population sampling context.\nWe introduce a reserve estimator in a frequency and severity distribution-free\nmanner that utilizes inverse probability weights (IPW) driven by individual\ninformation, akin to propensity scores. We demonstrate that the Chain-Ladder\nmethod emerges as a particular case of such an IPW estimator, thereby\ninheriting a statistically sound foundation based on population sampling theory\nthat enables the use of granular information, and other extensions.",
        "authors": [
            "Sebastian Calcetero-Vanegas",
            "Andrei L. Badescu",
            "X. Sheldon Lin"
        ],
        "categories": "econ.EM",
        "published": "2023-07-05T07:24:23Z",
        "updated": "2024-06-11T18:51:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.01779v1",
        "title": "Asymptotics for the Generalized Autoregressive Conditional Duration Model",
        "abstract": "Engle and Russell (1998, Econometrica, 66:1127--1162) apply results from the\nGARCH literature to prove consistency and asymptotic normality of the\n(exponential) QMLE for the generalized autoregressive conditional duration\n(ACD) model, the so-called ACD(1,1), under the assumption of strict\nstationarity and ergodicity. The GARCH results, however, do not account for the\nfact that the number of durations over a given observation period is random.\nThus, in contrast with Engle and Russell (1998), we show that strict\nstationarity and ergodicity alone are not sufficient for consistency and\nasymptotic normality, and provide additional sufficient conditions to account\nfor the random number of durations. In particular, we argue that the durations\nneed to satisfy the stronger requirement that they have finite mean.",
        "authors": [
            "Giuseppe Cavaliere",
            "Thomas Mikosch",
            "Anders Rahbek",
            "Frederik Vilandt"
        ],
        "categories": "econ.EM",
        "published": "2023-07-04T15:31:21Z",
        "updated": "2023-07-04T15:31:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.01449v2",
        "title": "A Double Machine Learning Approach to Combining Experimental and Observational Data",
        "abstract": "Experimental and observational studies often lack validity due to untestable\nassumptions. We propose a double machine learning approach to combine\nexperimental and observational studies, allowing practitioners to test for\nassumption violations and estimate treatment effects consistently. Our\nframework tests for violations of external validity and ignorability under\nmilder assumptions. When only one of these assumptions is violated, we provide\nsemiparametrically efficient treatment effect estimators. However, our\nno-free-lunch theorem highlights the necessity of accurately identifying the\nviolated assumption for consistent treatment effect estimation. Through\ncomparative analyses, we show our framework's superiority over existing data\nfusion methods. The practical utility of our approach is further exemplified by\nthree real-world case studies, underscoring its potential for widespread\napplication in empirical research.",
        "authors": [
            "Harsh Parikh",
            "Marco Morucci",
            "Vittorio Orlandi",
            "Sudeepa Roy",
            "Cynthia Rudin",
            "Alexander Volfovsky"
        ],
        "categories": "stat.ME",
        "published": "2023-07-04T02:53:11Z",
        "updated": "2024-04-03T02:26:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.01357v3",
        "title": "Adaptive Principal Component Regression with Applications to Panel Data",
        "abstract": "Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for (regularized) PCR\nwhenever data is collected adaptively. Since the proof techniques for analyzing\nPCR in the fixed design setting do not readily extend to the online setting,\nour results rely on adapting tools from modern martingale concentration to the\nerror-in-variables setting. We demonstrate the usefulness of our bounds by\napplying them to the domain of panel data, a ubiquitous setting in econometrics\nand statistics. As our first application, we provide a framework for experiment\ndesign in panel data settings when interventions are assigned adaptively. Our\nframework may be thought of as a generalization of the synthetic control and\nsynthetic interventions frameworks, where data is collected via an adaptive\nintervention assignment policy. Our second application is a procedure for\nlearning such an intervention assignment policy in a setting where units arrive\nsequentially to be treated. In addition to providing theoretical performance\nguarantees (as measured by regret), we show that our method empirically\noutperforms a baseline which does not leverage error-in-variables regression.",
        "authors": [
            "Anish Agarwal",
            "Keegan Harris",
            "Justin Whitehouse",
            "Zhiwei Steven Wu"
        ],
        "categories": "cs.LG",
        "published": "2023-07-03T21:13:40Z",
        "updated": "2024-08-04T22:31:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.01348v1",
        "title": "Nonparametric Estimation of Large Spot Volatility Matrices for High-Frequency Financial Data",
        "abstract": "In this paper, we consider estimating spot/instantaneous volatility matrices\nof high-frequency data collected for a large number of assets. We first combine\nclassic nonparametric kernel-based smoothing with a generalised shrinkage\ntechnique in the matrix estimation for noise-free data under a uniform sparsity\nassumption, a natural extension of the approximate sparsity commonly used in\nthe literature. The uniform consistency property is derived for the proposed\nspot volatility matrix estimator with convergence rates comparable to the\noptimal minimax one. For the high-frequency data contaminated by microstructure\nnoise, we introduce a localised pre-averaging estimation method that reduces\nthe effective magnitude of the noise. We then use the estimation tool developed\nin the noise-free scenario, and derive the uniform convergence rates for the\ndeveloped spot volatility matrix estimator. We further combine the kernel\nsmoothing with the shrinkage technique to estimate the time-varying volatility\nmatrix of the high-dimensional noise vector. In addition, we consider large\nspot volatility matrix estimation in time-varying factor models with observable\nrisk factors and derive the uniform convergence property. We provide numerical\nstudies including simulation and empirical application to examine the\nperformance of the proposed estimation methods in finite samples.",
        "authors": [
            "Ruijun Bu",
            "Degui Li",
            "Oliver Linton",
            "Hanchao Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-07-03T20:43:48Z",
        "updated": "2023-07-03T20:43:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.01328v1",
        "title": "A maximal inequality for local empirical processes under weak dependence",
        "abstract": "We introduce a maximal inequality for a local empirical process under\nstrongly mixing data. Local empirical processes are defined as the (local)\naverages $\\frac{1}{nh}\\sum_{i=1}^n \\mathbf{1}\\{x - h \\leq X_i \\leq\nx+h\\}f(Z_i)$, where $f$ belongs to a class of functions, $x \\in \\mathbb{R}$ and\n$h > 0$ is a bandwidth. Our nonasymptotic bounds control estimation error\nuniformly over the function class, evaluation point $x$ and bandwidth $h$. They\nare also general enough to accomodate function classes whose complexity\nincreases with $n$. As an application, we apply our bounds to function classes\nthat exhibit polynomial decay in their uniform covering numbers. When\nspecialized to the problem of kernel density estimation, our bounds reveal\nthat, under weak dependence with exponential decay, these estimators achieve\nthe same (up to a logarithmic factor) sharp uniform-in-bandwidth rates derived\nin the iid setting by \\cite{Einmahl2005}.",
        "authors": [
            "Luis Alvarez",
            "Cristine Pinto"
        ],
        "categories": "econ.EM",
        "published": "2023-07-03T20:02:40Z",
        "updated": "2023-07-03T20:02:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.01284v4",
        "title": "Does regional variation in wage levels identify the effects of a national minimum wage?",
        "abstract": "This paper investigates the validity of estimators that exploit regional\ndifferences in wage levels to identify the labor market effects of a national\nminimum wage. Specifically, it examines variations of the ``fraction affected''\nand ``effective minimum wage'' designs. The study finds that these estimators\nare prone to biases from correlated measurement errors and functional form\nmisspecification, even when identification assumptions from previous literature\nare met. Additionally, minor deviations from these assumptions can introduce\nsignificant biases. Through a series of simulation exercises and a detailed\ncase study of Brazil's federal minimum wage increase starting in 1995, the\npaper documents the practical relevance of these biases and evaluates the\neffectiveness of potential solutions and diagnostic tools.",
        "authors": [
            "Daniel Haanwinckel"
        ],
        "categories": "econ.EM",
        "published": "2023-07-03T18:18:24Z",
        "updated": "2024-11-20T22:47:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.01049v1",
        "title": "Doubly Robust Estimation of Direct and Indirect Quantile Treatment Effects with Machine Learning",
        "abstract": "We suggest double/debiased machine learning estimators of direct and indirect\nquantile treatment effects under a selection-on-observables assumption. This\npermits disentangling the causal effect of a binary treatment at a specific\noutcome rank into an indirect component that operates through an intermediate\nvariable called mediator and an (unmediated) direct impact. The proposed method\nis based on the efficient score functions of the cumulative distribution\nfunctions of potential outcomes, which are robust to certain misspecifications\nof the nuisance parameters, i.e., the outcome, treatment, and mediator models.\nWe estimate these nuisance parameters by machine learning and use cross-fitting\nto reduce overfitting bias in the estimation of direct and indirect quantile\ntreatment effects. We establish uniform consistency and asymptotic normality of\nour effect estimators. We also propose a multiplier bootstrap for statistical\ninference and show the validity of the multiplier bootstrap. Finally, we\ninvestigate the finite sample performance of our method in a simulation study\nand apply it to empirical data from the National Job Corp Study to assess the\ndirect and indirect earnings effects of training.",
        "authors": [
            "Yu-Chin Hsu",
            "Martin Huber",
            "Yu-Min Yen"
        ],
        "categories": "econ.EM",
        "published": "2023-07-03T14:27:15Z",
        "updated": "2023-07-03T14:27:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.01033v2",
        "title": "Expected Shortfall LASSO",
        "abstract": "We propose an $\\ell_1$-penalized estimator for high-dimensional models of\nExpected Shortfall (ES). The estimator is obtained as the solution to a\nleast-squares problem for an auxiliary dependent variable, which is defined as\na transformation of the dependent variable and a pre-estimated tail quantile.\nLeveraging a sparsity condition, we derive a nonasymptotic bound on the\nprediction and estimator errors of the ES estimator, accounting for the\nestimation error in the dependent variable, and provide conditions under which\nthe estimator is consistent. Our estimator is applicable to heavy-tailed\ntime-series data and we find that the amount of parameters in the model may\ngrow with the sample size at a rate that depends on the dependence and\nheavy-tailedness in the data. In an empirical application, we consider the\nsystemic risk measure CoES and consider a set of regressors that consists of\nnonlinear transformations of a set of state variables. We find that the\nnonlinear model outperforms an unpenalized and untransformed benchmark\nconsiderably.",
        "authors": [
            "Sander Barendse"
        ],
        "categories": "econ.EM",
        "published": "2023-07-03T14:08:09Z",
        "updated": "2024-01-24T11:17:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.00779v1",
        "title": "Quantifying Distributional Model Risk in Marginal Problems via Optimal Transport",
        "abstract": "This paper studies distributional model risk in marginal problems, where each\nmarginal measure is assumed to lie in a Wasserstein ball centered at a fixed\nreference measure with a given radius. Theoretically, we establish several\nfundamental results including strong duality, finiteness of the proposed\nWasserstein distributional model risk, and the existence of an optimizer at\neach radius. In addition, we show continuity of the Wasserstein distributional\nmodel risk as a function of the radius. Using strong duality, we extend the\nwell-known Makarov bounds for the distribution function of the sum of two\nrandom variables with given marginals to Wasserstein distributionally robust\nMarkarov bounds. Practically, we illustrate our results on four distinct\napplications when the sample information comes from multiple data sources and\nonly some marginal reference measures are identified. They are: partial\nidentification of treatment effects; externally valid treatment choice via\nrobust welfare functions; Wasserstein distributionally robust estimation under\ndata combination; and evaluation of the worst aggregate risk measures.",
        "authors": [
            "Yanqin Fan",
            "Hyeonseok Park",
            "Gaoqian Xu"
        ],
        "categories": "math.OC",
        "published": "2023-07-03T06:46:10Z",
        "updated": "2023-07-03T06:46:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2307.00369v1",
        "title": "The Yule-Frisch-Waugh-Lovell Theorem",
        "abstract": "This paper traces the historical and analytical development of what is known\nin the econometrics literature as the Frisch-Waugh-Lovell theorem. This theorem\ndemonstrates that the coefficients on any subset of covariates in a multiple\nregression is equal to the coefficients in a regression of the residualized\noutcome variable on the residualized subset of covariates, where\nresidualization uses the complement of the subset of covariates of interest. In\nthis paper, I suggest that the theorem should be renamed as the\nYule-Frisch-Waugh-Lovell (YFWL) theorem to recognize the pioneering\ncontribution of the statistician G. Udny Yule in its development. Second, I\nhighlight recent work by the statistician, P. Ding, which has extended the YFWL\ntheorem to a comparison of estimated covariance matrices of coefficients from\nmultiple and partial, i.e. residualized regressions. Third, I show that, in\ncases where Ding's results do not apply, one can still resort to a\ncomputational method to conduct statistical inference about coefficients in\nmultiple regressions using information from partial regressions.",
        "authors": [
            "Deepankar Basu"
        ],
        "categories": "econ.EM",
        "published": "2023-07-01T15:44:33Z",
        "updated": "2023-07-01T15:44:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.17095v2",
        "title": "Decomposing cryptocurrency high-frequency price dynamics into recurring and noisy components",
        "abstract": "This paper investigates the temporal patterns of activity in the\ncryptocurrency market with a focus on Bitcoin, Ethereum, Dogecoin, and WINkLink\nfrom January 2020 to December 2022. Market activity measures - logarithmic\nreturns, volume, and transaction number, sampled every 10 seconds, were divided\ninto intraday and intraweek periods and then further decomposed into recurring\nand noise components via correlation matrix formalism. The key findings include\nthe distinctive market behavior from traditional stock markets due to the\nnonexistence of trade opening and closing. This was manifest in three\nenhanced-activity phases aligning with Asian, European, and U.S. trading\nsessions. An intriguing pattern of activity surge in 15-minute intervals,\nparticularly at full hours, was also noticed, implying the potential role of\nalgorithmic trading. Most notably, recurring bursts of activity in bitcoin and\nether were identified to coincide with the release times of significant U.S.\nmacroeconomic reports such as Nonfarm payrolls, Consumer Price Index data, and\nFederal Reserve statements. The most correlated daily patterns of activity\noccurred in 2022, possibly reflecting the documented correlations with U.S.\nstock indices in the same period. Factors that are external to the inner market\ndynamics are found to be responsible for the repeatable components of the\nmarket dynamics, while the internal factors appear to be substantially random,\nwhich manifests itself in a good agreement between the empirical eigenvalue\ndistributions in their bulk and the random matrix theory predictions expressed\nby the Marchenko-Pastur distribution. The findings reported support the growing\nintegration of cryptocurrencies into the global financial markets.",
        "authors": [
            "Marcin W\u0105torek",
            "Maria Skupie\u0144",
            "Jaros\u0142aw Kwapie\u0144",
            "Stanis\u0142aw Dro\u017cd\u017c"
        ],
        "categories": "q-fin.TR",
        "published": "2023-06-29T16:51:08Z",
        "updated": "2023-08-21T16:59:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.16591v3",
        "title": "Nonparametric Causal Decomposition of Group Disparities",
        "abstract": "We introduce a new nonparametric causal decomposition approach that\nidentifies the mechanisms by which a treatment variable contributes to a\ngroup-based outcome disparity. Our approach distinguishes three mechanisms:\ngroup differences in 1) treatment prevalence, 2) average treatment effects, and\n3) selection into treatment based on individual-level treatment effects. Our\napproach reformulates classic Kitagawa-Blinder-Oaxaca decompositions in causal\nand nonparametric terms, complements causal mediation analysis by explaining\ngroup disparities instead of group effects, and isolates conceptually distinct\nmechanisms conflated in recent random equalization decompositions. In contrast\nto all prior approaches, our framework uniquely identifies differential\nselection into treatment as a novel disparity-generating mechanism. Our\napproach can be used for both the retrospective causal explanation of\ndisparities and the prospective planning of interventions to change\ndisparities. We present both an unconditional and a conditional decomposition,\nwhere the latter quantifies the contributions of the treatment within levels of\ncertain covariates. We develop nonparametric estimators that are\n$\\sqrt{n}$-consistent, asymptotically normal, semiparametrically efficient, and\nmultiply robust. We apply our approach to analyze the mechanisms by which\ncollege graduation causally contributes to intergenerational income persistence\n(the disparity in income attainment between parental income groups).",
        "authors": [
            "Ang Yu",
            "Felix Elwert"
        ],
        "categories": "stat.ME",
        "published": "2023-06-28T23:01:44Z",
        "updated": "2024-07-29T16:05:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.16393v2",
        "title": "High-Dimensional Canonical Correlation Analysis",
        "abstract": "This paper studies high-dimensional canonical correlation analysis (CCA) with\nan emphasis on the vectors that define canonical variables. The paper shows\nthat when two dimensions of data grow to infinity jointly and proportionally,\nthe classical CCA procedure for estimating those vectors fails to deliver a\nconsistent estimate. This provides the first result on the impossibility of\nidentification of canonical variables in the CCA procedure when all dimensions\nare large. As a countermeasure, the paper derives the magnitude of the\nestimation error, which can be used in practice to assess the precision of CCA\nestimates. Applications of the results to cyclical vs. non-cyclical stocks and\nto a limestone grassland data set are provided.",
        "authors": [
            "Anna Bykhovskaya",
            "Vadim Gorin"
        ],
        "categories": "econ.EM",
        "published": "2023-06-28T17:43:01Z",
        "updated": "2023-08-20T19:38:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.15048v1",
        "title": "Assessing Heterogeneity of Treatment Effects",
        "abstract": "Treatment effect heterogeneity is of major interest in economics, but its\nassessment is often hindered by the fundamental lack of identification of the\nindividual treatment effects. For example, we may want to assess the effect of\ninsurance on the health of otherwise unhealthy individuals, but it is\ninfeasible to insure only the unhealthy, and thus the causal effects for those\nare not identified. Or, we may be interested in the shares of winners from a\nminimum wage increase, while without observing the counterfactual, the winners\nare not identified. Such heterogeneity is often assessed by quantile treatment\neffects, which do not come with clear interpretation and the takeaway can\nsometimes be equivocal. We show that, with the quantiles of the treated and\ncontrol outcomes, the ranges of these quantities are identified and can be\ninformative even when the average treatment effects are not significant. Two\napplications illustrate how these ranges can inform us about heterogeneity of\nthe treatment effects.",
        "authors": [
            "Tetsuya Kaji",
            "Jianfei Cao"
        ],
        "categories": "econ.EM",
        "published": "2023-06-26T20:12:21Z",
        "updated": "2023-06-26T20:12:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.15000v2",
        "title": "Identifying Socially Disruptive Policies",
        "abstract": "Social disruption occurs when a policy creates or destroys many network\nconnections between agents. It is a costly side effect of many interventions\nand so a growing empirical literature recommends measuring and accounting for\nsocial disruption when evaluating the welfare impact of a policy. However,\nthere is currently little work characterizing what can actually be learned\nabout social disruption from data in practice. In this paper, we consider the\nproblem of identifying social disruption in a research design that is popular\nin the literature. We provide two sets of identification results. First, we\nshow that social disruption is not generally point identified, but informative\nbounds can be constructed using the eigenvalues of the network adjacency\nmatrices observed by the researcher. Second, we show that point identification\nfollows from a theoretically motivated monotonicity condition, and we derive a\nclosed form representation. We apply our methods in two empirical illustrations\nand find large policy effects that otherwise might be missed by alternatives in\nthe literature.",
        "authors": [
            "Eric Auerbach",
            "Yong Cai"
        ],
        "categories": "econ.EM",
        "published": "2023-06-26T18:31:43Z",
        "updated": "2023-06-28T19:42:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.14862v3",
        "title": "Marginal Effects for Probit and Tobit with Endogeneity",
        "abstract": "When evaluating partial effects, it is important to distinguish between\nstructural endogeneity and measurement errors. In contrast to linear models,\nthese two sources of endogeneity affect partial effects differently in\nnonlinear models. We study this issue focusing on the Instrumental Variable\n(IV) Probit and Tobit models. We show that even when a valid IV is available,\nfailing to differentiate between the two types of endogeneity can lead to\neither under- or over-estimation of the partial effects. We develop simple\nestimators of the bounds on the partial effects and provide easy to implement\nconfidence intervals that correctly account for both types of endogeneity. We\nillustrate the methods in a Monte Carlo simulation and an empirical\napplication.",
        "authors": [
            "Kirill S. Evdokimov",
            "Ilze Kalnina",
            "Andrei Zeleneev"
        ],
        "categories": "econ.EM",
        "published": "2023-06-26T17:19:45Z",
        "updated": "2024-08-23T14:22:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.14653v3",
        "title": "Optimization of the Generalized Covariance Estimator in Noncausal Processes",
        "abstract": "This paper investigates the performance of the Generalized Covariance\nestimator (GCov) in estimating and identifying mixed causal and noncausal\nmodels. The GCov estimator is a semi-parametric method that minimizes an\nobjective function without making any assumptions about the error distribution\nand is based on nonlinear autocovariances to identify the causal and noncausal\norders. When the number and type of nonlinear autocovariances included in the\nobjective function of a GCov estimator is insufficient/inadequate, or the error\ndensity is too close to the Gaussian, identification issues can arise. These\nissues result in local minima in the objective function, which correspond to\nparameter values associated with incorrect causal and noncausal orders. Then,\ndepending on the starting point and the optimization algorithm employed, the\nalgorithm can converge to a local minimum. The paper proposes the use of the\nSimulated Annealing (SA) optimization algorithm as an alternative to\nconventional numerical optimization methods. The results demonstrate that SA\nperforms well when applied to mixed causal and noncausal models, successfully\neliminating the effects of local minima. The proposed approach is illustrated\nby an empirical application involving a bivariate commodity price series.",
        "authors": [
            "Gianluca Cubadda",
            "Francesco Giancaterini",
            "Alain Hecq",
            "Joann Jasiak"
        ],
        "categories": "econ.EM",
        "published": "2023-06-26T12:46:24Z",
        "updated": "2024-01-10T10:21:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.14445v1",
        "title": "Hybrid unadjusted Langevin methods for high-dimensional latent variable models",
        "abstract": "The exact estimation of latent variable models with big data is known to be\nchallenging. The latents have to be integrated out numerically, and the\ndimension of the latent variables increases with the sample size. This paper\ndevelops a novel approximate Bayesian method based on the Langevin diffusion\nprocess. The method employs the Fisher identity to integrate out the latent\nvariables, which makes it accurate and computationally feasible when applied to\nbig data. In contrast to other approximate estimation methods, it does not\nrequire the choice of a parametric distribution for the unknowns, which often\nleads to inaccuracies. In an empirical discrete choice example with a million\nobservations, the proposed method accurately estimates the posterior choice\nprobabilities using only 2% of the computation time of exact MCMC.",
        "authors": [
            "Ruben Loaiza-Maya",
            "Didier Nibbering",
            "Dan Zhu"
        ],
        "categories": "econ.EM",
        "published": "2023-06-26T06:21:21Z",
        "updated": "2023-06-26T06:21:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.14311v2",
        "title": "Simple Estimation of Semiparametric Models with Measurement Errors",
        "abstract": "We develop a practical way of addressing the Errors-In-Variables (EIV)\nproblem in the Generalized Method of Moments (GMM) framework. We focus on the\nsettings in which the variability of the EIV is a fraction of that of the\nmismeasured variables, which is typical for empirical applications. For any\ninitial set of moment conditions our approach provides a \"corrected\" set of\nmoment conditions that are robust to the EIV. We show that the GMM estimator\nbased on these moments is root-n-consistent, with the standard tests and\nconfidence intervals providing valid inference. This is true even when the EIV\nare so large that naive estimators (that ignore the EIV problem) are heavily\nbiased with their confidence intervals having 0% coverage. Our approach\ninvolves no nonparametric estimation, which is especially important for\napplications with many covariates, and settings with multivariate or\nnon-classical EIV. In particular, the approach makes it easy to use\ninstrumental variables to address EIV in nonlinear models.",
        "authors": [
            "Kirill S. Evdokimov",
            "Andrei Zeleneev"
        ],
        "categories": "econ.EM",
        "published": "2023-06-25T18:52:29Z",
        "updated": "2024-03-17T19:08:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.14004v2",
        "title": "Latent Factor Analysis in Short Panels",
        "abstract": "We develop inferential tools for latent factor analysis in short panels. The\npseudo maximum likelihood setting under a large cross-sectional dimension n and\na fixed time series dimension T relies on a diagonal TxT covariance matrix of\nthe errors without imposing sphericity nor Gaussianity. We outline the\nasymptotic distributions of the latent factor and error covariance estimates as\nwell as of an asymptotically uniformly most powerful invariant (AUMPI) test for\nthe number of factors based on the likelihood ratio statistic. We derive the\nAUMPI characterization from inequalities ensuring the monotone likelihood ratio\nproperty for positive definite quadratic forms in normal variables. An\nempirical application to a large panel of monthly U.S. stock returns separates\nmonth after month systematic and idiosyncratic risks in short subperiods of\nbear vs. bull market based on the selected number of factors. We observe an\nuptrend in the paths of total and idiosyncratic volatilities while the\nsystematic risk explains a large part of the cross-sectional total variance in\nbear markets but is not driven by a single factor. Rank tests show that\nobserved factors struggle spanning latent factors with a discrepancy between\nthe dimensions of the two factor spaces decreasing over time.",
        "authors": [
            "Alain-Philippe Fortin",
            "Patrick Gagliardini",
            "Olivier Scaillet"
        ],
        "categories": "econ.EM",
        "published": "2023-06-24T15:31:26Z",
        "updated": "2024-05-30T07:32:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.13419v1",
        "title": "Multivariate Simulation-based Forecasting for Intraday Power Markets: Modelling Cross-Product Price Effects",
        "abstract": "Intraday electricity markets play an increasingly important role in balancing\nthe intermittent generation of renewable energy resources, which creates a need\nfor accurate probabilistic price forecasts. However, research to date has\nfocused on univariate approaches, while in many European intraday electricity\nmarkets all delivery periods are traded in parallel. Thus, the dependency\nstructure between different traded products and the corresponding cross-product\neffects cannot be ignored. We aim to fill this gap in the literature by using\ncopulas to model the high-dimensional intraday price return vector. We model\nthe marginal distribution as a zero-inflated Johnson's $S_U$ distribution with\nlocation, scale and shape parameters that depend on market and fundamental\ndata. The dependence structure is modelled using latent beta regression to\naccount for the particular market structure of the intraday electricity market,\nsuch as overlapping but independent trading sessions for different delivery\ndays. We allow the dependence parameter to be time-varying. We validate our\napproach in a simulation study for the German intraday electricity market and\nfind that modelling the dependence structure improves the forecasting\nperformance. Additionally, we shed light on the impact of the single intraday\ncoupling (SIDC) on the trading activity and price distribution and interpret\nour results in light of the market efficiency hypothesis. The approach is\ndirectly applicable to other European electricity markets.",
        "authors": [
            "Simon Hirsch",
            "Florian Ziel"
        ],
        "categories": "q-fin.ST",
        "published": "2023-06-23T10:12:31Z",
        "updated": "2023-06-23T10:12:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.13362v3",
        "title": "Factor-augmented sparse MIDAS regressions with an application to nowcasting",
        "abstract": "This article investigates factor-augmented sparse MIDAS (Mixed Data Sampling)\nregressions for high-dimensional time series data, which may be observed at\ndifferent frequencies. Our novel approach integrates sparse and dense\ndimensionality reduction techniques. We derive the convergence rate of our\nestimator under misspecification, $\\tau$-mixing dependence, and polynomial\ntails. Our method's finite sample performance is assessed via Monte Carlo\nsimulations. We apply the methodology to nowcasting U.S. GDP growth and\ndemonstrate that it outperforms both sparse regression and standard\nfactor-augmented regression during the COVID-19 pandemic. To ensure the\nrobustness of these results, we also implement factor-augmented sparse logistic\nregression, which further confirms the superior accuracy of our nowcast\nprobabilities during recessions. These findings indicate that recessions are\ninfluenced by both idiosyncratic (sparse) and common (dense) shocks.",
        "authors": [
            "Jad Beyhum",
            "Jonas Striaukas"
        ],
        "categories": "econ.EM",
        "published": "2023-06-23T08:28:39Z",
        "updated": "2024-11-12T07:34:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.13005v1",
        "title": "A Discrimination Report Card",
        "abstract": "We develop an Empirical Bayes grading scheme that balances the\ninformativeness of the assigned grades against the expected frequency of\nranking errors. Applying the method to a massive correspondence experiment, we\ngrade the racial biases of 97 U.S. employers. A four-grade ranking limits the\nchances that a randomly selected pair of firms is mis-ranked to 5% while\nexplaining nearly half of the variation in firms' racial contact gaps. The\ngrades are presented alongside measures of uncertainty about each firm's\ncontact gap in an accessible rubric that is easily adapted to other settings\nwhere ranks and levels are of simultaneous interest.",
        "authors": [
            "Patrick Kline",
            "Evan K. Rose",
            "Christopher R. Walters"
        ],
        "categories": "econ.EM",
        "published": "2023-06-22T16:05:49Z",
        "updated": "2023-06-22T16:05:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.12863v1",
        "title": "Price elasticity of electricity demand: Using instrumental variable regressions to address endogeneity and autocorrelation of high-frequency time series",
        "abstract": "This paper examines empirical methods for estimating the response of\naggregated electricity demand to high-frequency price signals, the short-term\nelasticity of electricity demand. We investigate how the endogeneity of prices\nand the autocorrelation of the time series, which are particularly pronounced\nat hourly granularity, affect and distort common estimators. After developing a\ncontrolled test environment with synthetic data that replicate key statistical\nproperties of electricity demand, we show that not only the ordinary least\nsquare (OLS) estimator is inconsistent (due to simultaneity), but so is a\nregular instrumental variable (IV) regression (due to autocorrelation). Using\nwind as an instrument, as it is commonly done, may result in an estimate of the\ndemand elasticity that is inflated by an order of magnitude. We visualize the\nreason for the Thams bias using causal graphs and show that its magnitude\ndepends on the autocorrelation of both the instrument, and the dependent\nvariable. We further incorporate and adapt two extensions of the IV estimation,\nconditional IV and nuisance IV, which have recently been proposed by Thams et\nal. (2022). We show that these extensions can identify the true short-term\nelasticity in a synthetic setting and are thus particularly promising for\nfuture empirical research in this field.",
        "authors": [
            "Silvana Tiedemann",
            "Raffaele Sgarlato",
            "Lion Hirth"
        ],
        "categories": "econ.EM",
        "published": "2023-06-22T13:19:43Z",
        "updated": "2023-06-22T13:19:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.13681v2",
        "title": "Estimating the Value of Evidence-Based Decision Making",
        "abstract": "Business/policy decisions are often based on evidence from randomized\nexperiments and observational studies. In this article we propose an empirical\nframework to estimate the value of evidence-based decision making (EBDM) and\nthe return on the investment in statistical precision.",
        "authors": [
            "Alberto Abadie",
            "Anish Agarwal",
            "Guido Imbens",
            "Siwei Jia",
            "James McQueen",
            "Serguei Stepaniants"
        ],
        "categories": "stat.ME",
        "published": "2023-06-21T19:59:08Z",
        "updated": "2023-09-09T12:49:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.12271v3",
        "title": "A Nonparametric Test of $m$th-degree Inverse Stochastic Dominance",
        "abstract": "This paper proposes a nonparametric test for $m$th-degree inverse stochastic\ndominance which is a powerful tool for ranking distribution functions according\nto social welfare. We construct the test based on empirical process theory. The\ntest is shown to be asymptotically size controlled and consistent. The good\nfinite sample properties of the test are illustrated via Monte Carlo\nsimulations. We apply our test to the inequality growth in the United Kingdom\nfrom 1995 to 2010.",
        "authors": [
            "Hongyi Jiang",
            "Zhenting Sun",
            "Shiyun Hu"
        ],
        "categories": "econ.EM",
        "published": "2023-06-21T13:49:20Z",
        "updated": "2023-07-14T18:31:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.12003v5",
        "title": "Difference-in-Differences with Interference",
        "abstract": "In many scenarios, such as the evaluation of place-based policies, potential\noutcomes are not only dependent upon the unit's own treatment but also its\nneighbors' treatment. Despite this, \"difference-in-differences\" (DID) type\nestimators typically ignore such interference among neighbors. I show in this\npaper that the canonical DID estimators generally fail to identify interesting\ncausal effects in the presence of neighborhood interference. To incorporate\ninterference structure into DID estimation, I propose doubly robust estimators\nfor the direct average treatment effect on the treated as well as the average\nspillover effects under a modified parallel trends assumption. The approach in\nthis paper relaxes common restrictions in the literature, such as partial\ninterference and correctly specified spillover functions. Moreover, robust\ninference is discussed based on the asymptotic distribution of the proposed\nestimators.",
        "authors": [
            "Ruonan Xu"
        ],
        "categories": "econ.EM",
        "published": "2023-06-21T03:46:14Z",
        "updated": "2024-05-30T02:49:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.11689v2",
        "title": "Statistical Tests for Replacing Human Decision Makers with Algorithms",
        "abstract": "This paper proposes a statistical framework of using artificial intelligence\nto improve human decision making. The performance of each human decision maker\nis benchmarked against that of machine predictions. We replace the diagnoses\nmade by a subset of the decision makers with the recommendation from the\nmachine learning algorithm. We apply both a heuristic frequentist approach and\na Bayesian posterior loss function approach to abnormal birth detection using a\nnationwide dataset of doctor diagnoses from prepregnancy checkups of\nreproductive age couples and pregnancy outcomes. We find that our algorithm on\na test dataset results in a higher overall true positive rate and a lower false\npositive rate than the diagnoses made by doctors only.",
        "authors": [
            "Kai Feng",
            "Han Hong",
            "Ke Tang",
            "Jingyuan Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-06-20T17:09:04Z",
        "updated": "2024-12-07T09:24:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.10590v4",
        "title": "Assumption-lean falsification tests of rate double-robustness of double-machine-learning estimators",
        "abstract": "The class of doubly-robust (DR) functionals studied by Rotnitzky et al.\n(2021) is of central importance in economics and biostatistics. It strictly\nincludes both (i) the class of mean-square continuous functionals that can be\nwritten as an expectation of an affine functional of a conditional expectation\nstudied by Chernozhukov et al. (2022b) and (ii) the class of functionals\nstudied by Robins et al. (2008). The present state-of-the-art estimators for DR\nfunctionals $\\psi$ are double-machine-learning (DML) estimators (Chernozhukov\net al., 2018). A DML estimator $\\widehat{\\psi}_{1}$ of $\\psi$ depends on\nestimates $\\widehat{p} (x)$ and $\\widehat{b} (x)$ of a pair of nuisance\nfunctions $p(x)$ and $b(x)$, and is said to satisfy \"rate double-robustness\" if\nthe Cauchy--Schwarz upper bound of its bias is $o (n^{- 1/2})$. Were it\nachievable, our scientific goal would have been to construct valid,\nassumption-lean (i.e. no complexity-reducing assumptions on $b$ or $p$) tests\nof the validity of a nominal $(1 - \\alpha)$ Wald confidence interval (CI)\ncentered at $\\widehat{\\psi}_{1}$. But this would require a test of the bias to\nbe $o (n^{-1/2})$, which can be shown not to exist. We therefore adopt the less\nambitious goal of falsifying, when possible, an analyst's justification for her\nclaim that the reported $(1 - \\alpha)$ Wald CI is valid. In many instances, an\nanalyst justifies her claim by imposing complexity-reducing assumptions on $b$\nand $p$ to ensure \"rate double-robustness\". Here we exhibit valid,\nassumption-lean tests of $H_{0}$: \"rate double-robustness holds\", with\nnon-trivial power against certain alternatives. If $H_{0}$ is rejected, we will\nhave falsified her justification. However, no assumption-lean test of $H_{0}$,\nincluding ours, can be a consistent test. Thus, the failure of our test to\nreject is not meaningful evidence in favor of $H_{0}$.",
        "authors": [
            "Lin Liu",
            "Rajarshi Mukherjee",
            "James M. Robins"
        ],
        "categories": "stat.ME",
        "published": "2023-06-18T15:55:51Z",
        "updated": "2023-08-28T14:19:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.10562v1",
        "title": "Formal Covariate Benchmarking to Bound Omitted Variable Bias",
        "abstract": "Covariate benchmarking is an important part of sensitivity analysis about\nomitted variable bias and can be used to bound the strength of the unobserved\nconfounder using information and judgments about observed covariates. It is\ncommon to carry out formal covariate benchmarking after residualizing the\nunobserved confounder on the set of observed covariates. In this paper, I\nexplain the rationale and details of this procedure. I clarify some important\ndetails of the process of formal covariate benchmarking and highlight some of\nthe difficulties of interpretation that researchers face in reasoning about the\nresidualized part of unobserved confounders. I explain all the points with\nseveral empirical examples.",
        "authors": [
            "Deepankar Basu"
        ],
        "categories": "econ.EM",
        "published": "2023-06-18T13:52:53Z",
        "updated": "2023-06-18T13:52:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.09806v3",
        "title": "Testing for Peer Effects without Specifying the Network Structure",
        "abstract": "This paper proposes an Anderson-Rubin (AR) test for the presence of peer\neffects in panel data without the need to specify the network structure. The\nunrestricted model of our test is a linear panel data model of social\ninteractions with dyad-specific peer effect coefficients for all potential\npeers. The proposed AR test evaluates if these peer effect coefficients are all\nzero. As the number of peer effect coefficients increases with the sample size,\nso does the number of instrumental variables (IVs) employed to test the\nrestrictions under the null, rendering Bekker's many-IV environment. By\nextending existing many-IV asymptotic results to panel data, we establish the\nasymptotic validity of the proposed AR test. Our Monte Carlo simulations show\nthe robustness and superior performance of the proposed test compared to some\nexisting tests with misspecified networks. We provide two applications to\ndemonstrate its empirical relevance.",
        "authors": [
            "Hyunseok Jung",
            "Xiaodong Liu"
        ],
        "categories": "econ.EM",
        "published": "2023-06-16T12:43:48Z",
        "updated": "2024-07-30T20:25:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.09287v2",
        "title": "Modelling and Forecasting Macroeconomic Risk with Time Varying Skewness Stochastic Volatility Models",
        "abstract": "Monitoring downside risk and upside risk to the key macroeconomic indicators\nis critical for effective policymaking aimed at maintaining economic stability.\nIn this paper I propose a parametric framework for modelling and forecasting\nmacroeconomic risk based on stochastic volatility models with Skew-Normal and\nSkew-t shocks featuring time varying skewness. Exploiting a mixture stochastic\nrepresentation of the Skew-Normal and Skew-t random variables, in the paper I\ndevelop efficient posterior simulation samplers for Bayesian estimation of both\nunivariate and VAR models of this type. In an application, I use the models to\npredict downside risk to GDP growth in the US and I show that these models\nrepresent a competitive alternative to semi-parametric approaches such as\nquantile regression. Finally, estimating a medium scale VAR on US data I show\nthat time varying skewness is a relevant feature of macroeconomic and financial\nshocks.",
        "authors": [
            "Andrea Renzetti"
        ],
        "categories": "econ.EM",
        "published": "2023-06-15T17:15:03Z",
        "updated": "2023-11-20T14:49:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.08559v2",
        "title": "Inference in IV models with clustered dependence, many instruments and weak identification",
        "abstract": "Data clustering reduces the effective sample size from the number of\nobservations towards the number of clusters. For instrumental variable models I\nshow that this reduced effective sample size makes the instruments more likely\nto be weak, in the sense that they contain little information about the\nendogenous regressor, and many, in the sense that their number is large\ncompared to the sample size. Clustered data therefore increases the need for\nmany and weak instrument robust tests. However, none of the previously\ndeveloped many and weak instrument robust tests can be applied to this type of\ndata as they all require independent observations. I therefore adapt two types\nof such tests to clustered data. First, I derive cluster jackknife\nAnderson-Rubin and score tests by removing clusters rather than individual\nobservations from the statistics. Second, I propose a cluster many instrument\nAnderson-Rubin test which improves on the first type of tests by using a more\noptimal, but more complex, weighting matrix. I show that if the clusters\nsatisfy an invariance assumption the higher complexity poses no problems. By\nrevisiting a study on the effect of queenly reign on war I show the empirical\nrelevance of the new tests.",
        "authors": [
            "Johannes W. Ligtenberg"
        ],
        "categories": "econ.EM",
        "published": "2023-06-14T15:05:48Z",
        "updated": "2024-03-01T09:10:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.08165v1",
        "title": "Machine Learning for Zombie Hunting: Predicting Distress from Firms' Accounts and Missing Values",
        "abstract": "In this contribution, we propose machine learning techniques to predict\nzombie firms. First, we derive the risk of failure by training and testing our\nalgorithms on disclosed financial information and non-random missing values of\n304,906 firms active in Italy from 2008 to 2017. Then, we spot the highest\nfinancial distress conditional on predictions that lies above a threshold for\nwhich a combination of false positive rate (false prediction of firm failure)\nand false negative rate (false prediction of active firms) is minimized.\nTherefore, we identify zombies as firms that persist in a state of financial\ndistress, i.e., their forecasts fall into the risk category above the threshold\nfor at least three consecutive years. For our purpose, we implement a gradient\nboosting algorithm (XGBoost) that exploits information about missing values.\nThe inclusion of missing values in our predictive model is crucial because\npatterns of undisclosed accounts are correlated with firm failure. Finally, we\nshow that our preferred machine learning algorithm outperforms (i) proxy models\nsuch as Z-scores and the Distance-to-Default, (ii) traditional econometric\nmethods, and (iii) other widely used machine learning techniques. We provide\nevidence that zombies are on average less productive and smaller, and that they\ntend to increase in times of crisis. Finally, we argue that our application can\nhelp financial institutions and public authorities design evidence-based\npolicies-e.g., optimal bankruptcy laws and information disclosure policies.",
        "authors": [
            "Falco J. Bargagli-Stoffi",
            "Fabio Incerti",
            "Massimo Riccaboni",
            "Armando Rungi"
        ],
        "categories": "econ.EM",
        "published": "2023-06-13T22:33:44Z",
        "updated": "2023-06-13T22:33:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.07619v2",
        "title": "Kernel Choice Matters for Boundary Inference Using Local Polynomial Density: With Application to Manipulation Testing",
        "abstract": "The local polynomial density (LPD) estimator has been a useful tool for\ninference concerning boundary points of density functions. While it is commonly\nbelieved that kernel selection is not crucial for the performance of\nkernel-based estimators, this paper argues that this does not hold true for LPD\nestimators at boundary points. We find that the commonly used kernels with\ncompact support lead to larger asymptotic and finite-sample variances.\nFurthermore, we present theoretical and numerical evidence showing that such\nunfavorable variance properties negatively affect the performance of\nmanipulation testing in regression discontinuity designs, which typically\nsuffer from low power. Notably, we demonstrate that these issues of increased\nvariance and reduced power can be significantly improved just by using a kernel\nfunction with unbounded support. We recommend the use of the spline-type kernel\n(the Laplace density) and illustrate its superior performance.",
        "authors": [
            "Shunsuke Imai",
            "Yuta Okamoto"
        ],
        "categories": "econ.EM",
        "published": "2023-06-13T08:29:00Z",
        "updated": "2024-01-29T05:12:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.07018v1",
        "title": "Instrument-based estimation of full treatment effects with movers",
        "abstract": "The effect of the full treatment is a primary parameter of interest in policy\nevaluation, while often only the effect of a subset of treatment is estimated.\nWe partially identify the local average treatment effect of receiving full\ntreatment (LAFTE) using an instrumental variable that may induce individuals\ninto only a subset of treatment (movers). We show that movers violate the\nstandard exclusion restriction, necessary conditions on the presence of movers\nare testable, and partial identification holds under a double exclusion\nrestriction. We identify movers in four empirical applications and estimate\ninformative bounds on the LAFTE in three of them.",
        "authors": [
            "Didier Nibbering",
            "Matthijs Oosterveen"
        ],
        "categories": "econ.EM",
        "published": "2023-06-12T10:44:34Z",
        "updated": "2023-06-12T10:44:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.05593v2",
        "title": "Localized Neural Network Modelling of Time Series: A Case Study on US Monetary Policy",
        "abstract": "In this paper, we investigate a semiparametric regression model under the\ncontext of treatment effects via a localized neural network (LNN) approach. Due\nto a vast number of parameters involved, we reduce the number of effective\nparameters by (i) exploring the use of identification restrictions; and (ii)\nadopting a variable selection method based on the group-LASSO technique.\nSubsequently, we derive the corresponding estimation theory and propose a\ndependent wild bootstrap procedure to construct valid inferences accounting for\nthe dependence of data. Finally, we validate our theoretical findings through\nextensive numerical studies. In an empirical study, we revisit the impacts of a\ntightening monetary policy action on a variety of economic variables, including\nshort-/long-term interest rate, inflation, unemployment rate, industrial price\nand equity return via the newly proposed framework using a monthly dataset of\nthe US.",
        "authors": [
            "Jiti Gao",
            "Fei Liu",
            "Bin Peng",
            "Yanrong Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-06-08T23:41:06Z",
        "updated": "2024-07-20T10:58:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.05568v2",
        "title": "Maximally Machine-Learnable Portfolios",
        "abstract": "When it comes to stock returns, any form of predictability can bolster\nrisk-adjusted profitability. We develop a collaborative machine learning\nalgorithm that optimizes portfolio weights so that the resulting synthetic\nsecurity is maximally predictable. Precisely, we introduce MACE, a multivariate\nextension of Alternating Conditional Expectations that achieves the\naforementioned goal by wielding a Random Forest on one side of the equation,\nand a constrained Ridge Regression on the other. There are two key improvements\nwith respect to Lo and MacKinlay's original maximally predictable portfolio\napproach. First, it accommodates for any (nonlinear) forecasting algorithm and\npredictor set. Second, it handles large portfolios. We conduct exercises at the\ndaily and monthly frequency and report significant increases in predictability\nand profitability using very little conditioning information. Interestingly,\npredictability is found in bad as well as good times, and MACE successfully\nnavigates the debacle of 2022.",
        "authors": [
            "Philippe Goulet Coulombe",
            "Maximilian Goebel"
        ],
        "categories": "econ.EM",
        "published": "2023-06-08T21:24:38Z",
        "updated": "2024-04-04T23:15:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.05299v3",
        "title": "Heterogeneous Autoregressions in Short T Panel Data Models",
        "abstract": "This paper considers a first-order autoregressive panel data model with\nindividual-specific effects and heterogeneous autoregressive coefficients\ndefined on the interval (-1,1], thus allowing for some of the individual\nprocesses to have unit roots. It proposes estimators for the moments of the\ncross-sectional distribution of the autoregressive (AR) coefficients, assuming\na random coefficient model for the autoregressive coefficients without imposing\nany restrictions on the fixed effects. It is shown the standard generalized\nmethod of moments estimators obtained under homogeneous slopes are biased.\nSmall sample properties of the proposed estimators are investigated by Monte\nCarlo experiments and compared with a number of alternatives, both under\nhomogeneous and heterogeneous slopes. It is found that a simple moment\nestimator of the mean of heterogeneous AR coefficients performs very well even\nfor moderate sample sizes, but to reliably estimate the variance of AR\ncoefficients much larger samples are required. It is also required that the\ntrue value of this variance is not too close to zero. The utility of the\nheterogeneous approach is illustrated in the case of earnings dynamics.",
        "authors": [
            "M. Hashem Pesaran",
            "Liying Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-06-08T15:44:35Z",
        "updated": "2024-06-25T14:41:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.05169v1",
        "title": "Matrix GARCH Model: Inference and Application",
        "abstract": "Matrix-variate time series data are largely available in applications.\nHowever, no attempt has been made to study their conditional heteroskedasticity\nthat is often observed in economic and financial data. To address this gap, we\npropose a novel matrix generalized autoregressive conditional\nheteroskedasticity (GARCH) model to capture the dynamics of conditional row and\ncolumn covariance matrices of matrix time series. The key innovation of the\nmatrix GARCH model is the use of a univariate GARCH specification for the trace\nof conditional row or column covariance matrix, which allows for the\nidentification of conditional row and column covariance matrices. Moreover, we\nintroduce a quasi maximum likelihood estimator (QMLE) for model estimation and\ndevelop a portmanteau test for model diagnostic checking. Simulation studies\nare conducted to assess the finite-sample performance of the QMLE and\nportmanteau test. To handle large dimensional matrix time series, we also\npropose a matrix factor GARCH model. Finally, we demonstrate the superiority of\nthe matrix GARCH and matrix factor GARCH models over existing multivariate\nGARCH-type models in volatility forecasting and portfolio allocations using\nthree applications on credit default swap prices, global stock sector indices,\nand future prices.",
        "authors": [
            "Cheng Yu",
            "Dong Li",
            "Feiyu Jiang",
            "Ke Zhu"
        ],
        "categories": "stat.ME",
        "published": "2023-06-08T13:06:13Z",
        "updated": "2023-06-08T13:06:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.04606v1",
        "title": "Network-based Representations and Dynamic Discrete Choice Models for Multiple Discrete Choice Analysis",
        "abstract": "In many choice modeling applications, people demand is frequently\ncharacterized as multiple discrete, which means that people choose multiple\nitems simultaneously. The analysis and prediction of people behavior in\nmultiple discrete choice situations pose several challenges. In this paper, to\naddress this, we propose a random utility maximization (RUM) based model that\nconsiders each subset of choice alternatives as a composite alternative, where\nindividuals choose a subset according to the RUM framework. While this approach\noffers a natural and intuitive modeling approach for multiple-choice analysis,\nthe large number of subsets of choices in the formulation makes its estimation\nand application intractable. To overcome this challenge, we introduce directed\nacyclic graph (DAG) based representations of choices where each node of the DAG\nis associated with an elemental alternative and additional information such\nthat the number of selected elemental alternatives. Our innovation is to show\nthat the multi-choice model is equivalent to a recursive route choice model on\nthe DAG, leading to the development of new efficient estimation algorithms\nbased on dynamic programming. In addition, the DAG representations enable us to\nbring some advanced route choice models to capture the correlation between\nsubset choice alternatives. Numerical experiments based on synthetic and real\ndatasets show many advantages of our modeling approach and the proposed\nestimation algorithms.",
        "authors": [
            "Hung Tran",
            "Tien Mai"
        ],
        "categories": "econ.EM",
        "published": "2023-06-07T17:16:41Z",
        "updated": "2023-06-07T17:16:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.04494v2",
        "title": "Evaluating the Impact of Regulatory Policies on Social Welfare in Difference-in-Difference Settings",
        "abstract": "Quantifying the impact of regulatory policies on social welfare generally\nrequires the identification of counterfactual distributions. Many of these\npolicies (e.g. minimum wages or minimum working time) generate mass points\nand/or discontinuities in the outcome distribution. Existing approaches in the\ndifference-in-difference literature cannot accommodate these discontinuities\nwhile accounting for selection on unobservables and non-stationary outcome\ndistributions. We provide a unifying partial identification result that can\naccount for these features. Our main identifying assumption is the stability of\nthe dependence (copula) between the distribution of the untreated potential\noutcome and group membership (treatment assignment) across time. Exploiting\nthis copula stability assumption allows us to provide an identification result\nthat is invariant to monotonic transformations. We provide sharp bounds on the\ncounterfactual distribution of the treatment group suitable for any outcome,\nwhether discrete, continuous, or mixed. Our bounds collapse to the\npoint-identification result in Athey and Imbens (2006) for continuous outcomes\nwith strictly increasing distribution functions. We illustrate our approach and\nthe informativeness of our bounds by analyzing the impact of an increase in the\nlegal minimum wage using data from a recent minimum wage study (Cengiz, Dube,\nLindner, and Zipperer, 2019).",
        "authors": [
            "Dalia Ghanem",
            "D\u00e9sir\u00e9 K\u00e9dagni",
            "Ismael Mourifi\u00e9"
        ],
        "categories": "econ.EM",
        "published": "2023-06-07T15:04:50Z",
        "updated": "2023-06-11T12:00:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.04177v3",
        "title": "Semiparametric Efficiency Gains From Parametric Restrictions on Propensity Scores",
        "abstract": "We explore how much knowing a parametric restriction on propensity scores\nimproves semiparametric efficiency bounds in the potential outcome framework.\nFor stratified propensity scores, considered as a parametric model, we derive\nexplicit formulas for the efficiency gain from knowing how the covariate space\nis split. Based on these, we find that the efficiency gain decreases as the\npartition of the stratification becomes finer. For general parametric models,\nwhere it is hard to obtain explicit representations of efficiency bounds, we\npropose a novel framework that enables us to see whether knowing a parametric\nmodel is valuable in terms of efficiency even when it is high-dimensional. In\naddition to the intuitive fact that knowing the parametric model does not help\nmuch if it is sufficiently flexible, we discover that the efficiency gain can\nbe nearly zero even though the parametric assumption significantly restricts\nthe space of possible propensity scores.",
        "authors": [
            "Haruki Kono"
        ],
        "categories": "econ.EM",
        "published": "2023-06-07T06:06:48Z",
        "updated": "2024-07-02T18:19:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.04135v3",
        "title": "Semiparametric Discrete Choice Models for Bundles",
        "abstract": "We propose two approaches to estimate semiparametric discrete choice models\nfor bundles. Our first approach is a kernel-weighted rank estimator based on a\nmatching-based identification strategy. We establish its complete asymptotic\nproperties and prove the validity of the nonparametric bootstrap for inference.\nWe then introduce a new multi-index least absolute deviations (LAD) estimator\nas an alternative, of which the main advantage is its capacity to estimate\npreference parameters on both alternative- and agent-specific regressors. Both\nmethods can account for arbitrary correlation in disturbances across choices,\nwith the former also allowing for interpersonal heteroskedasticity. We also\ndemonstrate that the identification strategy underlying these procedures can be\nextended naturally to panel data settings, producing an analogous localized\nmaximum score estimator and a LAD estimator for estimating bundle choice models\nwith fixed effects. We derive the limiting distribution of the former and\nverify the validity of the numerical bootstrap as an inference tool. All our\nproposed methods can be applied to general multi-index models. Monte Carlo\nexperiments show that they perform well in finite samples.",
        "authors": [
            "Fu Ouyang",
            "Thomas T. Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-06-07T04:12:02Z",
        "updated": "2023-11-10T02:05:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.10031v1",
        "title": "Marijuana on Main Streets? The Story Continues in Colombia: An Endogenous Three-part Model",
        "abstract": "Cannabis is the most common illicit drug, and understanding its demand is\nrelevant to analyze the potential implications of its legalization. This paper\nproposes an endogenous three-part model taking into account incidental\ntruncation and access restrictions to study demand for marijuana in Colombia,\nand analyze the potential effects of its legalization. Our application suggests\nthat modeling simultaneously access, intensive and extensive margin is\nrelevant, and that selection into access is important for the intensive margin.\nWe find that younger men that have consumed alcohol and cigarettes, living in a\nneighborhood with drug suppliers, and friends that consume marijuana face\nhigher probability of having access and using this drug. In addition, we find\nthat marijuana is an inelastic good (-0.45 elasticity). Our results are robust\nto different specifications and definitions. If marijuana were legalized,\nyounger individuals with a medium or low risk perception about marijuana use\nwould increase the probability of use in 3.8 percentage points, from 13.6% to\n17.4%. Overall, legalization would increase the probability of consumption in\n0.7 p.p. (2.3% to 3.0%). Different price settings suggest that annual tax\nrevenues fluctuate between USD 11.0 million and USD 54.2 million, a potential\nbenchmark is USD 32 million.",
        "authors": [
            "A. Ramirez-Hassan",
            "C. Gomez",
            "S. Velasquez",
            "K. Tangarife"
        ],
        "categories": "econ.GN",
        "published": "2023-06-06T21:05:55Z",
        "updated": "2023-06-06T21:05:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.03816v4",
        "title": "Parametrization, Prior Independence, and the Semiparametric Bernstein-von Mises Theorem for the Partially Linear Model",
        "abstract": "I prove a semiparametric Bernstein-von Mises theorem for a partially linear\nregression model with independent priors for the low-dimensional parameter of\ninterest and the infinite-dimensional nuisance parameters. My result avoids a\nprior invariance condition that arises from a loss of information in not\nknowing the nuisance parameter. The key idea is a feasible reparametrization of\nthe regression function that mimics the Gaussian profile likelihood. This\nallows a researcher to assume independent priors for the model parameters while\nautomatically accounting for the loss of information associated with not\nknowing the nuisance parameter. As these prior stability conditions often\nimpose strong restrictions on the underlying data-generating process, my\nresults provide a more robust asymptotic normality theorem than the original\nparametrization of the partially linear model.",
        "authors": [
            "Christopher D. Walker"
        ],
        "categories": "math.ST",
        "published": "2023-06-06T16:02:22Z",
        "updated": "2024-02-27T14:30:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.03632v2",
        "title": "Uniform Inference for Cointegrated Vector Autoregressive Processes",
        "abstract": "Uniformly valid inference for cointegrated vector autoregressive processes\nhas so far proven difficult due to certain discontinuities arising in the\nasymptotic distribution of the least squares estimator. We extend asymptotic\nresults from the univariate case to multiple dimensions and show how inference\ncan be based on these results. Furthermore, we show that lag augmentation and a\nrecent instrumental variable procedure can also yield uniformly valid tests and\nconfidence regions. We verify the theoretical findings and investigate finite\nsample properties in simulation experiments for two specific examples.",
        "authors": [
            "Christian Holberg",
            "Susanne Ditlevsen"
        ],
        "categories": "math.ST",
        "published": "2023-06-06T12:41:58Z",
        "updated": "2023-12-07T13:23:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.03620v1",
        "title": "Forecasting the Performance of US Stock Market Indices During COVID-19: RF vs LSTM",
        "abstract": "The US stock market experienced instability following the recession\n(2007-2009). COVID-19 poses a significant challenge to US stock traders and\ninvestors. Traders and investors should keep up with the stock market. This is\nto mitigate risks and improve profits by using forecasting models that account\nfor the effects of the pandemic. With consideration of the COVID-19 pandemic\nafter the recession, two machine learning models, including Random Forest and\nLSTM are used to forecast two major US stock market indices. Data on historical\nprices after the big recession is used for developing machine learning models\nand forecasting index returns. To evaluate the model performance during\ntraining, cross-validation is used. Additionally, hyperparameter optimizing,\nregularization, such as dropouts and weight decays, and preprocessing improve\nthe performances of Machine Learning techniques. Using high-accuracy machine\nlearning techniques, traders and investors can forecast stock market behavior,\nstay ahead of their competition, and improve profitability. Keywords: COVID-19,\nLSTM, S&P500, Random Forest, Russell 2000, Forecasting, Machine Learning, Time\nSeries JEL Code: C6, C8, G4.",
        "authors": [
            "Reza Nematirad",
            "Amin Ahmadisharaf",
            "Ali Lashgari"
        ],
        "categories": "econ.EM",
        "published": "2023-06-06T12:15:45Z",
        "updated": "2023-06-06T12:15:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.03363v1",
        "title": "Robust inference for the treatment effect variance in experiments using machine learning",
        "abstract": "Experimenters often collect baseline data to study heterogeneity. I propose\nthe first valid confidence intervals for the VCATE, the treatment effect\nvariance explained by observables. Conventional approaches yield incorrect\ncoverage when the VCATE is zero. As a result, practitioners could be prone to\ndetect heterogeneity even when none exists. The reason why coverage worsens at\nthe boundary is that all efficient estimators have a locally-degenerate\ninfluence function and may not be asymptotically normal. I solve the problem\nfor a broad class of multistep estimators with a predictive first stage. My\nconfidence intervals account for higher-order terms in the limiting\ndistribution and are fast to compute. I also find new connections between the\nVCATE and the problem of deciding whom to treat. The gains of targeting\ntreatment are (sharply) bounded by half the square root of the VCATE. Finally,\nI document excellent performance in simulation and reanalyze an experiment from\nMalawi.",
        "authors": [
            "Alejandro Sanchez-Becerra"
        ],
        "categories": "econ.EM",
        "published": "2023-06-06T02:30:10Z",
        "updated": "2023-06-06T02:30:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.03073v2",
        "title": "Inference for Local Projections",
        "abstract": "Inference for impulse responses estimated with local projections presents\ninteresting challenges and opportunities. Analysts typically want to assess the\nprecision of individual estimates, explore the dynamic evolution of the\nresponse over particular regions, and generally determine whether the impulse\ngenerates a response that is any different from the null of no effect. Each of\nthese goals requires a different approach to inference. In this article, we\nprovide an overview of results that have appeared in the literature in the past\n20 years along with some new procedures that we introduce here.",
        "authors": [
            "Atsushi Inoue",
            "\u00d2scar Jord\u00e0",
            "Guido M. Kuersteiner"
        ],
        "categories": "econ.EM",
        "published": "2023-06-05T17:50:07Z",
        "updated": "2024-08-13T20:20:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.02977v1",
        "title": "Improving the accuracy of bubble date estimators under time-varying volatility",
        "abstract": "In this study, we consider a four-regime bubble model under the assumption of\ntime-varying volatility and propose the algorithm of estimating the break dates\nwith volatility correction: First, we estimate the emerging date of the\nexplosive bubble, its collapsing date, and the recovering date to the normal\nmarket under assumption of homoskedasticity; second, we collect the residuals\nand then employ the WLS-based estimation of the bubble dates. We demonstrate by\nMonte Carlo simulations that the accuracy of the break dates estimators improve\nsignificantly by this two-step procedure in some cases compared to those based\non the OLS method.",
        "authors": [
            "Eiji Kurozumi",
            "Anton Skrobotov"
        ],
        "categories": "econ.EM",
        "published": "2023-06-05T15:49:32Z",
        "updated": "2023-06-05T15:49:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.02584v2",
        "title": "Synthetic Regressing Control Method",
        "abstract": "Estimating weights in the synthetic control method, typically resulting in\nsparse weights where only a few control units have non-zero weights, involves\nan optimization procedure that simultaneously selects and aligns control units\nto closely match the treated unit. However, this simultaneous selection and\nalignment of control units may lead to a loss of efficiency. Another concern\narising from the aforementioned procedure is its susceptibility to\nunder-fitting due to imperfect pre-treatment fit. It is not uncommon for the\nlinear combination, using nonnegative weights, of pre-treatment period outcomes\nfor the control units to inadequately approximate the pre-treatment outcomes\nfor the treated unit. To address both of these issues, this paper proposes a\nsimple and effective method called Synthetic Regressing Control (SRC). The SRC\nmethod begins by performing the univariate linear regression to appropriately\nalign the pre-treatment periods of the control units with the treated unit.\nSubsequently, a SRC estimator is obtained by synthesizing (taking a weighted\naverage) the fitted controls. To determine the weights in the synthesis\nprocedure, we propose an approach that utilizes a criterion of unbiased risk\nestimator. Theoretically, we show that the synthesis way is asymptotically\noptimal in the sense of achieving the lowest possible squared error. Extensive\nnumerical experiments highlight the advantages of the SRC method.",
        "authors": [
            "Rong J. B. Zhu"
        ],
        "categories": "econ.EM",
        "published": "2023-06-05T04:23:54Z",
        "updated": "2023-10-23T07:07:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.01969v1",
        "title": "Individual Causal Inference Using Panel Data With Multiple Outcomes",
        "abstract": "Policy evaluation in empirical microeconomics has been focusing on estimating\nthe average treatment effect and more recently the heterogeneous treatment\neffects, often relying on the unconfoundedness assumption. We propose a method\nbased on the interactive fixed effects model to estimate treatment effects at\nthe individual level, which allows both the treatment assignment and the\npotential outcomes to be correlated with the unobserved individual\ncharacteristics. This method is suitable for panel datasets where multiple\nrelated outcomes are observed for a large number of individuals over a small\nnumber of time periods. Monte Carlo simulations show that our method\noutperforms related methods. To illustrate our method, we provide an example of\nestimating the effect of health insurance coverage on individual usage of\nhospital emergency departments using the Oregon Health Insurance Experiment\ndata.",
        "authors": [
            "Wei Tian"
        ],
        "categories": "econ.EM",
        "published": "2023-06-03T00:33:44Z",
        "updated": "2023-06-03T00:33:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.01967v1",
        "title": "The Synthetic Control Method with Nonlinear Outcomes: Estimating the Impact of the 2019 Anti-Extradition Law Amendments Bill Protests on Hong Kong's Economy",
        "abstract": "The synthetic control estimator (Abadie et al., 2010) is asymptotically\nunbiased assuming that the outcome is a linear function of the underlying\npredictors and that the treated unit can be well approximated by the synthetic\ncontrol before the treatment. When the outcome is nonlinear, the bias of the\nsynthetic control estimator can be severe. In this paper, we provide conditions\nfor the synthetic control estimator to be asymptotically unbiased when the\noutcome is nonlinear, and propose a flexible and data-driven method to choose\nthe synthetic control weights. Monte Carlo simulations show that compared with\nthe competing methods, the nonlinear synthetic control method has similar or\nbetter performance when the outcome is linear, and better performance when the\noutcome is nonlinear, and that the confidence intervals have good coverage\nprobabilities across settings. In the empirical application, we illustrate the\nmethod by estimating the impact of the 2019 anti-extradition law amendments\nbill protests on Hong Kong's economy, and find that the year-long protests\nreduced real GDP per capita in Hong Kong by 11.27% in the first quarter of\n2020, which was larger in magnitude than the economic decline during the 1997\nAsian financial crisis or the 2008 global financial crisis.",
        "authors": [
            "Wei Tian"
        ],
        "categories": "econ.EM",
        "published": "2023-06-03T00:25:53Z",
        "updated": "2023-06-03T00:25:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.01687v1",
        "title": "Load Asymptotics and Dynamic Speed Optimization for the Greenest Path Problem: A Comprehensive Analysis",
        "abstract": "We study the effect of using high-resolution elevation data on the selection\nof the most fuel-efficient (greenest) path for different trucks in various\nurban environments. We adapt a variant of the Comprehensive Modal Emission\nModel (CMEM) to show that the optimal speed and the greenest path are slope\ndependent (dynamic). When there are no elevation changes in a road network, the\nmost fuel-efficient path is the shortest path with a constant (static) optimal\nspeed throughout. However, if the network is not flat, then the shortest path\nis not necessarily the greenest path, and the optimal driving speed is dynamic.\nWe prove that the greenest path converges to an asymptotic greenest path as the\npayload approaches infinity and that this limiting path is attained for a\nfinite load. In a set of extensive numerical experiments, we benchmark the CO2\nemissions reduction of our dynamic speed and the greenest path policies against\npolicies that ignore elevation data. We use the geo-spatial data of 25 major\ncities across 6 continents, such as Los Angeles, Mexico City, Johannesburg,\nAthens, Ankara, and Canberra. Our results show that, on average, traversing the\ngreenest path with a dynamic optimal speed policy can reduce the CO2 emissions\nby 1.19% to 10.15% depending on the city and truck type for a moderate payload.\nThey also demonstrate that the average CO2 reduction of the optimal dynamic\nspeed policy is between 2% to 4% for most of the cities, regardless of the\ntruck type. We confirm that disregarding elevation data yields sub-optimal\npaths that are significantly less CO2 efficient than the greenest paths.",
        "authors": [
            "Poulad Moradi",
            "Joachim Arts",
            "Josu\u00e9 Vel\u00e1zquez-Mart\u00ednez"
        ],
        "categories": "math.OC",
        "published": "2023-06-02T17:02:25Z",
        "updated": "2023-06-02T17:02:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.01544v1",
        "title": "Social Interactions with Endogenous Group Formation",
        "abstract": "This paper explores the identification and estimation of social interaction\nmodels with endogenous group formation. We characterize group formation using a\ntwo-sided many-to-one matching model, where individuals select groups based on\ntheir preferences, while groups rank individuals according to their\nqualifications, accepting the most qualified until reaching capacities. The\nselection into groups leads to a bias in standard estimates of peer effects,\nwhich is difficult to correct for due to equilibrium effects. We employ the\nlimiting approximation of a market as the market size grows large to simplify\nthe selection bias. Assuming exchangeable unobservables, we can express the\nselection bias of an individual as a group-invariant nonparametric function of\nher preference and qualification indices. In addition to the selection\ncorrection, we show that the excluded variables in group formation can serve as\ninstruments to tackle the reflection problem. We propose semiparametric\ndistribution-free estimators that are root-n consistent and asymptotically\nnormal.",
        "authors": [
            "Shuyang Sheng",
            "Xiaoting Sun"
        ],
        "categories": "econ.EM",
        "published": "2023-06-02T13:47:53Z",
        "updated": "2023-06-02T13:47:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.01801v1",
        "title": "Rank-heterogeneous Preference Models for School Choice",
        "abstract": "School choice mechanism designers use discrete choice models to understand\nand predict families' preferences. The most widely-used choice model, the\nmultinomial logit (MNL), is linear in school and/or household attributes. While\nthe model is simple and interpretable, it assumes the ranked preference lists\narise from a choice process that is uniform throughout the ranking, from top to\nbottom. In this work, we introduce two strategies for rank-heterogeneous choice\nmodeling tailored for school choice. First, we adapt a context-dependent random\nutility model (CDM), considering down-rank choices as occurring in the context\nof earlier up-rank choices. Second, we consider stratifying the choice modeling\nby rank, regularizing rank-adjacent models towards one another when\nappropriate. Using data on household preferences from the San Francisco Unified\nSchool District (SFUSD) across multiple years, we show that the contextual\nmodels considerably improve our out-of-sample evaluation metrics across all\nrank positions over the non-contextual models in the literature. Meanwhile,\nstratifying the model by rank can yield more accurate first-choice predictions\nwhile down-rank predictions are relatively unimproved. These models provide\nperformance upgrades that school choice researchers can adopt to improve\npredictions and counterfactual analyses.",
        "authors": [
            "Amel Awadelkarim",
            "Arjun Seshadri",
            "Itai Ashlagi",
            "Irene Lo",
            "Johan Ugander"
        ],
        "categories": "stat.AP",
        "published": "2023-06-01T16:31:29Z",
        "updated": "2023-06-01T16:31:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.00485v1",
        "title": "Causal Estimation of User Learning in Personalized Systems",
        "abstract": "In online platforms, the impact of a treatment on an observed outcome may\nchange over time as 1) users learn about the intervention, and 2) the system\npersonalization, such as individualized recommendations, change over time. We\nintroduce a non-parametric causal model of user actions in a personalized\nsystem. We show that the Cookie-Cookie-Day (CCD) experiment, designed for the\nmeasurement of the user learning effect, is biased when there is\npersonalization. We derive new experimental designs that intervene in the\npersonalization system to generate the variation necessary to separately\nidentify the causal effect mediated through user learning and personalization.\nMaking parametric assumptions allows for the estimation of long-term causal\neffects based on medium-term experiments. In simulations, we show that our new\ndesigns successfully recover the dynamic causal effects of interest.",
        "authors": [
            "Evan Munro",
            "David Jones",
            "Jennifer Brennan",
            "Roland Nelet",
            "Vahab Mirrokni",
            "Jean Pouget-Abadie"
        ],
        "categories": "stat.ME",
        "published": "2023-06-01T09:37:43Z",
        "updated": "2023-06-01T09:37:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.00296v2",
        "title": "Inference in Predictive Quantile Regressions",
        "abstract": "This paper studies inference in predictive quantile regressions when the\npredictive regressor has a near-unit root. We derive asymptotic distributions\nfor the quantile regression estimator and its heteroskedasticity and\nautocorrelation consistent (HAC) t-statistic in terms of functionals of\nOrnstein-Uhlenbeck processes. We then propose a switching-fully modified (FM)\npredictive test for quantile predictability. The proposed test employs an FM\nstyle correction with a Bonferroni bound for the local-to-unity parameter when\nthe predictor has a near unit root. It switches to a standard predictive\nquantile regression test with a slightly conservative critical value when the\nlargest root of the predictor lies in the stationary range. Simulations\nindicate that the test has a reliable size in small samples and good power. We\nemploy this new methodology to test the ability of three commonly employed,\nhighly persistent and endogenous lagged valuation regressors - the dividend\nprice ratio, earnings price ratio, and book-to-market ratio - to predict the\nmedian, shoulders, and tails of the stock return distribution.",
        "authors": [
            "Alex Maynard",
            "Katsumi Shimotsu",
            "Nina Kuriyama"
        ],
        "categories": "econ.EM",
        "published": "2023-06-01T02:32:47Z",
        "updated": "2024-05-04T23:55:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.19921v1",
        "title": "Deep Neural Network Estimation in Panel Data Models",
        "abstract": "In this paper we study neural networks and their approximating power in panel\ndata models. We provide asymptotic guarantees on deep feed-forward neural\nnetwork estimation of the conditional mean, building on the work of Farrell et\nal. (2021), and explore latent patterns in the cross-section. We use the\nproposed estimators to forecast the progression of new COVID-19 cases across\nthe G7 countries during the pandemic. We find significant forecasting gains\nover both linear panel and nonlinear time series models. Containment or\nlockdown policies, as instigated at the national-level by governments, are\nfound to have out-of-sample predictive power for new COVID-19 cases. We\nillustrate how the use of partial derivatives can help open the \"black-box\" of\nneural networks and facilitate semi-structural analysis: school and workplace\nclosures are found to have been effective policies at restricting the\nprogression of the pandemic across the G7 countries. But our methods illustrate\nsignificant heterogeneity and time-variation in the effectiveness of specific\ncontainment policies.",
        "authors": [
            "Ilias Chronopoulos",
            "Katerina Chrysikou",
            "George Kapetanios",
            "James Mitchell",
            "Aristeidis Raftapostolos"
        ],
        "categories": "econ.EM",
        "published": "2023-05-31T14:58:31Z",
        "updated": "2023-05-31T14:58:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.19721v1",
        "title": "Quasi-Score Matching Estimation for Spatial Autoregressive Model with Random Weights Matrix and Regressors",
        "abstract": "With the rapid advancements in technology for data collection, the\napplication of the spatial autoregressive (SAR) model has become increasingly\nprevalent in real-world analysis, particularly when dealing with large\ndatasets. However, the commonly used quasi-maximum likelihood estimation (QMLE)\nfor the SAR model is not computationally scalable to handle the data with a\nlarge size. In addition, when establishing the asymptotic properties of the\nparameter estimators of the SAR model, both weights matrix and regressors are\nassumed to be nonstochastic in classical spatial econometrics, which is perhaps\nnot realistic in real applications. Motivated by the machine learning\nliterature, this paper proposes quasi-score matching estimation for the SAR\nmodel. This new estimation approach is still likelihood-based, but\nsignificantly reduces the computational complexity of the QMLE. The asymptotic\nproperties of parameter estimators under the random weights matrix and\nregressors are established, which provides a new theoretical framework for the\nasymptotic inference of the SAR-type models. The usefulness of the quasi-score\nmatching estimation and its asymptotic inference is illustrated via extensive\nsimulation studies and a case study of an anti-conflict social network\nexperiment for middle school students.",
        "authors": [
            "Xuan Liang",
            "Tao Zou"
        ],
        "categories": "econ.EM",
        "published": "2023-05-31T10:26:06Z",
        "updated": "2023-05-31T10:26:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.19484v2",
        "title": "A Simple Method for Predicting Covariance Matrices of Financial Returns",
        "abstract": "We consider the well-studied problem of predicting the time-varying\ncovariance matrix of a vector of financial returns. Popular methods range from\nsimple predictors like rolling window or exponentially weighted moving average\n(EWMA) to more sophisticated predictors such as generalized autoregressive\nconditional heteroscedastic (GARCH) type methods. Building on a specific\ncovariance estimator suggested by Engle in 2002, we propose a relatively simple\nextension that requires little or no tuning or fitting, is interpretable, and\nproduces results at least as good as MGARCH, a popular extension of GARCH that\nhandles multiple assets. To evaluate predictors we introduce a novel approach,\nevaluating the regret of the log-likelihood over a time period such as a\nquarter. This metric allows us to see not only how well a covariance predictor\ndoes over all, but also how quickly it reacts to changes in market conditions.\nOur simple predictor outperforms MGARCH in terms of regret. We also test\ncovariance predictors on downstream applications such as portfolio optimization\nmethods that depend on the covariance matrix. For these applications our simple\ncovariance predictor and MGARCH perform similarly.",
        "authors": [
            "Kasper Johansson",
            "Mehmet Giray Ogut",
            "Markus Pelger",
            "Thomas Schmelzer",
            "Stephen Boyd"
        ],
        "categories": "econ.EM",
        "published": "2023-05-31T01:41:24Z",
        "updated": "2023-11-21T21:25:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.19089v4",
        "title": "Impulse Response Analysis of Structural Nonlinear Time Series Models",
        "abstract": "This paper proposes a semiparametric sieve approach to estimate impulse\nresponse functions of nonlinear time series within a general class of\nstructural autoregressive models. We prove that a two-step procedure can\nflexibly accommodate nonlinear specifications while avoiding the need to choose\nof fixed parametric forms. Sieve impulse responses are proven to be consistent\nby deriving uniform estimation guarantees, and an iterative algorithm makes it\nstraightforward to compute them in practice. With simulations, we show that the\nproposed semiparametric approach proves effective against misspecification\nwhile suffering only minor efficiency losses. In a US monetary policy\napplication, we find that the pointwise sieve GDP response associated with an\ninterest rate increase is larger than that of a linear model. Finally, in an\nanalysis of interest rate uncertainty shocks, sieve responses imply\nsignificantly more substantial contractionary effects both on production and\ninflation.",
        "authors": [
            "Giovanni Ballarin"
        ],
        "categories": "econ.EM",
        "published": "2023-05-30T14:52:29Z",
        "updated": "2024-06-20T08:49:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2306.00016v1",
        "title": "Incorporating Domain Knowledge in Deep Neural Networks for Discrete Choice Models",
        "abstract": "Discrete choice models (DCM) are widely employed in travel demand analysis as\na powerful theoretical econometric framework for understanding and predicting\nchoice behaviors. DCMs are formed as random utility models (RUM), with their\nkey advantage of interpretability. However, a core requirement for the\nestimation of these models is a priori specification of the associated utility\nfunctions, making them sensitive to modelers' subjective beliefs. Recently,\nmachine learning (ML) approaches have emerged as a promising avenue for\nlearning unobserved non-linear relationships in DCMs. However, ML models are\nconsidered \"black box\" and may not correspond with expected relationships. This\npaper proposes a framework that expands the potential of data-driven approaches\nfor DCM by supporting the development of interpretable models that incorporate\ndomain knowledge and prior beliefs through constraints. The proposed framework\nincludes pseudo data samples that represent required relationships and a loss\nfunction that measures their fulfillment, along with observed data, for model\ntraining. The developed framework aims to improve model interpretability by\ncombining ML's specification flexibility with econometrics and interpretable\nbehavioral analysis. A case study demonstrates the potential of this framework\nfor discrete choice analysis.",
        "authors": [
            "Shadi Haj-Yahia",
            "Omar Mansour",
            "Tomer Toledo"
        ],
        "categories": "cs.LG",
        "published": "2023-05-30T12:53:55Z",
        "updated": "2023-05-30T12:53:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.18991v1",
        "title": "Generalized Autoregressive Score Trees and Forests",
        "abstract": "We propose methods to improve the forecasts from generalized autoregressive\nscore (GAS) models (Creal et. al, 2013; Harvey, 2013) by localizing their\nparameters using decision trees and random forests. These methods avoid the\ncurse of dimensionality faced by kernel-based approaches, and allow one to draw\non information from multiple state variables simultaneously. We apply the new\nmodels to four distinct empirical analyses, and in all applications the\nproposed new methods significantly outperform the baseline GAS model. In our\napplications to stock return volatility and density prediction, the optimal GAS\ntree model reveals a leverage effect and a variance risk premium effect. Our\nstudy of stock-bond dependence finds evidence of a flight-to-quality effect in\nthe optimal GAS forest forecasts, while our analysis of high-frequency trade\ndurations uncovers a volume-volatility effect.",
        "authors": [
            "Andrew J. Patton",
            "Yasin Simsek"
        ],
        "categories": "econ.EM",
        "published": "2023-05-30T12:41:52Z",
        "updated": "2023-05-30T12:41:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.18145v1",
        "title": "Nonlinear Impulse Response Functions and Local Projections",
        "abstract": "The goal of this paper is to extend the method of estimating Impluse Response\nFunctions (IRFs) by means of Local Projection (LP) in a nonlinear dynamic\nframework. We discuss the existence of a nonlinear autoregressive\nrepresentation for a Markov process, and explain how their Impulse Response\nFunctions are directly linked to the nonlinear Local Projection, as in the case\nfor the linear setting. We then present a nonparametric LP estimator, and\ncompare its asymptotic properties to that of IRFs obtained through direct\nestimation. We also explore issues of identification for the nonlinear IRF in\nthe multivariate framework, which remarkably differs in comparison to the\nGaussian linear case. In particular, we show that identification is conditional\non the uniqueness of deconvolution. Then, we consider IRF and LP in augmented\nMarkov models.",
        "authors": [
            "Christian Gourieroux",
            "Quinlan Lee"
        ],
        "categories": "econ.EM",
        "published": "2023-05-29T15:16:49Z",
        "updated": "2023-05-29T15:16:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.18114v2",
        "title": "Identifying Dynamic LATEs with a Static Instrument",
        "abstract": "In many situations, researchers are interested in identifying dynamic effects\nof an irreversible treatment with a static binary instrumental variable (IV).\nFor example, in evaluations of dynamic effects of training programs, with a\nsingle lottery determining eligibility. A common approach in these situations\nis to report per-period IV estimates. Under a dynamic extension of standard IV\nassumptions, we show that such IV estimators identify a weighted sum of\ntreatment effects for different latent groups and treatment exposures. However,\nthere is possibility of negative weights. We consider point and partial\nidentification of dynamic treatment effects in this setting under different\nsets of assumptions.",
        "authors": [
            "Bruno Ferman",
            "Ot\u00e1vio Tecchio"
        ],
        "categories": "econ.EM",
        "published": "2023-05-29T14:27:41Z",
        "updated": "2023-12-09T01:35:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.17829v1",
        "title": "Time-Varying Vector Error-Correction Models: Estimation and Inference",
        "abstract": "This paper considers a time-varying vector error-correction model that allows\nfor different time series behaviours (e.g., unit-root and locally stationary\nprocesses) to interact with each other to co-exist. From practical\nperspectives, this framework can be used to estimate shifts in the\npredictability of non-stationary variables, test whether economic theories hold\nperiodically, etc. We first develop a time-varying Granger Representation\nTheorem, which facilitates the establishment of asymptotic properties for the\nmodel, and then propose estimation and inferential methods and theory for both\nshort-run and long-run coefficients. We also propose an information criterion\nto estimate the lag length, a singular-value ratio test to determine the\ncointegration rank, and a hypothesis test to examine the parameter stability.\nTo validate the theoretical findings, we conduct extensive simulations.\nFinally, we demonstrate the empirical relevance by applying the framework to\ninvestigate the rational expectations hypothesis of the U.S. term structure.",
        "authors": [
            "Jiti Gao",
            "Bin Peng",
            "Yayi Yan"
        ],
        "categories": "econ.EM",
        "published": "2023-05-28T23:52:09Z",
        "updated": "2023-05-28T23:52:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.17615v5",
        "title": "Estimating overidentified linear models with heteroskedasticity and outliers",
        "abstract": "A large degree of overidentification causes severe bias in TSLS. A\nconventional heuristic rule used to motivate new estimators in this context is\napproximate bias. This paper formalizes the definition of approximate bias and\nexpands the applicability of approximate bias to various classes of estimators\nthat bridge OLS, TSLS, and Jackknife IV estimators (JIVEs). By evaluating their\napproximate biases, I propose new approximately unbiased estimators, including\nUOJIVE1 and UOJIVE2. UOJIVE1 can be interpreted as a generalization of an\nexisting estimator UIJIVE1. Both UOJIVEs are proven to be consistent and\nasymptotically normal under a fixed number of instruments and controls. The\nasymptotic proofs for UOJIVE1 in this paper require the absence of high\nleverage points, whereas proofs for UOJIVE2 do not. In addition, UOJIVE2 is\nconsistent under many-instrument asymptotic. The simulation results align with\nthe theorems in this paper: (i) Both UOJIVEs perform well under many instrument\nscenarios with or without heteroskedasticity, (ii) When a high leverage point\ncoincides with a high variance of the error term, an outlier is generated and\nthe performance of UOJIVE1 is much poorer than that of UOJIVE2.",
        "authors": [
            "Lei Bill Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-05-28T03:04:57Z",
        "updated": "2024-08-20T19:41:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.17206v1",
        "title": "Using Limited Trial Evidence to Credibly Choose Treatment Dosage when Efficacy and Adverse Effects Weakly Increase with Dose",
        "abstract": "In medical treatment and elsewhere, it has become standard to base treatment\nintensity (dosage) on evidence in randomized trials. Yet it has been rare to\nstudy how outcomes vary with dosage. In trials to obtain drug approval, the\nnorm has been to specify some dose of a new drug and compare it with an\nestablished therapy or placebo. Design-based trial analysis views each trial\narm as qualitatively different, but it may be highly credible to assume that\nefficacy and adverse effects (AEs) weakly increase with dosage. Optimization of\npatient care requires joint attention to both, as well as to treatment cost.\nThis paper develops methodology to credibly use limited trial evidence to\nchoose dosage when efficacy and AEs weakly increase with dose. I suppose that\ndosage is an integer choice t in (0, 1, . . . , T), T being a specified maximum\ndose. I study dosage choice when trial evidence on outcomes is available for\nonly K dose levels, where K < T + 1. Then the population distribution of dose\nresponse is partially rather than point identified. The identification region\nis a convex polygon determined by linear equalities and inequalities. I\ncharacterize clinical and public-health decision making using the\nminimax-regret criterion. A simple analytical solution exists when T = 2 and\ncomputation is tractable when T is larger.",
        "authors": [
            "Charles F. Manski"
        ],
        "categories": "econ.EM",
        "published": "2023-05-26T18:58:44Z",
        "updated": "2023-05-26T18:58:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.17083v2",
        "title": "A Policy Gradient Method for Confounded POMDPs",
        "abstract": "In this paper, we propose a policy gradient method for confounded partially\nobservable Markov decision processes (POMDPs) with continuous state and\nobservation spaces in the offline setting. We first establish a novel\nidentification result to non-parametrically estimate any history-dependent\npolicy gradient under POMDPs using the offline data. The identification enables\nus to solve a sequence of conditional moment restrictions and adopt the min-max\nlearning procedure with general function approximation for estimating the\npolicy gradient. We then provide a finite-sample non-asymptotic bound for\nestimating the gradient uniformly over a pre-specified policy class in terms of\nthe sample size, length of horizon, concentratability coefficient and the\nmeasure of ill-posedness in solving the conditional moment restrictions.\nLastly, by deploying the proposed gradient estimation in the gradient ascent\nalgorithm, we show the global convergence of the proposed algorithm in finding\nthe history-dependent optimal policy under some technical conditions. To the\nbest of our knowledge, this is the first work studying the policy gradient\nmethod for POMDPs under the offline setting.",
        "authors": [
            "Mao Hong",
            "Zhengling Qi",
            "Yanxun Xu"
        ],
        "categories": "stat.ML",
        "published": "2023-05-26T16:48:05Z",
        "updated": "2023-12-01T02:21:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.16915v2",
        "title": "When is cross impact relevant?",
        "abstract": "Trading pressure from one asset can move the price of another, a phenomenon\nreferred to as cross impact. Using tick-by-tick data spanning 5 years for 500\nassets listed in the United States, we identify the features that make\ncross-impact relevant to explain the variance of price returns. We show that\nprice formation occurs endogenously within highly liquid assets. Then, trades\nin these assets influence the prices of less liquid correlated products, with\nan impact velocity constrained by their minimum trading frequency. We\ninvestigate the implications of such a multidimensional price formation\nmechanism on interest rate markets. We find that the 10-year bond future serves\nas the primary liquidity reservoir, influencing the prices of cash bonds and\nfutures contracts within the interest rate curve. Such behaviour challenges the\nvalidity of the theory in Financial Economics that regards long-term rates as\nagents anticipations of future short term rates.",
        "authors": [
            "Victor Le Coz",
            "Iacopo Mastromatteo",
            "Damien Challet",
            "Michael Benzaquen"
        ],
        "categories": "q-fin.TR",
        "published": "2023-05-26T13:27:27Z",
        "updated": "2024-03-26T11:17:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.16827v1",
        "title": "Fast and Order-invariant Inference in Bayesian VARs with Non-Parametric Shocks",
        "abstract": "The shocks which hit macroeconomic models such as Vector Autoregressions\n(VARs) have the potential to be non-Gaussian, exhibiting asymmetries and fat\ntails. This consideration motivates the VAR developed in this paper which uses\na Dirichlet process mixture (DPM) to model the shocks. However, we do not\nfollow the obvious strategy of simply modeling the VAR errors with a DPM since\nthis would lead to computationally infeasible Bayesian inference in larger VARs\nand potentially a sensitivity to the way the variables are ordered in the VAR.\nInstead we develop a particular additive error structure inspired by Bayesian\nnonparametric treatments of random effects in panel data models. We show that\nthis leads to a model which allows for computationally fast and order-invariant\ninference in large VARs with nonparametric shocks. Our empirical results with\nnonparametric VARs of various dimensions shows that nonparametric treatment of\nthe VAR errors is particularly useful in periods such as the financial crisis\nand the pandemic.",
        "authors": [
            "Florian Huber",
            "Gary Koop"
        ],
        "categories": "econ.EM",
        "published": "2023-05-26T11:11:23Z",
        "updated": "2023-05-26T11:11:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.16255v1",
        "title": "Hierarchical forecasting for aggregated curves with an application to day-ahead electricity price auctions",
        "abstract": "Aggregated curves are common structures in economics and finance, and the\nmost prominent examples are supply and demand curves. In this study, we exploit\nthe fact that all aggregated curves have an intrinsic hierarchical structure,\nand thus hierarchical reconciliation methods can be used to improve the\nforecast accuracy. We provide an in-depth theory on how aggregated curves can\nbe constructed or deconstructed, and conclude that these methods are equivalent\nunder weak assumptions. We consider multiple reconciliation methods for\naggregated curves, including previously established bottom-up, top-down, and\nlinear optimal reconciliation approaches. We also present a new benchmark\nreconciliation method called 'aggregated-down' with similar complexity to\nbottom-up and top-down approaches, but it tends to provide better accuracy in\nthis setup. We conducted an empirical forecasting study on the German day-ahead\npower auction market by predicting the demand and supply curves, where their\nequilibrium determines the electricity price for the next day. Our results\ndemonstrate that hierarchical reconciliation methods can be used to improve the\nforecasting accuracy of aggregated curves.",
        "authors": [
            "Paul Ghelasi",
            "Florian Ziel"
        ],
        "categories": "stat.AP",
        "published": "2023-05-25T17:10:54Z",
        "updated": "2023-05-25T17:10:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.16377v2",
        "title": "Validating a dynamic input-output model for the propagation of supply and demand shocks during the COVID-19 pandemic in Belgium",
        "abstract": "This work validates a dynamic production network model, used to quantify the\nimpact of economic shocks caused by COVID-19 in the UK, using data for Belgium.\nBecause the model was published early during the 2020 COVID-19 pandemic, it\nrelied on several assumptions regarding the magnitude of the observed economic\nshocks, for which more accurate data have become available in the meantime. We\nrefined the propagated shocks to align with observed data collected during the\npandemic and calibrated some less well-informed parameters using 115 economic\ntime series. The refined model effectively captures the evolution of GDP,\nrevenue, and employment during the COVID-19 pandemic in Belgium at both\nindividual economic activity and aggregate levels. However, the reduction in\nbusiness-to-business demand is overestimated, revealing structural shortcomings\nin accounting for businesses' motivations to sustain trade despite the\npandemic's induced shocks. We confirm that the relaxation of the stringent\nLeontief production function by a survey on the criticality of inputs\nsignificantly improved the model's accuracy. However, despite a large dataset,\ndistinguishing between varying degrees of relaxation proved challenging.\nOverall, this work demonstrates the model's validity in assessing the impact of\neconomic shocks caused by an epidemic in Belgium.",
        "authors": [
            "Tijs W. Alleman",
            "Koen Schoors",
            "Jan M. Baetens"
        ],
        "categories": "econ.GN",
        "published": "2023-05-25T15:28:01Z",
        "updated": "2024-01-11T13:39:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.14265v4",
        "title": "Adapting to Misspecification",
        "abstract": "Empirical research typically involves a robustness-efficiency tradeoff. A\nresearcher seeking to estimate a scalar parameter can invoke strong assumptions\nto motivate a restricted estimator that is precise but may be heavily biased,\nor they can relax some of these assumptions to motivate a more robust, but\nvariable, unrestricted estimator. When a bound on the bias of the restricted\nestimator is available, it is optimal to shrink the unrestricted estimator\ntowards the restricted estimator. For settings where a bound on the bias of the\nrestricted estimator is unknown, we propose adaptive estimators that minimize\nthe percentage increase in worst case risk relative to an oracle that knows the\nbound. We show that adaptive estimators solve a weighted convex minimax problem\nand provide lookup tables facilitating their rapid computation. Revisiting some\nwell known empirical studies where questions of model specification arise, we\nexamine the advantages of adapting to -- rather than testing for --\nmisspecification.",
        "authors": [
            "Timothy B. Armstrong",
            "Patrick Kline",
            "Liyang Sun"
        ],
        "categories": "econ.EM",
        "published": "2023-05-23T17:16:09Z",
        "updated": "2024-08-27T21:40:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.13687v2",
        "title": "Flexible Bayesian Quantile Analysis of Residential Rental Rates",
        "abstract": "This article develops a random effects quantile regression model for panel\ndata that allows for increased distributional flexibility, multivariate\nheterogeneity, and time-invariant covariates in situations where mean\nregression may be unsuitable. Our approach is Bayesian and builds upon the\ngeneralized asymmetric Laplace distribution to decouple the modeling of\nskewness from the quantile parameter. We derive an efficient simulation-based\nestimation algorithm, demonstrate its properties and performance in targeted\nsimulation studies, and employ it in the computation of marginal likelihoods to\nenable formal Bayesian model comparisons. The methodology is applied in a study\nof U.S. residential rental rates following the Global Financial Crisis. Our\nempirical results provide interesting insights on the interaction between rents\nand economic, demographic and policy variables, weigh in on key modeling\nfeatures, and overwhelmingly support the additional flexibility at nearly all\nquantiles and across several sub-samples. The practical differences that arise\nas a result of allowing for flexible modeling can be nontrivial, especially for\nquantiles away from the median.",
        "authors": [
            "Ivan Jeliazkov",
            "Shubham Karnawat",
            "Mohammad Arshad Rahman",
            "Angela Vossmeyer"
        ],
        "categories": "econ.EM",
        "published": "2023-05-23T04:49:12Z",
        "updated": "2023-09-06T07:12:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.12883v3",
        "title": "Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors",
        "abstract": "In recent years, there has been a significant growth in research focusing on\nminimum $\\ell_2$ norm (ridgeless) interpolation least squares estimators.\nHowever, the majority of these analyses have been limited to an unrealistic\nregression error structure, assuming independent and identically distributed\nerrors with zero mean and common variance. In this paper, we explore prediction\nrisk as well as estimation risk under more general regression error\nassumptions, highlighting the benefits of overparameterization in a more\nrealistic setting that allows for clustered or serial dependence. Notably, we\nestablish that the estimation difficulties associated with the variance\ncomponents of both risks can be summarized through the trace of the\nvariance-covariance matrix of the regression errors. Our findings suggest that\nthe benefits of overparameterization can extend to time series, panel and\ngrouped data.",
        "authors": [
            "Sungyoon Lee",
            "Sokbae Lee"
        ],
        "categories": "math.ST",
        "published": "2023-05-22T10:04:20Z",
        "updated": "2024-06-13T01:16:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.12407v2",
        "title": "Federated Offline Policy Learning",
        "abstract": "We consider the problem of learning personalized decision policies from\nobservational bandit feedback data across multiple heterogeneous data sources.\nIn our approach, we introduce a novel regret analysis that establishes\nfinite-sample upper bounds on distinguishing notions of global regret for all\ndata sources on aggregate and of local regret for any given data source. We\ncharacterize these regret bounds by expressions of source heterogeneity and\ndistribution shift. Moreover, we examine the practical considerations of this\nproblem in the federated setting where a central server aims to train a policy\non data distributed across the heterogeneous sources without collecting any of\ntheir raw data. We present a policy learning algorithm amenable to federation\nbased on the aggregation of local policies trained with doubly robust offline\npolicy evaluation strategies. Our analysis and supporting experimental results\nprovide insights into tradeoffs in the participation of heterogeneous data\nsources in offline policy learning.",
        "authors": [
            "Aldo Gael Carranza",
            "Susan Athey"
        ],
        "categories": "cs.LG",
        "published": "2023-05-21T09:08:09Z",
        "updated": "2024-10-11T05:46:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.12067v1",
        "title": "Identification and Estimation of Production Function with Unobserved Heterogeneity",
        "abstract": "This paper examines the nonparametric identifiability of production\nfunctions, considering firm heterogeneity beyond Hicks-neutral technology\nterms. We propose a finite mixture model to account for unobserved\nheterogeneity in production technology and productivity growth processes. Our\nanalysis demonstrates that the production function for each latent type can be\nnonparametrically identified using four periods of panel data, relying on\nassumptions similar to those employed in existing literature on production\nfunction and panel data identification. By analyzing Japanese plant-level panel\ndata, we uncover significant disparities in estimated input elasticities and\nproductivity growth processes among latent types within narrowly defined\nindustries. We further show that neglecting unobserved heterogeneity in input\nelasticities may lead to substantial and systematic bias in the estimation of\nproductivity growth.",
        "authors": [
            "Hiroyuki Kasahara",
            "Paul Schrimpf",
            "Michio Suzuki"
        ],
        "categories": "econ.EM",
        "published": "2023-05-20T03:08:06Z",
        "updated": "2023-05-20T03:08:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.11282v2",
        "title": "Statistical Estimation for Covariance Structures with Tail Estimates using Nodewise Quantile Predictive Regression Models",
        "abstract": "This paper considers the specification of covariance structures with tail\nestimates. We focus on two aspects: (i) the estimation of the VaR-CoVaR risk\nmatrix in the case of larger number of time series observations than assets in\na portfolio using quantile predictive regression models without assuming the\npresence of nonstationary regressors and; (ii) the construction of a novel\nvariable selection algorithm, so-called, Feature Ordering by Centrality\nExclusion (FOCE), which is based on an assumption-lean regression framework,\nhas no tuning parameters and is proved to be consistent under general sparsity\nassumptions. We illustrate the usefulness of our proposed methodology with\nnumerical studies of real and simulated datasets when modelling systemic risk\nin a network.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-05-18T19:57:23Z",
        "updated": "2023-07-24T15:20:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.10934v1",
        "title": "Context-Dependent Heterogeneous Preferences: A Comment on Barseghyan and Molinari (2023)",
        "abstract": "Barseghyan and Molinari (2023) give sufficient conditions for\nsemi-nonparametric point identification of parameters of interest in a mixture\nmodel of decision-making under risk, allowing for unobserved heterogeneity in\nutility functions and limited consideration. A key assumption in the model is\nthat the heterogeneity of risk preferences is unobservable but\ncontext-independent. In this comment, we build on their insights and present\nidentification results in a setting where the risk preferences are allowed to\nbe context-dependent.",
        "authors": [
            "Matias D. Cattaneo",
            "Xinwei Ma",
            "Yusufcan Masatlioglu"
        ],
        "categories": "econ.TH",
        "published": "2023-05-18T12:50:58Z",
        "updated": "2023-05-18T12:50:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.10728v2",
        "title": "Modeling Interference Using Experiment Roll-out",
        "abstract": "Experiments on online marketplaces and social networks suffer from\ninterference, where the outcome of a unit is impacted by the treatment status\nof other units. We propose a framework for modeling interference using a\nubiquitous deployment mechanism for experiments, staggered roll-out designs,\nwhich slowly increase the fraction of units exposed to the treatment to\nmitigate any unanticipated adverse side effects. Our main idea is to leverage\nthe temporal variations in treatment assignments introduced by roll-outs to\nmodel the interference structure. Since there are often multiple competing\nmodels of interference in practice we first develop a model selection method\nthat evaluates models based on their ability to explain outcome variation\nobserved along the roll-out. Through simulations, we show that our heuristic\nmodel selection method, Leave-One-Period-Out, outperforms other baselines.\nNext, we present a set of model identification conditions under which the\nestimation of common estimands is possible and show how these conditions are\naided by roll-out designs. We conclude with a set of considerations, robustness\nchecks, and potential limitations for practitioners wishing to use our\nframework.",
        "authors": [
            "Ariel Boyarsky",
            "Hongseok Namkoong",
            "Jean Pouget-Abadie"
        ],
        "categories": "stat.ME",
        "published": "2023-05-18T05:57:37Z",
        "updated": "2023-08-16T21:44:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.10256v1",
        "title": "Nowcasting with signature methods",
        "abstract": "Key economic variables are often published with a significant delay of over a\nmonth. The nowcasting literature has arisen to provide fast, reliable estimates\nof delayed economic indicators and is closely related to filtering methods in\nsignal processing. The path signature is a mathematical object which captures\ngeometric properties of sequential data; it naturally handles missing data from\nmixed frequency and/or irregular sampling -- issues often encountered when\nmerging multiple data sources -- by embedding the observed data in continuous\ntime. Calculating path signatures and using them as features in models has\nachieved state-of-the-art results in fields such as finance, medicine, and\ncyber security. We look at the nowcasting problem by applying regression on\nsignatures, a simple linear model on these nonlinear objects that we show\nsubsumes the popular Kalman filter. We quantify the performance via a\nsimulation exercise, and through application to nowcasting US GDP growth, where\nwe see a lower error than a dynamic factor model based on the New York Fed\nstaff nowcasting model. Finally we demonstrate the flexibility of this method\nby applying regression on signatures to nowcast weekly fuel prices using daily\ndata. Regression on signatures is an easy-to-apply approach that allows great\nflexibility for data with complex sampling patterns.",
        "authors": [
            "Samuel N. Cohen",
            "Silvia Lui",
            "Will Malpass",
            "Giulia Mantoan",
            "Lars Nesheim",
            "\u00c1ureo de Paula",
            "Andrew Reeves",
            "Craig Scott",
            "Emma Small",
            "Lingyi Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-05-17T14:44:06Z",
        "updated": "2023-05-17T14:44:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.09563v1",
        "title": "Monitoring multicountry macroeconomic risk",
        "abstract": "We propose a multicountry quantile factor augmeneted vector autoregression\n(QFAVAR) to model heterogeneities both across countries and across\ncharacteristics of the distributions of macroeconomic time series. The presence\nof quantile factors allows for summarizing these two heterogeneities in a\nparsimonious way. We develop two algorithms for posterior inference that\nfeature varying level of trade-off between estimation precision and\ncomputational speed. Using monthly data for the euro area, we establish the\ngood empirical properties of the QFAVAR as a tool for assessing the effects of\nglobal shocks on country-level macroeconomic risks. In particular, QFAVAR\nshort-run tail forecasts are more accurate compared to a FAVAR with symmetric\nGaussian errors, as well as univariate quantile autoregressions that ignore\ncomovements among quantiles of macroeconomic variables. We also illustrate how\nquantile impulse response functions and quantile connectedness measures,\nresulting from the new model, can be used to implement joint risk scenario\nanalysis.",
        "authors": [
            "Dimitris Korobilis",
            "Maximilian Schr\u00f6der"
        ],
        "categories": "econ.EM",
        "published": "2023-05-16T15:59:07Z",
        "updated": "2023-05-16T15:59:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.09052v1",
        "title": "Grenander-type Density Estimation under Myerson Regularity",
        "abstract": "This study presents a novel approach to the density estimation of private\nvalues from second-price auctions, diverging from the conventional use of\nsmoothing-based estimators. We introduce a Grenander-type estimator,\nconstructed based on a shape restriction in the form of a convexity constraint.\nThis constraint corresponds to the renowned Myerson regularity condition in\nauction theory, which is equivalent to the concavity of the revenue function\nfor selling the auction item. Our estimator is nonparametric and does not\nrequire any tuning parameters. Under mild assumptions, we establish the\ncube-root consistency and show that the estimator asymptotically follows the\nscaled Chernoff's distribution. Moreover, we demonstrate that the estimator\nachieves the minimax optimal convergence rate.",
        "authors": [
            "Haitian Xie"
        ],
        "categories": "econ.EM",
        "published": "2023-05-15T22:24:46Z",
        "updated": "2023-05-15T22:24:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.08559v3",
        "title": "Designing Discontinuities",
        "abstract": "Discontinuities can be fairly arbitrary but also cause a significant impact\non outcomes in larger systems. Indeed, their arbitrariness is why they have\nbeen used to infer causal relationships among variables in numerous settings.\nRegression discontinuity from econometrics assumes the existence of a\ndiscontinuous variable that splits the population into distinct partitions to\nestimate the causal effects of a given phenomenon. Here we consider the design\nof partitions for a given discontinuous variable to optimize a certain effect\npreviously studied using regression discontinuity. To do so, we propose a\nquantization-theoretic approach to optimize the effect of interest, first\nlearning the causal effect size of a given discontinuous variable and then\napplying dynamic programming for optimal quantization design of discontinuities\nto balance the gain and loss in that effect size. We also develop a\ncomputationally-efficient reinforcement learning algorithm for the dynamic\nprogramming formulation of optimal quantization. We demonstrate our approach by\ndesigning optimal time zone borders for counterfactuals of social capital,\nsocial mobility, and health. This is based on regression discontinuity analyses\nwe perform on novel data, which may be of independent empirical interest.",
        "authors": [
            "Ibtihal Ferwana",
            "Suyoung Park",
            "Ting-Yi Wu",
            "Lav R. Varshney"
        ],
        "categories": "cs.IT",
        "published": "2023-05-15T11:41:30Z",
        "updated": "2023-12-28T04:25:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.08488v2",
        "title": "Hierarchical DCC-HEAVY Model for High-Dimensional Covariance Matrices",
        "abstract": "We introduce a HD DCC-HEAVY class of hierarchical-type factor models for\nhigh-dimensional covariance matrices, employing the realized measures built\nfrom higher-frequency data. The modelling approach features straightforward\nestimation and forecasting schemes, independent of the cross-sectional\ndimension of the assets under consideration, and accounts for sophisticated\nasymmetric dynamics in the covariances. Empirical analyses suggest that the HD\nDCC-HEAVY models have a better in-sample fit and deliver statistically and\neconomically significant out-of-sample gains relative to the existing\nhierarchical factor model and standard benchmarks. The results are robust under\ndifferent frequencies and market conditions.",
        "authors": [
            "Emilija Dzuverovic",
            "Matteo Barigozzi"
        ],
        "categories": "econ.EM",
        "published": "2023-05-15T09:44:24Z",
        "updated": "2024-07-16T13:25:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.08340v1",
        "title": "Efficient Semiparametric Estimation of Average Treatment Effects Under Covariate Adaptive Randomization",
        "abstract": "Experiments that use covariate adaptive randomization (CAR) are commonplace\nin applied economics and other fields. In such experiments, the experimenter\nfirst stratifies the sample according to observed baseline covariates and then\nassigns treatment randomly within these strata so as to achieve balance\naccording to pre-specified stratum-specific target assignment proportions. In\nthis paper, we compute the semiparametric efficiency bound for estimating the\naverage treatment effect (ATE) in such experiments with binary treatments\nallowing for the class of CAR procedures considered in Bugni, Canay, and Shaikh\n(2018, 2019). This is a broad class of procedures and is motivated by those\nused in practice. The stratum-specific target proportions play the role of the\npropensity score conditional on all baseline covariates (and not just the\nstrata) in these experiments. Thus, the efficiency bound is a special case of\nthe bound in Hahn (1998), but conditional on all baseline covariates.\nAdditionally, this efficiency bound is shown to be achievable under the same\nconditions as those used to derive the bound by using a cross-fitted\nNadaraya-Watson kernel estimator to form nonparametric regression adjustments.",
        "authors": [
            "Ahnaf Rafi"
        ],
        "categories": "econ.EM",
        "published": "2023-05-15T04:23:28Z",
        "updated": "2023-05-15T04:23:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.07993v3",
        "title": "The Nonstationary Newsvendor with (and without) Predictions",
        "abstract": "The classic newsvendor model yields an optimal decision for a \"newsvendor\"\nselecting a quantity of inventory, under the assumption that the demand is\ndrawn from a known distribution. Motivated by applications such as cloud\nprovisioning and staffing, we consider a setting in which newsvendor-type\ndecisions must be made sequentially, in the face of demand drawn from a\nstochastic process that is both unknown and nonstationary. All prior work on\nthis problem either (a) assumes that the level of nonstationarity is known, or\n(b) imposes additional statistical assumptions that enable accurate predictions\nof the unknown demand.\n  We study the Nonstationary Newsvendor, with and without predictions. We\nfirst, in the setting without predictions, design a policy which we prove (via\nmatching upper and lower bounds) achieves order-optimal regret -- ours is the\nfirst policy to accomplish this without being given the level of\nnonstationarity of the underlying demand. We then, for the first time,\nintroduce a model for generic (i.e. with no statistical assumptions)\npredictions with arbitrary accuracy, and propose a policy that incorporates\nthese predictions without being given their accuracy. We upper bound the regret\nof this policy, and show that it matches the best achievable regret had the\naccuracy of the predictions been known. Finally, we empirically validate our\nnew policy with experiments based on three real-world datasets containing\nthousands of time-series, showing that it succeeds in closing approximately 74%\nof the gap between the best approaches based on nonstationarity and predictions\nalone.",
        "authors": [
            "Lin An",
            "Andrew A. Li",
            "Benjamin Moseley",
            "R. Ravi"
        ],
        "categories": "math.OC",
        "published": "2023-05-13T20:14:50Z",
        "updated": "2024-07-05T23:18:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.08880v1",
        "title": "Semiparametrically Optimal Cointegration Test",
        "abstract": "This paper aims to address the issue of semiparametric efficiency for\ncointegration rank testing in finite-order vector autoregressive models, where\nthe innovation distribution is considered an infinite-dimensional nuisance\nparameter. Our asymptotic analysis relies on Le Cam's theory of limit\nexperiment, which in this context takes the form of Locally Asymptotically\nBrownian Functional (LABF). By leveraging the structural version of LABF, an\nOrnstein-Uhlenbeck experiment, we develop the asymptotic power envelopes of\nasymptotically invariant tests for both cases with and without a time trend. We\npropose feasible tests based on a nonparametrically estimated density and\ndemonstrate that their power can achieve the semiparametric power envelopes,\nmaking them semiparametrically optimal. We validate the theoretical results\nthrough large-sample simulations and illustrate satisfactory size control and\nexcellent power performance of our tests under small samples. In both cases\nwith and without time trend, we show that a remarkable amount of additional\npower can be obtained from non-Gaussian distributions.",
        "authors": [
            "Bo Zhou"
        ],
        "categories": "econ.EM",
        "published": "2023-05-13T15:44:09Z",
        "updated": "2023-05-13T15:44:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.06618v1",
        "title": "Band-Pass Filtering with High-Dimensional Time Series",
        "abstract": "The paper deals with the construction of a synthetic indicator of economic\ngrowth, obtained by projecting a quarterly measure of aggregate economic\nactivity, namely gross domestic product (GDP), into the space spanned by a\nfinite number of smooth principal components, representative of the\nmedium-to-long-run component of economic growth of a high-dimensional time\nseries, available at the monthly frequency. The smooth principal components\nresult from applying a cross-sectional filter distilling the low-pass component\nof growth in real time. The outcome of the projection is a monthly nowcast of\nthe medium-to-long-run component of GDP growth. After discussing the\ntheoretical properties of the indicator, we deal with the assessment of its\nreliability and predictive validity with reference to a panel of macroeconomic\nU.S. time series.",
        "authors": [
            "Alessandro Giovannelli",
            "Marco Lippi",
            "Tommaso Proietti"
        ],
        "categories": "econ.EM",
        "published": "2023-05-11T07:28:30Z",
        "updated": "2023-05-11T07:28:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.06076v1",
        "title": "The price elasticity of Gleevec in patients with Chronic Myeloid Leukemia enrolled in Medicare Part D: Evidence from a regression discontinuity design",
        "abstract": "Objective To assess the price elasticity of branded imatinib in chronic\nmyeloid leukemia (CML) patients on Medicare Part D to determine if high\nout-of-pocket payments (OOP) are driving the substantial levels of\nnon-adherence observed in this population.\n  Data sources and study setting We use data from the TriNetX Diamond Network\n(TDN) United States database for the period from first availability in 2011\nthrough the end of patent exclusivity following the introduction of generic\nimatinib in early 2016.\n  Study design We implement a fuzzy regression discontinuity design to\nseparately estimate the effect of Medicare Part D enrollment at age 65 on\nadherence and OOP in newly-diagnosed CML patients initiating branded imatinib.\nThe corresponding price elasticity of demand (PED) is estimated and results are\nassessed across a variety of specifications and robustness checks.\n  Data collection/extraction methods Data from eligible patients following the\napplication of inclusion and exclusion criteria were analyzed.\n  Principal findings Our analysis suggests that there is a significant increase\nin initial OOP of $232 (95% Confidence interval (CI): $102 to $362) for\nindividuals that enrolled in Part D due to expanded eligibility at age 65. The\nrelatively smaller and non-significant decrease in adherence of only 6\npercentage points (95% CI: -0.21 to 0.08) led to a PED of -0.02 (95% CI:\n-0.056, 0.015).\n  Conclusion This study provides evidence regarding the financial impact of\ncoinsurance-based benefit designs on Medicare-age patients with CML initiating\nbranded imatinib. Results indicate that factors besides high OOP are driving\nthe substantial non-adherence observed in this population and add to the\ngrowing literature on PED for specialty drugs.",
        "authors": [
            "Samantha E. Clark",
            "Ruth Etzioni",
            "Jerry Radich",
            "Zachary Marcum",
            "Anirban Basu"
        ],
        "categories": "econ.EM",
        "published": "2023-05-10T11:56:09Z",
        "updated": "2023-05-10T11:56:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.05998v4",
        "title": "On the Time-Varying Structure of the Arbitrage Pricing Theory using the Japanese Sector Indices",
        "abstract": "This paper is the first study to examine the time instability of the APT in\nthe Japanese stock market. In particular, we measure how changes in each risk\nfactor affect the stock risk premiums to investigate the validity of the APT\nover time, applying the rolling window method to Fama and MacBeth's (1973)\ntwo-step regression and Kamstra and Shi's (2023) generalized GRS test. We\nsummarize our empirical results as follows: (1) the changes in monetary policy\nby major central banks greatly affect the validity of the APT in Japan, and (2)\nthe time-varying estimates of the risk premiums for each factor are also\nunstable over time, and they are affected by the business cycle and economic\ncrises. Therefore, we conclude that the validity of the APT as an appropriate\nmodel to explain the Japanese sector index is not stable over time.",
        "authors": [
            "Koichiro Moriya",
            "Akihiko Noda"
        ],
        "categories": "q-fin.ST",
        "published": "2023-05-10T09:15:14Z",
        "updated": "2024-03-15T00:30:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.05934v2",
        "title": "Does Principal Component Analysis Preserve the Sparsity in Sparse Weak Factor Models?",
        "abstract": "This paper studies the principal component (PC) method-based estimation of\nweak factor models with sparse loadings. We uncover an intrinsic near-sparsity\npreservation property for the PC estimators of loadings, which comes from the\napproximately upper triangular (block) structure of the rotation matrix. It\nimplies an asymmetric relationship among factors: the rotated loadings for a\nstronger factor can be contaminated by those from a weaker one, but the\nloadings for a weaker factor is almost free of the impact of those from a\nstronger one. More importantly, the finding implies that there is no need to\nuse complicated penalties to sparsify the loading estimators. Instead, we adopt\na simple screening method to recover the sparsity and construct estimators for\nvarious factor strengths. In addition, for sparse weak factor models, we\nprovide a singular value thresholding-based approach to determine the number of\nfactors and establish uniform convergence rates for PC estimators, which\ncomplement Bai and Ng (2023). The accuracy and efficiency of the proposed\nestimators are investigated via Monte Carlo simulations. The application to the\nFRED-QD dataset reveals the underlying factor strengths and loading sparsity as\nwell as their dynamic features.",
        "authors": [
            "Jie Wei",
            "Yonghui Zhang"
        ],
        "categories": "econ.EM",
        "published": "2023-05-10T07:10:24Z",
        "updated": "2024-11-07T11:35:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.04137v2",
        "title": "Volatility of Volatility and Leverage Effect from Options",
        "abstract": "We propose model-free (nonparametric) estimators of the volatility of\nvolatility and leverage effect using high-frequency observations of short-dated\noptions. At each point in time, we integrate available options into estimates\nof the conditional characteristic function of the price increment until the\noptions' expiration and we use these estimates to recover spot volatility. Our\nvolatility of volatility estimator is then formed from the sample variance and\nfirst-order autocovariance of the spot volatility increments, with the latter\ncorrecting for the bias in the former due to option observation errors. The\nleverage effect estimator is the sample covariance between price increments and\nthe estimated volatility increments. The rate of convergence of the estimators\ndepends on the diffusive innovations in the latent volatility process as well\nas on the observation error in the options with strikes in the vicinity of the\ncurrent spot price. Feasible inference is developed in a way that does not\nrequire prior knowledge of the source of estimation error that is\nasymptotically dominating.",
        "authors": [
            "Carsten H. Chong",
            "Viktor Todorov"
        ],
        "categories": "econ.EM",
        "published": "2023-05-06T21:33:06Z",
        "updated": "2024-01-23T04:57:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.03205v2",
        "title": "Risk management in the use of published statistical results for policy decisions",
        "abstract": "Statistical inferential results generally come with a measure of reliability\nfor decision-making purposes. For a policy implementer, the value of\nimplementing published policy research depends critically upon this\nreliability. For a policy researcher, the value of policy implementation may\ndepend weakly or not at all upon the policy's outcome. Some researchers might\nbenefit from overstating the reliability of statistical results. Implementers\nmay find it difficult or impossible to determine whether researchers are\noverstating reliability. This information asymmetry between researchers and\nimplementers can lead to an adverse selection problem where, at best, the full\nbenefits of a policy are not realized or, at worst, a policy is deemed too\nrisky to implement at any scale. Researchers can remedy this by guaranteeing\nthe policy outcome. Researchers can overcome their own risk aversion and wealth\nconstraints by exchanging risks with other researchers or offering only partial\ninsurance. The problem and remedy are illustrated using a confidence interval\nfor the success probability of a binomial policy outcome.",
        "authors": [
            "Duncan Ermini Leaf"
        ],
        "categories": "stat.OT",
        "published": "2023-05-04T23:28:34Z",
        "updated": "2024-08-20T04:30:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.03134v3",
        "title": "Debiased Inference for Dynamic Nonlinear Panels with Multi-dimensional Heterogeneities",
        "abstract": "We introduce a generic class of dynamic nonlinear heterogeneous parameter\nmodels that incorporate individual and time effects in both the intercept and\nslope. To address the incidental parameter problem inherent in this class of\nmodels, we develop an analytical bias correction procedure to construct a\nbias-corrected likelihood. The resulting maximum likelihood estimators are\nautomatically bias-corrected. Moreover, likelihood-based tests statistics --\nincluding likelihood-ratio, Lagrange-multiplier, and Wald tests -- follow the\nlimiting chi-square distribution under the null hypothesis. Simulations\ndemonstrate the effectiveness of the proposed correction method, and an\nempirical application on the labor force participation of single mothers\nunderscores its practical importance.",
        "authors": [
            "Xuan Leng",
            "Jiaming Mao",
            "Yutao Sun"
        ],
        "categories": "econ.EM",
        "published": "2023-05-04T20:29:48Z",
        "updated": "2024-11-22T07:16:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.02185v2",
        "title": "Doubly Robust Uniform Confidence Bands for Group-Time Conditional Average Treatment Effects in Difference-in-Differences",
        "abstract": "We consider a panel data analysis to examine the heterogeneity in treatment\neffects with respect to a pre-treatment covariate of interest in the staggered\ndifference-in-differences setting of Callaway and Sant'Anna (2021). Under\nstandard identification conditions, a doubly robust estimand conditional on the\ncovariate identifies the group-time conditional average treatment effect given\nthe covariate. Focusing on the case of a continuous covariate, we propose a\nthree-step estimation procedure based on nonparametric local polynomial\nregressions and parametric estimation methods. Using uniformly valid\ndistributional approximation results for empirical processes and multiplier\nbootstrapping, we develop doubly robust inference methods to construct uniform\nconfidence bands for the group-time conditional average treatment effect\nfunction. The accompanying R package didhetero allows for easy implementation\nof the proposed methods.",
        "authors": [
            "Shunsuke Imai",
            "Lei Qin",
            "Takahide Yanagi"
        ],
        "categories": "econ.EM",
        "published": "2023-05-03T15:29:22Z",
        "updated": "2023-11-29T00:43:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.01464v3",
        "title": "Large Global Volatility Matrix Analysis Based on Observation Structural Information",
        "abstract": "In this paper, we develop a novel large volatility matrix estimation\nprocedure for analyzing global financial markets. Practitioners often use\nlower-frequency data, such as weekly or monthly returns, to address the issue\nof different trading hours in the international financial market. However, this\napproach can lead to inefficiency due to information loss. To mitigate this\nproblem, our proposed method, called Structured Principal Orthogonal complEment\nThresholding (Structured-POET), incorporates observation structural information\nfor both global and national factor models. We establish the asymptotic\nproperties of the Structured-POET estimator, and also demonstrate the drawbacks\nof conventional covariance matrix estimation procedures when using\nlower-frequency data. Finally, we apply the Structured-POET estimator to an\nout-of-sample portfolio allocation study using international stock market data.",
        "authors": [
            "Sung Hoon Choi",
            "Donggyu Kim"
        ],
        "categories": "econ.EM",
        "published": "2023-05-02T14:46:02Z",
        "updated": "2024-02-20T16:50:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.01435v6",
        "title": "Transfer Estimates for Causal Effects across Heterogeneous Sites",
        "abstract": "We consider the problem of extrapolating treatment effects across\nheterogeneous populations (``sites\"/``contexts\"). We consider an idealized\nscenario in which the researcher observes cross-sectional data for a large\nnumber of units across several ``experimental\" sites in which an intervention\nhas already been implemented to a new ``target\" site for which a baseline\nsurvey of unit-specific, pre-treatment outcomes and relevant attributes is\navailable. Our approach treats the baseline as functional data, and this choice\nis motivated by the observation that unobserved site-specific confounders\nmanifest themselves not only in average levels of outcomes, but also how these\ninteract with observed unit-specific attributes. We consider the problem of\ndetermining the optimal finite-dimensional feature space in which to solve that\nprediction problem. Our approach is design-based in the sense that the\nperformance of the predictor is evaluated given the specific, finite selection\nof experimental and target sites. Our approach is nonparametric, and our formal\nresults concern the construction of an optimal basis of predictors as well as\nconvergence rates for the estimated conditional average treatment effect\nrelative to the constrained-optimal population predictor for the target site.\nWe quantify the potential gains from adapting experimental estimates to a\ntarget location in an application to conditional cash transfer (CCT) programs\nusing a combined data set from five multi-site randomized controlled trials.",
        "authors": [
            "Konrad Menzel"
        ],
        "categories": "econ.EM",
        "published": "2023-05-02T14:02:15Z",
        "updated": "2024-05-21T11:22:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.01201v3",
        "title": "Estimating Input Coefficients for Regional Input-Output Tables Using Deep Learning with Mixup",
        "abstract": "An input-output table is an important data for analyzing the economic\nsituation of a region. Generally, the input-output table for each region\n(regional input-output table) in Japan is not always publicly available, so it\nis necessary to estimate the table. In particular, various methods have been\ndeveloped for estimating input coefficients, which are an important part of the\ninput-output table. Currently, non-survey methods are often used to estimate\ninput coefficients because they require less data and computation, but these\nmethods have some problems, such as discarding information and requiring\nadditional data for estimation.\n  In this study, the input coefficients are estimated by approximating the\ngeneration process with an artificial neural network (ANN) to mitigate the\nproblems of the non-survey methods and to estimate the input coefficients with\nhigher precision. To avoid over-fitting due to the small data used, data\naugmentation, called mixup, is introduced to increase the data size by\ngenerating virtual regions through region composition and scaling.\n  By comparing the estimates of the input coefficients with those of Japan as a\nwhole, it is shown that the accuracy of the method of this research is higher\nand more stable than that of the conventional non-survey methods. In addition,\nthe estimated input coefficients for the three cities in Japan are generally\nclose to the published values for each city.",
        "authors": [
            "Shogo Fukui"
        ],
        "categories": "econ.EM",
        "published": "2023-05-02T04:34:09Z",
        "updated": "2024-06-14T17:14:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.00860v3",
        "title": "Estimation and Inference in Threshold Predictive Regression Models with Locally Explosive Regressors",
        "abstract": "In this paper, we study the estimation of the threshold predictive regression\nmodel with hybrid stochastic local unit root predictors. We demonstrate the\nestimation procedure and derive the asymptotic distribution of the least square\nestimator and the IV based estimator proposed by Magdalinos and Phillips\n(2009), under the null hypothesis of a diminishing threshold effect. Simulation\nexperiments focus on the finite sample performance of our proposed estimators\nand the corresponding predictability tests as in Gonzalo and Pitarakis (2012),\nunder the presence of threshold effects with stochastic local unit roots. An\nempirical application to stock return equity indices, illustrate the usefulness\nof our framework in uncovering regimes of predictability during certain\nperiods. In particular, we focus on an aspect not previously examined in the\npredictability literature, that is, the effect of economic policy uncertainty.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-05-01T15:00:02Z",
        "updated": "2023-05-14T18:22:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.00700v3",
        "title": "Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control",
        "abstract": "Motivated by a recent literature on the double-descent phenomenon in machine\nlearning, we consider highly over-parameterized models in causal inference,\nincluding synthetic control with many control units. In such models, there may\nbe so many free parameters that the model fits the training data perfectly. We\nfirst investigate high-dimensional linear regression for imputing wage data and\nestimating average treatment effects, where we find that models with many more\ncovariates than sample size can outperform simple ones. We then document the\nperformance of high-dimensional synthetic control estimators with many control\nunits. We find that adding control units can help improve imputation\nperformance even beyond the point where the pre-treatment fit is perfect. We\nprovide a unified theoretical perspective on the performance of these\nhigh-dimensional models. Specifically, we show that more complex models can be\ninterpreted as model-averaging estimators over simpler ones, which we link to\nan improvement in average performance. This perspective yields concrete\ninsights into the use of synthetic control when control units are many relative\nto the number of pre-treatment periods.",
        "authors": [
            "Jann Spiess",
            "Guido Imbens",
            "Amar Venugopal"
        ],
        "categories": "econ.EM",
        "published": "2023-05-01T07:54:53Z",
        "updated": "2023-10-12T21:25:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2305.00403v2",
        "title": "Optimal tests following sequential experiments",
        "abstract": "Recent years have seen tremendous advances in the theory and application of\nsequential experiments. While these experiments are not always designed with\nhypothesis testing in mind, researchers may still be interested in performing\ntests after the experiment is completed. The purpose of this paper is to aid in\nthe development of optimal tests for sequential experiments by analyzing their\nasymptotic properties. Our key finding is that the asymptotic power function of\nany test can be matched by a test in a limit experiment where a Gaussian\nprocess is observed for each treatment, and inference is made for the drifts of\nthese processes. This result has important implications, including a powerful\nsufficiency result: any candidate test only needs to rely on a fixed set of\nstatistics, regardless of the type of sequential experiment. These statistics\nare the number of times each treatment has been sampled by the end of the\nexperiment, along with final value of the score (for parametric models) or\nefficient influence function (for non-parametric models) process for each\ntreatment. We then characterize asymptotically optimal tests under various\nrestrictions such as unbiasedness, \\alpha-spending constraints etc. Finally, we\napply our our results to three key classes of sequential experiments: costly\nsampling, group sequential trials, and bandit experiments, and show how optimal\ninference can be conducted in these scenarios.",
        "authors": [
            "Karun Adusumilli"
        ],
        "categories": "econ.EM",
        "published": "2023-04-30T06:09:49Z",
        "updated": "2023-06-28T14:21:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.14545v3",
        "title": "Augmented balancing weights as linear regression",
        "abstract": "We provide a novel characterization of augmented balancing weights, also\nknown as automatic debiased machine learning (AutoDML). These popular doubly\nrobust or de-biased machine learning estimators combine outcome modeling with\nbalancing weights - weights that achieve covariate balance directly in lieu of\nestimating and inverting the propensity score. When the outcome and weighting\nmodels are both linear in some (possibly infinite) basis, we show that the\naugmented estimator is equivalent to a single linear model with coefficients\nthat combine the coefficients from the original outcome model and coefficients\nfrom an unpenalized ordinary least squares (OLS) fit on the same data. We see\nthat, under certain choices of regularization parameters, the augmented\nestimator often collapses to the OLS estimator alone; this occurs for example\nin a re-analysis of the Lalonde 1986 dataset. We then extend these results to\nspecific choices of outcome and weighting models. We first show that the\naugmented estimator that uses (kernel) ridge regression for both outcome and\nweighting models is equivalent to a single, undersmoothed (kernel) ridge\nregression. This holds numerically in finite samples and lays the groundwork\nfor a novel analysis of undersmoothing and asymptotic rates of convergence.\nWhen the weighting model is instead lasso-penalized regression, we give\nclosed-form expressions for special cases and demonstrate a ``double\nselection'' property. Our framework opens the black box on this increasingly\npopular class of estimators, bridges the gap between existing results on the\nsemiparametric efficiency of undersmoothed and doubly robust estimators, and\nprovides new insights into the performance of augmented balancing weights.",
        "authors": [
            "David Bruns-Smith",
            "Oliver Dukes",
            "Avi Feller",
            "Elizabeth L. Ogburn"
        ],
        "categories": "stat.ME",
        "published": "2023-04-27T21:53:54Z",
        "updated": "2024-06-05T22:53:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.14544v1",
        "title": "Assessing Text Mining and Technical Analyses on Forecasting Financial Time Series",
        "abstract": "Forecasting financial time series (FTS) is an essential field in finance and\neconomics that anticipates market movements in financial markets. This paper\ninvestigates the accuracy of text mining and technical analyses in forecasting\nfinancial time series. It focuses on the S&P500 stock market index during the\npandemic, which tracks the performance of the largest publicly traded companies\nin the US. The study compares two methods of forecasting the future price of\nthe S&P500: text mining, which uses NLP techniques to extract meaningful\ninsights from financial news, and technical analysis, which uses historical\nprice and volume data to make predictions. The study examines the advantages\nand limitations of both methods and analyze their performance in predicting the\nS&P500. The FinBERT model outperforms other models in terms of S&P500 price\nprediction, as evidenced by its lower RMSE value, and has the potential to\nrevolutionize financial analysis and prediction using financial news data.\nKeywords: ARIMA, BERT, FinBERT, Forecasting Financial Time Series, GARCH, LSTM,\nTechnical Analysis, Text Mining JEL classifications: G4, C8",
        "authors": [
            "Ali Lashgari"
        ],
        "categories": "econ.EM",
        "published": "2023-04-27T21:52:36Z",
        "updated": "2023-04-27T21:52:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.14386v1",
        "title": "Convexity Not Required: Estimation of Smooth Moment Condition Models",
        "abstract": "Generalized and Simulated Method of Moments are often used to estimate\nstructural Economic models. Yet, it is commonly reported that optimization is\nchallenging because the corresponding objective function is non-convex. For\nsmooth problems, this paper shows that convexity is not required: under a\nglobal rank condition involving the Jacobian of the sample moments, certain\nalgorithms are globally convergent. These include a gradient-descent and a\nGauss-Newton algorithm with appropriate choice of tuning parameters. The\nresults are robust to 1) non-convexity, 2) one-to-one non-linear\nreparameterizations, and 3) moderate misspecification. In contrast,\nNewton-Raphson and quasi-Newton methods can fail to converge for the same\nestimation because of non-convexity. A simple example illustrates a non-convex\nGMM estimation problem that satisfies the aforementioned rank condition.\nEmpirical applications to random coefficient demand estimation and impulse\nresponse matching further illustrate the results.",
        "authors": [
            "Jean-Jacques Forneron",
            "Liang Zhong"
        ],
        "categories": "econ.EM",
        "published": "2023-04-27T17:52:41Z",
        "updated": "2023-04-27T17:52:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.13934v1",
        "title": "A universal model for the Lorenz curve with novel applications for datasets containing zeros and/or exhibiting extreme inequality",
        "abstract": "Given that the existing parametric functional forms for the Lorenz curve do\nnot fit all possible size distributions, a universal parametric functional form\nis introduced. By using the empirical data from different scientific\ndisciplines and also the hypothetical data, this study shows that, the proposed\nmodel fits not only the data whose actual Lorenz plots have a typical convex\nsegment but also the data whose actual Lorenz plots have both horizontal and\nconvex segments practically well. It also perfectly fits the data whose\nobservation is larger in size while the rest of observations are smaller and\nequal in size as characterized by 2 positive-slope linear segments. In\naddition, the proposed model has a closed-form expression for the Gini index,\nmaking it computationally convenient to calculate. Considering that the Lorenz\ncurve and the Gini index are widely used in various disciplines of sciences,\nthe proposed model and the closed-form expression for the Gini index could be\nused as alternative tools to analyze size distributions of non-negative\nquantities and examine their inequalities or unevennesses.",
        "authors": [
            "Thitithep Sitthiyot",
            "Kanyarat Holasut"
        ],
        "categories": "physics.data-an",
        "published": "2023-04-27T03:04:05Z",
        "updated": "2023-04-27T03:04:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.13925v1",
        "title": "Difference-in-Differences with Compositional Changes",
        "abstract": "This paper studies difference-in-differences (DiD) setups with repeated\ncross-sectional data and potential compositional changes across time periods.\nWe begin our analysis by deriving the efficient influence function and the\nsemiparametric efficiency bound for the average treatment effect on the treated\n(ATT). We introduce nonparametric estimators that attain the semiparametric\nefficiency bound under mild rate conditions on the estimators of the nuisance\nfunctions, exhibiting a type of rate doubly-robust (DR) property. Additionally,\nwe document a trade-off related to compositional changes: We derive the\nasymptotic bias of DR DiD estimators that erroneously exclude compositional\nchanges and the efficiency loss when one fails to correctly rule out\ncompositional changes. We propose a nonparametric Hausman-type test for\ncompositional changes based on these trade-offs. The finite sample performance\nof the proposed DiD tools is evaluated through Monte Carlo experiments and an\nempirical application. As a by-product of our analysis, we present a new\nuniform stochastic expansion of the local polynomial multinomial logit\nestimator, which may be of independent interest.",
        "authors": [
            "Pedro H. C. Sant'Anna",
            "Qi Xu"
        ],
        "categories": "econ.EM",
        "published": "2023-04-27T02:30:24Z",
        "updated": "2023-04-27T02:30:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.13206v1",
        "title": "Estimation of Characteristics-based Quantile Factor Models",
        "abstract": "This paper studies the estimation of characteristic-based quantile factor\nmodels where the factor loadings are unknown functions of observed individual\ncharacteristics while the idiosyncratic error terms are subject to conditional\nquantile restrictions. We propose a three-stage estimation procedure that is\neasily implementable in practice and has nice properties. The convergence\nrates, the limiting distributions of the estimated factors and loading\nfunctions, and a consistent selection criterion for the number of factors at\neach quantile are derived under general conditions. The proposed estimation\nmethodology is shown to work satisfactorily when: (i) the idiosyncratic errors\nhave heavy tails, (ii) the time dimension of the panel dataset is not large,\nand (iii) the number of factors exceeds the number of characteristics. Finite\nsample simulations and an empirical application aimed at estimating the loading\nfunctions of the daily returns of a large panel of S\\&P500 index securities\nhelp illustrate these properties.",
        "authors": [
            "Liang Chen",
            "Juan Jose Dolado",
            "Jesus Gonzalo",
            "Haozi Pan"
        ],
        "categories": "econ.EM",
        "published": "2023-04-26T00:16:58Z",
        "updated": "2023-04-26T00:16:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.13199v1",
        "title": "Common Correlated Effects Estimation of Nonlinear Panel Data Models",
        "abstract": "This paper focuses on estimating the coefficients and average partial effects\nof observed regressors in nonlinear panel data models with interactive fixed\neffects, using the common correlated effects (CCE) framework. The proposed\ntwo-step estimation method involves applying principal component analysis to\nestimate latent factors based on cross-sectional averages of the regressors in\nthe first step, and jointly estimating the coefficients of the regressors and\nfactor loadings in the second step. The asymptotic distributions of the\nproposed estimators are derived under general conditions, assuming that the\nnumber of time-series observations is comparable to the number of\ncross-sectional observations. To correct for asymptotic biases of the\nestimators, we introduce both analytical and split-panel jackknife methods, and\nconfirm their good performance in finite samples using Monte Carlo simulations.\nAn empirical application utilizes the proposed method to study the arbitrage\nbehaviour of nonfinancial firms across different security markets.",
        "authors": [
            "Liang Chen",
            "Minyuan Zhang"
        ],
        "categories": "econ.EM",
        "published": "2023-04-25T23:55:25Z",
        "updated": "2023-04-25T23:55:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.12698v2",
        "title": "Enhanced multilayer perceptron with feature selection and grid search for travel mode choice prediction",
        "abstract": "Accurate and reliable prediction of individual travel mode choices is crucial\nfor developing multi-mode urban transportation systems, conducting\ntransportation planning and formulating traffic demand management strategies.\nTraditional discrete choice models have dominated the modelling methods for\ndecades yet suffer from strict model assumptions and low prediction accuracy.\nIn recent years, machine learning (ML) models, such as neural networks and\nboosting models, are widely used by researchers for travel mode choice\nprediction and have yielded promising results. However, despite the superior\nprediction performance, a large body of ML methods, especially the branch of\nneural network models, is also limited by overfitting and tedious model\nstructure determination process. To bridge this gap, this study proposes an\nenhanced multilayer perceptron (MLP; a neural network) with two hidden layers\nfor travel mode choice prediction; this MLP is enhanced by XGBoost (a boosting\nmethod) for feature selection and a grid search method for optimal hidden\nneurone determination of each hidden layer. The proposed method was trained and\ntested on a real resident travel diary dataset collected in Chengdu, China.",
        "authors": [
            "Li Tang",
            "Chuanli Tang",
            "Qi Fu"
        ],
        "categories": "econ.EM",
        "published": "2023-04-25T10:05:30Z",
        "updated": "2023-10-22T11:27:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.12554v1",
        "title": "The Ordinary Least Eigenvalues Estimator",
        "abstract": "We propose a rate optimal estimator for the linear regression model on\nnetwork data with interacted (unobservable) individual effects. The estimator\nachieves a faster rate of convergence $N$ compared to the standard estimators'\n$\\sqrt{N}$ rate and is efficient in cases that we discuss. We observe that the\nindividual effects alter the eigenvalue distribution of the data's matrix\nrepresentation in significant and distinctive ways. We subsequently offer a\ncorrection for the \\textit{ordinary least squares}' objective function to\nattenuate the statistical noise that arises due to the individual effects, and\nin some cases, completely eliminate it. The new estimator is asymptotically\nnormal and we provide a valid estimator for its asymptotic covariance matrix.\nWhile this paper only considers models accounting for first-order interactions\nbetween individual effects, our estimation procedure is naturally extendable to\nhigher-order interactions and more general specifications of the error terms.",
        "authors": [
            "Yassine Sbai Sassi"
        ],
        "categories": "econ.EM",
        "published": "2023-04-25T03:42:27Z",
        "updated": "2023-04-25T03:42:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.12134v2",
        "title": "Determination of the effective cointegration rank in high-dimensional time-series predictive regressions",
        "abstract": "This paper proposes a new approach to identifying the effective cointegration\nrank in high-dimensional unit-root (HDUR) time series from a prediction\nperspective using reduced-rank regression. For a HDUR process $\\mathbf{x}_t\\in\n\\mathbb{R}^N$ and a stationary series $\\mathbf{y}_t\\in \\mathbb{R}^p$ of\ninterest, our goal is to predict future values of $\\mathbf{y}_t$ using\n$\\mathbf{x}_t$ and lagged values of $\\mathbf{y}_t$. The proposed framework\nconsists of a two-step estimation procedure. First, the Principal Component\nAnalysis is used to identify all cointegrating vectors of $\\mathbf{x}_t$.\nSecond, the co-integrated stationary series are used as regressors, together\nwith some lagged variables of $\\mathbf{y}_t$, to predict $\\mathbf{y}_t$. The\nestimated reduced rank is then defined as the effective cointegration rank of\n$\\mathbf{x}_t$. Under the scenario that the autoregressive coefficient matrices\nare sparse (or of low-rank), we apply the Least Absolute Shrinkage and\nSelection Operator (or the reduced-rank techniques) to estimate the\nautoregressive coefficients when the dimension involved is high. Theoretical\nproperties of the estimators are established under the assumptions that the\ndimensions $p$ and $N$ and the sample size $T \\to \\infty$. Both simulated and\nreal examples are used to illustrate the proposed framework, and the empirical\napplication suggests that the proposed procedure fares well in predicting stock\nreturns.",
        "authors": [
            "Puyi Fang",
            "Zhaoxing Gao",
            "Ruey S. Tsay"
        ],
        "categories": "econ.EM",
        "published": "2023-04-24T14:41:34Z",
        "updated": "2023-04-25T03:45:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.11735v1",
        "title": "Policy Learning under Biased Sample Selection",
        "abstract": "Practitioners often use data from a randomized controlled trial to learn a\ntreatment assignment policy that can be deployed on a target population. A\nrecurring concern in doing so is that, even if the randomized trial was\nwell-executed (i.e., internal validity holds), the study participants may not\nrepresent a random sample of the target population (i.e., external validity\nfails)--and this may lead to policies that perform suboptimally on the target\npopulation. We consider a model where observable attributes can impact sample\nselection probabilities arbitrarily but the effect of unobservable attributes\nis bounded by a constant, and we aim to learn policies with the best possible\nperformance guarantees that hold under any sampling bias of this type. In\nparticular, we derive the partial identification result for the worst-case\nwelfare in the presence of sampling bias and show that the optimal max-min,\nmax-min gain, and minimax regret policies depend on both the conditional\naverage treatment effect (CATE) and the conditional value-at-risk (CVaR) of\npotential outcomes given covariates. To avoid finite-sample inefficiencies of\nplug-in estimates, we further provide an end-to-end procedure for learning the\noptimal max-min and max-min gain policies that does not require the separate\nestimation of nuisance parameters.",
        "authors": [
            "Lihua Lei",
            "Roshni Sahoo",
            "Stefan Wager"
        ],
        "categories": "econ.EM",
        "published": "2023-04-23T19:54:17Z",
        "updated": "2023-04-23T19:54:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.09775v1",
        "title": "The Impact of Industrial Zone:Evidence from China's National High-tech Zone Policy",
        "abstract": "Based on the statistical yearbook data and related patent data of 287 cities\nin China from 2000 to 2020, this study regards the policy of establishing the\nnational high-tech zones as a quasi-natural experiment. Using this experiment,\nthis study firstly estimated the treatment effect of the policy and checked the\nrobustness of the estimation. Then the study examined the heterogeneity in\ndifferent geographic demarcation of China and in different city level of China.\nAfter that, this study explored the possible influence mechanism of the policy.\nIt shows that the possible mechanism of the policy is financial support,\nindustrial agglomeration of secondary industry and the spillovers. In the end,\nthis study examined the spillovers deeply and showed the distribution of\nspillover effect.",
        "authors": [
            "Li Han"
        ],
        "categories": "econ.EM",
        "published": "2023-04-19T15:58:15Z",
        "updated": "2023-04-19T15:58:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.09336v1",
        "title": "A hybrid model for day-ahead electricity price forecasting: Combining fundamental and stochastic modelling",
        "abstract": "The accurate prediction of short-term electricity prices is vital for\neffective trading strategies, power plant scheduling, profit maximisation and\nefficient system operation. However, uncertainties in supply and demand make\nsuch predictions challenging. We propose a hybrid model that combines a\ntechno-economic energy system model with stochastic models to address this\nchallenge. The techno-economic model in our hybrid approach provides a deep\nunderstanding of the market. It captures the underlying factors and their\nimpacts on electricity prices, which is impossible with statistical models\nalone. The statistical models incorporate non-techno-economic aspects, such as\nthe expectations and speculative behaviour of market participants, through the\ninterpretation of prices. The hybrid model generates both conventional point\npredictions and probabilistic forecasts, providing a comprehensive\nunderstanding of the market landscape. Probabilistic forecasts are particularly\nvaluable because they account for market uncertainty, facilitating informed\ndecision-making and risk management. Our model delivers state-of-the-art\nresults, helping market participants to make informed decisions and operate\ntheir systems more efficiently.",
        "authors": [
            "Mira Watermeyer",
            "Thomas M\u00f6bius",
            "Oliver Grothe",
            "Felix M\u00fcsgens"
        ],
        "categories": "econ.EM",
        "published": "2023-04-18T22:53:47Z",
        "updated": "2023-04-18T22:53:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.09078v6",
        "title": "Club coefficients in the UEFA Champions League: Time for shift to an Elo-based formula",
        "abstract": "One of the most popular club football tournaments, the UEFA Champions League,\nwill see a fundamental reform from the 2024/25 season: the traditional group\nstage will be replaced by one league where each of the 36 teams plays eight\nmatches. To guarantee that the opponents of the clubs are of the same strength\nin the new design, it is crucial to forecast the performance of the teams\nbefore the tournament as well as possible. This paper investigates whether the\ncurrently used rating of the teams, the UEFA club coefficient, can be improved\nby taking the games played in the national leagues into account. According to\nour logistic regression models, a variant of the Elo method provides a higher\naccuracy in terms of explanatory power in the Champions League matches. The\nUnion of European Football Associations (UEFA) is encouraged to follow the\nexample of the FIFA World Ranking and reform the calculation of the club\ncoefficients in order to avoid unbalanced schedules in the novel tournament\nformat of the Champions League.",
        "authors": [
            "L\u00e1szl\u00f3 Csat\u00f3"
        ],
        "categories": "stat.AP",
        "published": "2023-04-18T15:51:13Z",
        "updated": "2023-10-31T15:09:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.08974v2",
        "title": "Doubly Robust Estimators with Weak Overlap",
        "abstract": "In this paper, we derive a new class of doubly robust estimators for\ntreatment effect estimands that is also robust against weak covariate overlap.\nOur proposed estimator relies on trimming observations with extreme propensity\nscores and uses a bias correction device for trimming bias. Our framework\naccommodates many research designs, such as unconfoundedness, local treatment\neffects, and difference-in-differences. Simulation exercises illustrate that\nour proposed tools indeed have attractive finite sample properties, which are\naligned with our theoretical asymptotic results.",
        "authors": [
            "Yukun Ma",
            "Pedro H. C. Sant'Anna",
            "Yuya Sasaki",
            "Takuya Ura"
        ],
        "categories": "econ.EM",
        "published": "2023-04-18T13:13:11Z",
        "updated": "2023-04-22T11:28:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.08184v4",
        "title": "Adjustment with Many Regressors Under Covariate-Adaptive Randomizations",
        "abstract": "Our paper discovers a new trade-off of using regression adjustments (RAs) in\ncausal inference under covariate-adaptive randomizations (CARs). On one hand,\nRAs can improve the efficiency of causal estimators by incorporating\ninformation from covariates that are not used in the randomization. On the\nother hand, RAs can degrade estimation efficiency due to their estimation\nerrors, which are not asymptotically negligible when the number of regressors\nis of the same order as the sample size. Ignoring the estimation errors of RAs\nmay result in serious over-rejection of causal inference under the null\nhypothesis. To address the issue, we construct a new ATE estimator by optimally\nlinearly combining the estimators with and without RAs. We then develop a\nunified inference theory for this estimator under CARs. It has two features:\n(1) the Wald test based on it achieves the exact asymptotic size under the null\nhypothesis, regardless of whether the number of covariates is fixed or diverges\nno faster than the sample size; and (2) it guarantees weak efficiency\nimprovement over estimators both with and without RAs.",
        "authors": [
            "Liang Jiang",
            "Liyao Li",
            "Ke Miao",
            "Yichong Zhang"
        ],
        "categories": "econ.EM",
        "published": "2023-04-17T11:50:35Z",
        "updated": "2024-11-20T02:31:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.07856v2",
        "title": "Coarsened Bayesian VARs -- Correcting BVARs for Incorrect Specification",
        "abstract": "Model mis-specification in multivariate econometric models can strongly\ninfluence quantities of interest such as structural parameters, forecast\ndistributions or responses to structural shocks, even more so if higher-order\nforecasts or responses are considered, due to parameter convolution. We propose\na simple method for addressing these specification issues in the context of\nBayesian VARs. Our method, called coarsened Bayesian VARs (cBVARs), replaces\nthe exact likelihood with a coarsened likelihood that takes into account that\nthe model might be mis-specified along important but unknown dimensions.\nCoupled with a conjugate prior, this results in a computationally simple model.\nAs opposed to more flexible specifications, our approach avoids overfitting, is\nsimple to implement and estimation is fast. The resulting cBVAR performs well\nin simulations for several types of mis-specification. Applied to US data,\ncBVARs improve point and density forecasts compared to standard BVARs, and lead\nto milder but more persistent negative effects of uncertainty shocks on output.",
        "authors": [
            "Florian Huber",
            "Massimiliano Marcellino"
        ],
        "categories": "econ.EM",
        "published": "2023-04-16T18:40:52Z",
        "updated": "2023-05-26T11:14:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.07855v1",
        "title": "Penalized Likelihood Inference with Survey Data",
        "abstract": "This paper extends three Lasso inferential methods, Debiased Lasso,\n$C(\\alpha)$ and Selective Inference to a survey environment. We establish the\nasymptotic validity of the inference procedures in generalized linear models\nwith survey weights and/or heteroskedasticity. Moreover, we generalize the\nmethods to inference on nonlinear parameter functions e.g. the average marginal\neffect in survey logit models. We illustrate the effectiveness of the approach\nin simulated data and Canadian Internet Use Survey 2020 data.",
        "authors": [
            "Joann Jasiak",
            "Purevdorj Tuvaandorj"
        ],
        "categories": "econ.EM",
        "published": "2023-04-16T18:38:14Z",
        "updated": "2023-04-16T18:38:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.07480v3",
        "title": "Gini-stable Lorenz curves and their relation to the generalised Pareto distribution",
        "abstract": "We introduce an iterative discrete information production process where we\ncan extend ordered normalised vectors by new elements based on a simple affine\ntransformation, while preserving the predefined level of inequality, G, as\nmeasured by the Gini index.\n  Then, we derive the family of empirical Lorenz curves of the corresponding\nvectors and prove that it is stochastically ordered with respect to both the\nsample size and G which plays the role of the uncertainty parameter. We prove\nthat asymptotically, we obtain all, and only, Lorenz curves generated by a new,\nintuitive parametrisation of the finite-mean Pickands' Generalised Pareto\nDistribution (GPD) that unifies three other families, namely: the Pareto Type\nII, exponential, and scaled beta distributions. The family is not only totally\nordered with respect to the parameter G, but also, thanks to our derivations,\nhas a nice underlying interpretation. Our result may thus shed a new light on\nthe genesis of this family of distributions.\n  Our model fits bibliometric, informetric, socioeconomic, and environmental\ndata reasonably well. It is quite user-friendly for it only depends on the\nsample size and its Gini index.",
        "authors": [
            "Lucio Bertoli-Barsotti",
            "Marek Gagolewski",
            "Grzegorz Siudem",
            "Barbara \u017boga\u0142a-Siudem"
        ],
        "categories": "physics.soc-ph",
        "published": "2023-04-15T05:50:39Z",
        "updated": "2024-01-15T23:26:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.07479v1",
        "title": "Equivalence of inequality indices: Three dimensions of impact revisited",
        "abstract": "Inequality is an inherent part of our lives: we see it in the distribution of\nincomes, talents, resources, and citations, amongst many others. Its intensity\nvaries across different environments: from relatively evenly distributed ones,\nto where a small group of stakeholders controls the majority of the available\nresources. We would like to understand why inequality naturally arises as a\nconsequence of the natural evolution of any system. Studying simple\nmathematical models governed by intuitive assumptions can bring many insights\ninto this problem. In particular, we recently observed (Siudem et al., PNAS\n117:13896-13900, 2020) that impact distribution might be modelled accurately by\na time-dependent agent-based model involving a mixture of the rich-get-richer\nand sheer chance components. Here we point out its relationship to an iterative\nprocess that generates rank distributions of any length and a predefined level\nof inequality, as measured by the Gini index.\n  Many indices quantifying the degree of inequality have been proposed. Which\nof them is the most informative? We show that, under our model, indices such as\nthe Bonferroni, De Vergottini, and Hoover ones are equivalent. Given one of\nthem, we can recreate the value of any other measure using the derived\nfunctional relationships. Also, thanks to the obtained formulae, we can\nunderstand how they depend on the sample size. An empirical analysis of a large\nsample of citation records in economics (RePEc) as well as countrywise family\nincome data, confirms our theoretical observations. Therefore, we can safely\nand effectively remain faithful to the simplest measure: the Gini index.",
        "authors": [
            "Lucio Bertoli-Barsotti",
            "Marek Gagolewski",
            "Grzegorz Siudem",
            "Barbara \u017boga\u0142a-Siudem"
        ],
        "categories": "physics.soc-ph",
        "published": "2023-04-15T05:47:52Z",
        "updated": "2023-04-15T05:47:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.07331v1",
        "title": "Generalized Automatic Least Squares: Efficiency Gains from Misspecified Heteroscedasticity Models",
        "abstract": "It is well known that in the presence of heteroscedasticity ordinary least\nsquares estimator is not efficient. I propose a generalized automatic least\nsquares estimator (GALS) that makes partial correction of heteroscedasticity\nbased on a (potentially) misspecified model without a pretest. Such an\nestimator is guaranteed to be at least as efficient as either OLS or WLS but\ncan provide some asymptotic efficiency gains over OLS if the misspecified model\nis approximately correct. If the heteroscedasticity model is correct, the\nproposed estimator achieves full asymptotic efficiency. The idea is to frame\nmoment conditions corresponding to OLS and WLS squares based on miss-specified\nheteroscedasticity as a joint generalized method of moments estimation problem.\nThe resulting optimal GMM estimator is equivalent to a feasible GLS with\nestimated weight matrix. I also propose an optimal GMM variance-covariance\nestimator for GALS to account for any remaining heteroscedasticity in the\nresiduals.",
        "authors": [
            "Bulat Gafarov"
        ],
        "categories": "econ.EM",
        "published": "2023-04-14T18:05:30Z",
        "updated": "2023-04-14T18:05:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.07003v1",
        "title": "Detection and Estimation of Structural Breaks in High-Dimensional Functional Time Series",
        "abstract": "In this paper, we consider detecting and estimating breaks in heterogeneous\nmean functions of high-dimensional functional time series which are allowed to\nbe cross-sectionally correlated and temporally dependent. A new test statistic\ncombining the functional CUSUM statistic and power enhancement component is\nproposed with asymptotic null distribution theory comparable to the\nconventional CUSUM theory derived for a single functional time series. In\nparticular, the extra power enhancement component enlarges the region where the\nproposed test has power, and results in stable power performance when breaks\nare sparse in the alternative hypothesis. Furthermore, we impose a latent group\nstructure on the subjects with heterogeneous break points and introduce an\neasy-to-implement clustering algorithm with an information criterion to\nconsistently estimate the unknown group number and membership. The estimated\ngroup structure can subsequently improve the convergence property of the\npost-clustering break point estimate. Monte-Carlo simulation studies and\nempirical applications show that the proposed estimation and testing techniques\nhave satisfactory performance in finite samples.",
        "authors": [
            "Degui Li",
            "Runze Li",
            "Han Lin Shang"
        ],
        "categories": "stat.ME",
        "published": "2023-04-14T08:56:31Z",
        "updated": "2023-04-14T08:56:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.06828v1",
        "title": "Predictive Incrementality by Experimentation (PIE) for Ad Measurement",
        "abstract": "We present a novel approach to causal measurement for advertising, namely to\nuse exogenous variation in advertising exposure (RCTs) for a subset of ad\ncampaigns to build a model that can predict the causal effect of ad campaigns\nthat were run without RCTs. This approach -- Predictive Incrementality by\nExperimentation (PIE) -- frames the task of estimating the causal effect of an\nad campaign as a prediction problem, with the unit of observation being an RCT\nitself. In contrast, traditional causal inference approaches with observational\ndata seek to adjust covariate imbalance at the user level. A key insight is to\nuse post-campaign features, such as last-click conversion counts, that do not\nrequire an RCT, as features in our predictive model. We find that our PIE model\nrecovers RCT-derived incremental conversions per dollar (ICPD) much better than\nthe program evaluation approaches analyzed in Gordon et al. (forthcoming). The\nprediction errors from the best PIE model are 48%, 42%, and 62% of the\nRCT-based average ICPD for upper-, mid-, and lower-funnel conversion outcomes,\nrespectively. In contrast, across the same data, the average prediction error\nof stratified propensity score matching exceeds 491%, and that of\ndouble/debiased machine learning exceeds 2,904%. Using a decision-making\nframework inspired by industry, we show that PIE leads to different decisions\ncompared to RCTs for only 6% of upper-funnel, 7% of mid-funnel, and 13% of\nlower-funnel outcomes. We conclude that PIE could enable advertising platforms\nto scale causal ad measurement by extrapolating from a limited number of RCTs\nto a large set of non-experimental ad campaigns.",
        "authors": [
            "Brett R. Gordon",
            "Robert Moakler",
            "Florian Zettelmeyer"
        ],
        "categories": "econ.EM",
        "published": "2023-04-13T21:37:04Z",
        "updated": "2023-04-13T21:37:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.05805v3",
        "title": "GDP nowcasting with artificial neural networks: How much does long-term memory matter?",
        "abstract": "We apply artificial neural networks (ANNs) to nowcast quarterly GDP growth\nfor the U.S. economy. Using the monthly FRED-MD database, we compare the\nnowcasting performance of five different ANN architectures: the multilayer\nperceptron (MLP), the one-dimensional convolutional neural network (1D CNN),\nthe Elman recurrent neural network (RNN), the long short-term memory network\n(LSTM), and the gated recurrent unit (GRU). The empirical analysis presents\nresults from two distinctively different evaluation periods. The first (2012:Q1\n-- 2019:Q4) is characterized by balanced economic growth, while the second\n(2012:Q1 -- 2022:Q4) also includes periods of the COVID-19 recession. According\nto our results, longer input sequences result in more accurate nowcasts in\nperiods of balanced economic growth. However, this effect ceases above a\nrelatively low threshold value of around six quarters (eighteen months). During\nperiods of economic turbulence (e.g., during the COVID-19 recession), longer\ninput sequences do not help the models' predictive performance; instead, they\nseem to weaken their generalization capability. Combined results from the two\nevaluation periods indicate that architectural features enabling long-term\nmemory do not result in more accurate nowcasts. Comparing network\narchitectures, the 1D CNN has proved to be a highly suitable model for GDP\nnowcasting. The network has shown good nowcasting performance among the\ncompetitors during the first evaluation period and achieved the overall best\naccuracy during the second evaluation period. Consequently, first in the\nliterature, we propose the application of the 1D CNN for economic nowcasting.",
        "authors": [
            "Krist\u00f3f N\u00e9meth",
            "D\u00e1niel Hadh\u00e1zi"
        ],
        "categories": "econ.EM",
        "published": "2023-04-12T12:29:58Z",
        "updated": "2024-02-29T15:32:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.04912v1",
        "title": "Financial Time Series Forecasting using CNN and Transformer",
        "abstract": "Time series forecasting is important across various domains for\ndecision-making. In particular, financial time series such as stock prices can\nbe hard to predict as it is difficult to model short-term and long-term\ntemporal dependencies between data points. Convolutional Neural Networks (CNN)\nare good at capturing local patterns for modeling short-term dependencies.\nHowever, CNNs cannot learn long-term dependencies due to the limited receptive\nfield. Transformers on the other hand are capable of learning global context\nand long-term dependencies. In this paper, we propose to harness the power of\nCNNs and Transformers to model both short-term and long-term dependencies\nwithin a time series, and forecast if the price would go up, down or remain the\nsame (flat) in the future. In our experiments, we demonstrated the success of\nthe proposed method in comparison to commonly adopted statistical and deep\nlearning methods on forecasting intraday stock price change of S&P 500\nconstituents.",
        "authors": [
            "Zhen Zeng",
            "Rachneet Kaur",
            "Suchetha Siddagangappa",
            "Saba Rahimi",
            "Tucker Balch",
            "Manuela Veloso"
        ],
        "categories": "cs.LG",
        "published": "2023-04-11T00:56:57Z",
        "updated": "2023-04-11T00:56:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.03069v2",
        "title": "Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series",
        "abstract": "The real life time series are usually nonstationary, bringing a difficult\nquestion of model adaptation. Classical approaches like ARMA-ARCH assume\narbitrary type of dependence. To avoid such bias, we will focus on recently\nproposed agnostic philosophy of moving estimator: in time $t$ finding\nparameters optimizing e.g. $F_t=\\sum_{\\tau<t} (1-\\eta)^{t-\\tau} \\ln(\\rho_\\theta\n(x_\\tau))$ moving log-likelihood, evolving in time. It allows for example to\nestimate parameters using inexpensive exponential moving averages (EMA), like\nabsolute central moments $E[|x-\\mu|^p]$ evolving for one or multiple powers\n$p\\in\\mathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + \\eta (|x_t-\\mu_t|^p-m_{p,t})$.\nApplication of such general adaptive methods of moments will be presented on\nStudent's t-distribution, popular especially in economical applications, here\napplied to log-returns of DJIA companies. While standard ARMA-ARCH approaches\nprovide evolution of $\\mu$ and $\\sigma$, here we also get evolution of $\\nu$\ndescribing $\\rho(x)\\sim |x|^{-\\nu-1}$ tail shape, probability of extreme events\n- which might turn out catastrophic, destabilizing the market.",
        "authors": [
            "Jarek Duda"
        ],
        "categories": "stat.ME",
        "published": "2023-04-06T13:37:27Z",
        "updated": "2023-04-12T14:12:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.02171v3",
        "title": "Faster estimation of dynamic discrete choice models using index invertibility",
        "abstract": "Many estimators of dynamic discrete choice models with persistent unobserved\nheterogeneity have desirable statistical properties but are computationally\nintensive. In this paper we propose a method to quicken estimation for a broad\nclass of dynamic discrete choice problems by exploiting semiparametric index\nrestrictions. Specifically, we propose an estimator for models whose reduced\nform parameters are invertible functions of one or more linear indices (Ahn,\nIchimura, Powell and Ruud 2018), a property we term index invertibility. We\nestablish that index invertibility implies a set of equality constraints on the\nmodel parameters. Our proposed estimator uses the equality constraints to\ndecrease the dimension of the optimization problem, thereby generating\ncomputational gains. Our main result shows that the proposed estimator is\nasymptotically equivalent to the unconstrained, computationally heavy\nestimator. In addition, we provide a series of results on the number of\nindependent index restrictions on the model parameters, providing theoretical\nguidance on the extent of computational gains. Finally, we demonstrate the\nadvantages of our approach via Monte Carlo simulations.",
        "authors": [
            "Jackson Bunting",
            "Takuya Ura"
        ],
        "categories": "econ.EM",
        "published": "2023-04-05T00:06:28Z",
        "updated": "2024-07-16T22:59:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.01921v4",
        "title": "Individual Welfare Analysis: Random Quasilinear Utility, Independence, and Confidence Bounds",
        "abstract": "We introduce a novel framework for individual-level welfare analysis. It\nbuilds on a parametric model for continuous demand with a quasilinear utility\nfunction, allowing for heterogeneous coefficients and unobserved\nindividual-good-level preference shocks. We obtain bounds on the\nindividual-level consumer welfare loss at any confidence level due to a\nhypothetical price increase, solving a scalable optimization problem\nconstrained by a novel confidence set under an independence restriction. This\nconfidence set is computationally simple and robust to weak instruments,\nnonlinearity, and partial identification. The validity of the confidence set is\nguaranteed by our new results on the joint limiting distribution of the\nindependence test by Chatterjee (2021). These results together with the\nconfidence set may have applications beyond welfare analysis. Monte Carlo\nsimulations and two empirical applications on gasoline and food demand\ndemonstrate the effectiveness of our method.",
        "authors": [
            "Junlong Feng",
            "Sokbae Lee"
        ],
        "categories": "econ.EM",
        "published": "2023-04-04T16:15:38Z",
        "updated": "2024-11-22T08:27:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.01906v3",
        "title": "Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python",
        "abstract": "The $\\texttt{torch-choice}$ is an open-source library for flexible, fast\nchoice modeling with Python and PyTorch. $\\texttt{torch-choice}$ provides a\n$\\texttt{ChoiceDataset}$ data structure to manage databases flexibly and\nmemory-efficiently. The paper demonstrates constructing a\n$\\texttt{ChoiceDataset}$ from databases of various formats and functionalities\nof $\\texttt{ChoiceDataset}$. The package implements two widely used models,\nnamely the multinomial logit and nested logit models, and supports\nregularization during model estimation. The package incorporates the option to\ntake advantage of GPUs for estimation, allowing it to scale to massive datasets\nwhile being computationally efficient. Models can be initialized using either\nR-style formula strings or Python dictionaries. We conclude with a comparison\nof the computational efficiencies of $\\texttt{torch-choice}$ and\n$\\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the\nnumber of covariates increases, and (3) the expansion of item sets. Finally, we\ndemonstrate the scalability of $\\texttt{torch-choice}$ on large-scale datasets.",
        "authors": [
            "Tianyu Du",
            "Ayush Kanodia",
            "Susan Athey"
        ],
        "categories": "cs.LG",
        "published": "2023-04-04T16:00:48Z",
        "updated": "2023-07-14T21:42:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.01273v3",
        "title": "Heterogeneity-robust granular instruments",
        "abstract": "Granular instrumental variables (GIV) has experienced sharp growth in\nempirical macro-finance. The methodology's rise showcases granularity's\npotential for identification across many economic environments, like the\nestimation of spillovers and demand systems. I propose a new estimator--called\nrobust granular instrumental variables (RGIV)--that enables studying unit-level\nheterogeneity in spillovers. Unlike existing methods that assume heterogeneity\nis a function of observables, RGIV leaves heterogeneity unrestricted. In\ncontrast to the baseline GIV estimator, RGIV allows for unknown shock variances\nand equal-sized units. Applied to the Euro area, I find strong evidence of\ncountry-level heterogeneity in sovereign yield spillovers.",
        "authors": [
            "Eric Qian"
        ],
        "categories": "econ.EM",
        "published": "2023-04-03T18:07:56Z",
        "updated": "2024-06-06T16:14:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.01141v1",
        "title": "Testing for idiosyncratic Treatment Effect Heterogeneity",
        "abstract": "This paper provides asymptotically valid tests for the null hypothesis of no\ntreatment effect heterogeneity. Importantly, I consider the presence of\nheterogeneity that is not explained by observed characteristics, or so-called\nidiosyncratic heterogeneity. When examining this heterogeneity, common\nstatistical tests encounter a nuisance parameter problem in the average\ntreatment effect which renders the asymptotic distribution of the test\nstatistic dependent on that parameter. I propose an asymptotically valid test\nthat circumvents the estimation of that parameter using the empirical\ncharacteristic function. A simulation study illustrates not only the test's\nvalidity but its higher power in rejecting a false null as compared to current\ntests. Furthermore, I show the method's usefulness through its application to a\nmicrofinance experiment in Bosnia and Herzegovina. In this experiment and for\noutcomes related to loan take-up and self-employment, the tests suggest that\ntreatment effect heterogeneity does not seem to be completely accounted for by\nbaseline characteristics. For those outcomes, researchers could potentially try\nto collect more baseline characteristics to inspect the remaining treatment\neffect heterogeneity, and potentially, improve treatment targeting.",
        "authors": [
            "Jaime Ramirez-Cuellar"
        ],
        "categories": "econ.EM",
        "published": "2023-04-03T17:07:55Z",
        "updated": "2023-04-03T17:07:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.01025v1",
        "title": "Artificial neural networks and time series of counts: A class of nonlinear INGARCH models",
        "abstract": "Time series of counts are frequently analyzed using generalized\ninteger-valued autoregressive models with conditional heteroskedasticity\n(INGARCH). These models employ response functions to map a vector of past\nobservations and past conditional expectations to the conditional expectation\nof the present observation. In this paper, it is shown how INGARCH models can\nbe combined with artificial neural network (ANN) response functions to obtain a\nclass of nonlinear INGARCH models. The ANN framework allows for the\ninterpretation of many existing INGARCH models as a degenerate version of a\ncorresponding neural model. Details on maximum likelihood estimation, marginal\neffects and confidence intervals are given. The empirical analysis of time\nseries of bounded and unbounded counts reveals that the neural INGARCH models\nare able to outperform reasonable degenerate competitor models in terms of the\ninformation loss.",
        "authors": [
            "Malte Jahn"
        ],
        "categories": "stat.ME",
        "published": "2023-04-03T14:26:16Z",
        "updated": "2023-04-03T14:26:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.00678v1",
        "title": "Testing and Identifying Substitution and Complementarity Patterns",
        "abstract": "This paper studies semiparametric identification of substitution and\ncomplementarity patterns between two goods using a panel multinomial choice\nmodel with bundles. The model allows the two goods to be either substitutes or\ncomplements and admits heterogeneous complementarity through observed\ncharacteristics. I first provide testable implications for the complementarity\nrelationship between goods. I then characterize the sharp identified set for\nthe model parameters and provide sufficient conditions for point\nidentification. The identification analysis accommodates endogenous covariates\nthrough flexible dependence structures between observed characteristics and\nfixed effects while placing no distributional assumptions on unobserved\npreference shocks. My method is shown to perform more robustly than the\nparametric method through Monte Carlo simulations. As an extension, I allow for\nunobserved heterogeneity in the complementarity, investigate scenarios\ninvolving more than two goods, and study a class of nonseparable utility\nfunctions.",
        "authors": [
            "Rui Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-04-03T01:45:09Z",
        "updated": "2023-04-03T01:45:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2304.00626v3",
        "title": "IV Regressions without Exclusion Restrictions",
        "abstract": "We study identification and estimation of endogenous linear and nonlinear\nregression models without excluded instrumental variables, based on the\nstandard mean independence condition and a nonlinear relevance condition. Based\non the identification results, we propose two semiparametric estimators as well\nas a discretization-based estimator that does not require any nonparametric\nregressions. We establish their asymptotic normality and demonstrate via\nsimulations their robust finite-sample performances with respect to exclusion\nrestrictions violations and endogeneity. Our approach is applied to study the\nreturns to education, and to test the direct effects of college proximity\nindicators as well as family background variables on the outcome.",
        "authors": [
            "Wayne Yuan Gao",
            "Rui Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-04-02T20:54:19Z",
        "updated": "2023-07-31T02:37:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.18233v2",
        "title": "Inference on eigenvectors of non-symmetric matrices",
        "abstract": "This paper argues that the symmetrisability condition in Tyler (1981) is not\nnecessary to establish asymptotic inference procedures for eigenvectors. We\nestablish distribution theory for a Wald and t-test for full-vector and\nindividual coefficient hypotheses, respectively. Our test statistics originate\nfrom eigenprojections of non-symmetric matrices. Representing projections as a\nmapping from the underlying matrix to its spectral data, we find derivatives\nthrough analytic perturbation theory. These results demonstrate how the\nanalytic perturbation theory of Sun (1991) is a useful tool in multivariate\nstatistics and are of independent interest. As an application, we define\nconfidence sets for Bonacich centralities estimated from adjacency matrices\ninduced by directed graphs.",
        "authors": [
            "Jerome R. Simons"
        ],
        "categories": "math.ST",
        "published": "2023-03-31T17:48:20Z",
        "updated": "2023-04-04T12:50:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.15170v1",
        "title": "Under-Identification of Structural Models Based on Timing and Information Set Assumptions",
        "abstract": "We revisit identification based on timing and information set assumptions in\nstructural models, which have been used in the context of production functions,\ndemand equations, and hedonic pricing models (e.g. Olley and Pakes (1996),\nBlundell and Bond (2000)). First, we demonstrate a general under-identification\nproblem using these assumptions in a simple version of the Blundell-Bond\ndynamic panel model. In particular, the basic moment conditions can yield\nmultiple discrete solutions: one at the persistence parameter in the main\nequation and another at the persistence parameter governing the regressor. We\nthen show that the problem can persist in a broader set of models but\ndisappears in models under stronger timing assumptions. We then propose\npossible solutions in the simple setting by enforcing an assumed sign\nrestriction and conclude by using lessons from our basic identification\napproach to propose more general practical advice for empirical researchers.",
        "authors": [
            "Daniel Ackerberg",
            "Garth Frazer",
            "Kyoo il Kim",
            "Yao Luo",
            "Yingjun Su"
        ],
        "categories": "econ.EM",
        "published": "2023-03-27T13:02:07Z",
        "updated": "2023-03-27T13:02:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.14298v3",
        "title": "Sensitivity Analysis in Unconditional Quantile Effects",
        "abstract": "This paper proposes a framework to analyze the effects of counterfactual\npolicies on the unconditional quantiles of an outcome variable. For a given\ncounterfactual policy, we obtain identified sets for the effect of both\nmarginal and global changes in the proportion of treated individuals. To\nconduct a sensitivity analysis, we introduce the quantile breakdown frontier, a\ncurve that (i) indicates whether a sensitivity analysis is possible or not, and\n(ii) when a sensitivity analysis is possible, quantifies the amount of\nselection bias consistent with a given conclusion of interest across different\nquantiles. To illustrate our method, we perform a sensitivity analysis on the\neffect of unionizing low income workers on the quantiles of the distribution of\n(log) wages.",
        "authors": [
            "Julian Martinez-Iriarte"
        ],
        "categories": "econ.EM",
        "published": "2023-03-24T23:18:20Z",
        "updated": "2024-06-19T11:51:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.14226v2",
        "title": "Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions",
        "abstract": "Consider a setting where there are $N$ heterogeneous units and $p$\ninterventions. Our goal is to learn unit-specific potential outcomes for any\ncombination of these $p$ interventions, i.e., $N \\times 2^p$ causal parameters.\nChoosing a combination of interventions is a problem that naturally arises in a\nvariety of applications such as factorial design experiments, recommendation\nengines, combination therapies in medicine, conjoint analysis, etc. Running $N\n\\times 2^p$ experiments to estimate the various parameters is likely expensive\nand/or infeasible as $N$ and $p$ grow. Further, with observational data there\nis likely confounding, i.e., whether or not a unit is seen under a combination\nis correlated with its potential outcome under that combination. To address\nthese challenges, we propose a novel latent factor model that imposes structure\nacross units (i.e., the matrix of potential outcomes is approximately rank\n$r$), and combinations of interventions (i.e., the coefficients in the Fourier\nexpansion of the potential outcomes is approximately $s$ sparse). We establish\nidentification for all $N \\times 2^p$ parameters despite unobserved\nconfounding. We propose an estimation procedure, Synthetic Combinations, and\nestablish it is finite-sample consistent and asymptotically normal under\nprecise conditions on the observation pattern. Our results imply consistent\nestimation given $\\text{poly}(r) \\times \\left( N + s^2p\\right)$ observations,\nwhile previous methods have sample complexity scaling as $\\min(N \\times s^2p, \\\n\\ \\text{poly(r)} \\times (N + 2^p))$. We use Synthetic Combinations to propose a\ndata-efficient experimental design. Empirically, Synthetic Combinations\noutperforms competing approaches on a real-world dataset on movie\nrecommendations. Lastly, we extend our analysis to do causal inference where\nthe intervention is a permutation over $p$ items (e.g., rankings).",
        "authors": [
            "Abhineet Agarwal",
            "Anish Agarwal",
            "Suhas Vijaykumar"
        ],
        "categories": "stat.ME",
        "published": "2023-03-24T18:45:44Z",
        "updated": "2024-01-15T09:14:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.14088v2",
        "title": "On the failure of the bootstrap for Chatterjee's rank correlation",
        "abstract": "While researchers commonly use the bootstrap for statistical inference, many\nof us have realized that the standard bootstrap, in general, does not work for\nChatterjee's rank correlation. In this paper, we provide proof of this issue\nunder an additional independence assumption, and complement our theory with\nsimulation evidence for general settings. Chatterjee's rank correlation thus\nfalls into a category of statistics that are asymptotically normal but\nbootstrap inconsistent. Valid inferential methods in this case are Chatterjee's\noriginal proposal (for testing independence) and Lin and Han (2022)'s analytic\nasymptotic variance estimator (for more general purposes).",
        "authors": [
            "Zhexiao Lin",
            "Fang Han"
        ],
        "categories": "math.ST",
        "published": "2023-03-24T15:52:41Z",
        "updated": "2023-04-05T16:34:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.13795v1",
        "title": "Point Identification of LATE with Two Imperfect Instruments",
        "abstract": "This paper characterizes point identification results of the local average\ntreatment effect (LATE) using two imperfect instruments. The classical approach\n(Imbens and Angrist (1994)) establishes the identification of LATE via an\ninstrument that satisfies exclusion, monotonicity, and independence. However,\nit may be challenging to find a single instrument that satisfies all these\nassumptions simultaneously. My paper uses two instruments but imposes weaker\nassumptions on both instruments. The first instrument is allowed to violate the\nexclusion restriction and the second instrument does not need to satisfy\nmonotonicity. Therefore, the first instrument can affect the outcome via both\ndirect effects and a shift in the treatment status. The direct effects can be\nidentified via exogenous variation in the second instrument and therefore the\nlocal average treatment effect is identified. An estimator is proposed, and\nusing Monte Carlo simulations, it is shown to perform more robustly than the\ninstrumental variable estimand.",
        "authors": [
            "Rui Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-03-24T04:22:26Z",
        "updated": "2023-03-24T04:22:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.13598v3",
        "title": "Bootstrap-Assisted Inference for Generalized Grenander-type Estimators",
        "abstract": "Westling and Carone (2020) proposed a framework for studying the large sample\ndistributional properties of generalized Grenander-type estimators, a versatile\nclass of nonparametric estimators of monotone functions. The limiting\ndistribution of those estimators is representable as the left derivative of the\ngreatest convex minorant of a Gaussian process whose monomial mean can be of\nunknown order (when the degree of flatness of the function of interest is\nunknown). The standard nonparametric bootstrap is unable to consistently\napproximate the large sample distribution of the generalized Grenander-type\nestimators even if the monomial order of the mean is known, making statistical\ninference a challenging endeavour in applications. To address this inferential\nproblem, we present a bootstrap-assisted inference procedure for generalized\nGrenander-type estimators. The procedure relies on a carefully crafted, yet\nautomatic, transformation of the estimator. Moreover, our proposed method can\nbe made ``flatness robust'' in the sense that it can be made adaptive to the\n(possibly unknown) degree of flatness of the function of interest. The method\nrequires only the consistent estimation of a single scalar quantity, for which\nwe propose an automatic procedure based on numerical derivative estimation and\nthe generalized jackknife. Under random sampling, our inference method can be\nimplemented using a computationally attractive exchangeable bootstrap\nprocedure. We illustrate our methods with examples and we also provide a small\nsimulation study. The development of formal results is made possible by some\ntechnical results that may be of independent interest.",
        "authors": [
            "Matias D. Cattaneo",
            "Michael Jansson",
            "Kenichi Nagasawa"
        ],
        "categories": "math.ST",
        "published": "2023-03-23T18:24:43Z",
        "updated": "2024-07-04T18:02:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.13406v2",
        "title": "Sequential Cauchy Combination Test for Multiple Testing Problems with Financial Applications",
        "abstract": "We introduce a simple tool to control for false discoveries and identify\nindividual signals in scenarios involving many tests, dependent test\nstatistics, and potentially sparse signals. The tool applies the Cauchy\ncombination test recursively on a sequence of expanding subsets of $p$-values\nand is referred to as the sequential Cauchy combination test. While the\noriginal Cauchy combination test aims to make a global statement about a set of\nnull hypotheses by summing transformed $p$-values, our sequential version\ndetermines which $p$-values trigger the rejection of the global null. The\nsequential test achieves strong familywise error rate control, exhibits less\nconservatism compared to existing controlling procedures when dealing with\ndependent test statistics, and provides a power boost. As illustrations, we\nrevisit two well-known large-scale multiple testing problems in finance for\nwhich the test statistics have either serial dependence or cross-sectional\ndependence, namely monitoring drift bursts in asset prices and searching for\nassets with a nonzero alpha. In both applications, the sequential Cauchy\ncombination test proves to be a preferable alternative. It overcomes many of\nthe drawbacks inherent to inequality-based controlling procedures, extreme\nvalue approaches, resampling and screening methods, and it improves the power\nin simulations, leading to distinct empirical outcomes.",
        "authors": [
            "Nabil Bouamara",
            "S\u00e9bastien Laurent",
            "Shuping Shi"
        ],
        "categories": "econ.EM",
        "published": "2023-03-23T16:28:16Z",
        "updated": "2023-06-01T09:56:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.13281v2",
        "title": "Uncertain Short-Run Restrictions and Statistically Identified Structural Vector Autoregressions",
        "abstract": "This study proposes a combination of a statistical identification approach\nwith potentially invalid short-run zero restrictions. The estimator shrinks\ntowards imposed restrictions and stops shrinkage when the data provide evidence\nagainst a restriction. Simulation results demonstrate how incorporating valid\nrestrictions through the shrinkage approach enhances the accuracy of the\nstatistically identified estimator and how the impact of invalid restrictions\ndecreases with the sample size. The estimator is applied to analyze the\ninteraction between the stock and oil market. The results indicate that\nincorporating stock market data into the analysis is crucial, as it enables the\nidentification of information shocks, which are shown to be important drivers\nof the oil price.",
        "authors": [
            "Sascha A. Keweloh"
        ],
        "categories": "econ.EM",
        "published": "2023-03-23T14:02:54Z",
        "updated": "2024-04-03T07:34:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.13218v1",
        "title": "Functional-Coefficient Quantile Regression for Panel Data with Latent Group Structure",
        "abstract": "This paper considers estimating functional-coefficient models in panel\nquantile regression with individual effects, allowing the cross-sectional and\ntemporal dependence for large panel observations. A latent group structure is\nimposed on the heterogenous quantile regression models so that the number of\nnonparametric functional coefficients to be estimated can be reduced\nconsiderably. With the preliminary local linear quantile estimates of the\nsubject-specific functional coefficients, a classic agglomerative clustering\nalgorithm is used to estimate the unknown group structure and an\neasy-to-implement ratio criterion is proposed to determine the group number.\nThe estimated group number and structure are shown to be consistent.\nFurthermore, a post-grouping local linear smoothing method is introduced to\nestimate the group-specific functional coefficients, and the relevant\nasymptotic normal distribution theory is derived with a normalisation rate\ncomparable to that in the literature. The developed methodologies and theory\nare verified through a simulation study and showcased with an application to\nhouse price data from UK local authority districts, which reveals different\nhomogeneity structures at different quantile levels.",
        "authors": [
            "Xiaorong Yang",
            "Jia Chen",
            "Degui Li",
            "Runze Li"
        ],
        "categories": "econ.EM",
        "published": "2023-03-23T12:29:40Z",
        "updated": "2023-03-23T12:29:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.14125v1",
        "title": "sparseDFM: An R Package to Estimate Dynamic Factor Models with Sparse Loadings",
        "abstract": "sparseDFM is an R package for the implementation of popular estimation\nmethods for dynamic factor models (DFMs) including the novel Sparse DFM\napproach of Mosley et al. (2023). The Sparse DFM ameliorates interpretability\nissues of factor structure in classic DFMs by constraining the loading matrices\nto have few non-zero entries (i.e. are sparse). Mosley et al. (2023) construct\nan efficient expectation maximisation (EM) algorithm to enable estimation of\nmodel parameters using a regularised quasi-maximum likelihood. We provide\ndetail on the estimation strategy in this paper and show how we implement this\nin a computationally efficient way. We then provide two real-data case studies\nto act as tutorials on how one may use the sparseDFM package. The first case\nstudy focuses on summarising the structure of a small subset of quarterly CPI\n(consumer price inflation) index data for the UK, while the second applies the\npackage onto a large-scale set of monthly time series for the purpose of\nnowcasting nine of the main trade commodities the UK exports worldwide.",
        "authors": [
            "Luke Mosley",
            "Tak-Shing Chan",
            "Alex Gibberd"
        ],
        "categories": "stat.CO",
        "published": "2023-03-23T10:19:24Z",
        "updated": "2023-03-23T10:19:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.16151v1",
        "title": "Forecasting Large Realized Covariance Matrices: The Benefits of Factor Models and Shrinkage",
        "abstract": "We propose a model to forecast large realized covariance matrices of returns,\napplying it to the constituents of the S\\&P 500 daily. To address the curse of\ndimensionality, we decompose the return covariance matrix using standard\nfirm-level factors (e.g., size, value, and profitability) and use sectoral\nrestrictions in the residual covariance matrix. This restricted model is then\nestimated using vector heterogeneous autoregressive (VHAR) models with the\nleast absolute shrinkage and selection operator (LASSO). Our methodology\nimproves forecasting precision relative to standard benchmarks and leads to\nbetter estimates of minimum variance portfolios.",
        "authors": [
            "Rafael Alves",
            "Diego S. de Brito",
            "Marcelo C. Medeiros",
            "Ruy M. Ribeiro"
        ],
        "categories": "q-fin.ST",
        "published": "2023-03-22T16:38:22Z",
        "updated": "2023-03-22T16:38:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.12667v2",
        "title": "Don't (fully) exclude me, it's not necessary! Identification with semi-IVs",
        "abstract": "This paper proposes a novel tool to nonparametrically identify models with a\ndiscrete endogenous variable or treatment: semi-instrumental variables\n(semi-IVs). A semi-IV is a variable that is relevant but only partially\nexcluded from the potential outcomes, i.e., excluded from at least one, but not\nnecessarily all, potential outcome equations. It follows that standard\ninstrumental variables (IVs), which are fully excluded from all the potential\noutcomes, are a special (extreme) case of semi-IVs. I show that full exclusion\nis stronger than necessary because the same objects that are usually identified\nwith an IV (Imbens and Angrist, 1994; Heckman and Vytlacil, 2005; Chernozhukov\nand Hansen, 2005) can be identified with several semi-IVs instead, provided\nthere is (at least) one semi-IV excluded from each potential outcome. For\napplied work, tackling endogeneity with semi-IVs instead of IVs should be an\nattractive alternative, since semi-IVs are easier to find: most\nselection-specific costs or benefits can be valid semi-IVs, for example. The\npaper also provides a simple semi-IV GMM estimator for models with homogenous\ntreatment effects and uses it to estimate the returns to education.",
        "authors": [
            "Christophe Bruneel-Zupanc"
        ],
        "categories": "econ.EM",
        "published": "2023-03-22T15:44:59Z",
        "updated": "2023-07-11T11:34:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.11777v5",
        "title": "Quasi Maximum Likelihood Estimation of High-Dimensional Factor Models: A Critical Review",
        "abstract": "We review Quasi Maximum Likelihood estimation of factor models for\nhigh-dimensional panels of time series. We consider two cases: (1) estimation\nwhen no dynamic model for the factors is specified (Bai and Li, 2012, 2016);\n(2) estimation based on the Kalman smoother and the Expectation Maximization\nalgorithm thus allowing to model explicitly the factor dynamics (Doz et al.,\n2012, Barigozzi and Luciani, 2019). Our interest is in approximate factor\nmodels, i.e., when we allow for the idiosyncratic components to be mildly\ncross-sectionally, as well as serially, correlated. Although such setting\napparently makes estimation harder, we show, in fact, that factor models do not\nsuffer of the {\\it curse of dimensionality} problem, but instead they enjoy a\n{\\it blessing of dimensionality} property. In particular, given an approximate\nfactor structure, if the cross-sectional dimension of the data, $N$, grows to\ninfinity, we show that: (i) identification of the model is still possible, (ii)\nthe mis-specification error due to the use of an exact factor model\nlog-likelihood vanishes. Moreover, if we let also the sample size, $T$, grow to\ninfinity, we can also consistently estimate all parameters of the model and\nmake inference. The same is true for estimation of the latent factors which can\nbe carried out by weighted least-squares, linear projection, or Kalman\nfiltering/smoothing. We also compare the approaches presented with: Principal\nComponent analysis and the classical, fixed $N$, exact Maximum Likelihood\napproach. We conclude with a discussion on efficiency of the considered\nestimators.",
        "authors": [
            "Matteo Barigozzi"
        ],
        "categories": "econ.EM",
        "published": "2023-03-21T11:47:36Z",
        "updated": "2024-05-20T06:05:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.11721v2",
        "title": "Using Forests in Multivariate Regression Discontinuity Designs",
        "abstract": "We discuss estimation and inference of conditional treatment effects in\nregression discontinuity designs with multiple scores. Aside from the commonly\nused local linear regression approach and a minimax-optimal estimator recently\nproposed by Imbens and Wager (2019), we consider two estimators based on random\nforests -- honest regression forests and local linear forests -- whose\nconstruction resembles that of standard local regressions, with theoretical\nvalidity following from results in Wager and Athey (2018) and Friedberg et al.\n(2020). We design a systematic Monte Carlo study with data generating processes\nbuilt both from functional forms that we specify and from Wasserstein\nGenerative Adversarial Networks that can closely mimic the observed data. We\nfind that no single estimator dominates across all simulations: (i) local\nlinear regressions perform well in univariate settings, but can undercover when\nmultivariate scores are transformed into a univariate score -- which is\ncommonly done in practice -- possibly due to the \"zero-density\" issue of the\ncollapsed univariate score at the transformed cutoff; (ii) good performance of\nthe minimax-optimal estimator depends on accurate estimation of a nuisance\nparameter and its current implementation only accepts up to two scores; (iii)\nforest-based estimators are not designed for estimation at boundary points and\ncan suffer from bias in finite sample, but their flexibility in modeling\nmultivariate scores opens the door to a wide range of empirical applications in\nmultivariate regression discontinuity designs.",
        "authors": [
            "Yiqi Liu",
            "Yuan Qi"
        ],
        "categories": "econ.EM",
        "published": "2023-03-21T10:14:40Z",
        "updated": "2024-07-09T03:11:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.11418v2",
        "title": "On the Existence and Information of Orthogonal Moments",
        "abstract": "Locally Robust (LR)/Orthogonal/Debiased moments have proven useful with\nmachine learning first steps, but their existence has not been investigated for\ngeneral parameters. In this paper, we provide a necessary and sufficient\ncondition, referred to as Restricted Local Non-surjectivity (RLN), for the\nexistence of such orthogonal moments to conduct robust inference on general\nparameters of interest in regular semiparametric models. Importantly, RLN does\nnot require either identification of the parameters of interest or the nuisance\nparameters. However, for orthogonal moments to be informative, the efficient\nFisher Information matrix for the parameter must be non-zero (though possibly\nsingular). Thus, orthogonal moments exist and are informative under more\ngeneral conditions than previously recognized. We demonstrate the utility of\nour general results by characterizing orthogonal moments in a class of models\nwith Unobserved Heterogeneity (UH). For this class of models our method\ndelivers functional differencing as a special case. Orthogonality for general\nsmooth functionals of the distribution of UH is also characterized. As a second\nmajor application, we investigate the existence of orthogonal moments and their\nrelevance for models defined by moment restrictions with possibly different\nconditioning variables. We find orthogonal moments for the fully saturated two\nstage least squares, for heterogeneous parameters in treatment effects, for\nsample selection models, and for popular models of demand for differentiated\nproducts. We apply our results to the Oregon Health Experiment to study\nheterogeneous treatment effects of Medicaid on different health outcomes.",
        "authors": [
            "Facundo Arga\u00f1araz",
            "Juan Carlos Escanciano"
        ],
        "categories": "econ.EM",
        "published": "2023-03-20T19:51:43Z",
        "updated": "2023-06-18T15:17:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.11399v3",
        "title": "How Much Should We Trust Instrumental Variable Estimates in Political Science? Practical Advice Based on Over 60 Replicated Studies",
        "abstract": "Instrumental variable (IV) strategies are widely used in political science to\nestablish causal relationships. However, the identifying assumptions required\nby an IV design are demanding, and it remains challenging for researchers to\nassess their validity. In this paper, we replicate 67 papers published in three\ntop journals in political science during 2010-2022 and identify several\ntroubling patterns. First, researchers often overestimate the strength of their\nIVs due to non-i.i.d. errors, such as a clustering structure. Second, the most\ncommonly used t-test for the two-stage-least-squares (2SLS) estimates often\nseverely underestimates uncertainty. Using more robust inferential methods, we\nfind that around 19-30% of the 2SLS estimates in our sample are underpowered.\nThird, in the majority of the replicated studies, the 2SLS estimates are much\nlarger than the ordinary-least-squares estimates, and their ratio is negatively\ncorrelated with the strength of the IVs in studies where the IVs are not\nexperimentally generated, suggesting potential violations of unconfoundedness\nor the exclusion restriction. To help researchers avoid these pitfalls, we\nprovide a checklist for better practice.",
        "authors": [
            "Apoorva Lal",
            "Mac Lockhart",
            "Yiqing Xu",
            "Ziwen Zu"
        ],
        "categories": "econ.EM",
        "published": "2023-03-20T19:11:56Z",
        "updated": "2023-11-07T18:50:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.11064v1",
        "title": "Network log-ARCH models for forecasting stock market volatility",
        "abstract": "This paper presents a novel dynamic network autoregressive conditional\nheteroscedasticity (ARCH) model based on spatiotemporal ARCH models to forecast\nvolatility in the US stock market. To improve the forecasting accuracy, the\nmodel integrates temporally lagged volatility information and information from\nadjacent nodes, which may instantaneously spill across the entire network. The\nmodel is also suitable for high-dimensional cases where multivariate ARCH\nmodels are typically no longer applicable. We adopt the theoretical foundations\nfrom spatiotemporal statistics and transfer the dynamic ARCH model for\nprocesses to networks. This new approach is compared with independent\nunivariate log-ARCH models. We could quantify the improvements due to the\ninstantaneous network ARCH effects, which are studied for the first time in\nthis paper. The edges are determined based on various distance and correlation\nmeasures between the time series. The performances of the alternative networks'\ndefinitions are compared in terms of out-of-sample accuracy. Furthermore, we\nconsider ensemble forecasts based on different network definitions.",
        "authors": [
            "Raffaele Mattera",
            "Philipp Otto"
        ],
        "categories": "stat.AP",
        "published": "2023-03-20T12:37:26Z",
        "updated": "2023-03-20T12:37:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.10306v1",
        "title": "Standard errors when a regressor is randomly assigned",
        "abstract": "We examine asymptotic properties of the OLS estimator when the values of the\nregressor of interest are assigned randomly and independently of other\nregressors. We find that the OLS variance formula in this case is often\nsimplified, sometimes substantially. In particular, when the regressor of\ninterest is independent not only of other regressors but also of the error\nterm, the textbook homoskedastic variance formula is valid even if the error\nterm and auxiliary regressors exhibit a general dependence structure. In the\ncontext of randomized controlled trials, this conclusion holds in completely\nrandomized experiments with constant treatment effects. When the error term is\nheteroscedastic with respect to the regressor of interest, the variance formula\nhas to be adjusted not only for heteroscedasticity but also for correlation\nstructure of the error term. However, even in the latter case, some\nsimplifications are possible as only a part of the correlation structure of the\nerror term should be taken into account. In the context of randomized control\ntrials, this implies that the textbook homoscedastic variance formula is\ntypically not valid if treatment effects are heterogenous but\nheteroscedasticity-robust variance formulas are valid if treatment effects are\nindependent across units, even if the error term exhibits a general dependence\nstructure. In addition, we extend the results to the case when the regressor of\ninterest is assigned randomly at a group level, such as in randomized control\ntrials with treatment assignment determined at a group (e.g., school/village)\nlevel.",
        "authors": [
            "Denis Chetverikov",
            "Jinyong Hahn",
            "Zhipeng Liao",
            "Andres Santos"
        ],
        "categories": "econ.EM",
        "published": "2023-03-18T02:00:40Z",
        "updated": "2023-03-18T02:00:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.10117v2",
        "title": "Estimation of Grouped Time-Varying Network Vector Autoregression Models",
        "abstract": "This paper introduces a flexible time-varying network vector autoregressive\nmodel framework for large-scale time series. A latent group structure is\nimposed on the heterogeneous and node-specific time-varying momentum and\nnetwork spillover effects so that the number of unknown time-varying\ncoefficients to be estimated can be reduced considerably. A classic\nagglomerative clustering algorithm with nonparametrically estimated distance\nmatrix is combined with a ratio criterion to consistently estimate the latent\ngroup number and membership. A post-grouping local linear smoothing method is\nproposed to estimate the group-specific time-varying momentum and network\neffects, substantially improving the convergence rates of the preliminary\nestimates which ignore the latent structure. We further modify the methodology\nand theory to allow for structural breaks in either the group membership, group\nnumber or group-specific coefficient functions. Numerical studies including\nMonte-Carlo simulation and an empirical application are presented to examine\nthe finite-sample performance of the developed model and methodology.",
        "authors": [
            "Degui Li",
            "Bin Peng",
            "Songqiao Tang",
            "Weibiao Wu"
        ],
        "categories": "stat.ME",
        "published": "2023-03-17T16:57:29Z",
        "updated": "2024-03-10T15:49:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.10019v3",
        "title": "Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices",
        "abstract": "This paper presents a new method for combining (or aggregating or ensembling)\nmultivariate probabilistic forecasts, considering dependencies between\nquantiles and marginals through a smoothing procedure that allows for online\nlearning. We discuss two smoothing methods: dimensionality reduction using\nBasis matrices and penalized smoothing. The new online learning algorithm\ngeneralizes the standard CRPS learning framework into multivariate dimensions.\nIt is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic\nlearning properties. The procedure uses horizontal aggregation, i.e.,\naggregation across quantiles. We provide an in-depth discussion on possible\nextensions of the algorithm and several nested cases related to the existing\nliterature on online forecast combination. We apply the proposed methodology to\nforecasting day-ahead electricity prices, which are 24-dimensional\ndistributional forecasts. The proposed method yields significant improvements\nover uniform combination in terms of continuous ranked probability score\n(CRPS). We discuss the temporal evolution of the weights and hyperparameters\nand present the results of reduced versions of the preferred model. A fast C++\nimplementation of the proposed algorithm is provided in the open-source\nR-Package profoc on CRAN.",
        "authors": [
            "Jonathan Berrisch",
            "Florian Ziel"
        ],
        "categories": "stat.ML",
        "published": "2023-03-17T14:47:55Z",
        "updated": "2024-02-06T20:39:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.09680v2",
        "title": "Bootstrap based asymptotic refinements for high-dimensional nonlinear models",
        "abstract": "We consider penalized extremum estimation of a high-dimensional, possibly\nnonlinear model that is sparse in the sense that most of its parameters are\nzero but some are not. We use the SCAD penalty function, which provides model\nselection consistent and oracle efficient estimates under suitable conditions.\nHowever, asymptotic approximations based on the oracle model can be inaccurate\nwith the sample sizes found in many applications. This paper gives conditions\nunder which the bootstrap, based on estimates obtained through SCAD\npenalization with thresholding, provides asymptotic refinements of size \\(O\n\\left( n^{- 2} \\right)\\) for the error in the rejection (coverage) probability\nof a symmetric hypothesis test (confidence interval) and \\(O \\left( n^{- 1}\n\\right)\\) for the error in the rejection (coverage) probability of a one-sided\nor equal tailed test (confidence interval). The results of Monte Carlo\nexperiments show that the bootstrap can provide large reductions in errors in\nrejection and coverage probabilities. The bootstrap is consistent, though it\ndoes not necessarily provide asymptotic refinements, even if some parameters\nare close but not equal to zero. Random-coefficients logit and probit models\nand nonlinear moment models are examples of models to which the procedure\napplies.",
        "authors": [
            "Joel L. Horowitz",
            "Ahnaf Rafi"
        ],
        "categories": "econ.EM",
        "published": "2023-03-16T22:52:03Z",
        "updated": "2024-02-21T20:06:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.08653v2",
        "title": "On the robustness of posterior means",
        "abstract": "Consider a normal location model $X \\mid \\theta \\sim N(\\theta, \\sigma^2)$\nwith known $\\sigma^2$. Suppose $\\theta \\sim G_0$, where the prior $G_0$ has\nzero mean and variance bounded by $V$. Let $G_1$ be a possibly misspecified\nprior with zero mean and variance bounded by $V$. We show that the squared\nerror Bayes risk of the posterior mean under $G_1$ is bounded, subjected to an\nadditional tail condition on $G_1$, uniformly over $G_0, G_1, \\sigma^2 > 0$.",
        "authors": [
            "Jiafeng Chen"
        ],
        "categories": "math.ST",
        "published": "2023-03-15T14:36:08Z",
        "updated": "2024-12-11T05:56:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.08460v2",
        "title": "Identifying an Earnings Process With Dependent Contemporaneous Income Shocks",
        "abstract": "This paper proposes a novel approach for identifying coefficients in an\nearnings dynamics model with arbitrarily dependent contemporaneous income\nshocks. Traditional methods relying on second moments fail to identify these\ncoefficients, emphasizing the need for nongaussianity assumptions that capture\ninformation from higher moments. Our results contribute to the literature on\nearnings dynamics by allowing models of earnings to have, for example, the\npermanent income shock of a job change to be linked to the contemporaneous\ntransitory income shock of a relocation bonus.",
        "authors": [
            "Dan Ben-Moshe"
        ],
        "categories": "econ.EM",
        "published": "2023-03-15T09:04:10Z",
        "updated": "2023-05-03T09:21:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.07822v2",
        "title": "Identification- and many instrument-robust inference via invariant moment conditions",
        "abstract": "Identification-robust hypothesis tests are commonly based on the continuous\nupdating objective function or its score. When the number of moment conditions\ngrows proportionally with the sample size, the large-dimensional weighting\nmatrix prohibits the use of conventional asymptotic approximations and the\nbehavior of these tests remains unknown. We show that the structure of the\nweighting matrix opens up an alternative route to asymptotic results when,\nunder the null hypothesis, the distribution of the moment conditions is\nreflection invariant. In a heteroskedastic linear instrumental variables model,\nwe then establish asymptotic normality of conventional tests statistics under\nmany instrument sequences. A key result is that the additional terms that\nappear in the variance are negative. Revisiting a study on the elasticity of\nsubstitution between immigrant and native workers where the number of\ninstruments is over a quarter of the sample size, the many instrument-robust\napproximation indeed leads to substantially narrower confidence intervals.",
        "authors": [
            "Tom Boot",
            "Johannes W. Ligtenberg"
        ],
        "categories": "econ.EM",
        "published": "2023-03-14T11:54:59Z",
        "updated": "2023-09-04T15:08:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.07287v2",
        "title": "Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm",
        "abstract": "In non-asymptotic learning, variance-type parameters of sub-Gaussian\ndistributions are of paramount importance. However, directly estimating these\nparameters using the empirical moment generating function (MGF) is infeasible.\nTo address this, we suggest using the sub-Gaussian intrinsic moment norm\n[Buldygin and Kozachenko (2000), Theorem 1.3] achieved by maximizing a sequence\nof normalized moments. Significantly, the suggested norm can not only\nreconstruct the exponential moment bounds of MGFs but also provide tighter\nsub-Gaussian concentration inequalities. In practice, we provide an intuitive\nmethod for assessing whether data with a finite sample size is sub-Gaussian,\nutilizing the sub-Gaussian plot. The intrinsic moment norm can be robustly\nestimated via a simple plug-in approach. Our theoretical findings are also\napplicable to reinforcement learning, including the multi-armed bandit\nscenario.",
        "authors": [
            "Huiming Zhang",
            "Haoyu Wei",
            "Guang Cheng"
        ],
        "categories": "stat.ML",
        "published": "2023-03-13T17:03:19Z",
        "updated": "2024-01-20T03:20:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.15364v2",
        "title": "Inflation forecasting with attention based transformer neural networks",
        "abstract": "Inflation is a major determinant for allocation decisions and its forecast is\na fundamental aim of governments and central banks. However, forecasting\ninflation is not a trivial task, as its prediction relies on low frequency,\nhighly fluctuating data with unclear explanatory variables. While classical\nmodels show some possibility of predicting inflation, reliably beating the\nrandom walk benchmark remains difficult. Recently, (deep) neural networks have\nshown impressive results in a multitude of applications, increasingly setting\nthe new state-of-the-art. This paper investigates the potential of the\ntransformer deep neural network architecture to forecast different inflation\nrates. The results are compared to a study on classical time series and machine\nlearning models. We show that our adapted transformer, on average, outperforms\nthe baseline in 6 out of 16 experiments, showing best scores in two out of four\ninvestigated inflation rates. Our results demonstrate that a transformer based\nneural network can outperform classical regression and machine learning models\nin certain inflation rates and forecasting horizons.",
        "authors": [
            "Maximilian Tschuchnig",
            "Petra Tschuchnig",
            "Cornelia Ferner",
            "Michael Gadermayr"
        ],
        "categories": "econ.EM",
        "published": "2023-03-13T13:36:16Z",
        "updated": "2023-03-29T08:08:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.06658v1",
        "title": "Counterfactual Copula and Its Application to the Effects of College Education on Intergenerational Mobility",
        "abstract": "This paper proposes a nonparametric estimator of the counterfactual copula of\ntwo outcome variables that would be affected by a policy intervention. The\nproposed estimator allows policymakers to conduct ex-ante evaluations by\ncomparing the estimated counterfactual and actual copulas as well as their\ncorresponding measures of association. Asymptotic properties of the\ncounterfactual copula estimator are established under regularity conditions.\nThese conditions are also used to validate the nonparametric bootstrap for\ninference on counterfactual quantities. Simulation results indicate that our\nestimation and inference procedures perform well in moderately sized samples.\nApplying the proposed method to studying the effects of college education on\nintergenerational income mobility under two counterfactual scenarios, we find\nthat while providing some college education to all children is unlikely to\npromote mobility, offering a college degree to children from less educated\nfamilies can significantly reduce income persistence across generations.",
        "authors": [
            "Tsung-Chih Lai",
            "Jiun-Hua Su"
        ],
        "categories": "econ.EM",
        "published": "2023-03-12T13:35:44Z",
        "updated": "2023-03-12T13:35:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.04994v1",
        "title": "Distributional Vector Autoregression: Eliciting Macro and Financial Dependence",
        "abstract": "Vector autoregression is an essential tool in empirical macroeconomics and\nfinance for understanding the dynamic interdependencies among multivariate time\nseries. In this study, we expand the scope of vector autoregression by\nincorporating a multivariate distributional regression framework and\nintroducing a distributional impulse response function, providing a\ncomprehensive view of dynamic heterogeneity. We propose a straightforward yet\nflexible estimation method and establish its asymptotic properties under weak\ndependence assumptions. Our empirical analysis examines the conditional joint\ndistribution of GDP growth and financial conditions in the United States, with\na focus on the global financial crisis. Our results show that tight financial\nconditions lead to a multimodal conditional joint distribution of GDP growth\nand financial conditions, and easing financial conditions significantly impacts\nlong-term GDP growth, while improving the GDP growth during the global\nfinancial crisis has limited effects on financial conditions.",
        "authors": [
            "Yunyun Wang",
            "Tatsushi Oka",
            "Dan Zhu"
        ],
        "categories": "econ.EM",
        "published": "2023-03-09T02:38:15Z",
        "updated": "2023-03-09T02:38:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.04416v3",
        "title": "Inference on Optimal Dynamic Policies via Softmax Approximation",
        "abstract": "Estimating optimal dynamic policies from offline data is a fundamental\nproblem in dynamic decision making. In the context of causal inference, the\nproblem is known as estimating the optimal dynamic treatment regime. Even\nthough there exists a plethora of methods for estimation, constructing\nconfidence intervals for the value of the optimal regime and structural\nparameters associated with it is inherently harder, as it involves non-linear\nand non-differentiable functionals of unknown quantities that need to be\nestimated. Prior work resorted to sub-sample approaches that can deteriorate\nthe quality of the estimate. We show that a simple soft-max approximation to\nthe optimal treatment regime, for an appropriately fast growing temperature\nparameter, can achieve valid inference on the truly optimal regime. We\nillustrate our result for a two-period optimal dynamic regime, though our\napproach should directly extend to the finite horizon case. Our work combines\ntechniques from semi-parametric inference and $g$-estimation, together with an\nappropriate triangular array central limit theorem, as well as a novel analysis\nof the asymptotic influence and asymptotic bias of softmax approximations.",
        "authors": [
            "Qizhao Chen",
            "Morgane Austern",
            "Vasilis Syrgkanis"
        ],
        "categories": "econ.EM",
        "published": "2023-03-08T07:42:47Z",
        "updated": "2023-12-13T23:26:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.03009v2",
        "title": "Identification of Ex ante Returns Using Elicited Choice Probabilities: an Application to Preferences for Public-sector Jobs",
        "abstract": "Ex ante returns, the net value that agents perceive before they take an\ninvestment decision, are understood as the main drivers of individual\ndecisions. Hence, their distribution in a population is an important tool for\ncounterfactual analysis and policy evaluation. This paper studies the\nidentification of the population distribution of ex ante returns using stated\nchoice experiments, in the context of binary investment decisions. The\nenvironment is characterised by uncertainty about future outcomes, with some\nuncertainty being resolved over time. In this context, each individual holds a\nprobability distribution over different levels of returns. The paper provides\nnovel, nonparametric identification results for the population distribution of\nreturns, accounting for uncertainty. It complements these with a\nnonparametric/semiparametric estimation methodology, which is new to the\nstated-preference literature. Finally, it uses these results to study the\npreference of high ability students in Cote d'Ivoire for public-sector jobs and\nhow the competition for talent affects the expansion of the private sector.",
        "authors": [
            "Romuald Meango",
            "Esther Mirjam Girsberger"
        ],
        "categories": "econ.EM",
        "published": "2023-03-06T10:25:49Z",
        "updated": "2024-06-14T11:17:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.02820v1",
        "title": "EnsembleIV: Creating Instrumental Variables from Ensemble Learners for Robust Statistical Inference",
        "abstract": "Despite increasing popularity in empirical studies, the integration of\nmachine learning generated variables into regression models for statistical\ninference suffers from the measurement error problem, which can bias estimation\nand threaten the validity of inferences. In this paper, we develop a novel\napproach to alleviate associated estimation biases. Our proposed approach,\nEnsembleIV, creates valid and strong instrumental variables from weak learners\nin an ensemble model, and uses them to obtain consistent estimates that are\nrobust against the measurement error problem. Our empirical evaluations, using\nboth synthetic and real-world datasets, show that EnsembleIV can effectively\nreduce estimation biases across several common regression specifications, and\ncan be combined with modern deep learning techniques when dealing with\nunstructured data.",
        "authors": [
            "Gordon Burtch",
            "Edward McFowland III",
            "Mochen Yang",
            "Gediminas Adomavicius"
        ],
        "categories": "econ.EM",
        "published": "2023-03-06T01:23:49Z",
        "updated": "2023-03-06T01:23:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.02784v1",
        "title": "Censored Quantile Regression with Many Controls",
        "abstract": "This paper develops estimation and inference methods for censored quantile\nregression models with high-dimensional controls. The methods are based on the\napplication of double/debiased machine learning (DML) framework to the censored\nquantile regression estimator of Buchinsky and Hahn (1998). I provide valid\ninference for low-dimensional parameters of interest in the presence of\nhigh-dimensional nuisance parameters when implementing machine learning\nestimators. The proposed estimator is shown to be consistent and asymptotically\nnormal. The performance of the estimator with high-dimensional controls is\nillustrated with numerical simulation and an empirical application that\nexamines the effect of 401(k) eligibility on savings.",
        "authors": [
            "Seoyun Hong"
        ],
        "categories": "econ.EM",
        "published": "2023-03-05T21:52:23Z",
        "updated": "2023-03-05T21:52:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.02716v5",
        "title": "Deterministic, quenched and annealed parameter estimation for heterogeneous network models",
        "abstract": "At least two, different approaches to define and solve statistical models for\nthe analysis of economic systems exist: the typical, econometric one,\ninterpreting the Gravity Model specification as the expected link weight of an\narbitrary probability distribution, and the one rooted into statistical\nphysics, constructing maximum-entropy distributions constrained to satisfy\ncertain network properties. In a couple of recent, companion papers they have\nbeen successfully integrated within the framework induced by the constrained\nminimisation of the Kullback-Leibler divergence: specifically, two, broad\nclasses of models have been devised, i.e. the integrated and the conditional\nones, defined by different, probabilistic rules to place links, load them with\nweights and turn them into proper, econometric prescriptions. Still, the\nrecipes adopted by the two approaches to estimate the parameters entering into\nthe definition of each model differ. In econometrics, a likelihood that\ndecouples the binary and weighted parts of a model, treating a network as\ndeterministic, is typically maximised; to restore its random character, two\nalternatives exist: either solving the likelihood maximisation on each\nconfiguration of the ensemble and taking the average of the parameters\nafterwards or taking the average of the likelihood function and maximising the\nlatter one. The difference between these approaches lies in the order in which\nthe operations of averaging and maximisation are taken - a difference that is\nreminiscent of the quenched and annealed ways of averaging out the disorder in\nspin glasses. The results of the present contribution, devoted to comparing\nthese recipes in the case of continuous, conditional network models, indicate\nthat the annealed estimation recipe represents the best alternative to the\ndeterministic one.",
        "authors": [
            "Marzio Di Vece",
            "Diego Garlaschelli",
            "Tiziano Squartini"
        ],
        "categories": "physics.soc-ph",
        "published": "2023-03-05T17:06:53Z",
        "updated": "2023-11-02T18:15:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.01887v2",
        "title": "Fast Forecasting of Unstable Data Streams for On-Demand Service Platforms",
        "abstract": "On-demand service platforms face a challenging problem of forecasting a large\ncollection of high-frequency regional demand data streams that exhibit\ninstabilities. This paper develops a novel forecast framework that is fast and\nscalable, and automatically assesses changing environments without human\nintervention. We empirically test our framework on a large-scale demand data\nset from a leading on-demand delivery platform in Europe, and find strong\nperformance gains from using our framework against several industry benchmarks,\nacross all geographical regions, loss functions, and both pre- and post-Covid\nperiods. We translate forecast gains to economic impacts for this on-demand\nservice platform by computing financial gains and reductions in computing\ncosts.",
        "authors": [
            "Yu Jeffrey Hu",
            "Jeroen Rombouts",
            "Ines Wilms"
        ],
        "categories": "econ.EM",
        "published": "2023-03-03T12:33:32Z",
        "updated": "2024-05-31T14:53:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.01863v3",
        "title": "Constructing High Frequency Economic Indicators by Imputation",
        "abstract": "Monthly and weekly economic indicators are often taken to be the largest\ncommon factor estimated from high and low frequency data, either separately or\njointly. To incorporate mixed frequency information without directly modeling\nthem, we target a low frequency diffusion index that is already available, and\ntreat high frequency values as missing. We impute these values using multiple\nfactors estimated from the high frequency data. In the empirical examples\nconsidered, static matrix completion that does not account for serial\ncorrelation in the idiosyncratic errors yields imprecise estimates of the\nmissing values irrespective of how the factors are estimated. Single equation\nand systems-based dynamic procedures that account for serial correlation yield\nimputed values that are closer to the observed low frequency ones. This is the\ncase in the counterfactual exercise that imputes the monthly values of consumer\nsentiment series before 1978 when the data was released only on a quarterly\nbasis. This is also the case for a weekly version of the CFNAI index of\neconomic activity that is imputed using seasonally unadjusted data. The imputed\nseries reveals episodes of increased variability of weekly economic information\nthat are masked by the monthly data, notably around the 2014-15 collapse in oil\nprices.",
        "authors": [
            "Serena Ng",
            "Susannah Scanlan"
        ],
        "categories": "econ.EM",
        "published": "2023-03-03T11:38:23Z",
        "updated": "2023-10-08T17:02:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.00982v2",
        "title": "Aggregated Intersection Bounds and Aggregated Minimax Values",
        "abstract": "This paper proposes a novel framework of aggregated intersection bounds,\nwhere the target parameter is obtained by averaging the minimum (or maximum) of\na collection of regression functions over the covariate space. Examples of such\nquantities include the lower and upper bounds on distributional effects\n(Fr\\'echet-Hoeffding, Makarov) as well as the optimal welfare in statistical\ntreatment choice problems. The proposed estimator -- the envelope score\nestimator -- is shown to have an oracle property, where the oracle knows the\nidentity of the minimizer for each covariate value. Next, the result is\nextended to the aggregated minimax values of a collection of regression\nfunctions, covering optimal distributional welfare in worst-case and best-case,\nrespectively. This proposed estimator -- the envelope saddle value estimator --\nis shown to have an oracle property, where the oracle knows the identity of the\nsaddle point.",
        "authors": [
            "Vira Semenova"
        ],
        "categories": "econ.EM",
        "published": "2023-03-02T05:24:37Z",
        "updated": "2024-06-11T15:07:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.00845v1",
        "title": "$21^{st}$ Century Statistical Disclosure Limitation: Motivations and Challenges",
        "abstract": "This chapter examines the motivations and imperatives for modernizing how\nstatistical agencies approach statistical disclosure limitation for official\ndata product releases. It discusses the implications for agencies' broader data\ngovernance and decision-making, and it identifies challenges that agencies will\nlikely face along the way. In conclusion, the chapter proposes some principles\nand best practices that we believe can help guide agencies in navigating the\ntransformation of their confidentiality programs.",
        "authors": [
            "John M Abowd",
            "Michael B Hawes"
        ],
        "categories": "stat.AP",
        "published": "2023-03-01T22:23:35Z",
        "updated": "2023-03-01T22:23:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.00473v1",
        "title": "Generalized Cumulative Shrinkage Process Priors with Applications to Sparse Bayesian Factor Analysis",
        "abstract": "The paper discusses shrinkage priors which impose increasing shrinkage in a\nsequence of parameters. We review the cumulative shrinkage process (CUSP) prior\nof Legramanti et al. (2020), which is a spike-and-slab shrinkage prior where\nthe spike probability is stochastically increasing and constructed from the\nstick-breaking representation of a Dirichlet process prior. As a first\ncontribution, this CUSP prior is extended by involving arbitrary stick-breaking\nrepresentations arising from beta distributions. As a second contribution, we\nprove that exchangeable spike-and-slab priors, which are popular and widely\nused in sparse Bayesian factor analysis, can be represented as a finite\ngeneralized CUSP prior, which is easily obtained from the decreasing order\nstatistics of the slab probabilities. Hence, exchangeable spike-and-slab\nshrinkage priors imply increasing shrinkage as the column index in the loading\nmatrix increases, without imposing explicit order constraints on the slab\nprobabilities. An application to sparse Bayesian factor analysis illustrates\nthe usefulness of the findings of this paper. A new exchangeable spike-and-slab\nshrinkage prior based on the triple gamma prior of Cadonna et al. (2020) is\nintroduced and shown to be helpful for estimating the unknown number of factors\nin a simulation study.",
        "authors": [
            "Sylvia Fr\u00fchwirth-Schnatter"
        ],
        "categories": "stat.ME",
        "published": "2023-03-01T12:57:32Z",
        "updated": "2023-03-01T12:57:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.01231v3",
        "title": "Robust Hicksian Welfare Analysis under Individual Heterogeneity",
        "abstract": "Welfare effects of price changes are often estimated with cross-sections;\nthese do not identify demand with heterogeneous consumers. We develop a\ntheoretical method addressing this, utilizing uncompensated demand moments to\nconstruct local approximations for compensated demand moments, robust to\nunobserved preference heterogeneity. Our methodological contribution offers\nrobust approximations for average and distributional welfare estimates,\nextending to price indices, taxable income elasticities, and general\nequilibrium welfare. Our methods apply to any cross-section; we demonstrate\nthem via UK household budget survey data. We uncover an insight: simple\nnon-parametric representative agent models might be less biased than complex\nparametric models accounting for heterogeneity.",
        "authors": [
            "Sebastiaan Maes",
            "Raghav Malhotra"
        ],
        "categories": "econ.TH",
        "published": "2023-03-01T10:57:56Z",
        "updated": "2023-11-28T11:26:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.00178v2",
        "title": "Disentangling Structural Breaks in Factor Models for Macroeconomic Data",
        "abstract": "Through a routine normalization of the factor variance, standard methods for\nestimating factor models in macroeconomics do not distinguish between breaks of\nthe factor variance and factor loadings. We argue that it is important to\ndistinguish between structural breaks in the factor variance and loadings\nwithin factor models commonly employed in macroeconomics as both can lead to\nmarkedly different interpretations when viewed via the lens of the underlying\ndynamic factor model. We then develop a projection-based decomposition that\nleads to two standard and easy-to-implement Wald tests to disentangle\nstructural breaks in the factor variance and factor loadings. Applying our\nprocedure to U.S. macroeconomic data, we find evidence of both types of breaks\nassociated with the Great Moderation and the Great Recession. Through our\nprojection-based decomposition, we estimate that the Great Moderation is\nassociated with an over 60% reduction in the total factor variance,\nhighlighting the relevance of disentangling breaks in the factor structure.",
        "authors": [
            "Bonsoo Koo",
            "Benjamin Wong",
            "Ze-Yu Zhong"
        ],
        "categories": "stat.ME",
        "published": "2023-03-01T02:10:52Z",
        "updated": "2024-06-04T01:23:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2303.00083v2",
        "title": "Transition Probabilities and Moment Restrictions in Dynamic Fixed Effects Logit Models",
        "abstract": "Dynamic logit models are popular tools in economics to measure state\ndependence. This paper introduces a new method to derive moment restrictions in\na large class of such models with strictly exogenous regressors and fixed\neffects. We exploit the common structure of logit-type transition probabilities\nand elementary properties of rational fractions, to formulate a systematic\nprocedure that scales naturally with model complexity (e.g the lag order or the\nnumber of observed time periods). We detail the construction of moment\nrestrictions in binary response models of arbitrary lag order as well as\nfirst-order panel vector autoregressions and dynamic multinomial logit models.\nIdentification of common parameters and average marginal effects is also\ndiscussed for the binary response case. Finally, we illustrate our results by\nstudying the dynamics of drug consumption amongst young people inspired by Deza\n(2015).",
        "authors": [
            "Kevin Dano"
        ],
        "categories": "econ.EM",
        "published": "2023-02-28T21:00:46Z",
        "updated": "2023-12-04T18:23:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.14423v2",
        "title": "The First-stage F Test with Many Weak Instruments",
        "abstract": "A widely adopted approach for detecting weak instruments is to use the\nfirst-stage $F$ statistic. While this method was developed with a fixed number\nof instruments, its performance with many instruments remains insufficiently\nexplored. We show that the first-stage $F$ test exhibits distorted sizes for\ndetecting many weak instruments, regardless of the choice of pretested\nestimators or Wald tests. These distortions occur due to the inadequate\napproximation using classical noncentral Chi-squared distributions. As a\nbyproduct of our main result, we present an alternative approach to pre-test\nmany weak instruments with the corrected first-stage $F$ statistic. An\nempirical illustration with Angrist and Keueger (1991)'s returns to education\ndata confirms its usefulness.",
        "authors": [
            "Zhenhong Huang",
            "Chen Wang",
            "Jianfeng Yao"
        ],
        "categories": "econ.EM",
        "published": "2023-02-28T09:03:02Z",
        "updated": "2024-09-11T12:00:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.14396v1",
        "title": "A specification test for the strength of instrumental variables",
        "abstract": "This paper develops a new specification test for the instrument weakness when\nthe number of instruments $K_n$ is large with a magnitude comparable to the\nsample size $n$. The test relies on the fact that the difference between the\ntwo-stage least squares (2SLS) estimator and the ordinary least squares (OLS)\nestimator asymptotically disappears when there are many weak instruments, but\notherwise converges to a non-zero limit. We establish the limiting distribution\nof the difference within the above two specifications, and introduce a\ndelete-$d$ Jackknife procedure to consistently estimate the asymptotic\nvariance/covariance of the difference. Monte Carlo experiments demonstrate the\ngood performance of the test procedure for both cases of single and multiple\nendogenous variables. Additionally, we re-examine the analysis of returns to\neducation data in Angrist and Keueger (1991) using our proposed test. Both the\nsimulation results and empirical analysis indicate the reliability of the test.",
        "authors": [
            "Zhenhong Huang",
            "Chen Wang",
            "Jianfeng Yao"
        ],
        "categories": "econ.EM",
        "published": "2023-02-28T08:23:51Z",
        "updated": "2023-02-28T08:23:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.14387v1",
        "title": "Unified and robust Lagrange multiplier type tests for cross-sectional independence in large panel data models",
        "abstract": "This paper revisits the Lagrange multiplier type test for the null hypothesis\nof no cross-sectional dependence in large panel data models. We propose a\nunified test procedure and its power enhancement version, which show robustness\nfor a wide class of panel model contexts. Specifically, the two procedures are\napplicable to both heterogeneous and fixed effects panel data models with the\npresence of weakly exogenous as well as lagged dependent regressors, allowing\nfor a general form of nonnormal error distribution. With the tools from Random\nMatrix Theory, the asymptotic validity of the test procedures is established\nunder the simultaneous limit scheme where the number of time periods and the\nnumber of cross-sectional units go to infinity proportionally. The derived\ntheories are accompanied by detailed Monte Carlo experiments, which confirm the\nrobustness of the two tests and also suggest the validity of the power\nenhancement technique.",
        "authors": [
            "Zhenhong Huang",
            "Zhaoyuan Li",
            "Jianfeng Yao"
        ],
        "categories": "econ.EM",
        "published": "2023-02-28T08:15:52Z",
        "updated": "2023-02-28T08:15:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.14380v1",
        "title": "Identification and Estimation of Categorical Random Coefficient Models",
        "abstract": "This paper proposes a linear categorical random coefficient model, in which\nthe random coefficients follow parametric categorical distributions. The\ndistributional parameters are identified based on a linear recurrence structure\nof moments of the random coefficients. A Generalized Method of Moments\nestimation procedure is proposed also employed by Peter Schmidt and his\ncoauthors to address heterogeneity in time effects in panel data models. Using\nMonte Carlo simulations, we find that moments of the random coefficients can be\nestimated reasonably accurately, but large samples are required for estimation\nof the parameters of the underlying categorical distribution. The utility of\nthe proposed estimator is illustrated by estimating the distribution of returns\nto education in the U.S. by gender and educational levels. We find that rising\nheterogeneity between educational groups is mainly due to the increasing\nreturns to education for those with postsecondary education, whereas within\ngroup heterogeneity has been rising mostly in the case of individuals with high\nschool or less education.",
        "authors": [
            "Zhan Gao",
            "M. Hashem Pesaran"
        ],
        "categories": "econ.EM",
        "published": "2023-02-28T08:06:26Z",
        "updated": "2023-02-28T08:06:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.14180v3",
        "title": "Macroeconomic Forecasting using Dynamic Factor Models: The Case of Morocco",
        "abstract": "This article discusses the use of dynamic factor models in macroeconomic\nforecasting, with a focus on the Factor-Augmented Error Correction Model\n(FECM). The FECM combines the advantages of cointegration and dynamic factor\nmodels, providing a flexible and reliable approach to macroeconomic\nforecasting, especially for non-stationary variables. We evaluate the\nforecasting performance of the FECM model on a large dataset of 117 Moroccan\neconomic series with quarterly frequency. Our study shows that FECM outperforms\ntraditional econometric models in terms of forecasting accuracy and robustness.\nThe inclusion of long-term information and common factors in FECM enhances its\nability to capture economic dynamics and leads to better forecasting\nperformance than other competing models. Our results suggest that FECM can be a\nvaluable tool for macroeconomic forecasting in Morocco and other similar\neconomies.",
        "authors": [
            "Daoui Marouane"
        ],
        "categories": "econ.EM",
        "published": "2023-02-27T22:37:35Z",
        "updated": "2023-05-02T16:54:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.13999v2",
        "title": "Forecasting Macroeconomic Tail Risk in Real Time: Do Textual Data Add Value?",
        "abstract": "We examine the incremental value of news-based data relative to the FRED-MD\neconomic indicators for quantile predictions of employment, output, inflation\nand consumer sentiment in a high-dimensional setting. Our results suggest that\nnews data contain valuable information that is not captured by a large set of\neconomic indicators. We provide empirical evidence that this information can be\nexploited to improve tail risk predictions. The added value is largest when\nmedia coverage and sentiment are combined to compute text-based predictors.\nMethods that capture quantile-specific non-linearities produce overall superior\nforecasts relative to methods that feature linear predictive relationships. The\nresults are robust along different modeling choices.",
        "authors": [
            "Philipp Ad\u00e4mmer",
            "Jan Pr\u00fcser",
            "Rainer Sch\u00fcssler"
        ],
        "categories": "econ.EM",
        "published": "2023-02-27T17:44:34Z",
        "updated": "2024-05-14T14:41:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.13857v3",
        "title": "Multi-cell experiments for marginal treatment effect estimation of digital ads",
        "abstract": "Randomized experiments with treatment and control groups are an important\ntool to measure the impacts of interventions. However, in experimental settings\nwith one-sided noncompliance, extant empirical approaches may not produce the\nestimands a decision-maker needs to solve their problem of interest. For\nexample, these experimental designs are common in digital advertising settings,\nbut typical methods do not yield effects that inform the intensive margin --\nhow many consumers should be reached or how much should be spent on a campaign.\nWe propose a solution that combines a novel multi-cell experimental design with\nmodern estimation techniques that enables decision-makers to recover enough\ninformation to solve problems with an intensive margin. Our design is\nstraightforward to implement and does not require any additional budget to be\ncarried out. We illustrate our approach through a series of simulations that\nare calibrated using an advertising experiment at Facebook, finding that our\nmethod outperforms standard techniques in generating better decisions.",
        "authors": [
            "Caio Waisman",
            "Brett R. Gordon"
        ],
        "categories": "econ.EM",
        "published": "2023-02-27T14:56:15Z",
        "updated": "2024-01-08T17:59:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.13455v3",
        "title": "Nickell Bias in Panel Local Projection: Financial Crises Are Worse Than You Think",
        "abstract": "Local Projection is widely used for impulse response estimation, with the\nFixed Effect (FE) estimator being the default for panel data. This paper\nhighlights the presence of Nickell bias for all regressors in the FE estimator,\neven if lagged dependent variables are absent in the regression. This bias is\nthe consequence of the inherent panel predictive specification. We recommend\nusing the split-panel jackknife estimator to eliminate the asymptotic bias and\nrestore the standard statistical inference. Revisiting three macro-finance\nstudies on the linkage between financial crises and economic contraction, we\nfind that the FE estimator substantially underestimates the post-crisis\neconomic losses.",
        "authors": [
            "Ziwei Mei",
            "Liugang Sheng",
            "Zhentao Shi"
        ],
        "categories": "econ.EM",
        "published": "2023-02-27T00:54:33Z",
        "updated": "2023-10-10T14:26:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.13066v5",
        "title": "Estimating Fiscal Multipliers by Combining Statistical Identification with Potentially Endogenous Proxies",
        "abstract": "Different proxy variables used in fiscal policy SVARs lead to contradicting\nconclusions regarding the size of fiscal multipliers. We show that the\nconflicting results are due to violations of the exogeneity assumptions, i.e.\nthe commonly used proxies are endogenously related to the structural shocks. We\npropose a novel approach to include proxy variables into a Bayesian\nnon-Gaussian SVAR, tailored to accommodate for potentially endogenous proxy\nvariables. Using our model, we show that increasing government spending is a\nmore effective tool to stimulate the economy than reducing taxes.",
        "authors": [
            "Sascha A. Keweloh",
            "Mathias Klein",
            "Jan Pr\u00fcser"
        ],
        "categories": "econ.EM",
        "published": "2023-02-25T11:51:05Z",
        "updated": "2024-05-09T06:57:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.12777v1",
        "title": "On the Misspecification of Linear Assumptions in Synthetic Control",
        "abstract": "The synthetic control (SC) method is a popular approach for estimating\ntreatment effects from observational panel data. It rests on a crucial\nassumption that we can write the treated unit as a linear combination of the\nuntreated units. This linearity assumption, however, can be unlikely to hold in\npractice and, when violated, the resulting SC estimates are incorrect. In this\npaper we examine two questions: (1) How large can the misspecification error\nbe? (2) How can we limit it? First, we provide theoretical bounds to quantify\nthe misspecification error. The bounds are comforting: small misspecifications\ninduce small errors. With these bounds in hand, we then develop new SC\nestimators that are specially designed to minimize misspecification error. The\nestimators are based on additional data about each unit, which is used to\nproduce the SC weights. (For example, if the units are countries then the\nadditional data might be demographic information about each.) We study our\nestimators on synthetic data; we find they produce more accurate causal\nestimates than standard synthetic controls. We then re-analyze the California\ntobacco-program data of the original SC paper, now including additional data\nfrom the US census about per-state demographics. Our estimators show that the\nobservations in the pre-treatment period lie within the bounds of\nmisspecification error, and that the observations post-treatment lie outside of\nthose bounds. This is evidence that our SC methods have uncovered a true\neffect.",
        "authors": [
            "Achille Nazaret",
            "Claudia Shi",
            "David M. Blei"
        ],
        "categories": "stat.ME",
        "published": "2023-02-24T17:48:48Z",
        "updated": "2023-02-24T17:48:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.12670v1",
        "title": "Personalized Pricing with Invalid Instrumental Variables: Identification, Estimation, and Policy Learning",
        "abstract": "Pricing based on individual customer characteristics is widely used to\nmaximize sellers' revenues. This work studies offline personalized pricing\nunder endogeneity using an instrumental variable approach. Standard\ninstrumental variable methods in causal inference/econometrics either focus on\na discrete treatment space or require the exclusion restriction of instruments\nfrom having a direct effect on the outcome, which limits their applicability in\npersonalized pricing. In this paper, we propose a new policy learning method\nfor Personalized pRicing using Invalid iNsTrumental variables (PRINT) for\ncontinuous treatment that allow direct effects on the outcome. Specifically,\nrelying on the structural models of revenue and price, we establish the\nidentifiability condition of an optimal pricing strategy under endogeneity with\nthe help of invalid instrumental variables. Based on this new identification,\nwhich leads to solving conditional moment restrictions with generalized\nresidual functions, we construct an adversarial min-max estimator and learn an\noptimal pricing strategy. Furthermore, we establish an asymptotic regret bound\nto find an optimal pricing strategy. Finally, we demonstrate the effectiveness\nof the proposed method via extensive simulation studies as well as a real data\napplication from an US online auto loan company.",
        "authors": [
            "Rui Miao",
            "Zhengling Qi",
            "Cong Shi",
            "Lin Lin"
        ],
        "categories": "stat.ME",
        "published": "2023-02-24T14:50:47Z",
        "updated": "2023-02-24T14:50:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.11715v2",
        "title": "Variable Importance Matching for Causal Inference",
        "abstract": "Our goal is to produce methods for observational causal inference that are\nauditable, easy to troubleshoot, accurate for treatment effect estimation, and\nscalable to high-dimensional data. We describe a general framework called\nModel-to-Match that achieves these goals by (i) learning a distance metric via\noutcome modeling, (ii) creating matched groups using the distance metric, and\n(iii) using the matched groups to estimate treatment effects. Model-to-Match\nuses variable importance measurements to construct a distance metric, making it\na flexible framework that can be adapted to various applications. Concentrating\non the scalability of the problem in the number of potential confounders, we\noperationalize the Model-to-Match framework with LASSO. We derive performance\nguarantees for settings where LASSO outcome modeling consistently identifies\nall confounders (importantly without requiring the linear model to be correctly\nspecified). We also provide experimental results demonstrating the method's\nauditability, accuracy, and scalability as well as extensions to more general\nnonparametric outcome modeling.",
        "authors": [
            "Quinn Lanners",
            "Harsh Parikh",
            "Alexander Volfovsky",
            "Cynthia Rudin",
            "David Page"
        ],
        "categories": "stat.ME",
        "published": "2023-02-23T00:43:03Z",
        "updated": "2023-06-28T22:19:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.11505v4",
        "title": "Decomposition and Interpretation of Treatment Effects in Settings with Delayed Outcomes",
        "abstract": "This paper studies settings where the analyst is interested in identifying\nand estimating the average causal effect of a binary treatment on an outcome.\nWe consider a setup in which the outcome realization does not get immediately\nrealized after the treatment assignment, a feature that is ubiquitous in\nempirical settings. The period between the treatment and the realization of the\noutcome allows other observed actions to occur and affect the outcome. In this\ncontext, we study several regression-based estimands routinely used in\nempirical work to capture the average treatment effect and shed light on\ninterpreting them in terms of ceteris paribus effects, indirect causal effects,\nand selection terms. We obtain three main and related takeaways. First, the\nthree most popular estimands do not generally satisfy what we call \\emph{strong\nsign preservation}, in the sense that these estimands may be negative even when\nthe treatment positively affects the outcome conditional on any possible\ncombination of other actions. Second, the most popular regression that includes\nthe other actions as controls satisfies strong sign preservation \\emph{if and\nonly if} these actions are mutually exclusive binary variables. Finally, we\nshow that a linear regression that fully stratifies the other actions leads to\nestimands that satisfy strong sign preservation.",
        "authors": [
            "Federico A. Bugni",
            "Ivan A. Canay",
            "Steve McBride"
        ],
        "categories": "econ.EM",
        "published": "2023-02-22T17:22:58Z",
        "updated": "2024-09-20T16:37:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.09871v1",
        "title": "Attitudes and Latent Class Choice Models using Machine learning",
        "abstract": "Latent Class Choice Models (LCCM) are extensions of discrete choice models\n(DCMs) that capture unobserved heterogeneity in the choice process by\nsegmenting the population based on the assumption of preference similarities.\nWe present a method of efficiently incorporating attitudinal indicators in the\nspecification of LCCM, by introducing Artificial Neural Networks (ANN) to\nformulate latent variables constructs. This formulation overcomes structural\nequations in its capability of exploring the relationship between the\nattitudinal indicators and the decision choice, given the Machine Learning (ML)\nflexibility and power in capturing unobserved and complex behavioural features,\nsuch as attitudes and beliefs. All of this while still maintaining the\nconsistency of the theoretical assumptions presented in the Generalized Random\nUtility model and the interpretability of the estimated parameters. We test our\nproposed framework for estimating a Car-Sharing (CS) service subscription\nchoice with stated preference data from Copenhagen, Denmark. The results show\nthat our proposed approach provides a complete and realistic segmentation,\nwhich helps design better policies.",
        "authors": [
            "Lorena Torres Lahoz",
            "Francisco Camara Pereira",
            "Georges Sfeir",
            "Ioanna Arkoudi",
            "Mayara Moraes Monteiro",
            "Carlos Lima Azevedo"
        ],
        "categories": "econ.EM",
        "published": "2023-02-20T10:03:01Z",
        "updated": "2023-02-20T10:03:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.09756v4",
        "title": "Identification-robust inference for the LATE with high-dimensional covariates",
        "abstract": "This paper presents an inference method for the local average treatment\neffect (LATE) in the presence of high-dimensional covariates, irrespective of\nthe strength of identification. We propose a novel high-dimensional conditional\ntest statistic with uniformly correct asymptotic size. We provide an\neasy-to-implement algorithm to infer the high-dimensional LATE by inverting our\ntest statistic and employing the double/debiased machine learning method.\nSimulations indicate that our test is robust against both weak identification\nand high dimensionality concerning size control and power performance,\noutperforming other conventional tests. Applying the proposed method to\nrailroad and population data to study the effect of railroad access on urban\npopulation growth, we observe that our methodology yields confidence intervals\nthat are 49% to 92% shorter than conventional results, depending on\nspecifications.",
        "authors": [
            "Yukun Ma"
        ],
        "categories": "econ.EM",
        "published": "2023-02-20T04:28:50Z",
        "updated": "2023-11-09T16:48:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.09255v2",
        "title": "Clustered Covariate Regression",
        "abstract": "High covariate dimensionality is increasingly occurrent in model estimation,\nand existing techniques to address this issue typically require sparsity or\ndiscrete heterogeneity of the unobservable parameter vector. However, neither\nrestriction may be supported by economic theory in some empirical contexts,\nleading to severe bias and misleading inference. The clustering-based grouped\nparameter estimator (GPE) introduced in this paper drops both restrictions in\nfavour of the natural one that the parameter support be compact. GPE exhibits\nrobust large sample properties under standard conditions and accommodates both\nsparse and non-sparse parameters whose support can be bounded away from zero.\nExtensive Monte Carlo simulations demonstrate the excellent performance of GPE\nin terms of bias reduction and size control compared to competing estimators.\nAn empirical application of GPE to estimating price and income elasticities of\ndemand for gasoline highlights its practical utility.",
        "authors": [
            "Abdul-Nasah Soale",
            "Emmanuel Selorm Tsyawo"
        ],
        "categories": "econ.EM",
        "published": "2023-02-18T08:01:47Z",
        "updated": "2023-07-31T17:35:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.08854v3",
        "title": "Post Reinforcement Learning Inference",
        "abstract": "We consider estimation and inference using data collected from reinforcement\nlearning algorithms. These algorithms, characterized by their adaptive\nexperimentation, interact with individual units over multiple stages,\ndynamically adjusting their strategies based on previous interactions. Our goal\nis to evaluate a counterfactual policy post-data collection and estimate\nstructural parameters, like dynamic treatment effects, which can be used for\ncredit assignment and determining the effect of earlier actions on final\noutcomes. Such parameters of interest can be framed as solutions to moment\nequations, but not minimizers of a population loss function, leading to\nZ-estimation approaches for static data. However, in the adaptive data\ncollection environment of reinforcement learning, where algorithms deploy\nnonstationary behavior policies, standard estimators do not achieve asymptotic\nnormality due to the fluctuating variance. We propose a weighted Z-estimation\napproach with carefully designed adaptive weights to stabilize the time-varying\nestimation variance. We identify proper weighting schemes to restore the\nconsistency and asymptotic normality of the weighted Z-estimators for target\nparameters, which allows for hypothesis testing and constructing uniform\nconfidence regions. Primary applications include dynamic treatment effect\nestimation and dynamic off-policy evaluation.",
        "authors": [
            "Vasilis Syrgkanis",
            "Ruohan Zhan"
        ],
        "categories": "stat.ML",
        "published": "2023-02-17T12:53:15Z",
        "updated": "2024-05-11T03:31:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.08097v1",
        "title": "New $\\sqrt{n}$-consistent, numerically stable higher-order influence function estimators",
        "abstract": "Higher-Order Influence Functions (HOIFs) provide a unified theory for\nconstructing rate-optimal estimators for a large class of low-dimensional\n(smooth) statistical functionals/parameters (and sometimes even\ninfinite-dimensional functions) that arise in substantive fields including\nepidemiology, economics, and the social sciences. Since the introduction of\nHOIFs by Robins et al. (2008), they have been viewed mostly as a theoretical\nbenchmark rather than a useful tool for statistical practice. Works aimed to\nflip the script are scant, but a few recent papers Liu et al. (2017, 2021b)\nmake some partial progress. In this paper, we take a fresh attempt at achieving\nthis goal by constructing new, numerically stable HOIF estimators (or sHOIF\nestimators for short with ``s'' standing for ``stable'') with provable\nstatistical, numerical, and computational guarantees. This new class of sHOIF\nestimators (up to the 2nd order) was foreshadowed in synthetic experiments\nconducted by Liu et al. (2020a).",
        "authors": [
            "Lin Liu",
            "Chang Li"
        ],
        "categories": "math.ST",
        "published": "2023-02-16T05:25:21Z",
        "updated": "2023-02-16T05:25:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.08002v2",
        "title": "Deep Learning Enhanced Realized GARCH",
        "abstract": "We propose a new approach to volatility modeling by combining deep learning\n(LSTM) and realized volatility measures. This LSTM-enhanced realized GARCH\nframework incorporates and distills modeling advances from financial\neconometrics, high frequency trading data and deep learning. Bayesian inference\nvia the Sequential Monte Carlo method is employed for statistical inference and\nforecasting. The new framework can jointly model the returns and realized\nvolatility measures, has an excellent in-sample fit and superior predictive\nperformance compared to several benchmark models, while being able to adapt\nwell to the stylized facts in volatility. The performance of the new framework\nis tested using a wide range of metrics, from marginal likelihood, volatility\nforecasting, to tail risk forecasting and option pricing. We report on a\ncomprehensive empirical study using 31 widely traded stock indices over a time\nperiod that includes COVID-19 pandemic.",
        "authors": [
            "Chen Liu",
            "Chao Wang",
            "Minh-Ngoc Tran",
            "Robert Kohn"
        ],
        "categories": "econ.EM",
        "published": "2023-02-16T00:20:43Z",
        "updated": "2023-10-17T09:37:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.07413v2",
        "title": "A Guide to Regression Discontinuity Designs in Medical Applications",
        "abstract": "We present a practical guide for the analysis of regression discontinuity\n(RD) designs in biomedical contexts. We begin by introducing key concepts,\nassumptions, and estimands within both the continuity-based framework and the\nlocal randomization framework. We then discuss modern estimation and inference\nmethods within both frameworks, including approaches for bandwidth or local\nneighborhood selection, optimal treatment effect point estimation, and robust\nbias-corrected inference methods for uncertainty quantification. We also\noverview empirical falsification tests that can be used to support key\nassumptions. Our discussion focuses on two particular features that are\nrelevant in biomedical research: (i) fuzzy RD designs, which often arise when\ntherapeutic treatments are based on clinical guidelines but patients with\nscores near the cutoff are treated contrary to the assignment rule; and (ii) RD\ndesigns with discrete scores, which are ubiquitous in biomedical applications.\nWe illustrate our discussion with three empirical applications: the effect of\nCD4 guidelines for anti-retroviral therapy on retention of HIV patients in\nSouth Africa, the effect of genetic guidelines for chemotherapy on breast\ncancer recurrence in the United States, and the effects of age-based patient\ncost-sharing on healthcare utilization in Taiwan. We provide replication\nmaterials employing publicly available statistical software in Python, R and\nStata, offering researchers all necessary tools to conduct an RD analysis.",
        "authors": [
            "Matias D. Cattaneo",
            "Luke Keele",
            "Rocio Titiunik"
        ],
        "categories": "stat.ME",
        "published": "2023-02-15T00:30:20Z",
        "updated": "2023-05-16T15:15:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.07052v1",
        "title": "Sequential Estimation of Multivariate Factor Stochastic Volatility Models",
        "abstract": "We provide a simple method to estimate the parameters of multivariate\nstochastic volatility models with latent factor structures. These models are\nvery useful as they alleviate the standard curse of dimensionality, allowing\nthe number of parameters to increase only linearly with the number of the\nreturn series. Although theoretically very appealing, these models have only\nfound limited practical application due to huge computational burdens. Our\nestimation method is simple in implementation as it consists of two steps:\nfirst, we estimate the loadings and the unconditional variances by maximum\nlikelihood, and then we use the efficient method of moments to estimate the\nparameters of the stochastic volatility structure with GARCH as an auxiliary\nmodel. In a comprehensive Monte Carlo study we show the good performance of our\nmethod to estimate the parameters of interest accurately. The simulation study\nand an application to real vectors of daily returns of dimensions up to 148\nshow the method's computation advantage over the existing estimation\nprocedures.",
        "authors": [
            "Giorgio Calzolari",
            "Roxana Halbleib",
            "Christian M\u00fccher"
        ],
        "categories": "econ.EM",
        "published": "2023-02-14T14:06:16Z",
        "updated": "2023-02-14T14:06:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.06799v2",
        "title": "Quantiled conditional variance, skewness, and kurtosis by Cornish-Fisher expansion",
        "abstract": "The conditional variance, skewness, and kurtosis play a central role in time\nseries analysis. These three conditional moments (CMs) are often studied by\nsome parametric models but with two big issues: the risk of model\nmis-specification and the instability of model estimation. To avoid the above\ntwo issues, this paper proposes a novel method to estimate these three CMs by\nthe so-called quantiled CMs (QCMs). The QCM method first adopts the idea of\nCornish-Fisher expansion to construct a linear regression model, based on $n$\ndifferent estimated conditional quantiles. Next, it computes the QCMs simply\nand simultaneously by using the ordinary least squares estimator of this\nregression model, without any prior estimation of the conditional mean. Under\ncertain conditions, the QCMs are shown to be consistent with the convergence\nrate $n^{-1/2}$. Simulation studies indicate that the QCMs perform well under\ndifferent scenarios of Cornish-Fisher expansion errors and quantile estimation\nerrors. In the application, the study of QCMs for three exchange rates\ndemonstrates the effectiveness of financial rescue plans during the COVID-19\npandemic outbreak, and suggests that the existing ``news impact curve''\nfunctions for the conditional skewness and kurtosis may not be suitable.",
        "authors": [
            "Ningning Zhang",
            "Ke Zhu"
        ],
        "categories": "stat.ME",
        "published": "2023-02-14T02:45:01Z",
        "updated": "2023-06-06T05:42:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.05747v4",
        "title": "Individualized Treatment Allocation in Sequential Network Games",
        "abstract": "Designing individualized allocation of treatments so as to maximize the\nequilibrium welfare of interacting agents has many policy-relevant\napplications. Focusing on sequential decision games of interacting agents, this\npaper develops a method to obtain optimal treatment assignment rules that\nmaximize a social welfare criterion by evaluating stationary distributions of\noutcomes. Stationary distributions in sequential decision games are given by\nGibbs distributions, which are difficult to optimize with respect to a\ntreatment allocation due to analytical and computational complexity. We apply a\nvariational approximation to the stationary distribution and optimize the\napproximated equilibrium welfare with respect to treatment allocation using a\ngreedy optimization algorithm. We characterize the performance of the\nvariational approximation, deriving a performance guarantee for the greedy\noptimization algorithm via a welfare regret bound. We implement our proposed\nmethod in simulation exercises and an empirical application using the Indian\nmicrofinance data (Banerjee et al., 2013), and show it delivers significant\nwelfare gains.",
        "authors": [
            "Toru Kitagawa",
            "Guanyi Wang"
        ],
        "categories": "econ.EM",
        "published": "2023-02-11T17:19:32Z",
        "updated": "2024-07-10T21:54:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.05404v1",
        "title": "Minimax Instrumental Variable Regression and $L_2$ Convergence Guarantees without Identification or Closedness",
        "abstract": "In this paper, we study nonparametric estimation of instrumental variable\n(IV) regressions. Recently, many flexible machine learning methods have been\ndeveloped for instrumental variable estimation. However, these methods have at\nleast one of the following limitations: (1) restricting the IV regression to be\nuniquely identified; (2) only obtaining estimation error rates in terms of\npseudometrics (\\emph{e.g.,} projected norm) rather than valid metrics\n(\\emph{e.g.,} $L_2$ norm); or (3) imposing the so-called closedness condition\nthat requires a certain conditional expectation operator to be sufficiently\nsmooth. In this paper, we present the first method and analysis that can avoid\nall three limitations, while still permitting general function approximation.\nSpecifically, we propose a new penalized minimax estimator that can converge to\na fixed IV solution even when there are multiple solutions, and we derive a\nstrong $L_2$ error rate for our estimator under lax conditions. Notably, this\nguarantee only needs a widely-used source condition and realizability\nassumptions, but not the so-called closedness condition. We argue that the\nsource condition and the closedness condition are inherently conflicting, so\nrelaxing the latter significantly improves upon the existing literature that\nrequires both conditions. Our estimator can achieve this improvement because it\nbuilds on a novel formulation of the IV estimation problem as a constrained\noptimization problem.",
        "authors": [
            "Andrew Bennett",
            "Nathan Kallus",
            "Xiaojie Mao",
            "Whitney Newey",
            "Vasilis Syrgkanis",
            "Masatoshi Uehara"
        ],
        "categories": "stat.ML",
        "published": "2023-02-10T18:08:49Z",
        "updated": "2023-02-10T18:08:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.05260v2",
        "title": "Policy Learning with Rare Outcomes",
        "abstract": "Machine learning (ML) estimates of conditional average treatment effects\n(CATE) can guide policy decisions, either by allowing targeting of individuals\nwith beneficial CATE estimates, or as inputs to decision trees that optimise\noverall outcomes. There is limited information available regarding how well\nthese algorithms perform in real-world policy evaluation scenarios. Using\nsynthetic data, we compare the finite sample performance of different policy\nlearning algorithms, machine learning techniques employed during their learning\nphases, and methods for presenting estimated policy values. For each algorithm,\nwe assess the resulting treatment allocation by measuring deviation from the\nideal (\"oracle\") policy. Our main finding is that policy trees based on\nestimated CATEs outperform trees learned from doubly-robust scores. Across\nsettings, Causal Forests and the Normalised Double-Robust Learner perform\nconsistently well, while Bayesian Additive Regression Trees perform poorly.\nThese methods are then applied to a case study targeting optimal allocation of\nsubsidised health insurance, with the goal of reducing infant mortality in\nIndonesia.",
        "authors": [
            "Julia Hatamyar",
            "Noemi Kreif"
        ],
        "categories": "econ.EM",
        "published": "2023-02-10T14:16:55Z",
        "updated": "2023-10-03T10:33:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.05193v1",
        "title": "Structural Break Detection in Quantile Predictive Regression Models with Persistent Covariates",
        "abstract": "We propose an econometric environment for structural break detection in\nnonstationary quantile predictive regressions. We establish the limit\ndistributions for a class of Wald and fluctuation type statistics based on both\nthe ordinary least squares estimator and the endogenous instrumental regression\nestimator proposed by Phillips and Magdalinos (2009a, Econometric Inference in\nthe Vicinity of Unity. Working paper, Singapore Management University).\nAlthough the asymptotic distribution of these test statistics appears to depend\non the chosen estimator, the IVX based tests are shown to be asymptotically\nnuisance parameter-free regardless of the degree of persistence and consistent\nunder local alternatives. The finite-sample performance of both tests is\nevaluated via simulation experiments. An empirical application to house pricing\nindex returns demonstrates the practicality of the proposed break tests for\nregression quantiles of nonstationary time series data.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-02-10T11:45:10Z",
        "updated": "2023-02-10T11:45:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.05089v1",
        "title": "On semiparametric estimation of the intercept of the sample selection model: a kernel approach",
        "abstract": "This paper presents a new perspective on the identification at infinity for\nthe intercept of the sample selection model as identification at the boundary\nvia a transformation of the selection index. This perspective suggests\ngeneralizations of estimation at infinity to kernel regression estimation at\nthe boundary and further to local linear estimation at the boundary. The\nproposed kernel-type estimators with an estimated transformation are proven to\nbe nonparametric-rate consistent and asymptotically normal under mild\nregularity conditions. A fully data-driven method of selecting the optimal\nbandwidths for the estimators is developed. The Monte Carlo simulation shows\nthe desirable finite sample properties of the proposed estimators and bandwidth\nselection procedures.",
        "authors": [
            "Zhewen Pan"
        ],
        "categories": "econ.EM",
        "published": "2023-02-10T07:16:49Z",
        "updated": "2023-02-10T07:16:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.04380v3",
        "title": "Covariate Adjustment in Experiments with Matched Pairs",
        "abstract": "This paper studies inference on the average treatment effect in experiments\nin which treatment status is determined according to \"matched pairs\" and it is\nadditionally desired to adjust for observed, baseline covariates to gain\nfurther precision. By a \"matched pairs\" design, we mean that units are sampled\ni.i.d. from the population of interest, paired according to observed, baseline\ncovariates and finally, within each pair, one unit is selected at random for\ntreatment. Importantly, we presume that not all observed, baseline covariates\nare used in determining treatment assignment. We study a broad class of\nestimators based on a \"doubly robust\" moment condition that permits us to study\nestimators with both finite-dimensional and high-dimensional forms of covariate\nadjustment. We find that estimators with finite-dimensional, linear adjustments\nneed not lead to improvements in precision relative to the unadjusted\ndifference-in-means estimator. This phenomenon persists even if the adjustments\nare interacted with treatment; in fact, doing so leads to no changes in\nprecision. However, gains in precision can be ensured by including fixed\neffects for each of the pairs. Indeed, we show that this adjustment is the\n\"optimal\" finite-dimensional, linear adjustment. We additionally study two\nestimators with high-dimensional forms of covariate adjustment based on the\nLASSO. For each such estimator, we show that it leads to improvements in\nprecision relative to the unadjusted difference-in-means estimator and also\nprovide conditions under which it leads to the \"optimal\" nonparametric,\ncovariate adjustment. A simulation study confirms the practical relevance of\nour theoretical analysis, and the methods are employed to reanalyze data from\nan experiment using a \"matched pairs\" design to study the effect of\nmacroinsurance on microenterprise.",
        "authors": [
            "Yuehao Bai",
            "Liang Jiang",
            "Joseph P. Romano",
            "Azeem M. Shaikh",
            "Yichong Zhang"
        ],
        "categories": "econ.EM",
        "published": "2023-02-09T00:12:32Z",
        "updated": "2023-10-18T20:17:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.04354v3",
        "title": "Consider or Choose? The Role and Power of Consideration Sets",
        "abstract": "Consideration sets play a crucial role in discrete choice modeling, where\ncustomers are commonly assumed to go through a two-stage decision making\nprocess. Specifically, customers are assumed to form consideration sets in the\nfirst stage and then use a second-stage choice mechanism to pick the product\nwith the highest utility from the consideration sets. Recent studies mostly aim\nto propose more powerful choice mechanisms based on advanced non-parametric\nmodels to improve prediction accuracy. In contrast, this paper takes a step\nback from exploring more complex second-stage choice mechanisms and instead\nfocus on how effectively we can model customer choice relying only on the\nfirst-stage consideration set formation. To this end, we study a class of\nnonparametric choice models that is only specified by a distribution over\nconsideration sets and has a bounded rationality interpretation. We denote it\nas the consideration set model. Intriguingly, we show that this class of choice\nmodels can be characterized by the axiom of symmetric demand cannibalization,\nwhich enables complete statistical identification. We further consider the\nmodel's downstream assortment planning as an application. We first present an\nexact description of the optimal assortment, proving that it is revenue-ordered\nbased on the blocks defined by the consideration sets. Despite this compelling\nstructure, we establish that the assortment optimization problem under this\nmodel is NP-hard even to approximate. This result shows that accounting for\nconsideration sets in the model inevitably results in inapproximability in\nassortment planning, even though the consideration set model uses the simplest\npossible uniform second-stage choice mechanism. Finally, using a real-world\ndataset, we show the tremendous power of the first-stage consideration sets\nwhen modeling customers' decision-making processes.",
        "authors": [
            "Yi-Chun Akchen",
            "Dmitry Mitrofanov"
        ],
        "categories": "econ.EM",
        "published": "2023-02-08T22:05:02Z",
        "updated": "2024-06-14T18:07:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.03996v2",
        "title": "High-Dimensional Granger Causality for Climatic Attribution",
        "abstract": "In this paper we test for Granger causality in high-dimensional vector\nautoregressive models (VARs) to disentangle and interpret the complex causal\nchains linking radiative forcings and global temperatures. By allowing for high\ndimensionality in the model, we can enrich the information set with relevant\nnatural and anthropogenic forcing variables to obtain reliable causal\nrelations. This provides a step forward from existing climatology literature,\nwhich has mostly treated these variables in isolation in small models.\nAdditionally, our framework allows to disregard the order of integration of the\nvariables by directly estimating the VAR in levels, thus avoiding accumulating\nbiases coming from unit-root and cointegration tests. This is of particular\nappeal for climate time series which are well known to contain stochastic\ntrends and long memory. We are thus able to establish causal networks linking\nradiative forcings to global temperatures and to connect radiative forcings\namong themselves, thereby allowing for tracing the path of dynamic causal\neffects through the system.",
        "authors": [
            "Marina Friedrich",
            "Luca Margaritella",
            "Stephan Smeekes"
        ],
        "categories": "econ.EM",
        "published": "2023-02-08T11:12:12Z",
        "updated": "2024-06-03T07:44:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.08323v1",
        "title": "Reevaluating the Taylor Rule with Machine Learning",
        "abstract": "This paper aims to reevaluate the Taylor Rule, through a linear and a\nnonlinear method, such that its estimated federal funds rates match those\nactually previously implemented by the Federal Reserve Bank. In the linear\nmethod, this paper uses an OLS regression model to find more accurate\ncoefficients within the same Taylor Rule equation in which the dependent\nvariable is the federal funds rate, and the independent variables are the\ninflation rate, the inflation gap, and the output gap. The intercept in the OLS\nregression model would capture the constant equilibrium target real interest\nrate set at 2. The linear OLS method suggests that the Taylor Rule\noverestimates the output gap and standalone inflation rate's coefficients for\nthe Taylor Rule. The coefficients this paper suggests are shown in equation\n(2). In the nonlinear method, this paper uses a machine learning system in\nwhich the two inputs are the inflation rate and the output gap and the output\nis the federal funds rate. This system utilizes gradient descent error\nminimization to create a model that minimizes the error between the estimated\nfederal funds rate and the actual previously implemented federal funds rate.\nSince the machine learning system allows the model to capture the more\nrealistic nonlinear relationship between the variables, it significantly\nincreases the estimation accuracy as a result. The actual and estimated federal\nfunds rates are almost identical besides three recessions caused by bubble\nbursts, which the paper addresses in the concluding remarks. Overall, the first\nmethod provides theoretical insight while the second suggests a model with\nimproved applicability.",
        "authors": [
            "Alper Deniz Karakas"
        ],
        "categories": "econ.GN",
        "published": "2023-02-07T21:41:11Z",
        "updated": "2023-02-07T21:41:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.03687v4",
        "title": "Covariate Adjustment in Stratified Experiments",
        "abstract": "This paper studies covariate adjusted estimation of the average treatment\neffect in stratified experiments. We work in a general framework that includes\nmatched tuples designs, coarse stratification, and complete randomization as\nspecial cases. Regression adjustment with treatment-covariate interactions is\nknown to weakly improve efficiency for completely randomized designs. By\ncontrast, we show that for stratified designs such regression estimators are\ngenerically inefficient, potentially even increasing estimator variance\nrelative to the unadjusted benchmark. Motivated by this result, we derive the\nasymptotically optimal linear covariate adjustment for a given stratification.\nWe construct several feasible estimators that implement this efficient\nadjustment in large samples. In the special case of matched pairs, for example,\nthe regression including treatment, covariates, and pair fixed effects is\nasymptotically optimal. We also provide novel asymptotically exact inference\nmethods that allow researchers to report smaller confidence intervals, fully\nreflecting the efficiency gains from both stratification and adjustment.\nSimulations and an empirical application demonstrate the value of our proposed\nmethods.",
        "authors": [
            "Max Cytrynbaum"
        ],
        "categories": "econ.EM",
        "published": "2023-02-07T18:59:41Z",
        "updated": "2024-07-20T22:09:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.03172v1",
        "title": "High-Dimensional Conditionally Gaussian State Space Models with Missing Data",
        "abstract": "We develop an efficient sampling approach for handling complex missing data\npatterns and a large number of missing observations in conditionally Gaussian\nstate space models. Two important examples are dynamic factor models with\nunbalanced datasets and large Bayesian VARs with variables in multiple\nfrequencies. A key insight underlying the proposed approach is that the joint\ndistribution of the missing data conditional on the observed data is Gaussian.\nMoreover, the inverse covariance or precision matrix of this conditional\ndistribution is sparse, and this special structure can be exploited to\nsubstantially speed up computations. We illustrate the methodology using two\nempirical applications. The first application combines quarterly, monthly and\nweekly data using a large Bayesian VAR to produce weekly GDP estimates. In the\nsecond application, we extract latent factors from unbalanced datasets\ninvolving over a hundred monthly variables via a dynamic factor model with\nstochastic volatility.",
        "authors": [
            "Joshua C. C. Chan",
            "Aubrey Poon",
            "Dan Zhu"
        ],
        "categories": "econ.EM",
        "published": "2023-02-07T00:14:38Z",
        "updated": "2023-02-07T00:14:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.03131v1",
        "title": "Extensions for Inference in Difference-in-Differences with Few Treated Clusters",
        "abstract": "In settings with few treated units, Difference-in-Differences (DID)\nestimators are not consistent, and are not generally asymptotically normal.\nThis poses relevant challenges for inference. While there are inference methods\nthat are valid in these settings, some of these alternatives are not readily\navailable when there is variation in treatment timing and heterogeneous\ntreatment effects; or for deriving uniform confidence bands for event-study\nplots. We present alternatives in settings with few treated units that are\nvalid with variation in treatment timing and/or that allow for uniform\nconfidence bands.",
        "authors": [
            "Luis Alvarez",
            "Bruno Ferman"
        ],
        "categories": "econ.EM",
        "published": "2023-02-06T21:37:35Z",
        "updated": "2023-02-06T21:37:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.03117v1",
        "title": "Asymptotic Representations for Sequential Decisions, Adaptive Experiments, and Batched Bandits",
        "abstract": "We develop asymptotic approximation results that can be applied to sequential\nestimation and inference problems, adaptive randomized controlled trials, and\nother statistical decision problems that involve multiple decision nodes with\nstructured and possibly endogenous information sets. Our results extend the\nclassic asymptotic representation theorem used extensively in efficiency bound\ntheory and local power analysis. In adaptive settings where the decision at one\nstage can affect the observation of variables in later stages, we show that a\nlimiting data environment characterizes all limit distributions attainable\nthrough a joint choice of an adaptive design rule and statistics applied to the\nadaptively generated data, under local alternatives. We illustrate how the\ntheory can be applied to study the choice of adaptive rules and end-of-sample\nstatistical inference in batched (groupwise) sequential adaptive experiments.",
        "authors": [
            "Keisuke Hirano",
            "Jack R. Porter"
        ],
        "categories": "econ.EM",
        "published": "2023-02-06T20:54:08Z",
        "updated": "2023-02-06T20:54:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.02988v2",
        "title": "Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds",
        "abstract": "We investigate the problem of fixed-budget best arm identification (BAI) for\nminimizing expected simple regret. In an adaptive experiment, a decision maker\ndraws one of multiple treatment arms based on past observations and observes\nthe outcome of the drawn arm. After the experiment, the decision maker\nrecommends the treatment arm with the highest expected outcome. We evaluate the\ndecision based on the expected simple regret, which is the difference between\nthe expected outcomes of the best arm and the recommended arm. Due to inherent\nuncertainty, we evaluate the regret using the minimax criterion. First, we\nderive asymptotic lower bounds for the worst-case expected simple regret, which\nare characterized by the variances of potential outcomes (leading factor).\nBased on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder\n(HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) in\nrecommending the best arm. Our theoretical analysis shows that the TS-HIR\nstrategy is asymptotically minimax optimal, meaning that the leading factor of\nits worst-case expected simple regret matches our derived worst-case lower\nbound. Additionally, we consider extensions of our method, such as the\nasymptotic optimality for the probability of misidentification. Finally, we\nvalidate the proposed method's effectiveness through simulations.",
        "authors": [
            "Masahiro Kato",
            "Masaaki Imaizumi",
            "Takuya Ishihara",
            "Toru Kitagawa"
        ],
        "categories": "cs.LG",
        "published": "2023-02-06T18:27:11Z",
        "updated": "2023-07-12T16:06:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.02923v2",
        "title": "In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation",
        "abstract": "Personalized treatment effect estimates are often of interest in high-stakes\napplications -- thus, before deploying a model estimating such effects in\npractice, one needs to be sure that the best candidate from the ever-growing\nmachine learning toolbox for this task was chosen. Unfortunately, due to the\nabsence of counterfactual information in practice, it is usually not possible\nto rely on standard validation metrics for doing so, leading to a well-known\nmodel selection dilemma in the treatment effect estimation literature. While\nsome solutions have recently been investigated, systematic understanding of the\nstrengths and weaknesses of different model selection criteria is still\nlacking. In this paper, instead of attempting to declare a global `winner', we\ntherefore empirically investigate success- and failure modes of different\nselection criteria. We highlight that there is a complex interplay between\nselection strategies, candidate estimators and the data used for comparing\nthem, and provide interesting insights into the relative (dis)advantages of\ndifferent criteria alongside desiderata for the design of further illuminating\nempirical studies in this context.",
        "authors": [
            "Alicia Curth",
            "Mihaela van der Schaar"
        ],
        "categories": "stat.ML",
        "published": "2023-02-06T16:55:37Z",
        "updated": "2023-06-06T09:17:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.02867v1",
        "title": "Penalized Quasi-likelihood Estimation and Model Selection in Time Series Models with Parameters on the Boundary",
        "abstract": "We extend the theory from Fan and Li (2001) on penalized likelihood-based\nestimation and model-selection to statistical and econometric models which\nallow for non-negativity constraints on some or all of the parameters, as well\nas time-series dependence. It differs from classic non-penalized likelihood\nestimation, where limiting distributions of likelihood-based estimators and\ntest-statistics are non-standard, and depend on the unknown number of\nparameters on the boundary of the parameter space. Specifically, we establish\nthat the joint model selection and estimation, results in standard asymptotic\nGaussian distributed estimators. The results are applied to the rich class of\nautoregressive conditional heteroskedastic (ARCH) models for the modelling of\ntime-varying volatility. We find from simulations that the penalized estimation\nand model-selection works surprisingly well even for a large number of\nparameters. A simple empirical illustration for stock-market returns data\nconfirms the ability of the penalized estimation to select ARCH models which\nfit nicely the autocorrelation function, as well as confirms the stylized fact\nof long-memory in financial time series data.",
        "authors": [
            "Heino Bohn Nielsen",
            "Anders Rahbek"
        ],
        "categories": "econ.EM",
        "published": "2023-02-06T15:36:11Z",
        "updated": "2023-02-06T15:36:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.02866v2",
        "title": "Out of Sample Predictability in Predictive Regressions with Many Predictor Candidates",
        "abstract": "This paper is concerned with detecting the presence of out of sample\npredictability in linear predictive regressions with a potentially large set of\ncandidate predictors. We propose a procedure based on out of sample MSE\ncomparisons that is implemented in a pairwise manner using one predictor at a\ntime and resulting in an aggregate test statistic that is standard normally\ndistributed under the global null hypothesis of no linear predictability.\nPredictors can be highly persistent, purely stationary or a combination of\nboth. Upon rejection of the null hypothesis we subsequently introduce a\npredictor screening procedure designed to identify the most active predictors.\nAn empirical application to key predictors of US economic activity illustrates\nthe usefulness of our methods and highlights the important forward looking role\nplayed by the series of manufacturing new orders.",
        "authors": [
            "Jesus Gonzalo",
            "Jean-Yves Pitarakis"
        ],
        "categories": "econ.EM",
        "published": "2023-02-06T15:32:48Z",
        "updated": "2023-10-16T09:48:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.02747v2",
        "title": "Testing Quantile Forecast Optimality",
        "abstract": "Quantile forecasts made across multiple horizons have become an important\noutput of many financial institutions, central banks and international\norganisations. This paper proposes misspecification tests for such quantile\nforecasts that assess optimality over a set of multiple forecast horizons\nand/or quantiles. The tests build on multiple Mincer-Zarnowitz quantile\nregressions cast in a moment equality framework. Our main test is for the null\nhypothesis of autocalibration, a concept which assesses optimality with respect\nto the information contained in the forecasts themselves. We provide an\nextension that allows to test for optimality with respect to larger information\nsets and a multivariate extension. Importantly, our tests do not just inform\nabout general violations of optimality, but may also provide useful insights\ninto specific forms of sub-optimality. A simulation study investigates the\nfinite sample performance of our tests, and two empirical applications to\nfinancial returns and U.S. macroeconomic series illustrate that our tests can\nyield interesting insights into quantile forecast sub-optimality and its\ncauses.",
        "authors": [
            "Jack Fosten",
            "Daniel Gutknecht",
            "Marc-Oliver Pohle"
        ],
        "categories": "econ.EM",
        "published": "2023-02-06T12:47:34Z",
        "updated": "2023-10-12T20:32:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.02476v1",
        "title": "Estimating Time-Varying Networks for High-Dimensional Time Series",
        "abstract": "We explore time-varying networks for high-dimensional locally stationary time\nseries, using the large VAR model framework with both the transition and\n(error) precision matrices evolving smoothly over time. Two types of\ntime-varying graphs are investigated: one containing directed edges of Granger\ncausality linkages, and the other containing undirected edges of partial\ncorrelation linkages. Under the sparse structural assumption, we propose a\npenalised local linear method with time-varying weighted group LASSO to jointly\nestimate the transition matrices and identify their significant entries, and a\ntime-varying CLIME method to estimate the precision matrices. The estimated\ntransition and precision matrices are then used to determine the time-varying\nnetwork structures. Under some mild conditions, we derive the theoretical\nproperties of the proposed estimates including the consistency and oracle\nproperties. In addition, we extend the methodology and theory to cover\nhighly-correlated large-scale time series, for which the sparsity assumption\nbecomes invalid and we allow for common factors before estimating the\nfactor-adjusted time-varying networks. We provide extensive simulation studies\nand an empirical application to a large U.S. macroeconomic dataset to\nillustrate the finite-sample performance of our methods.",
        "authors": [
            "Jia Chen",
            "Degui Li",
            "Yuning Li",
            "Oliver Linton"
        ],
        "categories": "stat.ME",
        "published": "2023-02-05T20:27:09Z",
        "updated": "2023-02-05T20:27:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.02370v1",
        "title": "Testing for Structural Change under Nonstationarity",
        "abstract": "This Appendix (dated: July 2021) includes supplementary derivations related\nto the main limit results of the econometric framework for structural break\ntesting in predictive regression models based on the OLS-Wald and IVX-Wald test\nstatistics, developed by Katsouris C (2021). In particular, we derive the\nasymptotic distributions of the test statistics when the predictive regression\nmodel includes either mildly integrated or persistent regressors. Moreover, we\nconsider the case in which a model intercept is included in the model vis-a-vis\nthe case that the predictive regression model has no model intercept. In a\nsubsequent version of this study we reexamine these particular aspects in more\ndepth with respect to the demeaned versions of the variables of the predictive\nregression.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2023-02-05T12:28:36Z",
        "updated": "2023-02-05T12:28:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.01775v1",
        "title": "Using bayesmixedlogit and bayesmixedlogitwtp in Stata",
        "abstract": "This document presents an overview of the bayesmixedlogit and\nbayesmixedlogitwtp Stata packages. It mirrors closely the helpfile obtainable\nin Stata (i.e., through help bayesmixedlogit or help bayesmixedlogitwtp).\nFurther background for the packages can be found in Baker(2014).",
        "authors": [
            "Matthew J. Baker"
        ],
        "categories": "econ.EM",
        "published": "2023-02-03T14:33:25Z",
        "updated": "2023-02-03T14:33:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.01621v1",
        "title": "Agreed and Disagreed Uncertainty",
        "abstract": "When agents' information is imperfect and dispersed, existing measures of\nmacroeconomic uncertainty based on the forecast error variance have two\ndistinct drivers: the variance of the economic shock and the variance of the\ninformation dispersion. The former driver increases uncertainty and reduces\nagents' disagreement (agreed uncertainty). The latter increases both\nuncertainty and disagreement (disagreed uncertainty). We use these implications\nto identify empirically the effects of agreed and disagreed uncertainty shocks,\nbased on a novel measure of consumer disagreement derived from survey\nexpectations. Disagreed uncertainty has no discernible economic effects and is\nbenign for economic activity, but agreed uncertainty exerts significant\ndepressing effects on a broad spectrum of macroeconomic indicators.",
        "authors": [
            "Luca Gambetti",
            "Dimitris Korobilis",
            "John Tsoukalas",
            "Francesco Zanetti"
        ],
        "categories": "econ.EM",
        "published": "2023-02-03T09:47:41Z",
        "updated": "2023-02-03T09:47:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.01434v2",
        "title": "Inference in Non-stationary High-Dimensional VARs",
        "abstract": "In this paper we construct an inferential procedure for Granger causality in\nhigh-dimensional non-stationary vector autoregressive (VAR) models. Our method\ndoes not require knowledge of the order of integration of the time series under\nconsideration. We augment the VAR with at least as many lags as the suspected\nmaximum order of integration, an approach which has been proven to be robust\nagainst the presence of unit roots in low dimensions. We prove that we can\nrestrict the augmentation to only the variables of interest for the testing,\nthereby making the approach suitable for high dimensions. We combine this lag\naugmentation with a post-double-selection procedure in which a set of initial\npenalized regressions is performed to select the relevant variables for both\nthe Granger causing and caused variables. We then establish uniform asymptotic\nnormality of a second-stage regression involving only the selected variables.\nFinite sample simulations show good performance, an application to investigate\nthe (predictive) causes and effects of economic uncertainty illustrates the\nneed to allow for unknown orders of integration.",
        "authors": [
            "Alain Hecq",
            "Luca Margaritella",
            "Stephan Smeekes"
        ],
        "categories": "econ.EM",
        "published": "2023-02-02T21:56:36Z",
        "updated": "2023-09-15T10:01:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.01236v1",
        "title": "A Machine Learning Approach to Measuring Climate Adaptation",
        "abstract": "I measure adaptation to climate change by comparing elasticities from\nshort-run and long-run changes in damaging weather. I propose a debiased\nmachine learning approach to flexibly measure these elasticities in panel\nsettings. In a simulation exercise, I show that debiased machine learning has\nconsiderable benefits relative to standard machine learning or ordinary least\nsquares, particularly in high-dimensional settings. I then measure adaptation\nto damaging heat exposure in United States corn and soy production. Using rich\nsets of temperature and precipitation variation, I find evidence that short-run\nimpacts from damaging heat are significantly offset in the long run. I show\nthat this is because the impacts of long-run changes in heat exposure do not\nfollow the same functional form as short-run shocks to heat exposure.",
        "authors": [
            "Max Vilgalys"
        ],
        "categories": "stat.AP",
        "published": "2023-02-02T17:16:48Z",
        "updated": "2023-02-02T17:16:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.01233v1",
        "title": "Sparse High-Dimensional Vector Autoregressive Bootstrap",
        "abstract": "We introduce a high-dimensional multiplier bootstrap for time series data\nbased capturing dependence through a sparsely estimated vector autoregressive\nmodel. We prove its consistency for inference on high-dimensional means under\ntwo different moment assumptions on the errors, namely sub-gaussian moments and\na finite number of absolute moments. In establishing these results, we derive a\nGaussian approximation for the maximum mean of a linear process, which may be\nof independent interest.",
        "authors": [
            "Robert Adamek",
            "Stephan Smeekes",
            "Ines Wilms"
        ],
        "categories": "econ.EM",
        "published": "2023-02-02T17:14:54Z",
        "updated": "2023-02-02T17:14:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.00469v3",
        "title": "Regression adjustment in randomized controlled trials with many covariates",
        "abstract": "This paper is concerned with estimation and inference on average treatment\neffects in randomized controlled trials when researchers observe potentially\nmany covariates. By employing Neyman's (1923) finite population perspective, we\npropose a bias-corrected regression adjustment estimator using cross-fitting,\nand show that the proposed estimator has favorable properties over existing\nalternatives. For inference, we derive the first and second order terms in the\nstochastic component of the regression adjustment estimators, study higher\norder properties of the existing inference methods, and propose a\nbias-corrected version of the HC3 standard error. The proposed methods readily\nextend to stratified experiments with large strata. Simulation studies show our\ncross-fitted estimator, combined with the bias-corrected HC3, delivers precise\npoint estimates and robust size controls over a wide range of DGPs. To\nillustrate, the proposed methods are applied to real dataset on randomized\nexperiments of incentives and services for college achievement following\nAngrist, Lang, and Oreopoulos (2009).",
        "authors": [
            "Harold D Chiang",
            "Yukitoshi Matsushita",
            "Taisuke Otsu"
        ],
        "categories": "econ.EM",
        "published": "2023-02-01T14:29:30Z",
        "updated": "2023-11-13T22:50:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.00251v1",
        "title": "Adaptive hedging horizon and hedging performance estimation",
        "abstract": "In this study, we constitute an adaptive hedging method based on empirical\nmode decomposition (EMD) method to extract the adaptive hedging horizon and\nbuild a time series cross-validation method for robust hedging performance\nestimation. Basing on the variance reduction criterion and the value-at-risk\n(VaR) criterion, we find that the estimation of in-sample hedging performance\nis inconsistent with that of the out-sample hedging performance. The EMD\nhedging method family exhibits superior performance on the VaR criterion\ncompared with the minimum variance hedging method. The matching degree of the\nspot and futures contracts at the specific time scale is the key determinant of\nthe hedging performance in the corresponding hedging horizon.",
        "authors": [
            "Wang Haoyu",
            "Junpeng Di",
            "Qing Han"
        ],
        "categories": "econ.EM",
        "published": "2023-02-01T05:29:08Z",
        "updated": "2023-02-01T05:29:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2302.00117v1",
        "title": "Real Estate Property Valuation using Self-Supervised Vision Transformers",
        "abstract": "The use of Artificial Intelligence (AI) in the real estate market has been\ngrowing in recent years. In this paper, we propose a new method for property\nvaluation that utilizes self-supervised vision transformers, a recent\nbreakthrough in computer vision and deep learning. Our proposed algorithm uses\na combination of machine learning, computer vision and hedonic pricing models\ntrained on real estate data to estimate the value of a given property. We\ncollected and pre-processed a data set of real estate properties in the city of\nBoulder, Colorado and used it to train, validate and test our algorithm. Our\ndata set consisted of qualitative images (including house interiors, exteriors,\nand street views) as well as quantitative features such as the number of\nbedrooms, bathrooms, square footage, lot square footage, property age, crime\nrates, and proximity to amenities. We evaluated the performance of our model\nusing metrics such as Root Mean Squared Error (RMSE). Our findings indicate\nthat these techniques are able to accurately predict the value of properties,\nwith a low RMSE. The proposed algorithm outperforms traditional appraisal\nmethods that do not leverage property images and has the potential to be used\nin real-world applications.",
        "authors": [
            "Mahdieh Yazdani",
            "Maziar Raissi"
        ],
        "categories": "cs.CV",
        "published": "2023-01-31T21:54:15Z",
        "updated": "2023-01-31T21:54:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.13843v2",
        "title": "Factor Model of Mixtures",
        "abstract": "This paper proposes a new approach to estimating the distribution of a\nresponse variable conditioned on observing some factors. The proposed approach\npossesses desirable properties of flexibility, interpretability, tractability\nand extendability. The conditional quantile function is modeled by a mixture\n(weighted sum) of basis quantile functions, with the weights depending on\nfactors. The calibration problem is formulated as a convex optimization\nproblem. It can be viewed as conducting quantile regressions for all confidence\nlevels simultaneously while avoiding quantile crossing by definition. The\ncalibration problem is equivalent to minimizing the continuous ranked\nprobability score (CRPS). Based on the canonical polyadic (CP) decomposition of\ntensors, we propose a dimensionality reduction method that reduces the rank of\nthe parameter tensor and propose an alternating algorithm for estimation.\nAdditionally, based on Risk Quadrangle framework, we generalize the approach to\nconditional distributions defined by Conditional Value-at-Risk (CVaR),\nexpectile and other functions of uncertainty measures. Although this paper\nfocuses on using splines as the weight functions, it can be extended to neural\nnetworks. Numerical experiments demonstrate the effectiveness of our approach.",
        "authors": [
            "Cheng Peng",
            "Stanislav Uryasev"
        ],
        "categories": "stat.ME",
        "published": "2023-01-31T18:40:25Z",
        "updated": "2023-03-14T20:53:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.13775v1",
        "title": "On Using The Two-Way Cluster-Robust Standard Errors",
        "abstract": "Thousands of papers have reported two-way cluster-robust (TWCR) standard\nerrors. However, the recent econometrics literature points out the potential\nnon-gaussianity of two-way cluster sample means, and thus invalidity of the\ninference based on the TWCR standard errors. Fortunately, simulation studies\nnonetheless show that the gaussianity is rather common than exceptional. This\npaper provides theoretical support for this encouraging observation.\nSpecifically, we derive a novel central limit theorem for two-way clustered\ntriangular arrays that justifies the use of the TWCR under very mild and\ninterpretable conditions. We, therefore, hope that this paper will provide a\ntheoretical justification for the legitimacy of most, if not all, of the\nthousands of those empirical papers that have used the TWCR standard errors. We\nprovide a guide in practice as to when a researcher can employ the TWCR\nstandard errors.",
        "authors": [
            "Harold D Chiang",
            "Yuya Sasaki"
        ],
        "categories": "econ.EM",
        "published": "2023-01-31T17:24:23Z",
        "updated": "2023-01-31T17:24:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.13736v2",
        "title": "Approximate Functional Differencing",
        "abstract": "Inference on common parameters in panel data models with individual-specific\nfixed effects is a classic example of Neyman and Scott's (1948) incidental\nparameter problem (IPP). One solution to this IPP is functional differencing\n(Bonhomme 2012), which works when the number of time periods T is fixed (and\nmay be small), but this solution is not applicable to all panel data models of\ninterest. Another solution, which applies to a larger class of models, is\n\"large-T\" bias correction (pioneered by Hahn and Kuersteiner 2002 and Hahn and\nNewey 2004), but this is only guaranteed to work well when T is sufficiently\nlarge. This paper provides a unified approach that connects those two seemingly\ndisparate solutions to the IPP. In doing so, we provide an approximate version\nof functional differencing, that is, an approximate solution to the IPP that is\napplicable to a large class of panel data models even when T is relatively\nsmall.",
        "authors": [
            "Geert Dhaene",
            "Martin Weidner"
        ],
        "categories": "econ.EM",
        "published": "2023-01-31T16:16:30Z",
        "updated": "2023-05-04T15:53:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.13692v1",
        "title": "Bridging the Covid-19 Data and the Epidemiological Model using Time-Varying Parameter SIRD Model",
        "abstract": "This paper extends the canonical model of epidemiology, the SIRD model, to\nallow for time-varying parameters for real-time measurement and prediction of\nthe trajectory of the Covid-19 pandemic. Time variation in model parameters is\ncaptured using the generalized autoregressive score modeling structure designed\nfor the typical daily count data related to the pandemic. The resulting\nspecification permits a flexible yet parsimonious model with a low\ncomputational cost. The model is extended to allow for unreported cases using a\nmixed-frequency setting. Results suggest that these cases' effects on the\nparameter estimates might be sizeable. Full sample results show that the\nflexible framework accurately captures the successive waves of the pandemic. A\nreal-time exercise indicates that the proposed structure delivers timely and\nprecise information on the pandemic's current stance. This superior\nperformance, in turn, transforms into accurate predictions of the confirmed and\ndeath cases.",
        "authors": [
            "Cem Cakmakli",
            "Yasin Simsek"
        ],
        "categories": "econ.EM",
        "published": "2023-01-31T15:08:40Z",
        "updated": "2023-01-31T15:08:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.13604v2",
        "title": "Nonlinearities in Macroeconomic Tail Risk through the Lens of Big Data Quantile Regressions",
        "abstract": "Modeling and predicting extreme movements in GDP is notoriously difficult and\nthe selection of appropriate covariates and/or possible forms of nonlinearities\nare key in obtaining precise forecasts. In this paper, our focus is on using\nlarge datasets in quantile regression models to forecast the conditional\ndistribution of US GDP growth. To capture possible non-linearities, we include\nseveral nonlinear specifications. The resulting models will be huge dimensional\nand we thus rely on a set of shrinkage priors. Since Markov Chain Monte Carlo\nestimation becomes slow in these dimensions, we rely on fast variational Bayes\napproximations to the posterior distribution of the coefficients and the latent\nstates. We find that our proposed set of models produces precise forecasts.\nThese gains are especially pronounced in the tails. Using Gaussian processes to\napproximate the nonlinear component of the model further improves the good\nperformance, in particular in the right tail.",
        "authors": [
            "Jan Pr\u00fcser",
            "Florian Huber"
        ],
        "categories": "econ.EM",
        "published": "2023-01-31T13:02:59Z",
        "updated": "2023-09-22T07:45:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.13152v5",
        "title": "STEEL: Singularity-aware Reinforcement Learning",
        "abstract": "Batch reinforcement learning (RL) aims at leveraging pre-collected data to\nfind an optimal policy that maximizes the expected total rewards in a dynamic\nenvironment. The existing methods require absolutely continuous assumption\n(e.g., there do not exist non-overlapping regions) on the distribution induced\nby target policies with respect to the data distribution over either the state\nor action or both. We propose a new batch RL algorithm that allows for\nsingularity for both state and action spaces (e.g., existence of\nnon-overlapping regions between offline data distribution and the distribution\ninduced by the target policies) in the setting of an infinite-horizon Markov\ndecision process with continuous states and actions. We call our algorithm\nSTEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by\na new error analysis on off-policy evaluation, where we use maximum mean\ndiscrepancy, together with distributionally robust optimization, to\ncharacterize the error of off-policy evaluation caused by the possible\nsingularity and to enable model extrapolation. By leveraging the idea of\npessimism and under some technical conditions, we derive a first finite-sample\nregret guarantee for our proposed algorithm under singularity. Compared with\nexisting algorithms,by requiring only minimal data-coverage assumption, STEEL\nimproves the applicability and robustness of batch RL. In addition, a two-step\nadaptive STEEL, which is nearly tuning-free, is proposed. Extensive simulation\nstudies and one (semi)-real experiment on personalized pricing demonstrate the\nsuperior performance of our methods in dealing with possible singularity in\nbatch RL.",
        "authors": [
            "Xiaohong Chen",
            "Zhengling Qi",
            "Runzhe Wan"
        ],
        "categories": "stat.ML",
        "published": "2023-01-30T18:29:35Z",
        "updated": "2024-06-26T03:39:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.13099v1",
        "title": "Prediction of Customer Churn in Banking Industry",
        "abstract": "With the growing competition in banking industry, banks are required to\nfollow customer retention strategies while they are trying to increase their\nmarket share by acquiring new customers. This study compares the performance of\nsix supervised classification techniques to suggest an efficient model to\npredict customer churn in banking industry, given 10 demographic and personal\nattributes from 10000 customers of European banks. The effect of feature\nselection, class imbalance, and outliers will be discussed for ANN and random\nforest as the two competing models. As shown, unlike random forest, ANN does\nnot reveal any serious concern regarding overfitting and is also robust to\nnoise. Therefore, ANN structure with five nodes in a single hidden layer is\nrecognized as the best performing classifier.",
        "authors": [
            "Sina Esmaeilpour Charandabi"
        ],
        "categories": "stat.ML",
        "published": "2023-01-30T17:36:33Z",
        "updated": "2023-01-30T17:36:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.12710v1",
        "title": "Machine Learning with High-Cardinality Categorical Features in Actuarial Applications",
        "abstract": "High-cardinality categorical features are pervasive in actuarial data (e.g.\noccupation in commercial property insurance). Standard categorical encoding\nmethods like one-hot encoding are inadequate in these settings.\n  In this work, we present a novel _Generalised Linear Mixed Model Neural\nNetwork_ (\"GLMMNet\") approach to the modelling of high-cardinality categorical\nfeatures. The GLMMNet integrates a generalised linear mixed model in a deep\nlearning framework, offering the predictive power of neural networks and the\ntransparency of random effects estimates, the latter of which cannot be\nobtained from the entity embedding models. Further, its flexibility to deal\nwith any distribution in the exponential dispersion (ED) family makes it widely\napplicable to many actuarial contexts and beyond.\n  We illustrate and compare the GLMMNet against existing approaches in a range\nof simulation experiments as well as in a real-life insurance case study.\nNotably, we find that the GLMMNet often outperforms or at least performs\ncomparably with an entity embedded neural network, while providing the\nadditional benefit of transparency, which is particularly valuable in practical\napplications.\n  Importantly, while our model was motivated by actuarial applications, it can\nhave wider applicability. The GLMMNet would suit any applications that involve\nhigh-cardinality categorical variables and where the response cannot be\nsufficiently modelled by a Gaussian distribution.",
        "authors": [
            "Benjamin Avanzi",
            "Greg Taylor",
            "Melantha Wang",
            "Bernard Wong"
        ],
        "categories": "stat.ML",
        "published": "2023-01-30T07:35:18Z",
        "updated": "2023-01-30T07:35:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.12542v1",
        "title": "A Note on the Estimation of Job Amenities and Labor Productivity",
        "abstract": "This paper introduces a maximum likelihood estimator of the value of job\namenities and labor productivity in a single matching market based on the\nobservation of equilibrium matches and wages. The estimation procedure\nsimultaneously fits both the matching patterns and the wage curve. While our\nestimator is suited for a wide range of assignment problems, we provide an\napplication to the estimation of the Value of a Statistical Life using\ncompensating wage differentials for the risk of fatal injury on the job. Using\nUS data for 2017, we estimate the Value of Statistical Life at \\$ 6.3 million\n(\\$2017).",
        "authors": [
            "Arnaud Dupuy",
            "Alfred Galichon"
        ],
        "categories": "econ.EM",
        "published": "2023-01-29T21:08:44Z",
        "updated": "2023-01-29T21:08:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.12499v1",
        "title": "Multidimensional dynamic factor models",
        "abstract": "This paper generalises dynamic factor models for multidimensional dependent\ndata. In doing so, it develops an interpretable technique to study complex\ninformation sources ranging from repeated surveys with a varying number of\nrespondents to panels of satellite images. We specialise our results to model\nmicroeconomic data on US households jointly with macroeconomic aggregates. This\nresults in a powerful tool able to generate localised predictions,\ncounterfactuals and impulse response functions for individual households,\naccounting for traditional time-series complexities depicted in the state-space\nliterature. The model is also compatible with the growing focus of policymakers\nfor real-time economic analysis as it is able to process observations online,\nwhile handling missing values and asynchronous data releases.",
        "authors": [
            "Matteo Barigozzi",
            "Filippo Pellegrino"
        ],
        "categories": "econ.EM",
        "published": "2023-01-29T17:37:29Z",
        "updated": "2023-01-29T17:37:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.11859v3",
        "title": "Synthetic Difference In Differences Estimation",
        "abstract": "In this paper, we describe a computational implementation of the Synthetic\ndifference-in-differences (SDID) estimator of Arkhangelsky et al. (2021) for\nStata. Synthetic difference-in-differences can be used in a wide class of\ncircumstances where treatment effects on some particular policy or event are\ndesired, and repeated observations on treated and untreated units are available\nover time. We lay out the theory underlying SDID, both when there is a single\ntreatment adoption date and when adoption is staggered over time, and discuss\nestimation and inference in each of these cases. We introduce the sdid command\nwhich implements these methods in Stata, and provide a number of examples of\nuse, discussing estimation, inference, and visualization of results.",
        "authors": [
            "Damian Clarke",
            "Daniel Paila\u00f1ir",
            "Susan Athey",
            "Guido Imbens"
        ],
        "categories": "econ.EM",
        "published": "2023-01-27T17:05:42Z",
        "updated": "2023-02-13T13:21:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.11358v2",
        "title": "Simple Difference-in-Differences Estimation in Fixed-T Panels",
        "abstract": "The present paper proposes a new treatment effects estimator that is valid\nwhen the number of time periods is small, and the parallel trends condition\nholds conditional on covariates and unobserved heterogeneity in the form of\ninteractive fixed effects. The estimator also allow the control variables to be\naffected by treatment and it enables estimation of the resulting indirect\neffect on the outcome variable. The asymptotic properties of the estimator are\nestablished and their accuracy in small samples is investigated using Monte\nCarlo simulations. The empirical usefulness of the estimator is illustrated\nusing as an example the effect of increased trade competition on firm markups\nin China.",
        "authors": [
            "Nicholas Brown",
            "Kyle Butts",
            "Joakim Westerlund"
        ],
        "categories": "econ.EM",
        "published": "2023-01-26T19:11:32Z",
        "updated": "2023-06-14T14:55:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.10643v2",
        "title": "Automatic Locally Robust Estimation with Generated Regressors",
        "abstract": "Many economic and causal parameters of interest depend on generated\nregressors. Examples include structural parameters in models with endogenous\nvariables estimated by control functions and in models with sample selection,\ntreatment effect estimation with propensity score matching, and marginal\ntreatment effects. Inference with generated regressors is complicated by the\nvery complex expression for influence functions and asymptotic variances. To\naddress this problem, we propose Automatic Locally Robust/debiased GMM\nestimators in a general setting with generated regressors. Importantly, we\nallow for the generated regressors to be generated from machine learners, such\nas Random Forest, Neural Nets, Boosting, and many others. We use our results to\nconstruct novel Doubly Robust and Locally Robust estimators for the\nCounterfactual Average Structural Function and Average Partial Effects in\nmodels with endogeneity and sample selection, respectively. We provide\nsufficient conditions for the asymptotic normality of our debiased GMM\nestimators and investigate their finite sample performance through Monte Carlo\nsimulations.",
        "authors": [
            "Juan Carlos Escanciano",
            "Telmo P\u00e9rez-Izquierdo"
        ],
        "categories": "econ.EM",
        "published": "2023-01-25T15:26:18Z",
        "updated": "2023-11-07T14:28:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.10592v2",
        "title": "Hierarchical Regularizers for Reverse Unrestricted Mixed Data Sampling Regressions",
        "abstract": "Reverse Unrestricted MIxed DAta Sampling (RU-MIDAS) regressions are used to\nmodel high-frequency responses by means of low-frequency variables. However,\ndue to the periodic structure of RU-MIDAS regressions, the dimensionality grows\nquickly if the frequency mismatch between the high- and low-frequency variables\nis large. Additionally the number of high-frequency observations available for\nestimation decreases. We propose to counteract this reduction in sample size by\npooling the high-frequency coefficients and further reduce the dimensionality\nthrough a sparsity-inducing convex regularizer that accounts for the temporal\nordering among the different lags. To this end, the regularizer prioritizes the\ninclusion of lagged coefficients according to the recency of the information\nthey contain. We demonstrate the proposed method on two empirical applications,\none on realized volatility forecasting with macroeconomic data and another on\ndemand forecasting for a bicycle-sharing system with ridership data on other\ntransportation types.",
        "authors": [
            "Alain Hecq",
            "Marie Ternes",
            "Ines Wilms"
        ],
        "categories": "econ.EM",
        "published": "2023-01-25T13:53:16Z",
        "updated": "2024-11-01T09:27:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.10494v1",
        "title": "Sequential Bayesian Learning for Hidden Semi-Markov Models",
        "abstract": "In this paper, we explore the class of the Hidden Semi-Markov Model (HSMM), a\nflexible extension of the popular Hidden Markov Model (HMM) that allows the\nunderlying stochastic process to be a semi-Markov chain. HSMMs are typically\nused less frequently than their basic HMM counterpart due to the increased\ncomputational challenges when evaluating the likelihood function. Moreover,\nwhile both models are sequential in nature, parameter estimation is mainly\nconducted via batch estimation methods. Thus, a major motivation of this paper\nis to provide methods to estimate HSMMs (1) in a computationally feasible time,\n(2) in an exact manner, i.e. only subject to Monte Carlo error, and (3) in a\nsequential setting. We provide and verify an efficient computational scheme for\nBayesian parameter estimation on HSMMs. Additionally, we explore the\nperformance of HSMMs on the VIX time series using Autoregressive (AR) models\nwith hidden semi-Markov states and demonstrate how this algorithm can be used\nfor regime switching, model selection and clustering purposes.",
        "authors": [
            "Patrick Aschermayr",
            "Konstantinos Kalogeropoulos"
        ],
        "categories": "stat.AP",
        "published": "2023-01-25T10:05:17Z",
        "updated": "2023-01-25T10:05:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.09486v1",
        "title": "Processes analogous to ecological interactions and dispersal shape the dynamics of economic activities",
        "abstract": "The processes of ecological interactions, dispersal and mutations shape the\ndynamics of biological communities, and analogous eco-evolutionary processes\nacting upon economic entities have been proposed to explain economic change.\nThis hypothesis is compelling because it explains economic change through\nendogenous mechanisms, but it has not been quantitatively tested at the global\neconomy level. Here, we use an inverse modelling technique and 59 years of\neconomic data covering 77 countries to test whether the collective dynamics of\nnational economic activities can be characterised by eco-evolutionary\nprocesses. We estimate the statistical support of dynamic community models in\nwhich the dynamics of economic activities are coupled with positive and\nnegative interactions between the activities, the spatial dispersal of the\nactivities, and their transformations into other economic activities. We find\nstrong support for the models capturing positive interactions between economic\nactivities and spatial dispersal of the activities across countries. These\nresults suggest that processes akin to those occurring in ecosystems play a\nsignificant role in the dynamics of economic systems. The strength-of-evidence\nobtained for each model varies across countries and may be caused by\ndifferences in the distance between countries, specific institutional contexts,\nand historical contingencies. Overall, our study provides a new quantitative,\nbiologically inspired framework to study the forces shaping economic change.",
        "authors": [
            "Victor Boussange",
            "Didier Sornette",
            "Heike Lischke",
            "Lo\u00efc Pellissier"
        ],
        "categories": "econ.EM",
        "published": "2023-01-23T15:30:10Z",
        "updated": "2023-01-23T15:30:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.09397v3",
        "title": "ddml: Double/debiased machine learning in Stata",
        "abstract": "We introduce the package ddml for Double/Debiased Machine Learning (DDML) in\nStata. Estimators of causal parameters for five different econometric models\nare supported, allowing for flexible estimation of causal effects of endogenous\nvariables in settings with unknown functional forms and/or many exogenous\nvariables. ddml is compatible with many existing supervised machine learning\nprograms in Stata. We recommend using DDML in combination with stacking\nestimation which combines multiple machine learners into a final predictor. We\nprovide Monte Carlo evidence to support our recommendation.",
        "authors": [
            "Achim Ahrens",
            "Christian B. Hansen",
            "Mark E. Schaffer",
            "Thomas Wiemann"
        ],
        "categories": "econ.EM",
        "published": "2023-01-23T12:37:34Z",
        "updated": "2024-01-06T13:17:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.09379v5",
        "title": "Revisiting Panel Data Discrete Choice Models with Lagged Dependent Variables",
        "abstract": "This paper revisits the identification and estimation of a class of\nsemiparametric (distribution-free) panel data binary choice models with lagged\ndependent variables, exogenous covariates, and entity fixed effects. We provide\na novel identification strategy, using an \"identification at infinity\"\nargument. In contrast with the celebrated Honore and Kyriazidou (2000), our\nmethod permits time trends of any form and does not suffer from the \"curse of\ndimensionality\". We propose an easily implementable conditional maximum score\nestimator. The asymptotic properties of the proposed estimator are fully\ncharacterized. A small-scale Monte Carlo study demonstrates that our approach\nperforms satisfactorily in finite samples. We illustrate the usefulness of our\nmethod by presenting an empirical application to enrollment in private hospital\ninsurance using the Household, Income and Labour Dynamics in Australia (HILDA)\nSurvey data.",
        "authors": [
            "Christopher R. Dobronyi",
            "Fu Ouyang",
            "Thomas Tao Yang"
        ],
        "categories": "econ.EM",
        "published": "2023-01-23T11:47:52Z",
        "updated": "2024-08-22T22:40:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.09173v1",
        "title": "Labor Income Risk and the Cross-Section of Expected Returns",
        "abstract": "This paper explores asset pricing implications of unemployment risk from\nsectoral shifts. I proxy for this risk using cross-industry dispersion (CID),\ndefined as a mean absolute deviation of returns of 49 industry portfolios. CID\npeaks during periods of accelerated sectoral reallocation and heightened\nuncertainty. I find that expected stock returns are related cross-sectionally\nto the sensitivities of returns to innovations in CID. Annualized returns of\nthe stocks with high sensitivity to CID are 5.9% lower than the returns of the\nstocks with low sensitivity. Abnormal returns with respect to the best factor\nmodel are 3.5%, suggesting that common factors can not explain this return\nspread. Stocks with high sensitivity to CID are likely to be the stocks, which\nbenefited from sectoral shifts. CID positively predicts unemployment through\nits long-term component, consistent with the hypothesis that CID is a proxy for\nunemployment risk from sectoral shifts.",
        "authors": [
            "Mykola Pinchuk"
        ],
        "categories": "q-fin.PR",
        "published": "2023-01-22T18:18:11Z",
        "updated": "2023-01-22T18:18:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.09016v6",
        "title": "Inference for Two-stage Experiments under Covariate-Adaptive Randomization",
        "abstract": "This paper studies inference in two-stage randomized experiments under\ncovariate-adaptive randomization. In the initial stage of this experimental\ndesign, clusters (e.g., households, schools, or graph partitions) are\nstratified and randomly assigned to control or treatment groups based on\ncluster-level covariates. Subsequently, an independent second-stage design is\ncarried out, wherein units within each treated cluster are further stratified\nand randomly assigned to either control or treatment groups, based on\nindividual-level covariates. Under the homogeneous partial interference\nassumption, I establish conditions under which the proposed\ndifference-in-``average of averages'' estimators are consistent and\nasymptotically normal for the corresponding average primary and spillover\neffects and develop consistent estimators of their asymptotic variances.\nCombining these results establishes the asymptotic validity of tests based on\nthese estimators. My findings suggest that ignoring covariate information in\nthe design stage can result in efficiency loss, and commonly used inference\nmethods that ignore or improperly use covariate information can lead to either\nconservative or invalid inference. Then, I apply these results to studying\noptimal use of covariate information under covariate-adaptive randomization in\nlarge samples, and demonstrate that a specific generalized matched-pair design\nachieves minimum asymptotic variance for each proposed estimator. Finally, I\ndiscuss covariate adjustment, which incorporates additional baseline covariates\nnot used for treatment assignment. The practical relevance of the theoretical\nresults is illustrated through a simulation study and an empirical application.",
        "authors": [
            "Jizhou Liu"
        ],
        "categories": "econ.EM",
        "published": "2023-01-21T21:55:40Z",
        "updated": "2024-10-15T04:05:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.08958v2",
        "title": "A Practical Introduction to Regression Discontinuity Designs: Extensions",
        "abstract": "This monograph, together with its accompanying first part Cattaneo, Idrobo\nand Titiunik (2020), collects and expands the instructional materials we\nprepared for more than $50$ short courses and workshops on Regression\nDiscontinuity (RD) methodology that we taught between 2014 and 2023. In this\nsecond monograph, we discuss several topics in RD methodology that build on and\nextend the analysis of RD designs introduced in Cattaneo, Idrobo and Titiunik\n(2020). Our first goal is to present an alternative RD conceptual framework\nbased on local randomization ideas. This methodological approach can be useful\nin RD designs with discretely-valued scores, and can also be used more broadly\nas a complement to the continuity-based approach in other settings. Then,\nemploying both continuity-based and local randomization approaches, we extend\nthe canonical Sharp RD design in multiple directions: fuzzy RD designs, RD\ndesigns with discrete scores, and multi-dimensional RD designs. The goal of our\ntwo-part monograph is purposely practical and hence we focus on the empirical\nanalysis of RD designs.",
        "authors": [
            "Matias D. Cattaneo",
            "Nicolas Idrobo",
            "Rocio Titiunik"
        ],
        "categories": "stat.ME",
        "published": "2023-01-21T14:39:00Z",
        "updated": "2024-03-25T19:52:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.09438v1",
        "title": "Composite distributions in the social sciences: A comparative empirical study of firms' sales distribution for France, Germany, Italy, Japan, South Korea, and Spain",
        "abstract": "We study 17 different statistical distributions for sizes obtained {}from the\nclassical and recent literature to describe a relevant variable in the social\nsciences and Economics, namely the firms' sales distribution in six countries\nover an ample period. We find that the best results are obtained with mixtures\nof lognormal (LN), loglogistic (LL), and log Student's $t$ (LSt) distributions.\nThe single lognormal, in turn, is strongly not selected. We then find that the\nwhole firm size distribution is better described by a mixture, and there exist\nsubgroups of firms. Depending on the method of measurement, the best fitting\ndistribution cannot be defined by a single one, but as a mixture of at least\nthree distributions or even four or five. We assess a full sample analysis, an\nin-sample and out-of-sample analysis, and a doubly truncated sample analysis.\nWe also provide the formulation of the preferred models as solutions of the\nFokker--Planck or forward Kolmogorov equation.",
        "authors": [
            "Arturo Ramos",
            "Till Massing",
            "Atushi Ishikawa",
            "Shouji Fujimoto",
            "Takayuki Mizuno"
        ],
        "categories": "econ.GN",
        "published": "2023-01-20T11:04:06Z",
        "updated": "2023-01-20T11:04:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.07997v1",
        "title": "From prosumer to flexumer: Case study on the value of flexibility in decarbonizing the multi-energy system of a manufacturing company",
        "abstract": "Digitalization and sector coupling enable companies to turn into flexumers.\nBy using the flexibility of their multi-energy system (MES), they reduce costs\nand carbon emissions while stabilizing the electricity system. However, to\nidentify the necessary investments in energy conversion and storage\ntechnologies to leverage demand response (DR) potentials, companies need to\nassess the value of flexibility. Therefore, this study quantifies the\nflexibility value of a production company's MES by optimizing the synthesis,\ndesign, and operation of a decarbonizing MES considering self-consumption\noptimization, peak shaving, and integrated DR based on hourly prices and carbon\nemission factors (CEFs). The detailed case study of a beverage company in\nnorthern Germany considers vehicle-to-X of powered industrial trucks,\npower-to-heat on multiple temperatures, wind turbines, photovoltaic systems,\nand energy storage systems (thermal energy, electricity, and hydrogen). We\npropose and apply novel data-driven metrics to evaluate the intensity of\nprice-based and CEF-based DR. The results reveal that flexibility usage reduces\ndecarbonization costs (by 19-80% depending on electricity and carbon removal\nprices), total annual costs, operating carbon emissions, energy-weighted\naverage prices and CEFs, and fossil energy dependency. The results also suggest\nthat a net-zero operational carbon emission MES requires flexibility, which, in\nan economic case, is provided by a combination of different flexible\ntechnologies and storage systems that complement each other. While the value of\nflexibility depends on various market and consumer-specific factors such as\nelectricity or carbon removal prices, this study highlights the importance of\ndemand flexibility for the decarbonization of MESs.",
        "authors": [
            "Markus Fleschutz",
            "Markus Bohlayer",
            "Marco Braun",
            "Michael D. Murphy"
        ],
        "categories": "eess.SY",
        "published": "2023-01-19T10:55:15Z",
        "updated": "2023-01-19T10:55:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.07855v3",
        "title": "Digital Divide: Empirical Study of CIUS 2020",
        "abstract": "As Canada and other major economies consider implementing \"digital money\" or\nCentral Bank Digital Currencies, understanding how demographic and geographic\nfactors influence public engagement with digital technologies becomes\nincreasingly important. This paper uses data from the 2020 Canadian Internet\nUse Survey and employs survey-adapted Lasso inference methods to identify\nindividual socio-economic and demographic characteristics determining the\ndigital divide in Canada. We also introduce a score to measure and compare the\ndigital literacy of various segments of Canadian population. Our findings\nreveal that disparities in the use of e.g. online banking, emailing, and\ndigital payments exist across different demographic and socio-economic groups.\nIn addition, we document the effects of COVID-19 pandemic on internet use in\nCanada and describe changes in the characteristics of Canadian internet users\nover the last decade.",
        "authors": [
            "Joann Jasiak",
            "Peter MacKenzie",
            "Purevdorj Tuvaandorj"
        ],
        "categories": "econ.EM",
        "published": "2023-01-19T02:52:42Z",
        "updated": "2024-10-08T02:17:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.07782v1",
        "title": "An MCMC Approach to Classical Estimation",
        "abstract": "This paper studies computationally and theoretically attractive estimators\ncalled the Laplace type estimators (LTE), which include means and quantiles of\nQuasi-posterior distributions defined as transformations of general\n(non-likelihood-based) statistical criterion functions, such as those in GMM,\nnonlinear IV, empirical likelihood, and minimum distance methods. The approach\ngenerates an alternative to classical extremum estimation and also falls\noutside the parametric Bayesian approach. For example, it offers a new\nattractive estimation method for such important semi-parametric problems as\ncensored and instrumental quantile, nonlinear GMM and value-at-risk models. The\nLTE's are computed using Markov Chain Monte Carlo methods, which help\ncircumvent the computational curse of dimensionality. A large sample theory is\nobtained for regular cases.",
        "authors": [
            "Victor Chernozhukov",
            "Han Hong"
        ],
        "categories": "econ.EM",
        "published": "2023-01-18T20:46:56Z",
        "updated": "2023-01-18T20:46:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.07755v1",
        "title": "Optimal Transport for Counterfactual Estimation: A Method for Causal Inference",
        "abstract": "Many problems ask a question that can be formulated as a causal question:\n\"what would have happened if...?\" For example, \"would the person have had\nsurgery if he or she had been Black?\" To address this kind of questions,\ncalculating an average treatment effect (ATE) is often uninformative, because\none would like to know how much impact a variable (such as skin color) has on a\nspecific individual, characterized by certain covariates. Trying to calculate a\nconditional ATE (CATE) seems more appropriate. In causal inference, the\npropensity score approach assumes that the treatment is influenced by x, a\ncollection of covariates. Here, we will have the dual view: doing an\nintervention, or changing the treatment (even just hypothetically, in a thought\nexperiment, for example by asking what would have happened if a person had been\nBlack) can have an impact on the values of x. We will see here that optimal\ntransport allows us to change certain characteristics that are influenced by\nthe variable we are trying to quantify the effect of. We propose here a mutatis\nmutandis version of the CATE, which will be done simply in dimension one by\nsaying that the CATE must be computed relative to a level of probability,\nassociated to the proportion of x (a single covariate) in the control\npopulation, and by looking for the equivalent quantile in the test population.\nIn higher dimension, it will be necessary to go through transport, and an\napplication will be proposed on the impact of some variables on the probability\nof having an unnatural birth (the fact that the mother smokes, or that the\nmother is Black).",
        "authors": [
            "Arthur Charpentier",
            "Emmanuel Flachaire",
            "Ewen Gallic"
        ],
        "categories": "econ.EM",
        "published": "2023-01-18T19:42:38Z",
        "updated": "2023-01-18T19:42:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.07241v4",
        "title": "Unconditional Quantile Partial Effects via Conditional Quantile Regression",
        "abstract": "This paper develops a semi-parametric procedure for estimation of\nunconditional quantile partial effects using quantile regression coefficients.\nThe estimator is based on an identification result showing that, for continuous\ncovariates, unconditional quantile effects are a weighted average of\nconditional ones at particular quantile levels that depend on the covariates.\nWe propose a two-step estimator for the unconditional effects where in the\nfirst step one estimates a structural quantile regression model, and in the\nsecond step a nonparametric regression is applied to the first step\ncoefficients. We establish the asymptotic properties of the estimator, say\nconsistency and asymptotic normality. Monte Carlo simulations show numerical\nevidence that the estimator has very good finite sample performance and is\nrobust to the selection of bandwidth and kernel. To illustrate the proposed\nmethod, we study the canonical application of the Engel's curve, i.e. food\nexpenditures as a share of income.",
        "authors": [
            "Javier Alejo",
            "Antonio F. Galvao",
            "Julian Martinez-Iriarte",
            "Gabriel Montes-Rojas"
        ],
        "categories": "econ.EM",
        "published": "2023-01-18T00:48:11Z",
        "updated": "2023-12-30T03:46:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.07196v2",
        "title": "Noisy, Non-Smooth, Non-Convex Estimation of Moment Condition Models",
        "abstract": "A practical challenge for structural estimation is the requirement to\naccurately minimize a sample objective function which is often non-smooth,\nnon-convex, or both. This paper proposes a simple algorithm designed to find\naccurate solutions without performing an exhaustive search. It augments each\niteration from a new Gauss-Newton algorithm with a grid search step. A finite\nsample analysis derives its optimization and statistical properties\nsimultaneously using only econometric assumptions. After a finite number of\niterations, the algorithm automatically transitions from global to fast local\nconvergence, producing accurate estimates with high probability. Simulated\nexamples and an empirical application illustrate the results.",
        "authors": [
            "Jean-Jacques Forneron"
        ],
        "categories": "econ.EM",
        "published": "2023-01-17T21:22:27Z",
        "updated": "2023-02-14T13:57:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.06720v2",
        "title": "Testing Firm Conduct",
        "abstract": "Evaluating policy in imperfectly competitive markets requires understanding\nfirm behavior. While researchers test conduct via model selection and\nassessment, we present advantages of Rivers and Vuong (2002) (RV) model\nselection under misspecification. However, degeneracy of RV invalidates\ninference. With a novel definition of weak instruments for testing, we connect\ndegeneracy to instrument strength, derive weak instrument properties of RV, and\nprovide a diagnostic for weak instruments by extending the framework of Stock\nand Yogo (2005) to model selection. We test vertical conduct (Villas-Boas,\n2007) using common instrument sets. Some are weak, providing no power. Strong\ninstruments support manufacturers setting retail prices.",
        "authors": [
            "Marco Duarte",
            "Lorenzo Magnolfi",
            "Mikkel S\u00f8lvsten",
            "Christopher Sullivan"
        ],
        "categories": "econ.EM",
        "published": "2023-01-17T06:35:03Z",
        "updated": "2024-01-17T15:29:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.06665v5",
        "title": "Resolving the Conflict on Conduct Parameter Estimation in Homogeneous Goods Markets between Bresnahan (1982) and Perloff and Shen (2012)",
        "abstract": "We revisit conduct parameter estimation in homogeneous goods markets to\nresolve the conflict between Bresnahan (1982) and Perloff and Shen (2012)\nregarding the identification and the estimation of conduct parameters. We point\nout that Perloff and Shen's (2012) proof is incorrect and its simulation\nsetting is invalid. Our simulation shows that estimation becomes accurate when\ndemand shifters are properly added in supply estimation and sample sizes are\nincreased, supporting Bresnahan (1982).",
        "authors": [
            "Yuri Matsumura",
            "Suguru Otani"
        ],
        "categories": "econ.EM",
        "published": "2023-01-17T02:25:43Z",
        "updated": "2023-05-23T02:42:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.06658v1",
        "title": "Statistical inference for the logarithmic spatial heteroskedasticity model with exogenous variables",
        "abstract": "The spatial dependence in mean has been well studied by plenty of models in a\nlarge strand of literature, however, the investigation of spatial dependence in\nvariance is lagging significantly behind. The existing models for the spatial\ndependence in variance are scarce, with neither probabilistic structure nor\nstatistical inference procedure being explored. To circumvent this deficiency,\nthis paper proposes a new generalized logarithmic spatial heteroscedasticity\nmodel with exogenous variables (denoted by the log-SHE model) to study the\nspatial dependence in variance. For the log-SHE model, its spatial near-epoch\ndependence (NED) property is investigated, and a systematic statistical\ninference procedure is provided, including the maximum likelihood and\ngeneralized method of moments estimators, the Wald, Lagrange multiplier and\nlikelihood-ratio-type D tests for model parameter constraints, and the\noveridentification test for the model diagnostic checking. Using the tool of\nspatial NED, the asymptotics of all proposed estimators and tests are\nestablished under regular conditions. The usefulness of the proposed\nmethodology is illustrated by simulation results and a real data example on the\nhouse selling price.",
        "authors": [
            "Bing Su",
            "Fukang Zhu",
            "Ke Zhu"
        ],
        "categories": "econ.EM",
        "published": "2023-01-17T01:48:14Z",
        "updated": "2023-01-17T01:48:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.06631v1",
        "title": "Robust M-Estimation for Additive Single-Index Cointegrating Time Series Models",
        "abstract": "Robust M-estimation uses loss functions, such as least absolute deviation\n(LAD), quantile loss and Huber's loss, to construct its objective function, in\norder to for example eschew the impact of outliers, whereas the difficulty in\nanalysing the resultant estimators rests on the nonsmoothness of these losses.\nGeneralized functions have advantages over ordinary functions in several\naspects, especially generalized functions possess derivatives of any order.\nGeneralized functions incorporate local integrable functions, the so-called\nregular generalized functions, while the so-called singular generalized\nfunctions (e.g. Dirac delta function) can be obtained as the limits of a\nsequence of sufficient smooth functions, so-called regular sequence in\ngeneralized function context. This makes it possible to use these singular\ngeneralized functions through approximation. Nevertheless, a significant\ncontribution of this paper is to establish the convergence rate of regular\nsequence to nonsmooth loss that answers a call from the relevant literature.\nFor parameter estimation where objective function may be nonsmooth, this paper\nfirst shows as a general paradigm that how generalized function approach can be\nused to tackle the nonsmooth loss functions in Section two using a very simple\nmodel. This approach is of general interest and applicability. We further use\nthe approach in robust M-estimation for additive single-index cointegrating\ntime series models; the asymptotic theory is established for the proposed\nestimators. We evaluate the finite-sample performance of the proposed\nestimation method and theory by both simulated data and an empirical analysis\nof predictive regression of stock returns.",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Yundong Tu",
            "Bin Peng"
        ],
        "categories": "econ.EM",
        "published": "2023-01-16T23:04:50Z",
        "updated": "2023-01-16T23:04:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.06354v1",
        "title": "When it counts -- Econometric identification of the basic factor model based on GLT structures",
        "abstract": "Despite the popularity of factor models with sparse loading matrices, little\nattention has been given to formally address identifiability of these models\nbeyond standard rotation-based identification such as the positive lower\ntriangular (PLT) constraint. To fill this gap, we review the advantages of\nvariance identification in sparse factor analysis and introduce the generalized\nlower triangular (GLT) structures. We show that the GLT assumption is an\nimprovement over PLT without compromise: GLT is also unique but, unlike PLT, a\nnon-restrictive assumption. Furthermore, we provide a simple counting rule for\nvariance identification under GLT structures, and we demonstrate that within\nthis model class the unknown number of common factors can be recovered in an\nexploratory factor analysis. Our methodology is illustrated for simulated data\nin the context of post-processing posterior draws in Bayesian sparse factor\nanalysis.",
        "authors": [
            "Sylvia Fr\u00fchwirth-Schnatter",
            "Darjus Hosszejni",
            "Hedibert Freitas Lopes"
        ],
        "categories": "stat.ME",
        "published": "2023-01-16T10:54:45Z",
        "updated": "2023-01-16T10:54:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.06283v1",
        "title": "Doubly-Robust Inference for Conditional Average Treatment Effects with High-Dimensional Controls",
        "abstract": "Plausible identification of conditional average treatment effects (CATEs) may\nrely on controlling for a large number of variables to account for confounding\nfactors. In these high-dimensional settings, estimation of the CATE requires\nestimating first-stage models whose consistency relies on correctly specifying\ntheir parametric forms. While doubly-robust estimators of the CATE exist,\ninference procedures based on the second stage CATE estimator are not\ndoubly-robust. Using the popular augmented inverse propensity weighting signal,\nwe propose an estimator for the CATE whose resulting Wald-type confidence\nintervals are doubly-robust. We assume a logistic model for the propensity\nscore and a linear model for the outcome regression, and estimate the\nparameters of these models using an $\\ell_1$ (Lasso) penalty to address the\nhigh dimensional covariates. Our proposed estimator remains consistent at the\nnonparametric rate and our proposed pointwise and uniform confidence intervals\nremain asymptotically valid even if one of the logistic propensity score or\nlinear outcome regression models are misspecified. These results are obtained\nunder similar conditions to existing analyses in the high-dimensional and\nnonparametric literatures.",
        "authors": [
            "Adam Baybutt",
            "Manu Navjeevan"
        ],
        "categories": "econ.EM",
        "published": "2023-01-16T06:44:46Z",
        "updated": "2023-01-16T06:44:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.05733v2",
        "title": "Identification in a Binary Choice Panel Data Model with a Predetermined Covariate",
        "abstract": "We study identification in a binary choice panel data model with a single\n\\emph{predetermined} binary covariate (i.e., a covariate sequentially exogenous\nconditional on lagged outcomes and covariates). The choice model is indexed by\na scalar parameter $\\theta$, whereas the distribution of unit-specific\nheterogeneity, as well as the feedback process that maps lagged outcomes into\nfuture covariate realizations, are left unrestricted. We provide a simple\ncondition under which $\\theta$ is never point-identified, no matter the number\nof time periods available. This condition is satisfied in most models,\nincluding the logit one. We also characterize the identified set of $\\theta$\nand show how to compute it using linear programming techniques. While $\\theta$\nis not generally point-identified, its identified set is informative in the\nexamples we analyze numerically, suggesting that meaningful learning about\n$\\theta$ may be possible even in short panels with feedback. As a complement,\nwe report calculations of identified sets for an average partial effect, and\nfind informative sets in this case as well.",
        "authors": [
            "St\u00e9phane Bonhomme",
            "Kevin Dano",
            "Bryan S. Graham"
        ],
        "categories": "econ.EM",
        "published": "2023-01-13T19:25:21Z",
        "updated": "2023-07-22T17:03:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.05703v2",
        "title": "Stable Probability Weighting: Large-Sample and Finite-Sample Estimation and Inference Methods for Heterogeneous Causal Effects of Multivalued Treatments Under Limited Overlap",
        "abstract": "In this paper, I try to tame \"Basu's elephants\" (data with extreme selection\non observables). I propose new practical large-sample and finite-sample methods\nfor estimating and inferring heterogeneous causal effects (under\nunconfoundedness) in the empirically relevant context of limited overlap. I\ndevelop a general principle called \"Stable Probability Weighting\" (SPW) that\ncan be used as an alternative to the widely used Inverse Probability Weighting\n(IPW) technique, which relies on strong overlap. I show that IPW (or its\naugmented version), when valid, is a special case of the more general SPW (or\nits doubly robust version), which adjusts for the extremeness of the\nconditional probabilities of the treatment states. The SPW principle can be\nimplemented using several existing large-sample parametric, semiparametric, and\nnonparametric procedures for conditional moment models. In addition, I provide\nnew finite-sample results that apply when unconfoundedness is plausible within\nfine strata. Since IPW estimation relies on the problematic reciprocal of the\nestimated propensity score, I develop a \"Finite-Sample Stable Probability\nWeighting\" (FPW) set-estimator that is unbiased in a sense. I also propose new\nfinite-sample inference methods for testing a general class of weak null\nhypotheses. The associated computationally convenient methods, which can be\nused to construct valid confidence sets and to bound the finite-sample\nconfidence distribution, are of independent interest. My large-sample and\nfinite-sample frameworks extend to the setting of multivalued treatments.",
        "authors": [
            "Ganesh Karapakula"
        ],
        "categories": "econ.EM",
        "published": "2023-01-13T18:52:18Z",
        "updated": "2023-01-19T18:39:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.05682v1",
        "title": "Non-Stochastic CDF Estimation Using Threshold Queries",
        "abstract": "Estimating the empirical distribution of a scalar-valued data set is a basic\nand fundamental task. In this paper, we tackle the problem of estimating an\nempirical distribution in a setting with two challenging features. First, the\nalgorithm does not directly observe the data; instead, it only asks a limited\nnumber of threshold queries about each sample. Second, the data are not assumed\nto be independent and identically distributed; instead, we allow for an\narbitrary process generating the samples, including an adaptive adversary.\nThese considerations are relevant, for example, when modeling a seller\nexperimenting with posted prices to estimate the distribution of consumers'\nwillingness to pay for a product: offering a price and observing a consumer's\npurchase decision is equivalent to asking a single threshold query about their\nvalue, and the distribution of consumers' values may be non-stationary over\ntime, as early adopters may differ markedly from late adopters.\n  Our main result quantifies, to within a constant factor, the sample\ncomplexity of estimating the empirical CDF of a sequence of elements of $[n]$,\nup to $\\varepsilon$ additive error, using one threshold query per sample. The\ncomplexity depends only logarithmically on $n$, and our result can be\ninterpreted as extending the existing logarithmic-complexity results for noisy\nbinary search to the more challenging setting where noise is non-stochastic.\nAlong the way to designing our algorithm, we consider a more general model in\nwhich the algorithm is allowed to make a limited number of simultaneous\nthreshold queries on each sample. We solve this problem using Blackwell's\nApproachability Theorem and the exponential weights method. As a side result of\nindependent interest, we characterize the minimum number of simultaneous\nthreshold queries required by deterministic CDF estimation algorithms.",
        "authors": [
            "Princewill Okoroafor",
            "Vaishnavi Gupta",
            "Robert Kleinberg",
            "Eleanor Goh"
        ],
        "categories": "cs.LG",
        "published": "2023-01-13T18:00:57Z",
        "updated": "2023-01-13T18:00:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.05580v2",
        "title": "Randomization Test for the Specification of Interference Structure",
        "abstract": "This study considers testing the specification of spillover effects in causal\ninference. We focus on experimental settings in which the treatment assignment\nmechanism is known to researchers. We develop a new randomization test\nutilizing a hierarchical relationship between different exposures. Compared\nwith existing approaches, our approach is essentially applicable to any null\nexposure specifications and produces powerful test statistics without a priori\nknowledge of the true interference structure. As empirical illustrations, we\nrevisit two existing social network experiments: one on farmers' insurance\nadoption and the other on anti-conflict education programs.",
        "authors": [
            "Tadao Hoshino",
            "Takahide Yanagi"
        ],
        "categories": "stat.ME",
        "published": "2023-01-13T14:38:53Z",
        "updated": "2023-12-25T05:54:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.05130v6",
        "title": "Unbiased estimation and asymptotically valid inference in multivariable Mendelian randomization with many weak instrumental variables",
        "abstract": "Mendelian randomization (MR) is an instrumental variable (IV) approach to\ninfer causal relationships between exposures and outcomes with genome-wide\nassociation studies (GWAS) summary data. However, the multivariable\ninverse-variance weighting (IVW) approach, which serves as the foundation for\nmost MR approaches, cannot yield unbiased causal effect estimates in the\npresence of many weak IVs. To address this problem, we proposed the MR using\nBias-corrected Estimating Equation (MRBEE) that can infer unbiased causal\nrelationships with many weak IVs and account for horizontal pleiotropy\nsimultaneously. While the practical significance of MRBEE was demonstrated in\nour parallel work (Lorincz-Comi (2023)), this paper established the statistical\ntheories of multivariable IVW and MRBEE with many weak IVs. First, we showed\nthat the bias of the multivariable IVW estimate is caused by the\nerror-in-variable bias, whose scale and direction are inflated and influenced\nby weak instrument bias and sample overlaps of exposures and outcome GWAS\ncohorts, respectively. Second, we investigated the asymptotic properties of\nmultivariable IVW and MRBEE, showing that MRBEE outperforms multivariable IVW\nregarding unbiasedness of causal effect estimation and asymptotic validity of\ncausal inference. Finally, we applied MRBEE to examine myopia and revealed that\neducation and outdoor activity are causal to myopia whereas indoor activity is\nnot.",
        "authors": [
            "Yihe Yang",
            "Noah Lorincz-Comi",
            "Xiaofeng Zhu"
        ],
        "categories": "stat.ME",
        "published": "2023-01-12T16:32:08Z",
        "updated": "2024-02-10T23:58:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.04876v2",
        "title": "Interacting Treatments with Endogenous Takeup",
        "abstract": "We study causal inference in randomized experiments (or quasi-experiments)\nfollowing a $2\\times 2$ factorial design. There are two treatments, denoted $A$\nand $B$, and units are randomly assigned to one of four categories: treatment\n$A$ alone, treatment $B$ alone, joint treatment, or none. Allowing for\nendogenous non-compliance with the two binary instruments representing the\nintended assignment, as well as unrestricted interference across the two\ntreatments, we derive the causal interpretation of various instrumental\nvariable estimands under more general compliance conditions than in the\nliterature. In general, if treatment takeup is driven by both instruments for\nsome units, it becomes difficult to separate treatment interaction from\ntreatment effect heterogeneity. We provide auxiliary conditions and various\nbounding strategies that may help zero in on causally interesting parameters.\nAs an empirical illustration, we apply our results to a program randomly\noffering two different treatments, namely tutoring and financial incentives, to\nfirst year college students, in order to assess the treatments' effects on\nacademic performance.",
        "authors": [
            "Mate Kormos",
            "Robert P. Lieli",
            "Martin Huber"
        ],
        "categories": "econ.EM",
        "published": "2023-01-12T08:47:59Z",
        "updated": "2024-12-11T14:00:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.04853v2",
        "title": "Testing for Coefficient Randomness in Local-to-Unity Autoregressions",
        "abstract": "In this study, we propose a test for the coefficient randomness in\nautoregressive models where the autoregressive coefficient is local to unity,\nwhich is empirically relevant given the results of earlier studies. Under this\nspecification, we theoretically analyze the effect of the correlation between\nthe random coefficient and disturbance on tests' properties, which remains\nlargely unexplored in the literature. Our analysis reveals that the correlation\ncrucially affects the power of tests for coefficient randomness and that tests\nproposed by earlier studies can perform poorly when the degree of the\ncorrelation is moderate to large. The test we propose in this paper is designed\nto have a power function robust to the correlation. Because the asymptotic null\ndistribution of our test statistic depends on the correlation $\\psi$ between\nthe disturbance and its square as earlier tests do, we also propose a modified\nversion of the test statistic such that its asymptotic null distribution is\nfree from the nuisance parameter $\\psi$. The modified test is shown to have\nbetter power properties than existing ones in large and finite samples.",
        "authors": [
            "Mikihito Nishi"
        ],
        "categories": "econ.EM",
        "published": "2023-01-12T07:34:49Z",
        "updated": "2023-01-16T21:49:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.04776v1",
        "title": "A Framework for Generalization and Transportation of Causal Estimates Under Covariate Shift",
        "abstract": "Randomized experiments are an excellent tool for estimating internally valid\ncausal effects with the sample at hand, but their external validity is\nfrequently debated. While classical results on the estimation of Population\nAverage Treatment Effects (PATE) implicitly assume random selection into\nexperiments, this is typically far from true in many medical,\nsocial-scientific, and industry experiments. When the experimental sample is\ndifferent from the target sample along observable or unobservable dimensions,\nexperimental estimates may be of limited use for policy decisions. We begin by\ndecomposing the extrapolation bias from estimating the Target Average Treatment\nEffect (TATE) using the Sample Average Treatment Effect (SATE) into covariate\nshift, overlap, and effect modification components, which researchers can\nreason about in order to diagnose the severity of extrapolation bias. Next, We\ncast covariate shift as a sample selection problem and propose estimators that\nre-weight the doubly-robust scores from experimental subjects to estimate\ntreatment effects in the overall sample (=: generalization) or in an alternate\ntarget sample (=: transportation). We implement these estimators in the\nopen-source R package causalTransportR and illustrate its performance in a\nsimulation study and discuss diagnostics to evaluate its performance.",
        "authors": [
            "Apoorva Lal",
            "Wenjing Zheng",
            "Simon Ejdemyr"
        ],
        "categories": "stat.ME",
        "published": "2023-01-12T01:00:36Z",
        "updated": "2023-01-12T01:00:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.04687v2",
        "title": "Inference on quantile processes with a finite number of clusters",
        "abstract": "I introduce a generic method for inference on entire quantile and regression\nquantile processes in the presence of a finite number of large and arbitrarily\nheterogeneous clusters. The method asymptotically controls size by generating\nstatistics that exhibit enough distributional symmetry such that randomization\ntests can be applied. The randomization test does not require ex-ante matching\nof clusters, is free of user-chosen parameters, and performs well at\nconventional significance levels with as few as five clusters. The method tests\nstandard (non-sharp) hypotheses and can even be asymptotically similar in\nempirically relevant situations. The main focus of the paper is inference on\nquantile treatment effects but the method applies more broadly. Numerical and\nempirical examples are provided.",
        "authors": [
            "Andreas Hagemann"
        ],
        "categories": "econ.EM",
        "published": "2023-01-11T19:28:34Z",
        "updated": "2023-06-15T16:47:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.04527v2",
        "title": "Fast and Reliable Jackknife and Bootstrap Methods for Cluster-Robust Inference",
        "abstract": "We provide computationally attractive methods to obtain jackknife-based\ncluster-robust variance matrix estimators (CRVEs) for linear regression models\nestimated by least squares. We also propose several new variants of the wild\ncluster bootstrap, which involve these CRVEs, jackknife-based bootstrap\ndata-generating processes, or both. Extensive simulation experiments suggest\nthat the new methods can provide much more reliable inferences than existing\nones in cases where the latter are not trustworthy, such as when the number of\nclusters is small and/or cluster sizes vary substantially. Three empirical\nexamples illustrate the new methods.",
        "authors": [
            "James G. MacKinnon",
            "Morten \u00d8rregaard Nielsen",
            "Matthew D. Webb"
        ],
        "categories": "econ.EM",
        "published": "2023-01-11T15:46:18Z",
        "updated": "2023-02-10T21:17:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.04522v2",
        "title": "Testing for the appropriate level of clustering in linear regression models",
        "abstract": "The overwhelming majority of empirical research that uses cluster-robust\ninference assumes that the clustering structure is known, even though there are\noften several possible ways in which a dataset could be clustered. We propose\ntwo tests for the correct level of clustering in regression models. One test\nfocuses on inference about a single coefficient, and the other on inference\nabout two or more coefficients. We provide both asymptotic and wild bootstrap\nimplementations. The proposed tests work for a null hypothesis of either no\nclustering or ``fine'' clustering against alternatives of ``coarser''\nclustering. We also propose a sequential testing procedure to determine the\nappropriate level of clustering. Simulations suggest that the bootstrap tests\nperform very well under the null hypothesis and can have excellent power. An\nempirical example suggests that using the tests leads to sensible inferences.",
        "authors": [
            "James G. MacKinnon",
            "Morten \u00d8rregaard Nielsen",
            "Matthew D. Webb"
        ],
        "categories": "econ.EM",
        "published": "2023-01-11T15:39:00Z",
        "updated": "2023-03-11T14:47:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.04439v1",
        "title": "Uniform Inference in Linear Error-in-Variables Models: Divide-and-Conquer",
        "abstract": "It is customary to estimate error-in-variables models using higher-order\nmoments of observables. This moments-based estimator is consistent only when\nthe coefficient of the latent regressor is assumed to be non-zero. We develop a\nnew estimator based on the divide-and-conquer principle that is consistent for\nany value of the coefficient of the latent regressor. In an application on the\nrelation between investment, (mismeasured) Tobin's $q$ and cash flow, we find\ntime periods in which the effect of Tobin's $q$ is not statistically different\nfrom zero. The implausibly large higher-order moment estimates in these periods\ndisappear when using the proposed estimator.",
        "authors": [
            "Tom Boot",
            "Art\u016bras Juodis"
        ],
        "categories": "econ.EM",
        "published": "2023-01-11T12:46:09Z",
        "updated": "2023-01-11T12:46:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.03805v3",
        "title": "Asymptotic Theory for Two-Way Clustering",
        "abstract": "This paper proves a new central limit theorem for a sample that exhibits\ntwo-way dependence and heterogeneity across clusters. Statistical inference for\nsituations with both two-way dependence and cluster heterogeneity has thus far\nbeen an open issue. The existing theory for two-way clustering inference\nrequires identical distributions across clusters (implied by the so-called\nseparate exchangeability assumption). Yet no such homogeneity requirement is\nneeded in the existing theory for one-way clustering. The new result therefore\ntheoretically justifies the view that two-way clustering is a more robust\nversion of one-way clustering, consistent with applied practice. In an\napplication to linear regression, I show that a standard plug-in variance\nestimator is valid for inference.",
        "authors": [
            "Luther Yap"
        ],
        "categories": "econ.EM",
        "published": "2023-01-10T06:14:16Z",
        "updated": "2024-06-12T20:31:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.02937v1",
        "title": "Quantile Autoregression-based Non-causality Testing",
        "abstract": "Non-causal processes have been drawing attention recently in Macroeconomics\nand Finance for their ability to display nonlinear behaviors such as asymmetric\ndynamics, clustering volatility, and local explosiveness. In this paper, we\ninvestigate the statistical properties of empirical conditional quantiles of\nnon-causal processes. Specifically, we show that the quantile autoregression\n(QAR) estimates for non-causal processes do not remain constant across\ndifferent quantiles in contrast to their causal counterparts. Furthermore, we\ndemonstrate that non-causal autoregressive processes admit nonlinear\nrepresentations for conditional quantiles given past observations. Exploiting\nthese properties, we propose three novel testing strategies of non-causality\nfor non-Gaussian processes within the QAR framework. The tests are constructed\neither by verifying the constancy of the slope coefficients or by applying a\nmisspecification test of the linear QAR model over different quantiles of the\nprocess. Some numerical experiments are included to examine the finite sample\nperformance of the testing strategies, where we compare different specification\ntests for dynamic quantiles with the Kolmogorov-Smirnov constancy test. The new\nmethodology is applied to some time series from financial markets to\ninvestigate the presence of speculative bubbles. The extension of the approach\nbased on the specification tests to AR processes driven by innovations with\nheteroskedasticity is studied through simulations. The performance of QAR\nestimates of non-causal processes at extreme quantiles is also explored.",
        "authors": [
            "Weifeng Jin"
        ],
        "categories": "econ.EM",
        "published": "2023-01-07T21:15:17Z",
        "updated": "2023-01-07T21:15:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.02648v1",
        "title": "Climate change heterogeneity: A new quantitative approach",
        "abstract": "Climate change is a non-uniform phenomenon. This paper proposes a new\nquantitative methodology to characterize, measure, and test the existence of\nclimate change heterogeneity. It consists of three steps. First, we introduce a\nnew testable warming typology based on the evolution of the trend of the whole\ntemperature distribution and not only on the average. Second, we define the\nconcepts of warming acceleration and warming amplification in a testable\nformat. And third, we introduce the new testable concept of warming dominance\nto determine whether region A is suffering a worse warming process than region\nB. Applying this three-step methodology, we find that Spain and the Globe\nexperience a clear distributional warming process (beyond the standard average)\nbut of different types. In both cases, this process is accelerating over time\nand asymmetrically amplified. Overall, warming in Spain dominates the Globe in\nall the quantiles except the lower tail of the global temperature distribution\nthat corresponds to the Arctic region. Our climate change heterogeneity results\nopen the door to the need for a non-uniform causal-effect climate analysis that\ngoes beyond the standard causality in mean as well as for a more efficient\ndesign of the mitigation-adaptation policies. In particular, the heterogeneity\nwe find suggests that these policies should contain a common global component\nand a clear local-regional element. Future climate agreements should take the\nwhole temperature distribution into account.",
        "authors": [
            "Maria Dolores Gadea",
            "Jesus Gonzalo"
        ],
        "categories": "econ.EM",
        "published": "2023-01-06T18:49:24Z",
        "updated": "2023-01-06T18:49:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.02052v3",
        "title": "Relaxing Instrument Exogeneity with Common Confounders",
        "abstract": "Instruments can be used to identify causal effects in the presence of\nunobserved confounding, under the famous relevance and exogeneity\n(unconfoundedness and exclusion) assumptions. As exogeneity is difficult to\njustify and to some degree untestable, it often invites criticism in\napplications. Hoping to alleviate this problem, we propose a novel\nidentification approach, which relaxes traditional IV exogeneity to exogeneity\nconditional on some unobserved common confounders. We assume there exist some\nrelevant proxies for the unobserved common confounders. Unlike typical proxies,\nour proxies can have a direct effect on the endogenous regressor and the\noutcome. We provide point identification results with a linearly separable\noutcome model in the disturbance, and alternatively with strict monotonicity in\nthe first stage. General doubly robust and Neyman orthogonal moments are\nderived consecutively to enable the straightforward root-n estimation of\nlow-dimensional parameters despite the high-dimensionality of nuisances,\nthemselves non-uniquely defined by Fredholm integral equations. Using this\nnovel method with NLS97 data, we separate ability bias from general selection\nbias in the economic returns to education problem.",
        "authors": [
            "Christian Tien"
        ],
        "categories": "econ.EM",
        "published": "2023-01-05T12:59:57Z",
        "updated": "2023-08-27T17:58:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.01362v1",
        "title": "Measuring tail risk at high-frequency: An $L_1$-regularized extreme value regression approach with unit-root predictors",
        "abstract": "We study tail risk dynamics in high-frequency financial markets and their\nconnection with trading activity and market uncertainty. We introduce a dynamic\nextreme value regression model accommodating both stationary and local\nunit-root predictors to appropriately capture the time-varying behaviour of the\ndistribution of high-frequency extreme losses. To characterize trading activity\nand market uncertainty, we consider several volatility and liquidity\npredictors, and propose a two-step adaptive $L_1$-regularized maximum\nlikelihood estimator to select the most appropriate ones. We establish the\noracle property of the proposed estimator for selecting both stationary and\nlocal unit-root predictors, and show its good finite sample properties in an\nextensive simulation study. Studying the high-frequency extreme losses of nine\nlarge liquid U.S. stocks using 42 liquidity and volatility predictors, we find\nthe severity of extreme losses to be well predicted by low levels of price\nimpact in period of high volatility of liquidity and volatility.",
        "authors": [
            "Julien Hambuckers",
            "Li Sun",
            "Luca Trapin"
        ],
        "categories": "econ.EM",
        "published": "2023-01-03T21:31:00Z",
        "updated": "2023-01-03T21:31:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.01109v1",
        "title": "On the causality-preservation capabilities of generative modelling",
        "abstract": "Modeling lies at the core of both the financial and the insurance industry\nfor a wide variety of tasks. The rise and development of machine learning and\ndeep learning models have created many opportunities to improve our modeling\ntoolbox. Breakthroughs in these fields often come with the requirement of large\namounts of data. Such large datasets are often not publicly available in\nfinance and insurance, mainly due to privacy and ethics concerns. This lack of\ndata is currently one of the main hurdles in developing better models. One\npossible option to alleviating this issue is generative modeling. Generative\nmodels are capable of simulating fake but realistic-looking data, also referred\nto as synthetic data, that can be shared more freely. Generative Adversarial\nNetworks (GANs) is such a model that increases our capacity to fit very\nhigh-dimensional distributions of data. While research on GANs is an active\ntopic in fields like computer vision, they have found limited adoption within\nthe human sciences, like economics and insurance. Reason for this is that in\nthese fields, most questions are inherently about identification of causal\neffects, while to this day neural networks, which are at the center of the GAN\nframework, focus mostly on high-dimensional correlations. In this paper we\nstudy the causal preservation capabilities of GANs and whether the produced\nsynthetic data can reliably be used to answer causal questions. This is done by\nperforming causal analyses on the synthetic data, produced by a GAN, with\nincreasingly more lenient assumptions. We consider the cross-sectional case,\nthe time series case and the case with a complete structural model. It is shown\nthat in the simple cross-sectional scenario where correlation equals causation\nthe GAN preserves causality, but that challenges arise for more advanced\nanalyses.",
        "authors": [
            "Yves-C\u00e9dric Bauwelinckx",
            "Jan Dhaene",
            "Tim Verdonck",
            "Milan van den Heuvel"
        ],
        "categories": "cs.LG",
        "published": "2023-01-03T14:09:15Z",
        "updated": "2023-01-03T14:09:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.01091v1",
        "title": "Fitting mixed logit random regret minimization models using maximum simulated likelihood",
        "abstract": "This article describes the mixrandregret command, which extends the\nrandregret command introduced in Guti\\'errez-Vargas et al. (2021, The Stata\nJournal 21: 626-658) incorporating random coefficients for Random Regret\nMinimization models. The newly developed command mixrandregret allows the\ninclusion of random coefficients in the regret function of the classical RRM\nmodel introduced in Chorus (2010, European Journal of Transport and\nInfrastructure Research 10: 181-196). The command allows the user to specify a\ncombination of fixed and random coefficients. In addition, the user can specify\nnormal and log-normal distributions for the random coefficients using the\ncommands' options. The models are fitted using simulated maximum likelihood\nusing numerical integration to approximate the choice probabilities.",
        "authors": [
            "Ziyue Zhu",
            "\u00c1lvaro A. Guti\u00e9rrez-Vargas",
            "Martina Vandebroek"
        ],
        "categories": "econ.EM",
        "published": "2023-01-03T13:34:53Z",
        "updated": "2023-01-03T13:34:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.01085v3",
        "title": "The Chained Difference-in-Differences",
        "abstract": "This paper studies the identification, estimation, and inference of long-term\n(binary) treatment effect parameters when balanced panel data is not available,\nor consists of only a subset of the available data. We develop a new estimator:\nthe chained difference-in-differences, which leverages the overlapping\nstructure of many unbalanced panel data sets. This approach consists in\naggregating a collection of short-term treatment effects estimated on multiple\nincomplete panels. Our estimator accommodates (1) multiple time periods, (2)\nvariation in treatment timing, (3) treatment effect heterogeneity, (4) general\nmissing data patterns, and (5) sample selection on observables. We establish\nthe asymptotic properties of the proposed estimator and discuss identification\nand efficiency gains in comparison to existing methods. Finally, we illustrate\nits relevance through (i) numerical simulations, and (ii) an application about\nthe effects of an innovation policy in France.",
        "authors": [
            "Christophe Bell\u00e9go",
            "David Benatia",
            "Vincent Dortet-Bernardet"
        ],
        "categories": "econ.EM",
        "published": "2023-01-03T13:26:22Z",
        "updated": "2024-05-24T17:27:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.00509v1",
        "title": "Time-Varying Coefficient DAR Model and Stability Measures for Stablecoin Prices: An Application to Tether",
        "abstract": "This paper examines the dynamics of Tether, the stablecoin with the largest\nmarket capitalization. We show that the distributional and dynamic properties\nof Tether/USD rates have been evolving from 2017 to 2021. We use local analysis\nmethods to detect and describe the local patterns, such as short-lived trends,\ntime-varying volatility and persistence. To accommodate these patterns, we\nconsider a time varying parameter Double Autoregressive tvDAR(1) model under\nthe assumption of local stationarity of Tether/USD rates. We estimate the tvDAR\nmodel non-parametrically and test hypotheses on the functional parameters. In\nthe application to Tether, the model provides a good fit and reliable\nout-of-sample forecasts at short horizons, while being robust to time-varying\npersistence and volatility. In addition, the model yields a simple plug-in\nmeasure of stability for Tether and other stablecoins for assessing and\ncomparing their stability.",
        "authors": [
            "Antoine Djobenou",
            "Emre Inan",
            "Joann Jasiak"
        ],
        "categories": "econ.EM",
        "published": "2023-01-02T03:07:09Z",
        "updated": "2023-01-02T03:07:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.00292v6",
        "title": "Inference for Large Panel Data with Many Covariates",
        "abstract": "This paper proposes a novel testing procedure for selecting a sparse set of\ncovariates that explains a large dimensional panel. Our selection method\nprovides correct false detection control while having higher power than\nexisting approaches. We develop the inferential theory for large panels with\nmany covariates by combining post-selection inference with a novel multiple\ntesting adjustment. Our data-driven hypotheses are conditional on the sparse\ncovariate selection. We control for family-wise error rates for covariate\ndiscovery for large cross-sections. As an easy-to-use and practically relevant\nprocedure, we propose Panel-PoSI, which combines the data-driven adjustment for\npanel multiple testing with valid post-selection p-values of a generalized\nLASSO, that allows us to incorporate priors. In an empirical study, we select a\nsmall number of asset pricing factors that explain a large cross-section of\ninvestment strategies. Our method dominates the benchmarks out-of-sample due to\nits better size and power.",
        "authors": [
            "Markus Pelger",
            "Jiacheng Zou"
        ],
        "categories": "econ.EM",
        "published": "2022-12-31T21:07:24Z",
        "updated": "2023-03-06T19:21:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.00277v2",
        "title": "Higher-order Refinements of Small Bandwidth Asymptotics for Density-Weighted Average Derivative Estimators",
        "abstract": "The density weighted average derivative (DWAD) of a regression function is a\ncanonical parameter of interest in economics. Classical first-order large\nsample distribution theory for kernel-based DWAD estimators relies on tuning\nparameter restrictions and model assumptions that imply an asymptotic linear\nrepresentation of the point estimator. These conditions can be restrictive, and\nthe resulting distributional approximation may not be representative of the\nactual sampling distribution of the statistic of interest. In particular, the\napproximation is not robust to bandwidth choice. Small bandwidth asymptotics\noffers an alternative, more general distributional approximation for\nkernel-based DWAD estimators that allows for, but does not require, asymptotic\nlinearity. The resulting inference procedures based on small bandwidth\nasymptotics were found to exhibit superior finite sample performance in\nsimulations, but no formal theory justifying that empirical success is\navailable in the literature. Employing Edgeworth expansions, this paper shows\nthat small bandwidth asymptotic approximations lead to inference procedures\nwith higher-order distributional properties that are demonstrably superior to\nthose of procedures based on asymptotic linear approximations.",
        "authors": [
            "Matias D. Cattaneo",
            "Max H. Farrell",
            "Michael Jansson",
            "Ricardo Masini"
        ],
        "categories": "econ.EM",
        "published": "2022-12-31T19:50:50Z",
        "updated": "2024-02-15T18:55:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.00251v3",
        "title": "Feature Selection for Personalized Policy Analysis",
        "abstract": "In this paper, we propose Forest-PLS, a feature selection method for\nanalyzing policy effect heterogeneity in a more flexible and comprehensive\nmanner than is typically available with conventional methods. In particular,\nour method is able to capture policy effect heterogeneity both within and\nacross subgroups of the population defined by observable characteristics. To\nachieve this, we employ partial least squares to identify target components of\nthe population and causal forests to estimate personalized policy effects\nacross these components. We show that the method is consistent and leads to\nasymptotically normally distributed policy effects. To demonstrate the efficacy\nof our approach, we apply it to the data from the Pennsylvania Reemployment\nBonus Experiments, which were conducted in 1988-1989. The analysis reveals that\nfinancial incentives can motivate some young non-white individuals to enter the\nlabor market. However, these incentives may also provide a temporary financial\ncushion for others, dissuading them from actively seeking employment. Our\nfindings highlight the need for targeted, personalized measures for young\nnon-white male participants.",
        "authors": [
            "Maria Nareklishvili",
            "Nicholas Polson",
            "Vadim Sokolov"
        ],
        "categories": "econ.EM",
        "published": "2022-12-31T16:52:46Z",
        "updated": "2023-07-23T19:59:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2301.00092v2",
        "title": "Inference on Time Series Nonparametric Conditional Moment Restrictions Using General Sieves",
        "abstract": "General nonlinear sieve learnings are classes of nonlinear sieves that can\napproximate nonlinear functions of high dimensional variables much more\nflexibly than various linear sieves (or series). This paper considers general\nnonlinear sieve quasi-likelihood ratio (GN-QLR) based inference on expectation\nfunctionals of time series data, where the functionals of interest are based on\nsome nonparametric function that satisfy conditional moment restrictions and\nare learned using multilayer neural networks. While the asymptotic normality of\nthe estimated functionals depends on some unknown Riesz representer of the\nfunctional space, we show that the optimally weighted GN-QLR statistic is\nasymptotically Chi-square distributed, regardless whether the expectation\nfunctional is regular (root-$n$ estimable) or not. This holds when the data are\nweakly dependent beta-mixing condition. We apply our method to the off-policy\nevaluation in reinforcement learning, by formulating the Bellman equation into\nthe conditional moment restriction framework, so that we can make inference\nabout the state-specific value functional using the proposed GN-QLR method with\ntime series data. In addition, estimating the averaged partial means and\naveraged partial derivatives of nonparametric instrumental variables and\nquantile IV models are also presented as leading examples. Finally, a Monte\nCarlo study shows the finite sample performance of the procedure",
        "authors": [
            "Xiaohong Chen",
            "Yuan Liao",
            "Weichen Wang"
        ],
        "categories": "stat.ML",
        "published": "2022-12-31T01:44:17Z",
        "updated": "2023-01-03T02:37:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.14622v2",
        "title": "Causal identification with subjective outcomes",
        "abstract": "Survey questions often elicit responses on ordered scales for which the\ndefinitions of the categories are subjective, possibly varying by individual.\nThis paper clarifies what is learned when these subjective reports are used as\nan outcome in regression-based causal inference. When a continuous treatment\nvariable is statistically independent of both i) potential outcomes; and ii)\nheterogeneity in reporting styles, a nonparametric regression of integer\ncategory numbers on that variable uncovers a positively-weighted linear\ncombination of causal responses among individuals who are on the margin between\nadjacent response categories. Though the weights do not integrate to one, the\nratio of local regression derivatives with respect to two such explanatory\nvariables identifies the relative magnitudes of convex averages of their\neffects. When results are extended to discrete treatment variables, different\nweighting schemes apply to different regressors, making comparisons of\nmagnitude less informative. I obtain a partial identification result for\ncomparing the effects of a discrete treatment variable to those of another\ntreatment variable when there are many categories and individual reporting\nfunctions are linear. I also provide results for identification using\ninstrumental variables.",
        "authors": [
            "Leonard Goff"
        ],
        "categories": "econ.EM",
        "published": "2022-12-30T10:17:09Z",
        "updated": "2023-02-16T18:58:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.14444v5",
        "title": "Empirical Bayes When Estimation Precision Predicts Parameters",
        "abstract": "Gaussian empirical Bayes methods usually maintain a precision independence\nassumption: The unknown parameters of interest are independent from the known\nstandard errors of the estimates. This assumption is often theoretically\nquestionable and empirically rejected. This paper proposes to model the\nconditional distribution of the parameter given the standard errors as a\nflexibly parametrized location-scale family of distributions, leading to a\nfamily of methods that we call CLOSE. The CLOSE framework unifies and\ngeneralizes several proposals under precision dependence. We argue that the\nmost flexible member of the CLOSE family is a minimalist and computationally\nefficient default for accounting for precision dependence. We analyze this\nmethod and show that it is competitive in terms of the regret of subsequent\ndecisions rules. Empirically, using CLOSE leads to sizable gains for selecting\nhigh-mobility Census tracts.",
        "authors": [
            "Jiafeng Chen"
        ],
        "categories": "econ.EM",
        "published": "2022-12-29T19:44:59Z",
        "updated": "2024-12-11T05:47:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.14411v5",
        "title": "Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations",
        "abstract": "Sequential tests and their implied confidence sequences, which are valid at\narbitrary stopping times, promise flexible statistical inference and on-the-fly\ndecision making. However, strong guarantees are limited to parametric\nsequential tests that under-cover in practice or concentration-bound-based\nsequences that over-cover and have suboptimal rejection times. In this work, we\nconsider classic delayed-start normal-mixture sequential probability ratio\ntests, and we provide the first asymptotic type-I-error and\nexpected-rejection-time guarantees under general non-parametric data generating\nprocesses, where the asymptotics are indexed by the test's burn-in time. The\ntype-I-error results primarily leverage a martingale strong invariance\nprinciple and establish that these tests (and their implied confidence\nsequences) have type-I error rates asymptotically equivalent to the desired\n(possibly varying) $\\alpha$-level. The expected-rejection-time results\nprimarily leverage an identity inspired by It\\^o's lemma and imply that, in\ncertain asymptotic regimes, the expected rejection time is asymptotically\nequivalent to the minimum possible among $\\alpha$-level tests. We show how to\napply our results to sequential inference on parameters defined by estimating\nequations, such as average treatment effects. Together, our results establish\nthese (ostensibly parametric) tests as general-purpose, non-parametric, and\nnear-optimal. We illustrate this via numerical simulations and a real-data\napplication to A/B testing at Netflix.",
        "authors": [
            "Aurelien Bibaut",
            "Nathan Kallus",
            "Michael Lindon"
        ],
        "categories": "stat.ME",
        "published": "2022-12-29T18:37:08Z",
        "updated": "2024-03-11T17:33:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.14185v1",
        "title": "What Estimators Are Unbiased For Linear Models?",
        "abstract": "The recent thought-provoking paper by Hansen [2022, Econometrica] proved that\nthe Gauss-Markov theorem continues to hold without the requirement that\ncompeting estimators are linear in the vector of outcomes. Despite the elegant\nproof, it was shown by the authors and other researchers that the main result\nin the earlier version of Hansen's paper does not extend the classic\nGauss-Markov theorem because no nonlinear unbiased estimator exists under his\nconditions. To address the issue, Hansen [2022] added statements in the latest\nversion with new conditions under which nonlinear unbiased estimators exist.\n  Motivated by the lively discussion, we study a fundamental problem: what\nestimators are unbiased for a given class of linear models? We first review a\nline of highly relevant work dating back to the 1960s, which, unfortunately,\nhave not drawn enough attention. Then, we introduce notation that allows us to\nrestate and unify results from earlier work and Hansen [2022]. The new\nframework also allows us to highlight differences among previous conclusions.\nLastly, we establish new representation theorems for unbiased estimators under\ndifferent restrictions on the linear model, allowing the coefficients and\ncovariance matrix to take only a finite number of values, the higher moments of\nthe estimator and the dependent variable to exist, and the error distribution\nto be discrete, absolutely continuous, or dominated by another probability\nmeasure. Our results substantially generalize the claims of parallel\ncommentaries on Hansen [2022] and a remarkable result by Koopmann [1982].",
        "authors": [
            "Lihua Lei",
            "Jeffrey Wooldridge"
        ],
        "categories": "econ.EM",
        "published": "2022-12-29T06:19:44Z",
        "updated": "2022-12-29T06:19:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.14105v2",
        "title": "Supercompliers",
        "abstract": "In a binary-treatment instrumental variable framework, we define\nsupercompliers as the subpopulation whose treatment take-up positively responds\nto eligibility and whose outcome positively responds to take-up. Supercompliers\nare the only subpopulation to benefit from treatment eligibility and, hence,\nare of great policy interest. Given a set of jointly testable assumptions and a\nbinary outcome, we can completely identify the characteristics of\nsupercompliers. Specifically, we require the standard assumptions from the\nlocal average treatment effect literature along with an outcome monotonicity\nassumption (i.e., treatment is weakly beneficial). We can estimate and conduct\ninference on supercomplier characteristics using standard instrumental variable\nregression.",
        "authors": [
            "Matthew L. Comey",
            "Amanda R. Eng",
            "Zhuan Pei"
        ],
        "categories": "econ.EM",
        "published": "2022-12-28T21:53:57Z",
        "updated": "2023-08-17T15:56:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.14075v2",
        "title": "Forward Orthogonal Deviations GMM and the Absence of Large Sample Bias",
        "abstract": "It is well known that generalized method of moments (GMM) estimators of\ndynamic panel data regressions can have significant bias when the number of\ntime periods ($T$) is not small compared to the number of cross-sectional units\n($n$). The bias is attributed to the use of many instrumental variables. This\npaper shows that if the maximum number of instrumental variables used in a\nperiod increases with $T$ at a rate slower than $T^{1/2}$, then GMM estimators\nthat exploit the forward orthogonal deviations (FOD) transformation do not have\nasymptotic bias, regardless of how fast $T$ increases relative to $n$. This\nconclusion is specific to using the FOD transformation. A similar conclusion\ndoes not necessarily apply when other transformations are used to remove fixed\neffects. Monte Carlo evidence illustrating the analytical results is provided.",
        "authors": [
            "Robert F. Phillips"
        ],
        "categories": "econ.EM",
        "published": "2022-12-28T19:38:03Z",
        "updated": "2024-07-17T19:55:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.13996v1",
        "title": "Robustifying Markowitz",
        "abstract": "Markowitz mean-variance portfolios with sample mean and covariance as input\nparameters feature numerous issues in practice. They perform poorly out of\nsample due to estimation error, they experience extreme weights together with\nhigh sensitivity to change in input parameters. The heavy-tail characteristics\nof financial time series are in fact the cause for these erratic fluctuations\nof weights that consequently create substantial transaction costs. In\nrobustifying the weights we present a toolbox for stabilizing costs and weights\nfor global minimum Markowitz portfolios. Utilizing a projected gradient descent\n(PGD) technique, we avoid the estimation and inversion of the covariance\noperator as a whole and concentrate on robust estimation of the gradient\ndescent increment. Using modern tools of robust statistics we construct a\ncomputationally efficient estimator with almost Gaussian properties based on\nmedian-of-means uniformly over weights. This robustified Markowitz approach is\nconfirmed by empirical studies on equity markets. We demonstrate that\nrobustified portfolios reach the lowest turnover compared to shrinkage-based\nand constrained portfolios while preserving or slightly improving out-of-sample\nperformance.",
        "authors": [
            "Wolfgang Karl H\u00e4rdle",
            "Yegor Klochkov",
            "Alla Petukhina",
            "Nikita Zhivotovskiy"
        ],
        "categories": "econ.EM",
        "published": "2022-12-28T18:09:14Z",
        "updated": "2022-12-28T18:09:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.13324v2",
        "title": "Spectral and post-spectral estimators for grouped panel data models",
        "abstract": "In this paper, we develop spectral and post-spectral estimators for grouped\npanel data models. Both estimators are consistent in the asymptotics where the\nnumber of observations $N$ and the number of time periods $T$ simultaneously\ngrow large. In addition, the post-spectral estimator is $\\sqrt{NT}$-consistent\nand asymptotically normal with mean zero under the assumption of well-separated\ngroups even if $T$ is growing much slower than $N$. The post-spectral estimator\nhas, therefore, theoretical properties that are comparable to those of the\ngrouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In\ncontrast to the grouped fixed-effect estimator, however, our post-spectral\nestimator is computationally straightforward.",
        "authors": [
            "Denis Chetverikov",
            "Elena Manresa"
        ],
        "categories": "econ.EM",
        "published": "2022-12-26T23:30:37Z",
        "updated": "2022-12-29T19:36:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.13226v3",
        "title": "An Effective Treatment Approach to Difference-in-Differences with General Treatment Patterns",
        "abstract": "We consider a general difference-in-differences model in which the treatment\nvariable of interest may be non-binary and its value may change in each period.\nIt is generally difficult to estimate treatment parameters defined with the\npotential outcome given the entire path of treatment adoption, because each\ntreatment path may be experienced by only a small number of observations. We\npropose an alternative approach using the concept of effective treatment, which\nsummarizes the treatment path into an empirically tractable low-dimensional\nvariable, and develop doubly robust identification, estimation, and inference\nmethods. We also provide a companion R software package.",
        "authors": [
            "Takahide Yanagi"
        ],
        "categories": "econ.EM",
        "published": "2022-12-26T17:20:59Z",
        "updated": "2023-05-29T06:34:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.13145v1",
        "title": "Orthogonal Series Estimation for the Ratio of Conditional Expectation Functions",
        "abstract": "In various fields of data science, researchers are often interested in\nestimating the ratio of conditional expectation functions (CEFR). Specifically\nin causal inference problems, it is sometimes natural to consider ratio-based\ntreatment effects, such as odds ratios and hazard ratios, and even\ndifference-based treatment effects are identified as CEFR in some empirically\nrelevant settings. This chapter develops the general framework for estimation\nand inference on CEFR, which allows the use of flexible machine learning for\ninfinite-dimensional nuisance parameters. In the first stage of the framework,\nthe orthogonal signals are constructed using debiased machine learning\ntechniques to mitigate the negative impacts of the regularization bias in the\nnuisance estimates on the target estimates. The signals are then combined with\na novel series estimator tailored for CEFR. We derive the pointwise and uniform\nasymptotic results for estimation and inference on CEFR, including the validity\nof the Gaussian bootstrap, and provide low-level sufficient conditions to apply\nthe proposed framework to some specific examples. We demonstrate the\nfinite-sample performance of the series estimator constructed under the\nproposed framework by numerical simulations. Finally, we apply the proposed\nmethod to estimate the causal effect of the 401(k) program on household assets.",
        "authors": [
            "Kazuhiko Shinoda",
            "Takahiro Hoshino"
        ],
        "categories": "econ.EM",
        "published": "2022-12-26T13:01:17Z",
        "updated": "2022-12-26T13:01:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.12981v2",
        "title": "Tensor Principal Component Analysis",
        "abstract": "In this paper, we develop new methods for analyzing high-dimensional tensor\ndatasets. A tensor factor model describes a high-dimensional dataset as a sum\nof a low-rank component and an idiosyncratic noise, generalizing traditional\nfactor models for panel data. We propose an estimation algorithm, called tensor\nprincipal component analysis (TPCA), which generalizes the traditional PCA\napplicable to panel data. The algorithm involves unfolding the tensor into a\nsequence of matrices along different dimensions and applying PCA to the\nunfolded matrices. We provide theoretical results on the consistency and\nasymptotic distribution for the TPCA estimator of loadings and factors. We also\nintroduce a novel test for the number of factors in a tensor factor model. The\nTPCA and the test feature good performance in Monte Carlo experiments and are\napplied to sorted portfolios.",
        "authors": [
            "Andrii Babii",
            "Eric Ghysels",
            "Junsu Pan"
        ],
        "categories": "econ.EM",
        "published": "2022-12-26T01:47:06Z",
        "updated": "2023-08-22T15:39:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.11833v2",
        "title": "Efficient Sampling for Realized Variance Estimation in Time-Changed Diffusion Models",
        "abstract": "This paper analyzes the benefits of sampling intraday returns in intrinsic\ntime for the standard and pre-averaging realized variance (RV) estimators. We\ntheoretically show in finite samples and asymptotically that the RV estimator\nis most efficient under the new concept of realized business time, which\nsamples according to a combination of observed trades and estimated tick\nvariance. Our asymptotic results carry over to the pre-averaging RV estimator\nunder market microstructure noise. The analysis builds on the assumption that\nasset prices follow a diffusion that is time-changed with a jump process that\nseparately models the transaction times. This provides a flexible model that\nseparately captures the empirically varying trading intensity and tick variance\nprocesses, which are particularly relevant for disentangling the driving forces\nof the sampling schemes. Extensive simulations confirm our theoretical results\nand show that realized business time remains superior also under more general\nnoise and process specifications. An application to stock data provides\nempirical evidence for the benefits of using realized business time sampling to\nconstruct more efficient RV estimators as well as for an improved forecasting\nperformance.",
        "authors": [
            "Timo Dimitriadis",
            "Roxana Halbleib",
            "Jeannine Polivka",
            "Jasper Rennspies",
            "Sina Streicher",
            "Axel Friedrich Wolter"
        ],
        "categories": "econ.EM",
        "published": "2022-12-22T16:12:54Z",
        "updated": "2023-12-23T10:42:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.11112v2",
        "title": "A Bootstrap Specification Test for Semiparametric Models with Generated Regressors",
        "abstract": "This paper provides a specification test for semiparametric models with\nnonparametrically generated regressors. Such variables are not observed by the\nresearcher but are nonparametrically identified and estimable. Applications of\nthe test include models with endogenous regressors identified by control\nfunctions, semiparametric sample selection models, or binary games with\nincomplete information. The statistic is built from the residuals of the\nsemiparametric model. A novel wild bootstrap procedure is shown to provide\nvalid critical values. We consider nonparametric estimators with an automatic\nbias correction that makes the test implementable without undersmoothing. In\nsimulations the test exhibits good small sample performances, and an\napplication to women's labor force participation decisions shows its\nimplementation in a real data context.",
        "authors": [
            "Elia Lapenta"
        ],
        "categories": "econ.EM",
        "published": "2022-12-21T15:51:24Z",
        "updated": "2023-10-25T16:33:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.11012v2",
        "title": "Partly Linear Instrumental Variables Regressions without Smoothing on the Instruments",
        "abstract": "We consider a semiparametric partly linear model identified by instrumental\nvariables. We propose an estimation method that does not smooth on the\ninstruments and we extend the Landweber-Fridman regularization scheme to the\nestimation of this semiparametric model. We then show the asymptotic normality\nof the parametric estimator and obtain the convergence rate for the\nnonparametric estimator. Our estimator that does not smooth on the instruments\ncoincides with a typical estimator that does smooth on the instruments but\nkeeps the respective bandwidth fixed as the sample size increases. We propose a\ndata driven method for the selection of the regularization parameter, and in a\nsimulation study we show the attractive performance of our estimators.",
        "authors": [
            "Jean-Pierre Florens",
            "Elia Lapenta"
        ],
        "categories": "econ.EM",
        "published": "2022-12-21T13:37:17Z",
        "updated": "2023-10-25T14:23:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.10790v1",
        "title": "Inference for Model Misspecification in Interest Rate Term Structure using Functional Principal Component Analysis",
        "abstract": "Level, slope, and curvature are three commonly-believed principal components\nin interest rate term structure and are thus widely used in modeling. This\npaper characterizes the heterogeneity of how misspecified such models are\nthrough time. Presenting the orthonormal basis in the Nelson-Siegel model\ninterpretable as the three factors, we design two nonparametric tests for\nwhether the basis is equivalent to the data-driven functional principal\ncomponent basis underlying the yield curve dynamics, considering the ordering\nof eigenfunctions or not, respectively. Eventually, we discover high dispersion\nbetween the two bases when rare events occur, suggesting occasional\nmisspecification even if the model is overall expressive.",
        "authors": [
            "Kaiwen Hou"
        ],
        "categories": "econ.EM",
        "published": "2022-12-21T06:19:58Z",
        "updated": "2022-12-21T06:19:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.10301v3",
        "title": "Probabilistic Quantile Factor Analysis",
        "abstract": "This paper extends quantile factor analysis to a probabilistic variant that\nincorporates regularization and computationally efficient variational\napproximations. We establish through synthetic and real data experiments that\nthe proposed estimator can, in many cases, achieve better accuracy than a\nrecently proposed loss-based estimator. We contribute to the factor analysis\nliterature by extracting new indexes of \\emph{low}, \\emph{medium}, and\n\\emph{high} economic policy uncertainty, as well as \\emph{loose},\n\\emph{median}, and \\emph{tight} financial conditions. We show that the high\nuncertainty and tight financial conditions indexes have superior predictive\nability for various measures of economic activity. In a high-dimensional\nexercise involving about 1000 daily financial series, we find that quantile\nfactors also provide superior out-of-sample information compared to mean or\nmedian factors.",
        "authors": [
            "Dimitris Korobilis",
            "Maximilian Schr\u00f6der"
        ],
        "categories": "econ.EM",
        "published": "2022-12-20T14:49:27Z",
        "updated": "2024-08-15T06:09:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.09868v1",
        "title": "Quantifying fairness and discrimination in predictive models",
        "abstract": "The analysis of discrimination has long interested economists and lawyers. In\nrecent years, the literature in computer science and machine learning has\nbecome interested in the subject, offering an interesting re-reading of the\ntopic. These questions are the consequences of numerous criticisms of\nalgorithms used to translate texts or to identify people in images. With the\narrival of massive data, and the use of increasingly opaque algorithms, it is\nnot surprising to have discriminatory algorithms, because it has become easy to\nhave a proxy of a sensitive variable, by enriching the data indefinitely.\nAccording to Kranzberg (1986), \"technology is neither good nor bad, nor is it\nneutral\", and therefore, \"machine learning won't give you anything like gender\nneutrality `for free' that you didn't explicitely ask for\", as claimed by\nKearns et a. (2019). In this article, we will come back to the general context,\nfor predictive models in classification. We will present the main concepts of\nfairness, called group fairness, based on independence between the sensitive\nvariable and the prediction, possibly conditioned on this or that information.\nWe will finish by going further, by presenting the concepts of individual\nfairness. Finally, we will see how to correct a potential discrimination, in\norder to guarantee that a model is more ethical",
        "authors": [
            "Arthur Charpentier"
        ],
        "categories": "econ.EM",
        "published": "2022-12-19T21:38:13Z",
        "updated": "2022-12-19T21:38:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.09844v5",
        "title": "Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding",
        "abstract": "Predictive algorithms inform consequential decisions in settings where the\noutcome is selectively observed given choices made by human decision makers. We\npropose a unified framework for the robust design and evaluation of predictive\nalgorithms in selectively observed data. We impose general assumptions on how\nmuch the outcome may vary on average between unselected and selected units\nconditional on observed covariates and identified nuisance parameters,\nformalizing popular empirical strategies for imputing missing data such as\nproxy outcomes and instrumental variables. We develop debiased machine learning\nestimators for the bounds on a large class of predictive performance estimands,\nsuch as the conditional likelihood of the outcome, a predictive algorithm's\nmean square error, true/false positive rate, and many others, under these\nassumptions. In an administrative dataset from a large Australian financial\ninstitution, we illustrate how varying assumptions on unobserved confounding\nleads to meaningful changes in default risk predictions and evaluations of\ncredit scores across sensitive groups.",
        "authors": [
            "Ashesh Rambachan",
            "Amanda Coston",
            "Edward Kennedy"
        ],
        "categories": "econ.EM",
        "published": "2022-12-19T20:41:44Z",
        "updated": "2024-05-19T19:53:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.10359v2",
        "title": "Simultaneous Inference of a Partially Linear Model in Time Series",
        "abstract": "We introduce a new methodology to conduct simultaneous inference of the\nnonparametric component in partially linear time series regression models where\nthe nonparametric part is a multivariate unknown function. In particular, we\nconstruct a simultaneous confidence region (SCR) for the multivariate function\nby extending the high-dimensional Gaussian approximation to dependent processes\nwith continuous index sets. Our results allow for a more general dependence\nstructure compared to previous works and are widely applicable to a variety of\nlinear and nonlinear autoregressive processes. We demonstrate the validity of\nour proposed methodology by examining the finite-sample performance in the\nsimulation study. Finally, an application in time series, the forward premium\nregression, is presented, where we construct the SCR for the foreign exchange\nrisk premium from the exchange rate and macroeconomic data.",
        "authors": [
            "Jiaqi Li",
            "Likai Chen",
            "Kun Ho Kim",
            "Tianwei Zhou"
        ],
        "categories": "stat.ME",
        "published": "2022-12-19T17:16:47Z",
        "updated": "2023-09-02T21:57:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.09193v2",
        "title": "Identification of time-varying counterfactual parameters in nonlinear panel models",
        "abstract": "We develop a general framework for the identification of counterfactual\nparameters in a class of nonlinear semiparametric panel models with fixed\neffects and time effects. Our method applies to models for discrete outcomes\n(e.g., two-way fixed effects binary choice) or continuous outcomes (e.g.,\ncensored regression), with discrete or continuous regressors. Our results do\nnot require parametric assumptions on the error terms or time-homogeneity on\nthe outcome equation. Our main results focus on static models, with a set of\nresults applying to models without any exogeneity conditions. We show that the\nsurvival distribution of counterfactual outcomes is identified (point or\npartial) in this class of models. This parameter is a building block for most\npartial and marginal effects of interest in applied practice that are based on\nthe average structural function as defined by Blundell and Powell (2003, 2004).\nTo the best of our knowledge, ours are the first results on average partial and\nmarginal effects for binary choice and ordered choice models with two-way fixed\neffects and non-logistic errors.",
        "authors": [
            "Irene Botosaru",
            "Chris Muris"
        ],
        "categories": "econ.EM",
        "published": "2022-12-18T23:36:59Z",
        "updated": "2023-11-04T15:21:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.09007v2",
        "title": "PAC-Bayesian Treatment Allocation Under Budget Constraints",
        "abstract": "This paper considers the estimation of treatment assignment rules when the\npolicy maker faces a general budget or resource constraint. Utilizing the\nPAC-Bayesian framework, we propose new treatment assignment rules that allow\nfor flexible notions of treatment outcome, treatment cost, and a budget\nconstraint. For example, the constraint setting allows for cost-savings, when\nthe costs of non-treatment exceed those of treatment for a subpopulation, to be\nfactored into the budget. It also accommodates simpler settings, such as\nquantity constraints, and doesn't require outcome responses and costs to have\nthe same unit of measurement. Importantly, the approach accounts for settings\nwhere budget or resource limitations may preclude treating all that can\nbenefit, where costs may vary with individual characteristics, and where there\nmay be uncertainty regarding the cost of treatment rules of interest. Despite\nthe nomenclature, our theoretical analysis examines frequentist properties of\nthe proposed rules. For stochastic rules that typically approach\nbudget-penalized empirical welfare maximizing policies in larger samples, we\nderive non-asymptotic generalization bounds for the target population costs and\nsharp oracle-type inequalities that compare the rules' welfare regret to that\nof optimal policies in relevant budget categories. A closely related,\nnon-stochastic, model aggregation treatment assignment rule is shown to inherit\ndesirable attributes.",
        "authors": [
            "Daniel F. Pellatt"
        ],
        "categories": "econ.EM",
        "published": "2022-12-18T04:22:16Z",
        "updated": "2023-06-09T01:16:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.08615v1",
        "title": "A smooth transition autoregressive model for matrix-variate time series",
        "abstract": "In many applications, data are observed as matrices with temporal dependence.\nMatrix-variate time series modeling is a new branch of econometrics. Although\nstylized facts in several fields, the existing models do not account for regime\nswitches in the dynamics of matrices that are not abrupt. In this paper, we\nextend linear matrix-variate autoregressive models by introducing a\nregime-switching model capable of accounting for smooth changes, the matrix\nsmooth transition autoregressive model. We present the estimation processes\nwith the asymptotic properties demonstrated with simulated and real data.",
        "authors": [
            "Andrea Bucci"
        ],
        "categories": "stat.ME",
        "published": "2022-12-16T17:46:46Z",
        "updated": "2022-12-16T17:46:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.08509v1",
        "title": "Moate Simulation of Stochastic Processes",
        "abstract": "A novel approach called Moate Simulation is presented to provide an accurate\nnumerical evolution of probability distribution functions represented on grids\narising from stochastic differential processes where initial conditions are\nspecified. Where the variables of stochastic differential equations may be\ntransformed via It\\^o-Doeblin calculus into stochastic differentials with a\nconstant diffusion term, the probability distribution function for these\nvariables can be simulated in discrete time steps. The drift is applied\ndirectly to a volume element of the distribution while the stochastic diffusion\nterm is applied through the use of convolution techniques such as Fast or\nDiscrete Fourier Transforms. This allows for highly accurate distributions to\nbe efficiently simulated to a given time horizon and may be employed in one,\ntwo or higher dimensional expectation integrals, e.g. for pricing of financial\nderivatives. The Moate Simulation approach forms a more accurate and\nconsiderably faster alternative to Monte Carlo Simulation for many applications\nwhile retaining the opportunity to alter the distribution in mid-simulation.",
        "authors": [
            "Michael E. Mura"
        ],
        "categories": "q-fin.CP",
        "published": "2022-12-16T14:42:45Z",
        "updated": "2022-12-16T14:42:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.07379v1",
        "title": "The finite sample performance of instrumental variable-based estimators of the Local Average Treatment Effect when controlling for covariates",
        "abstract": "This paper investigates the finite sample performance of a range of\nparametric, semi-parametric, and non-parametric instrumental variable\nestimators when controlling for a fixed set of covariates to evaluate the local\naverage treatment effect. Our simulation designs are based on empirical labor\nmarket data from the US and vary in several dimensions, including effect\nheterogeneity, instrument selectivity, instrument strength, outcome\ndistribution, and sample size. Among the estimators and simulations considered,\nnon-parametric estimation based on the random forest (a machine learner\ncontrolling for covariates in a data-driven way) performs competitive in terms\nof the average coverage rates of the (bootstrap-based) 95% confidence\nintervals, while also being relatively precise. Non-parametric kernel\nregression as well as certain versions of semi-parametric radius matching on\nthe propensity score, pair matching on the covariates, and inverse probability\nweighting also have a decent coverage, but are less precise than the random\nforest-based method. In terms of the average root mean squared error of LATE\nestimation, kernel regression performs best, closely followed by the random\nforest method, which has the lowest average absolute bias.",
        "authors": [
            "Hugo Bodory",
            "Martin Huber",
            "Michael Lechner"
        ],
        "categories": "econ.EM",
        "published": "2022-12-14T18:02:46Z",
        "updated": "2022-12-14T18:02:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.07288v1",
        "title": "Smoothing volatility targeting",
        "abstract": "We propose an alternative approach towards cost mitigation in\nvolatility-managed portfolios based on smoothing the predictive density of an\notherwise standard stochastic volatility model. Specifically, we develop a\nnovel variational Bayes estimation method that flexibly encompasses different\nsmoothness assumptions irrespective of the persistence of the underlying latent\nstate. Using a large set of equity trading strategies, we show that smoothing\nvolatility targeting helps to regularise the extreme leverage/turnover that\nresults from commonly used realised variance estimates. This has important\nimplications for both the risk-adjusted returns and the mean-variance\nefficiency of volatility-managed portfolios, once transaction costs are\nfactored in. An extensive simulation study shows that our variational inference\nscheme compares favourably against existing state-of-the-art Bayesian\nestimation methods for stochastic volatility models.",
        "authors": [
            "Mauro Bernardi",
            "Daniele Bianchi",
            "Nicolas Bianco"
        ],
        "categories": "econ.EM",
        "published": "2022-12-14T15:43:22Z",
        "updated": "2022-12-14T15:43:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.07263v2",
        "title": "Robust Estimation of the non-Gaussian Dimension in Structural Linear Models",
        "abstract": "Statistical identification of possibly non-fundamental SVARMA models requires\nstructural errors: (i) to be an i.i.d process, (ii) to be mutually independent\nacross components, and (iii) each of them must be non-Gaussian distributed.\nHence, provided the first two requisites, it is crucial to evaluate the\nnon-Gaussian identification condition. We address this problem by relating the\nnon-Gaussian dimension of structural errors vector to the rank of a matrix\nbuilt from the higher-order spectrum of reduced-form errors. This makes our\nproposal robust to the roots location of the lag polynomials, and generalizes\nthe current procedures designed for the restricted case of a causal structural\nVAR model. Simulation exercises show that our procedure satisfactorily\nestimates the number of non-Gaussian components.",
        "authors": [
            "Miguel Cabello"
        ],
        "categories": "econ.EM",
        "published": "2022-12-14T14:57:22Z",
        "updated": "2023-09-25T13:30:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.07052v2",
        "title": "On LASSO for High Dimensional Predictive Regression",
        "abstract": "This paper examines LASSO, a widely-used $L_{1}$-penalized regression method,\nin high dimensional linear predictive regressions, particularly when the number\nof potential predictors exceeds the sample size and numerous unit root\nregressors are present. The consistency of LASSO is contingent upon two key\ncomponents: the deviation bound of the cross product of the regressors and the\nerror term, and the restricted eigenvalue of the Gram matrix. We present new\nprobabilistic bounds for these components, suggesting that LASSO's rates of\nconvergence are different from those typically observed in cross-sectional\ncases. When applied to a mixture of stationary, nonstationary, and cointegrated\npredictors, LASSO maintains its asymptotic guarantee if predictors are\nscale-standardized. Leveraging machine learning and macroeconomic domain\nexpertise, LASSO demonstrates strong performance in forecasting the\nunemployment rate, as evidenced by its application to the FRED-MD database.",
        "authors": [
            "Ziwei Mei",
            "Zhentao Shi"
        ],
        "categories": "econ.EM",
        "published": "2022-12-14T06:14:58Z",
        "updated": "2024-01-16T15:19:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.06312v2",
        "title": "Policy learning for many outcomes of interest: Combining optimal policy trees with multi-objective Bayesian optimisation",
        "abstract": "Methods for learning optimal policies use causal machine learning models to\ncreate human-interpretable rules for making choices around the allocation of\ndifferent policy interventions. However, in realistic policy-making contexts,\ndecision-makers often care about trade-offs between outcomes, not just\nsingle-mindedly maximising utility for one outcome. This paper proposes an\napproach termed Multi-Objective Policy Learning (MOPoL) which combines optimal\ndecision trees for policy learning with a multi-objective Bayesian optimisation\napproach to explore the trade-off between multiple outcomes. It does this by\nbuilding a Pareto frontier of non-dominated models for different hyperparameter\nsettings which govern outcome weighting. The key here is that a low-cost greedy\ntree can be an accurate proxy for the very computationally costly optimal tree\nfor the purposes of making decisions which means models can be repeatedly fit\nto learn a Pareto frontier. The method is applied to a real-world case-study of\nnon-price rationing of anti-malarial medication in Kenya.",
        "authors": [
            "Patrick Rehill",
            "Nicholas Biddle"
        ],
        "categories": "cs.LG",
        "published": "2022-12-13T01:39:14Z",
        "updated": "2023-10-17T05:37:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.06080v7",
        "title": "Logs with zeros? Some problems and solutions",
        "abstract": "When studying an outcome $Y$ that is weakly-positive but can equal zero (e.g.\nearnings), researchers frequently estimate an average treatment effect (ATE)\nfor a \"log-like\" transformation that behaves like $\\log(Y)$ for large $Y$ but\nis defined at zero (e.g. $\\log(1+Y)$, $\\mathrm{arcsinh}(Y)$). We argue that\nATEs for log-like transformations should not be interpreted as approximating\npercentage effects, since unlike a percentage, they depend on the units of the\noutcome. In fact, we show that if the treatment affects the extensive margin,\none can obtain a treatment effect of any magnitude simply by re-scaling the\nunits of $Y$ before taking the log-like transformation. This arbitrary\nunit-dependence arises because an individual-level percentage effect is not\nwell-defined for individuals whose outcome changes from zero to non-zero when\nreceiving treatment, and the units of the outcome implicitly determine how much\nweight the ATE for a log-like transformation places on the extensive margin. We\nfurther establish a trilemma: when the outcome can equal zero, there is no\ntreatment effect parameter that is an average of individual-level treatment\neffects, unit-invariant, and point-identified. We discuss several alternative\napproaches that may be sensible in settings with an intensive and extensive\nmargin, including (i) expressing the ATE in levels as a percentage (e.g. using\nPoisson regression), (ii) explicitly calibrating the value placed on the\nintensive and extensive margins, and (iii) estimating separate effects for the\ntwo margins (e.g. using Lee bounds). We illustrate these approaches in three\nempirical applications.",
        "authors": [
            "Jiafeng Chen",
            "Jonathan Roth"
        ],
        "categories": "econ.EM",
        "published": "2022-12-12T17:56:15Z",
        "updated": "2023-11-15T20:46:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.05866v3",
        "title": "Measuring the Driving Forces of Predictive Performance: Application to Credit Scoring",
        "abstract": "In credit scoring, machine learning models are known to outperform standard\nparametric models. As they condition access to credit, banking supervisors and\ninternal model validation teams need to monitor their predictive performance\nand to identify the features with the highest impact on performance. To\nfacilitate this, we introduce the XPER methodology to decompose a performance\nmetric (e.g., AUC, $R^2$) into specific contributions associated with the\nvarious features of a classification or regression model. XPER is theoretically\ngrounded on Shapley values and is both model-agnostic and performance\nmetric-agnostic. Furthermore, it can be implemented either at the model level\nor at the individual level. Using a novel dataset of car loans, we decompose\nthe AUC of a machine-learning model trained to forecast the default probability\nof loan applicants. We show that a small number of features can explain a\nsurprisingly large part of the model performance. Furthermore, we find that the\nfeatures that contribute the most to the predictive performance of the model\nmay not be the ones that contribute the most to individual forecasts (SHAP). We\nalso show how XPER can be used to deal with heterogeneity issues and\nsignificantly boost out-of-sample performance.",
        "authors": [
            "Hu\u00e9 Sullivan",
            "Hurlin Christophe",
            "P\u00e9rignon Christophe",
            "Saurin S\u00e9bastien"
        ],
        "categories": "stat.ML",
        "published": "2022-12-12T13:09:46Z",
        "updated": "2023-06-27T12:57:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.05841v1",
        "title": "Dominant Drivers of National Inflation",
        "abstract": "For western economies a long-forgotten phenomenon is on the horizon: rising\ninflation rates. We propose a novel approach christened D2ML to identify\ndrivers of national inflation. D2ML combines machine learning for model\nselection with time dependent data and graphical models to estimate the inverse\nof the covariance matrix, which is then used to identify dominant drivers.\nUsing a dataset of 33 countries, we find that the US inflation rate and oil\nprices are dominant drivers of national inflation rates. For a more general\nframework, we carry out Monte Carlo simulations to show that our estimator\ncorrectly identifies dominant drivers.",
        "authors": [
            "Jan Ditzen",
            "Francesco Ravazzolo"
        ],
        "categories": "econ.EM",
        "published": "2022-12-12T11:58:21Z",
        "updated": "2022-12-12T11:58:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.05554v1",
        "title": "Robust Inference in High Dimensional Linear Model with Cluster Dependence",
        "abstract": "Cluster standard error (Liang and Zeger, 1986) is widely used by empirical\nresearchers to account for cluster dependence in linear model. It is well known\nthat this standard error is biased. We show that the bias does not vanish under\nhigh dimensional asymptotics by revisiting Chesher and Jewitt (1987)'s\napproach. An alternative leave-cluster-out crossfit (LCOC) estimator that is\nunbiased, consistent and robust to cluster dependence is provided under high\ndimensional setting introduced by Cattaneo, Jansson and Newey (2018). Since\nLCOC estimator nests the leave-one-out crossfit estimator of Kline, Saggio and\nSolvsten (2019), the two papers are unified. Monte Carlo comparisons are\nprovided to give insights on its finite sample properties. The LCOC estimator\nis then applied to Angrist and Lavy's (2009) study of the effects of high\nschool achievement award and Donohue III and Levitt's (2001) study of the\nimpact of abortion on crime.",
        "authors": [
            "Ng Cheuk Fai"
        ],
        "categories": "econ.EM",
        "published": "2022-12-11T17:36:05Z",
        "updated": "2022-12-11T17:36:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.05424v2",
        "title": "On regression-adjusted imputation estimators of the average treatment effect",
        "abstract": "Imputing missing potential outcomes using an estimated regression function is\na natural idea for estimating causal effects. In the literature, estimators\nthat combine imputation and regression adjustments are believed to be\ncomparable to augmented inverse probability weighting. Accordingly, people for\na long time conjectured that such estimators, while avoiding directly\nconstructing the weights, are also doubly robust (Imbens, 2004; Stuart, 2010).\nGeneralizing an earlier result of the authors (Lin et al., 2021), this paper\nformalizes this conjecture, showing that a large class of regression-adjusted\nimputation methods are indeed doubly robust for estimating the average\ntreatment effect. In addition, they are provably semiparametrically efficient\nas long as both the density and regression models are correctly specified.\nNotable examples of imputation methods covered by our theory include kernel\nmatching, (weighted) nearest neighbor matching, local linear matching, and\n(honest) random forests.",
        "authors": [
            "Zhexiao Lin",
            "Fang Han"
        ],
        "categories": "math.ST",
        "published": "2022-12-11T06:36:47Z",
        "updated": "2023-01-19T18:57:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.04814v2",
        "title": "The Falsification Adaptive Set in Linear Models with Instrumental Variables that Violate the Exclusion or Conditional Exogeneity Restriction",
        "abstract": "Masten and Poirier (2021) introduced the falsification adaptive set (FAS) in\nlinear models with a single endogenous variable estimated with multiple\ncorrelated instrumental variables (IVs). The FAS reflects the model uncertainty\nthat arises from falsification of the baseline model. We show that it applies\nto cases where a conditional exogeneity assumption holds and invalid\ninstruments violate the exclusion assumption only. We propose a generalized FAS\nthat reflects the model uncertainty when some instruments violate the exclusion\nassumption and/or some instruments violate the conditional exogeneity\nassumption. Under the assumption that invalid instruments are not themselves\nendogenous explanatory variables, if there is at least one relevant instrument\nthat satisfies both the exclusion and conditional exogeneity assumptions then\nthis generalized FAS is guaranteed to contain the parameter of interest.",
        "authors": [
            "Nicolas Apfel",
            "Frank Windmeijer"
        ],
        "categories": "econ.EM",
        "published": "2022-12-09T12:42:17Z",
        "updated": "2024-04-22T14:26:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.04620v3",
        "title": "On the Non-Identification of Revenue Production Functions",
        "abstract": "Production functions are potentially misspecified when revenue is used as a\nproxy for output. I formalize and strengthen this common knowledge by showing\nthat neither the production function nor Hicks-neutral productivity can be\nidentified with such a revenue proxy. This result obtains when relaxing the\nstandard assumptions used in the literature to allow for imperfect competition.\nIt holds for a large class of production functions, including all commonly used\nparametric forms. Among the prevalent approaches to address this issue, only\nthose that impose assumptions on the underlying demand system can possibly\nidentify the production function.",
        "authors": [
            "David Van Dijcke"
        ],
        "categories": "econ.EM",
        "published": "2022-12-09T01:23:11Z",
        "updated": "2024-05-08T20:57:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.04043v1",
        "title": "Optimal Model Selection in RDD and Related Settings Using Placebo Zones",
        "abstract": "We propose a new model-selection algorithm for Regression Discontinuity\nDesign, Regression Kink Design, and related IV estimators. Candidate models are\nassessed within a 'placebo zone' of the running variable, where the true\neffects are known to be zero. The approach yields an optimal combination of\nbandwidth, polynomial, and any other choice parameters. It can also inform\nchoices between classes of models (e.g. RDD versus cohort-IV) and any other\nchoices, such as covariates, kernel, or other weights. We outline sufficient\nconditions under which the approach is asymptotically optimal. The approach\nalso performs favorably under more general conditions in a series of Monte\nCarlo simulations. We demonstrate the approach in an evaluation of changes to\nMinimum Supervised Driving Hours in the Australian state of New South Wales. We\nalso re-evaluate evidence on the effects of Head Start and Minimum Legal\nDrinking Age. Our Stata commands implement the procedure and compare its\nperformance to other approaches.",
        "authors": [
            "Nathan Kettlewell",
            "Peter Siminski"
        ],
        "categories": "econ.EM",
        "published": "2022-12-08T02:44:59Z",
        "updated": "2022-12-08T02:44:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.03704v1",
        "title": "Semiparametric Distribution Regression with Instruments and Monotonicity",
        "abstract": "This paper proposes IV-based estimators for the semiparametric distribution\nregression model in the presence of an endogenous regressor, which are based on\nan extension of IV probit estimators. We discuss the causal interpretation of\nthe estimators and two methods (monotone rearrangement and isotonic regression)\nto ensure a monotonically increasing distribution function. Asymptotic\nproperties and simulation evidence are provided. An application to wage\nequations reveals statistically significant and heterogeneous differences to\nthe inconsistent OLS-based estimator.",
        "authors": [
            "Dominik Wied"
        ],
        "categories": "econ.EM",
        "published": "2022-12-07T15:23:47Z",
        "updated": "2022-12-07T15:23:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.03683v1",
        "title": "Neighborhood Adaptive Estimators for Causal Inference under Network Interference",
        "abstract": "Estimating causal effects has become an integral part of most applied fields.\nSolving these modern causal questions requires tackling violations of many\nclassical causal assumptions. In this work we consider the violation of the\nclassical no-interference assumption, meaning that the treatment of one\nindividuals might affect the outcomes of another. To make interference\ntractable, we consider a known network that describes how interference may\ntravel. However, unlike previous work in this area, the radius (and intensity)\nof the interference experienced by a unit is unknown and can depend on\ndifferent sub-networks of those treated and untreated that are connected to\nthis unit.\n  We study estimators for the average direct treatment effect on the treated in\nsuch a setting. The proposed estimator builds upon a Lepski-like procedure that\nsearches over the possible relevant radii and treatment assignment patterns. In\ncontrast to previous work, the proposed procedure aims to approximate the\nrelevant network interference patterns. We establish oracle inequalities and\ncorresponding adaptive rates for the estimation of the interference function.\nWe leverage such estimates to propose and analyze two estimators for the\naverage direct treatment effect on the treated. We address several challenges\nsteaming from the data-driven creation of the patterns (i.e. feature\nengineering) and the network dependence. In addition to rates of convergence,\nunder mild regularity conditions, we show that one of the proposed estimators\nis asymptotically normal and unbiased.",
        "authors": [
            "Alexandre Belloni",
            "Fei Fang",
            "Alexander Volfovsky"
        ],
        "categories": "stat.ML",
        "published": "2022-12-07T14:53:47Z",
        "updated": "2022-12-07T14:53:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.03471v2",
        "title": "Bayesian Forecasting in Economics and Finance: A Modern Review",
        "abstract": "The Bayesian statistical paradigm provides a principled and coherent approach\nto probabilistic forecasting. Uncertainty about all unknowns that characterize\nany forecasting problem -- model, parameters, latent states -- is able to be\nquantified explicitly, and factored into the forecast distribution via the\nprocess of integration or averaging. Allied with the elegance of the method,\nBayesian forecasting is now underpinned by the burgeoning field of Bayesian\ncomputation, which enables Bayesian forecasts to be produced for virtually any\nproblem, no matter how large, or complex. The current state of play in Bayesian\nforecasting in economics and finance is the subject of this review. The aim is\nto provide the reader with an overview of modern approaches to the field, set\nin some historical context; and with sufficient computational detail given to\nassist the reader with implementation.",
        "authors": [
            "Gael M. Martin",
            "David T. Frazier",
            "Worapree Maneesoonthorn",
            "Ruben Loaiza-Maya",
            "Florian Huber",
            "Gary Koop",
            "John Maheu",
            "Didier Nibbering",
            "Anastasios Panagiotelis"
        ],
        "categories": "econ.EM",
        "published": "2022-12-07T05:41:04Z",
        "updated": "2023-07-29T03:47:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.03351v2",
        "title": "The long-term effect of childhood exposure to technology using surrogates",
        "abstract": "We study how childhood exposure to technology at ages 5-15 via the occupation\nof the parents affects the ability to climb the social ladder in terms of\nincome at ages 45-49 using the Danish micro data from years 1961-2019. Our\nmeasure of technology exposure covers the degree to which using computers\n(hardware and software) is required to perform an occupation, and it is created\nby merging occupational codes with detailed data from O*NET. The challenge in\nestimating this effect is that long-term outcome is observed over a different\ntime horizon than our treatment of interest. We therefore adapt the surrogate\nindex methodology, linking the effect of our childhood treatment on\nintermediate surrogates, such as income and education at ages 25-29, to the\neffect on adulthood income. We estimate that a one standard error increase in\nexposure to technology increases the income rank by 2\\%-points, which is\neconomically and statistically significant and robust to cluster-correlation\nwithin families. The derived policy recommendation is to update the educational\ncurriculum to expose children to computers to a higher degree, which may then\nact as a social leveler.",
        "authors": [
            "Sylvia Klosin",
            "Nicolaj S\u00f8ndergaard M\u00fchlbach"
        ],
        "categories": "econ.EM",
        "published": "2022-12-06T22:19:41Z",
        "updated": "2022-12-12T20:07:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.02585v1",
        "title": "Identification of Unobservables in Observations",
        "abstract": "In empirical studies, the data usually don't include all the variables of\ninterest in an economic model. This paper shows the identification of\nunobserved variables in observations at the population level. When the\nobservables are distinct in each observation, there exists a function mapping\nfrom the observables to the unobservables. Such a function guarantees the\nuniqueness of the latent value in each observation. The key lies in the\nidentification of the joint distribution of observables and unobservables from\nthe distribution of observables. The joint distribution of observables and\nunobservables then reveal the latent value in each observation. Three examples\nof this result are discussed.",
        "authors": [
            "Yingyao Hu"
        ],
        "categories": "econ.EM",
        "published": "2022-12-05T20:24:19Z",
        "updated": "2022-12-05T20:24:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.02407v3",
        "title": "Educational Inequality of Opportunity and Mobility in Europe",
        "abstract": "Educational attainment generates labor market returns, societal gains and has\nintrinsic value for individuals. We study Inequality of Opportunity (IOp) and\nintergenerational mobility in the distribution of educational attainment. We\npropose to use debiased IOp estimators based on the Gini coefficient and the\nMean Logarithmic Deviation (MLD) which are robust to machine learning biases.\nWe also measure the effect of each circumstance on IOp, we provide tests to\ncompare IOp in two populations and to test joint significance of a group of\ncircumstances. We find that circumstances explain between 38\\% and 74\\% of\ntotal educational inequality in European countries. Mother's education is the\nmost important circumstance in most countries. There is high intergenerational\npersistence and there is evidence of an educational Great Gatsby curve. We also\nconstruct IOp aware educational Great Gatsby curves and find that high income\nIOp countries are also high educational IOp and less mobile countries.",
        "authors": [
            "Jo\u00ebl Terschuur"
        ],
        "categories": "econ.EM",
        "published": "2022-12-05T16:47:08Z",
        "updated": "2023-03-08T16:49:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.02178v1",
        "title": "A Data Fusion Approach for Ride-sourcing Demand Estimation: A Discrete Choice Model with Sampling and Endogeneity Corrections",
        "abstract": "Ride-sourcing services offered by companies like Uber and Didi have grown\nrapidly in the last decade. Understanding the demand for these services is\nessential for planning and managing modern transportation systems. Existing\nstudies develop statistical models for ride-sourcing demand estimation at an\naggregate level due to limited data availability. These models lack foundations\nin microeconomic theory, ignore competition of ride-sourcing with other travel\nmodes, and cannot be seamlessly integrated into existing individual-level\n(disaggregate) activity-based models to evaluate system-level impacts of\nride-sourcing services. In this paper, we present and apply an approach for\nestimating ride-sourcing demand at a disaggregate level using discrete choice\nmodels and multiple data sources. We first construct a sample of trip-based\nmode choices in Chicago, USA by enriching household travel survey with publicly\navailable ride-sourcing and taxi trip records. We then formulate a multivariate\nextreme value-based discrete choice with sampling and endogeneity corrections\nto account for the construction of the estimation sample from multiple data\nsources and endogeneity biases arising from supply-side constraints and surge\npricing mechanisms in ride-sourcing systems. Our analysis of the constructed\ndataset reveals insights into the influence of various socio-economic, land use\nand built environment features on ride-sourcing demand. We also derive\nelasticities of ride-sourcing demand relative to travel cost and time. Finally,\nwe illustrate how the developed model can be employed to quantify the welfare\nimplications of ride-sourcing policies and regulations such as terminating\ncertain types of services and introducing ride-sourcing taxes.",
        "authors": [
            "Rico Krueger",
            "Michel Bierlaire",
            "Prateek Bansal"
        ],
        "categories": "econ.EM",
        "published": "2022-12-05T11:21:48Z",
        "updated": "2022-12-05T11:21:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.01925v1",
        "title": "Counterfactual Learning with General Data-generating Policies",
        "abstract": "Off-policy evaluation (OPE) attempts to predict the performance of\ncounterfactual policies using log data from a different policy. We extend its\napplicability by developing an OPE method for a class of both full support and\ndeficient support logging policies in contextual-bandit settings. This class\nincludes deterministic bandit (such as Upper Confidence Bound) as well as\ndeterministic decision-making based on supervised and unsupervised learning. We\nprove that our method's prediction converges in probability to the true\nperformance of a counterfactual policy as the sample size increases. We\nvalidate our method with experiments on partly and entirely deterministic\nlogging policies. Finally, we apply it to evaluate coupon targeting policies by\na major online platform and show how to improve the existing policy.",
        "authors": [
            "Yusuke Narita",
            "Kyohei Okumura",
            "Akihiro Shimizu",
            "Kohei Yata"
        ],
        "categories": "cs.LG",
        "published": "2022-12-04T21:07:46Z",
        "updated": "2022-12-04T21:07:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2212.00101v1",
        "title": "mCube: Multinomial Micro-level reserving Model",
        "abstract": "This paper presents a multinomial multi-state micro-level reserving model,\ndenoted mCube. We propose a unified framework for modelling the time and the\npayment process for IBNR and RBNS claims and for modeling IBNR claim counts. We\nuse multinomial distributions for the time process and spliced mixture models\nfor the payment process. We illustrate the excellent performance of the\nproposed model on a real data set of a major insurance company consisting of\nbodily injury claims. It is shown that the proposed model produces a best\nestimate distribution that is centered around the true reserve.",
        "authors": [
            "Emmanuel Jordy Menvouta",
            "Jolien Ponnet",
            "Robin Van Oirbeek",
            "Tim Verdonck"
        ],
        "categories": "stat.AP",
        "published": "2022-11-30T20:17:48Z",
        "updated": "2022-11-30T20:17:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.16714v3",
        "title": "Incorporating Prior Knowledge of Latent Group Structure in Panel Data Models",
        "abstract": "The assumption of group heterogeneity has become popular in panel data\nmodels. We develop a constrained Bayesian grouped estimator that exploits\nresearchers' prior beliefs on groups in a form of pairwise constraints,\nindicating whether a pair of units is likely to belong to a same group or\ndifferent groups. We propose a prior to incorporate the pairwise constraints\nwith varying degrees of confidence. The whole framework is built on the\nnonparametric Bayesian method, which implicitly specifies a distribution over\nthe group partitions, and so the posterior analysis takes the uncertainty of\nthe latent group structure into account. Monte Carlo experiments reveal that\nadding prior knowledge yields more accurate estimates of coefficient and scores\npredictive gains over alternative estimators. We apply our method to two\nempirical applications. In a first application to forecasting U.S. CPI\ninflation, we illustrate that prior knowledge of groups improves density\nforecasts when the data is not entirely informative. A second application\nrevisits the relationship between a country's income and its democratic\ntransition; we identify heterogeneous income effects on democracy with five\ndistinct groups over ninety countries.",
        "authors": [
            "Boyuan Zhang"
        ],
        "categories": "econ.EM",
        "published": "2022-11-30T03:32:58Z",
        "updated": "2023-10-28T18:46:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.16362v3",
        "title": "Score-based calibration testing for multivariate forecast distributions",
        "abstract": "Calibration tests based on the probability integral transform (PIT) are\nroutinely used to assess the quality of univariate distributional forecasts.\nHowever, PIT-based calibration tests for multivariate distributional forecasts\nface various challenges. We propose two new types of tests based on proper\nscoring rules, which overcome these challenges. They arise from a general\nframework for calibration testing in the multivariate case, introduced in this\nwork. The new tests have good size and power properties in simulations and\nsolve various problems of existing tests. We apply the tests to forecast\ndistributions for macroeconomic and financial time series data.",
        "authors": [
            "Malte Kn\u00fcppel",
            "Fabian Kr\u00fcger",
            "Marc-Oliver Pohle"
        ],
        "categories": "econ.EM",
        "published": "2022-11-29T16:44:36Z",
        "updated": "2023-12-12T11:45:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.16298v5",
        "title": "Double Robust Bayesian Inference on Average Treatment Effects",
        "abstract": "We propose a double robust Bayesian inference procedure on the average\ntreatment effect (ATE) under unconfoundedness. For our new Bayesian approach,\nwe first adjust the prior distributions of the conditional mean functions, and\nthen correct the posterior distribution of the resulting ATE. Both adjustments\nmake use of pilot estimators motivated by the semiparametric influence function\nfor ATE estimation. We prove asymptotic equivalence of our Bayesian procedure\nand efficient frequentist ATE estimators by establishing a new semiparametric\nBernstein-von Mises theorem under double robustness; i.e., the lack of\nsmoothness of conditional mean functions can be compensated by high regularity\nof the propensity score and vice versa. Consequently, the resulting Bayesian\ncredible sets form confidence intervals with asymptotically exact coverage\nprobability. In simulations, our method provides precise point estimates of the\nATE through the posterior mean and credible intervals that closely align with\nthe nominal coverage probability. Furthermore, our approach achieves a shorter\ninterval length in comparison to existing methods. We illustrate our method in\nan application to the National Supported Work Demonstration following LaLonde\n[1986] and Dehejia and Wahba [1999].",
        "authors": [
            "Christoph Breunig",
            "Ruixuan Liu",
            "Zhengfei Yu"
        ],
        "categories": "econ.EM",
        "published": "2022-11-29T15:32:25Z",
        "updated": "2024-10-09T08:23:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.16121v2",
        "title": "Bayesian Multivariate Quantile Regression with alternative Time-varying Volatility Specifications",
        "abstract": "This article proposes a novel Bayesian multivariate quantile regression to\nforecast the tail behavior of energy commodities, where the homoskedasticity\nassumption is relaxed to allow for time-varying volatility. In particular, we\nexploit the mixture representation of the multivariate asymmetric Laplace\nlikelihood and the Cholesky-type decomposition of the scale matrix to introduce\nstochastic volatility and GARCH processes and then provide an efficient MCMC to\nestimate them. The proposed models outperform the homoskedastic benchmark\nmainly when predicting the distribution's tails. We provide a model combination\nusing a quantile score-based weighting scheme, which leads to improved\nperformances, notably when no single model uniformly outperforms the other\nacross quantiles, time, or variables.",
        "authors": [
            "Matteo Iacopini",
            "Francesco Ravazzolo",
            "Luca Rossini"
        ],
        "categories": "econ.EM",
        "published": "2022-11-29T11:47:51Z",
        "updated": "2024-08-07T10:08:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.15241v1",
        "title": "Synthetic Principal Component Design: Fast Covariate Balancing with Synthetic Controls",
        "abstract": "The optimal design of experiments typically involves solving an NP-hard\ncombinatorial optimization problem. In this paper, we aim to develop a globally\nconvergent and practically efficient optimization algorithm. Specifically, we\nconsider a setting where the pre-treatment outcome data is available and the\nsynthetic control estimator is invoked. The average treatment effect is\nestimated via the difference between the weighted average outcomes of the\ntreated and control units, where the weights are learned from the observed\ndata. {Under this setting, we surprisingly observed that the optimal\nexperimental design problem could be reduced to a so-called \\textit{phase\nsynchronization} problem.} We solve this problem via a normalized variant of\nthe generalized power method with spectral initialization. On the theoretical\nside, we establish the first global optimality guarantee for experiment design\nwhen pre-treatment data is sampled from certain data-generating processes.\nEmpirically, we conduct extensive experiments to demonstrate the effectiveness\nof our method on both the US Bureau of Labor Statistics and the\nAbadie-Diemond-Hainmueller California Smoking Data. In terms of the root mean\nsquare error, our algorithm surpasses the random design by a large margin.",
        "authors": [
            "Yiping Lu",
            "Jiajin Li",
            "Lexing Ying",
            "Jose Blanchet"
        ],
        "categories": "econ.EM",
        "published": "2022-11-28T11:45:54Z",
        "updated": "2022-11-28T11:45:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.14903v4",
        "title": "Inference in Cluster Randomized Trials with Matched Pairs",
        "abstract": "This paper studies inference in cluster randomized trials where treatment\nstatus is determined according to a \"matched pairs\" design. Here, by a cluster\nrandomized experiment, we mean one in which treatment is assigned at the level\nof the cluster; by a \"matched pairs\" design, we mean that a sample of clusters\nis paired according to baseline, cluster-level covariates and, within each\npair, one cluster is selected at random for treatment. We study the\nlarge-sample behavior of a weighted difference-in-means estimator and derive\ntwo distinct sets of results depending on if the matching procedure does or\ndoes not match on cluster size. We then propose a single variance estimator\nwhich is consistent in either regime. Combining these results establishes the\nasymptotic exactness of tests based on these estimators. Next, we consider the\nproperties of two common testing procedures based on t-tests constructed from\nlinear regressions, and argue that both are generally conservative in our\nframework. We additionally study the behavior of a randomization test which\npermutes the treatment status for clusters within pairs, and establish its\nfinite-sample and asymptotic validity for testing specific null hypotheses.\nFinally, we propose a covariate-adjusted estimator which adjusts for additional\nbaseline covariates not used for treatment assignment, and establish conditions\nunder which such an estimator leads to strict improvements in precision. A\nsimulation study confirms the practical relevance of our theoretical results.",
        "authors": [
            "Yuehao Bai",
            "Jizhou Liu",
            "Azeem M. Shaikh",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2022-11-27T18:10:45Z",
        "updated": "2024-08-02T20:35:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.14870v2",
        "title": "Extreme Changes in Changes",
        "abstract": "Policy analysts are often interested in treating the units with extreme\noutcomes, such as infants with extremely low birth weights. Existing\nchanges-in-changes (CIC) estimators are tailored to middle quantiles and do not\nwork well for such subpopulations. This paper proposes a new CIC estimator to\naccurately estimate treatment effects at extreme quantiles. With its asymptotic\nnormality, we also propose a method of statistical inference, which is simple\nto implement. Based on simulation studies, we propose to use our extreme CIC\nestimator for extreme, such as below 5% and above 95%, quantiles, while the\nconventional CIC estimator should be used for intermediate quantiles. Applying\nthe proposed method, we study the effects of income gains from the 1993 EITC\nreform on infant birth weights for those in the most critical conditions. This\npaper is accompanied by a Stata command.",
        "authors": [
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "categories": "econ.EM",
        "published": "2022-11-27T15:59:30Z",
        "updated": "2023-05-20T23:26:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.14387v1",
        "title": "Machine Learning Algorithms for Time Series Analysis and Forecasting",
        "abstract": "Time series data is being used everywhere, from sales records to patients'\nhealth evolution metrics. The ability to deal with this data has become a\nnecessity, and time series analysis and forecasting are used for the same.\nEvery Machine Learning enthusiast would consider these as very important tools,\nas they deepen the understanding of the characteristics of data. Forecasting is\nused to predict the value of a variable in the future, based on its past\noccurrences. A detailed survey of the various methods that are used for\nforecasting has been presented in this paper. The complete process of\nforecasting, from preprocessing to validation has also been explained\nthoroughly. Various statistical and deep learning models have been considered,\nnotably, ARIMA, Prophet and LSTMs. Hybrid versions of Machine Learning models\nhave also been explored and elucidated. Our work can be used by anyone to\ndevelop a good understanding of the forecasting process, and to identify\nvarious state of the art models which are being used today.",
        "authors": [
            "Rameshwar Garg",
            "Shriya Barpanda",
            "Girish Rao Salanke N S",
            "Ramya S"
        ],
        "categories": "cs.LG",
        "published": "2022-11-25T22:12:03Z",
        "updated": "2022-11-25T22:12:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.14354v1",
        "title": "A Design-Based Approach to Spatial Correlation",
        "abstract": "When observing spatial data, what standard errors should we report? With the\nfinite population framework, we identify three channels of spatial correlation:\nsampling scheme, assignment design, and model specification. The\nEicker-Huber-White standard error, the cluster-robust standard error, and the\nspatial heteroskedasticity and autocorrelation consistent standard error are\ncompared under different combinations of the three channels. Then, we provide\nguidelines for whether standard errors should be adjusted for spatial\ncorrelation for both linear and nonlinear estimators. As it turns out, the\nanswer to this question also depends on the magnitude of the sampling\nprobability.",
        "authors": [
            "Ruonan Xu",
            "Jeffrey M. Wooldridge"
        ],
        "categories": "econ.EM",
        "published": "2022-11-25T19:21:14Z",
        "updated": "2022-11-25T19:21:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.14236v4",
        "title": "Strategyproof Decision-Making in Panel Data Settings and Beyond",
        "abstract": "We consider the problem of decision-making using panel data, in which a\ndecision-maker gets noisy, repeated measurements of multiple units (or agents).\nWe consider a setup where there is a pre-intervention period, when the\nprincipal observes the outcomes of each unit, after which the principal uses\nthese observations to assign a treatment to each unit. Unlike this classical\nsetting, we permit the units generating the panel data to be strategic, i.e.\nunits may modify their pre-intervention outcomes in order to receive a more\ndesirable intervention. The principal's goal is to design a strategyproof\nintervention policy, i.e. a policy that assigns units to their\nutility-maximizing interventions despite their potential strategizing. We first\nidentify a necessary and sufficient condition under which a strategyproof\nintervention policy exists, and provide a strategyproof mechanism with a simple\nclosed form when one does exist. Along the way, we prove impossibility results\nfor strategic multiclass classification, which may be of independent interest.\nWhen there are two interventions, we establish that there always exists a\nstrategyproof mechanism, and provide an algorithm for learning such a\nmechanism. For three or more interventions, we provide an algorithm for\nlearning a strategyproof mechanism if there exists a sufficiently large gap in\nthe principal's rewards between different interventions. Finally, we\nempirically evaluate our model using real-world panel data collected from\nproduct sales over 18 months. We find that our methods compare favorably to\nbaselines which do not take strategic interactions into consideration, even in\nthe presence of model misspecification.",
        "authors": [
            "Keegan Harris",
            "Anish Agarwal",
            "Chara Podimata",
            "Zhiwei Steven Wu"
        ],
        "categories": "econ.EM",
        "published": "2022-11-25T16:56:42Z",
        "updated": "2023-12-21T15:17:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.13830v1",
        "title": "Spectral estimation for mixed causal-noncausal autoregressive models",
        "abstract": "This paper investigates new ways of estimating and identifying causal,\nnoncausal, and mixed causal-noncausal autoregressive models driven by a\nnon-Gaussian error sequence. We do not assume any parametric distribution\nfunction for the innovations. Instead, we use the information of higher-order\ncumulants, combining the spectrum and the bispectrum in a minimum distance\nestimation. We show how to circumvent the nonlinearity of the parameters and\nthe multimodality in the noncausal and mixed models by selecting the\nappropriate initial values in the estimation. In addition, we propose a method\nof identification using a simple comparison criterion based on the global\nminimum of the estimation function. By means of a Monte Carlo study, we find\nunbiased estimated parameters and a correct identification as the data depart\nfrom normality. We propose an empirical application on eight monthly commodity\nprices, finding noncausal and mixed causal-noncausal dynamics.",
        "authors": [
            "Alain Hecq",
            "Daniel Velasquez-Gaviria"
        ],
        "categories": "econ.EM",
        "published": "2022-11-24T23:51:46Z",
        "updated": "2022-11-24T23:51:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.13610v4",
        "title": "Cross-Sectional Dynamics Under Network Structure: Theory and Macroeconomic Applications",
        "abstract": "Many environments in economics feature a cross-section of units linked by\nbilateral ties. I develop a framework for studying dynamics of cross-sectional\nvariables that exploits this network structure. The Network-VAR (NVAR) is a\nvector autoregression in which innovations transmit cross-sectionally via\nbilateral links and which can accommodate rich patterns of how network effects\nof higher order accumulate over time. It can be used to estimate dynamic\nnetwork effects, with the network given or inferred from dynamic\ncross-correlations in the data. It also offers a dimensionality-reduction\ntechnique for modeling high-dimensional (cross-sectional) processes, owing to\nnetworks' ability to summarize complex relations among variables (units) by\nrelatively few bilateral links. In a first application, consistent with an RBC\neconomy with lagged input-output conversion, I estimate how sectoral\nproductivity shocks transmit along supply chains and affect sectoral prices in\nthe US economy. In a second application, I forecast monthly industrial\nproduction growth across 44 countries by assuming and estimating a network\nunderlying the dynamics. This reduces out-of-sample mean squared errors by up\nto 23% relative to a factor model, consistent with an equivalence result I\nderive.",
        "authors": [
            "Marko Mlikota"
        ],
        "categories": "econ.EM",
        "published": "2022-11-24T13:53:15Z",
        "updated": "2024-09-09T08:58:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.13002v1",
        "title": "Simulation-based Forecasting for Intraday Power Markets: Modelling Fundamental Drivers for Location, Shape and Scale of the Price Distribution",
        "abstract": "During the last years, European intraday power markets have gained importance\nfor balancing forecast errors due to the rising volumes of intermittent\nrenewable generation. However, compared to day-ahead markets, the drivers for\nthe intraday price process are still sparsely researched. In this paper, we\npropose a modelling strategy for the location, shape and scale parameters of\nthe return distribution in intraday markets, based on fundamental variables. We\nconsider wind and solar forecasts and their intraday updates, outages, price\ninformation and a novel measure for the shape of the merit-order, derived from\nspot auction curves as explanatory variables. We validate our modelling by\nsimulating price paths and compare the probabilistic forecasting performance of\nour model to benchmark models in a forecasting study for the German market. The\napproach yields significant improvements in the forecasting performance,\nespecially in the tails of the distribution. At the same time, we are able to\nderive the contribution of the driving variables. We find that, apart from the\nfirst lag of the price changes, none of our fundamental variables have\nexplanatory power for the expected value of the intraday returns. This implies\nweak-form market efficiency as renewable forecast changes and outage\ninformation seems to be priced in by the market. We find that the volatility is\ndriven by the merit-order regime, the time to delivery and the closure of\ncross-border order books. The tail of the distribution is mainly influenced by\npast price differences and trading activity. Our approach is directly\ntransferable to other continuous intraday markets in Europe.",
        "authors": [
            "Simon Hirsch",
            "Florian Ziel"
        ],
        "categories": "q-fin.ST",
        "published": "2022-11-23T15:08:50Z",
        "updated": "2022-11-23T15:08:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.12437v1",
        "title": "Macroeconomic Effects of Active Labour Market Policies: A Novel Instrumental Variables Approach",
        "abstract": "This study evaluates the macroeconomic effects of active labour market\npolicies (ALMP) in Germany over the period 2005 to 2018. We propose a novel\nidentification strategy to overcome the simultaneity of ALMP and labour market\noutcomes at the regional level. It exploits the imperfect overlap of local\nlabour markets and local employment agencies that decide on the local\nimplementation of policies. Specifically, we instrument for the use of ALMP in\na local labour market with the mix of ALMP implemented outside this market but\nin local employment agencies that partially overlap with this market. We find\nno effects of short-term activation measures and further vocational training on\naggregate labour market outcomes. In contrast, wage subsidies substantially\nincrease the share of workers in unsubsidised employment while lowering\nlong-term unemployment and welfare dependency. Our results suggest that\nnegative externalities of ALMP partially offset the effects for program\nparticipants and that some segments of the labour market benefit more than\nothers.",
        "authors": [
            "Ulrike Unterhofer",
            "Conny Wunsch"
        ],
        "categories": "econ.EM",
        "published": "2022-11-22T17:45:34Z",
        "updated": "2022-11-22T17:45:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.12366v2",
        "title": "Peer Effects in Labor Market Training",
        "abstract": "This paper shows that the group composition matters for the effectiveness of\nlabor market training programs for jobseekers. Using rich administrative data\nfrom Germany, I document that greater average exposure to highly employable\npeers leads to increased employment stability after program participation. Peer\neffects on earnings are positive and long-lasting in classic vocational\ntraining and negative but of short duration in retraining, pointing to\ndifferent mechanisms. Finally, I also find evidence for non-linearities in\neffects and show that more heterogeneity in the peer group is detrimental.",
        "authors": [
            "Ulrike Unterhofer"
        ],
        "categories": "econ.EM",
        "published": "2022-11-22T16:00:32Z",
        "updated": "2023-06-08T15:16:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.12095v1",
        "title": "Asymptotic Properties of the Synthetic Control Method",
        "abstract": "This paper provides new insights into the asymptotic properties of the\nsynthetic control method (SCM). We show that the synthetic control (SC) weight\nconverges to a limiting weight that minimizes the mean squared prediction risk\nof the treatment-effect estimator when the number of pretreatment periods goes\nto infinity, and we also quantify the rate of convergence. Observing the link\nbetween the SCM and model averaging, we further establish the asymptotic\noptimality of the SC estimator under imperfect pretreatment fit, in the sense\nthat it achieves the lowest possible squared prediction error among all\npossible treatment effect estimators that are based on an average of control\nunits, such as matching, inverse probability weighting and\ndifference-in-differences. The asymptotic optimality holds regardless of\nwhether the number of control units is fixed or divergent. Thus, our results\nprovide justifications for the SCM in a wide range of applications. The\ntheoretical results are verified via simulations.",
        "authors": [
            "Xiaomeng Zhang",
            "Wendun Wang",
            "Xinyu Zhang"
        ],
        "categories": "econ.EM",
        "published": "2022-11-22T08:55:07Z",
        "updated": "2022-11-22T08:55:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.12004v1",
        "title": "Contextual Bandits in a Survey Experiment on Charitable Giving: Within-Experiment Outcomes versus Policy Learning",
        "abstract": "We design and implement an adaptive experiment (a ``contextual bandit'') to\nlearn a targeted treatment assignment policy, where the goal is to use a\nparticipant's survey responses to determine which charity to expose them to in\na donation solicitation. The design balances two competing objectives:\noptimizing the outcomes for the subjects in the experiment (``cumulative regret\nminimization'') and gathering data that will be most useful for policy\nlearning, that is, for learning an assignment rule that will maximize welfare\nif used after the experiment (``simple regret minimization''). We evaluate\nalternative experimental designs by collecting pilot data and then conducting a\nsimulation study. Next, we implement our selected algorithm. Finally, we\nperform a second simulation study anchored to the collected data that evaluates\nthe benefits of the algorithm we chose. Our first result is that the value of a\nlearned policy in this setting is higher when data is collected via a uniform\nrandomization rather than collected adaptively using standard cumulative regret\nminimization or policy learning algorithms. We propose a simple heuristic for\nadaptive experimentation that improves upon uniform randomization from the\nperspective of policy learning at the expense of increasing cumulative regret\nrelative to alternative bandit algorithms. The heuristic modifies an existing\ncontextual bandit algorithm by (i) imposing a lower bound on assignment\nprobabilities that decay slowly so that no arm is discarded too quickly, and\n(ii) after adaptively collecting data, restricting policy learning to select\nfrom arms where sufficient data has been gathered.",
        "authors": [
            "Susan Athey",
            "Undral Byambadalai",
            "Vitor Hadad",
            "Sanath Kumar Krishnamurthy",
            "Weiwen Leung",
            "Joseph Jay Williams"
        ],
        "categories": "econ.EM",
        "published": "2022-11-22T04:44:17Z",
        "updated": "2022-11-22T04:44:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.11915v1",
        "title": "A Misuse of Specification Tests",
        "abstract": "Empirical researchers often perform model specification tests, such as the\nHausman test and the overidentifying restrictions test, to confirm the validity\nof estimators rather than the validity of models. This paper examines the\neffectiveness of specification pretests in finding invalid estimators. We study\nthe local asymptotic properties of test statistics and estimators and show that\nlocally unbiased specification tests cannot determine whether asymptotically\nefficient estimators are asymptotically biased. The main message of the paper\nis that correct specification and valid estimation are different issues.\nCorrect specification is neither necessary nor sufficient for asymptotically\nunbiased estimation under local overidentification.",
        "authors": [
            "Naoya Sueishi"
        ],
        "categories": "econ.EM",
        "published": "2022-11-21T23:57:20Z",
        "updated": "2022-11-21T23:57:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.11876v1",
        "title": "Structural Modelling of Dynamic Networks and Identifying Maximum Likelihood",
        "abstract": "This paper considers nonlinear dynamic models where the main parameter of\ninterest is a nonnegative matrix characterizing the network (contagion)\neffects. This network matrix is usually constrained either by assuming a\nlimited number of nonzero elements (sparsity), or by considering a reduced rank\napproach for nonnegative matrix factorization (NMF). We follow the latter\napproach and develop a new probabilistic NMF method. We introduce a new\nIdentifying Maximum Likelihood (IML) method for consistent estimation of the\nidentified set of admissible NMF's and derive its asymptotic distribution.\nMoreover, we propose a maximum likelihood estimator of the parameter matrix for\na given non-negative rank, derive its asymptotic distribution and the\nassociated efficiency bound.",
        "authors": [
            "Christian Gourieroux",
            "Joann Jasiak"
        ],
        "categories": "econ.EM",
        "published": "2022-11-21T22:00:23Z",
        "updated": "2022-11-21T22:00:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.10235v1",
        "title": "Fractional integration and cointegration",
        "abstract": "In this chapter we present an overview of the main ideas and methods in the\nfractional integration and cointegration literature. We do not attempt to give\na complete survey of this enormous literature, but rather a more introductory\ntreatment suitable for a researcher or graduate student wishing to learn about\nthis exciting field of research. With this aim, we have surely overlooked many\nrelevant references for which we apologize in advance. Knowledge of standard\ntime series methods, and in particular methods related to nonstationary time\nseries, at the level of a standard graduate course or advanced undergraduate\ncourse is assumed.",
        "authors": [
            "Javier Hualde",
            "Morten \u00d8rregaard Nielsen"
        ],
        "categories": "econ.EM",
        "published": "2022-11-18T13:41:58Z",
        "updated": "2022-11-18T13:41:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.09604v2",
        "title": "Cointegration with Occasionally Binding Constraints",
        "abstract": "In the literature on nonlinear cointegration, a long-standing open problem\nrelates to how a (nonlinear) vector autoregression, which provides a unified\ndescription of the short- and long-run dynamics of a collection of time series,\ncan generate 'nonlinear cointegration' in the profound sense of those series\nsharing common nonlinear stochastic trends. We consider this problem in the\nsetting of the censored and kinked structural VAR (CKSVAR), which provides a\nflexible yet tractable framework within which to model time series that are\nsubject to threshold-type nonlinearities, such as those arising due to\noccasionally binding constraints, of which the zero lower bound (ZLB) on\nshort-term nominal interest rates provides a leading example. We provide a\ncomplete characterisation of how common linear and {nonlinear stochastic trends\nmay be generated in this model, via unit roots and appropriate generalisations\nof the usual rank conditions, providing the first extension to date of the\nGranger-Johansen representation theorem to a nonlinearly cointegrated setting,\nand thereby giving the first successful treatment of the open problem. The\nlimiting common trend processes include regulated, censored and kinked Brownian\nmotions, none of which have previously appeared in the literature on\ncointegrated VARs. Our results and running examples illustrate that the CKSVAR\nis capable of supporting a far richer variety of long-run behaviour than is a\nlinear VAR, in ways that may be particularly useful for the identification of\nstructural parameters.",
        "authors": [
            "James A. Duffy",
            "Sophocles Mavroeidis",
            "Sam Wycherley"
        ],
        "categories": "econ.EM",
        "published": "2022-11-17T15:50:00Z",
        "updated": "2023-07-13T08:09:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.09502v1",
        "title": "On the Role of the Zero Conditional Mean Assumption for Causal Inference in Linear Models",
        "abstract": "Many econometrics textbooks imply that under mean independence of the\nregressors and the error term, the OLS parameters have a causal interpretation.\nWe show that even when this assumption is satisfied, OLS might identify a\npseudo-parameter that does not have a causal interpretation. Even assuming that\nthe linear model is \"structural\" creates some ambiguity in what the regression\nerror represents and whether the OLS estimand is causal. This issue applies\nequally to linear IV and panel data models. To give these estimands a causal\ninterpretation, one needs to impose assumptions on a \"causal\" model, e.g.,\nusing the potential outcome framework. This highlights that causal inference\nrequires causal, and not just stochastic, assumptions.",
        "authors": [
            "Federico Crudu",
            "Michael C. Knaus",
            "Giovanni Mellace",
            "Joeri Smits"
        ],
        "categories": "econ.EM",
        "published": "2022-11-17T12:52:02Z",
        "updated": "2022-11-17T12:52:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.08995v1",
        "title": "Estimating Dynamic Spillover Effects along Multiple Networks in a Linear Panel Model",
        "abstract": "Spillover of economic outcomes often arises over multiple networks, and\ndistinguishing their separate roles is important in empirical research. For\nexample, the direction of spillover between two groups (such as banks and\nindustrial sectors linked in a bipartite graph) has important economic\nimplications, and a researcher may want to learn which direction is supported\nin the data. For this, we need to have an empirical methodology that allows for\nboth directions of spillover simultaneously. In this paper, we develop a\ndynamic linear panel model and asymptotic inference with large $n$ and small\n$T$, where both directions of spillover are accommodated through multiple\nnetworks. Using the methodology developed here, we perform an empirical study\nof spillovers between bank weakness and zombie-firm congestion in industrial\nsectors, using firm-bank matched data from Spain between 2005 and 2012.\nOverall, we find that there is positive spillover in both directions between\nbanks and sectors.",
        "authors": [
            "Clemens Possnig",
            "Andreea Rot\u0103rescu",
            "Kyungchul Song"
        ],
        "categories": "econ.EM",
        "published": "2022-11-16T15:49:44Z",
        "updated": "2022-11-16T15:49:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.08649v2",
        "title": "Causal Bandits: Online Decision-Making in Endogenous Settings",
        "abstract": "The deployment of Multi-Armed Bandits (MAB) has become commonplace in many\neconomic applications. However, regret guarantees for even state-of-the-art\nlinear bandit algorithms (such as Optimism in the Face of Uncertainty Linear\nbandit (OFUL)) make strong exogeneity assumptions w.r.t. arm covariates. This\nassumption is very often violated in many economic contexts and using such\nalgorithms can lead to sub-optimal decisions. Further, in social science\nanalysis, it is also important to understand the asymptotic distribution of\nestimated parameters. To this end, in this paper, we consider the problem of\nonline learning in linear stochastic contextual bandit problems with endogenous\ncovariates. We propose an algorithm we term $\\epsilon$-BanditIV, that uses\ninstrumental variables to correct for this bias, and prove an\n$\\tilde{\\mathcal{O}}(k\\sqrt{T})$ upper bound for the expected regret of the\nalgorithm. Further, we demonstrate the asymptotic consistency and normality of\nthe $\\epsilon$-BanditIV estimator. We carry out extensive Monte Carlo\nsimulations to demonstrate the performance of our algorithms compared to other\nmethods. We show that $\\epsilon$-BanditIV significantly outperforms other\nexisting methods in endogenous settings. Finally, we use data from real-time\nbidding (RTB) system to demonstrate how $\\epsilon$-BanditIV can be used to\nestimate the causal impact of advertising in such settings and compare its\nperformance with other existing methods.",
        "authors": [
            "Jingwen Zhang",
            "Yifang Chen",
            "Amandeep Singh"
        ],
        "categories": "econ.EM",
        "published": "2022-11-16T03:51:14Z",
        "updated": "2023-02-27T07:06:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.08205v1",
        "title": "Robust estimation for Threshold Autoregressive Moving-Average models",
        "abstract": "Threshold autoregressive moving-average (TARMA) models are popular in time\nseries analysis due to their ability to parsimoniously describe several complex\ndynamical features. However, neither theory nor estimation methods are\ncurrently available when the data present heavy tails or anomalous\nobservations, which is often the case in applications. In this paper, we\nprovide the first theoretical framework for robust M-estimation for TARMA\nmodels and also study its practical relevance. Under mild conditions, we show\nthat the robust estimator for the threshold parameter is super-consistent,\nwhile the estimators for autoregressive and moving-average parameters are\nstrongly consistent and asymptotically normal. The Monte Carlo study shows that\nthe M-estimator is superior, in terms of both bias and variance, to the least\nsquares estimator, which can be heavily affected by outliers. The findings\nsuggest that robust M-estimation should be generally preferred to the least\nsquares method. Finally, we apply our methodology to a set of commodity price\ntime series; the robust TARMA fit presents smaller standard errors and leads to\nsuperior forecasting accuracy compared to the least squares fit. The results\nsupport the hypothesis of a two-regime, asymmetric nonlinearity around zero,\ncharacterised by slow expansions and fast contractions.",
        "authors": [
            "Greta Goracci",
            "Davide Ferrari",
            "Simone Giannerini",
            "Francesco ravazzolo"
        ],
        "categories": "stat.ME",
        "published": "2022-11-15T15:14:15Z",
        "updated": "2022-11-15T15:14:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.07903v1",
        "title": "Identification and Auto-debiased Machine Learning for Outcome Conditioned Average Structural Derivatives",
        "abstract": "This paper proposes a new class of heterogeneous causal quantities, named\n\\textit{outcome conditioned} average structural derivatives (OASD) in a general\nnonseparable model. OASD is the average partial effect of a marginal change in\na continuous treatment on the individuals located at different parts of the\noutcome distribution, irrespective of individuals' characteristics. OASD\ncombines both features of ATE and QTE: it is interpreted as straightforwardly\nas ATE while at the same time more granular than ATE by breaking the entire\npopulation up according to the rank of the outcome distribution.\n  One contribution of this paper is that we establish some close relationships\nbetween the \\textit{outcome conditioned average partial effects} and a class of\nparameters measuring the effect of counterfactually changing the distribution\nof a single covariate on the unconditional outcome quantiles. By exploiting\nsuch relationship, we can obtain root-$n$ consistent estimator and calculate\nthe semi-parametric efficiency bound for these counterfactual effect\nparameters. We illustrate this point by two examples: equivalence between OASD\nand the unconditional partial quantile effect (Firpo et al. (2009)), and\nequivalence between the marginal partial distribution policy effect (Rothe\n(2012)) and a corresponding outcome conditioned parameter.\n  Because identification of OASD is attained under a conditional exogeneity\nassumption, by controlling for a rich information about covariates, a\nresearcher may ideally use high-dimensional controls in data. We propose for\nOASD a novel automatic debiased machine learning estimator, and present\nasymptotic statistical guarantees for it. We prove our estimator is root-$n$\nconsistent, asymptotically normal, and semiparametrically efficient. We also\nprove the validity of the bootstrap procedure for uniform inference on the OASD\nprocess.",
        "authors": [
            "Zequn Jin",
            "Lihua Lin",
            "Zhengyu Zhang"
        ],
        "categories": "econ.EM",
        "published": "2022-11-15T05:19:07Z",
        "updated": "2022-11-15T05:19:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.07823v3",
        "title": "Graph Neural Networks for Causal Inference Under Network Confounding",
        "abstract": "This paper studies causal inference with observational network data. A\nchallenging aspect of this setting is the possibility of interference in both\npotential outcomes and selection into treatment, for example due to peer\neffects in either stage. We therefore consider a nonparametric setup in which\nboth stages are reduced forms of simultaneous-equations models. This results in\nhigh-dimensional network confounding, where the network and covariates of all\nunits constitute sources of selection bias. The literature predominantly\nassumes that confounding can be summarized by a known, low-dimensional function\nof these objects, and it is unclear what selection models justify common\nchoices of functions. We show that graph neural networks (GNNs) are well suited\nto adjust for high-dimensional network confounding. We establish a network\nanalog of approximate sparsity under primitive conditions on interference. This\ndemonstrates that the model has low-dimensional structure that makes estimation\nfeasible and justifies the use of shallow GNN architectures.",
        "authors": [
            "Michael P. Leung",
            "Pantelis Loupos"
        ],
        "categories": "econ.EM",
        "published": "2022-11-15T01:00:04Z",
        "updated": "2024-03-20T16:43:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.07506v4",
        "title": "Type I Tobit Bayesian Additive Regression Trees for Censored Outcome Regression",
        "abstract": "Censoring occurs when an outcome is unobserved beyond some threshold value.\nMethods that do not account for censoring produce biased predictions of the\nunobserved outcome. This paper introduces Type I Tobit Bayesian Additive\nRegression Tree (TOBART-1) models for censored outcomes. Simulation results and\nreal data applications demonstrate that TOBART-1 produces accurate predictions\nof censored outcomes. TOBART-1 provides posterior intervals for the conditional\nexpectation and other quantities of interest. The error term distribution can\nhave a large impact on the expectation of the censored outcome. Therefore the\nerror is flexibly modeled as a Dirichlet process mixture of normal\ndistributions.",
        "authors": [
            "Eoghan O'Neill"
        ],
        "categories": "econ.EM",
        "published": "2022-11-14T16:40:08Z",
        "updated": "2024-02-20T18:21:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.06710v5",
        "title": "Robust Difference-in-differences Models",
        "abstract": "The difference-in-differences (DID) method identifies the average treatment\neffects on the treated (ATT) under mainly the so-called parallel trends (PT)\nassumption. The most common and widely used approach to justify the PT\nassumption is the pre-treatment period examination. If a null hypothesis of the\nsame trend in the outcome means for both treatment and control groups in the\npre-treatment periods is rejected, researchers believe less in PT and the DID\nresults. This paper develops a robust generalized DID method that utilizes all\nthe information available not only from the pre-treatment periods but also from\nmultiple data sources. Our approach interprets PT in a different way using a\nnotion of selection bias, which enables us to generalize the standard DID\nestimand by defining an information set that may contain multiple pre-treatment\nperiods or other baseline covariates. Our main assumption states that the\nselection bias in the post-treatment period lies within the convex hull of all\nselection biases in the pre-treatment periods. We provide a sufficient\ncondition for this assumption to hold. Based on the baseline information set we\nconstruct, we provide an identified set for the ATT that always contains the\ntrue ATT under our identifying assumption, and also the standard DID estimand.\nWe extend our proposed approach to multiple treatment periods DID settings. We\npropose a flexible and easy way to implement the method. Finally, we illustrate\nour methodology through some numerical and empirical examples.",
        "authors": [
            "Kyunghoon Ban",
            "D\u00e9sir\u00e9 K\u00e9dagni"
        ],
        "categories": "econ.EM",
        "published": "2022-11-12T17:22:22Z",
        "updated": "2023-08-22T02:03:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.06707v2",
        "title": "Multiple Structural Breaks in Interactive Effects Panel Data and the Impact of Quantitative Easing on Bank Lending",
        "abstract": "This paper develops a new toolbox for multiple structural break detection in\npanel data models with interactive effects. The toolbox includes tests for the\npresence of structural breaks, a break date estimator, and a break date\nconfidence interval. The new toolbox is applied to a large panel of US banks\nfor a period characterized by massive quantitative easing programs aimed at\nlessening the impact of the global financial crisis and the COVID--19 pandemic.\nThe question we ask is: Have these programs been successful in spurring bank\nlending in the US economy? The short answer turns out to be: ``No''.",
        "authors": [
            "Jan Ditzen",
            "Yiannis Karavias",
            "Joakim Westerlund"
        ],
        "categories": "econ.EM",
        "published": "2022-11-12T16:59:47Z",
        "updated": "2023-01-26T17:57:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.06288v3",
        "title": "A Residuals-Based Nonparametric Variance Ratio Test for Cointegration",
        "abstract": "This paper derives asymptotic theory for Breitung's (2002, Journal of\nEconometrics 108, 343-363) nonparameteric variance ratio unit root test when\napplied to regression residuals. The test requires neither the specification of\nthe correlation structure in the data nor the choice of tuning parameters.\nCompared with popular residuals-based no-cointegration tests, the variance\nratio test is less prone to size distortions but has smaller local asymptotic\npower. However, this paper shows that local asymptotic power properties do not\nserve as a useful indicator for the power of residuals-based no-cointegration\ntests in finite samples. In terms of size-corrected power, the variance ratio\ntest performs relatively well and, in particular, does not suffer from power\nreversal problems detected for, e.g., the frequently used augmented\nDickey-Fuller type no-cointegration test. An application to daily prices of\ncryptocurrencies illustrates the usefulness of the variance ratio test in\npractice.",
        "authors": [
            "Karsten Reichold"
        ],
        "categories": "econ.EM",
        "published": "2022-11-11T15:54:26Z",
        "updated": "2022-12-07T11:45:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.04752v4",
        "title": "Bayesian Neural Networks for Macroeconomic Analysis",
        "abstract": "Macroeconomic data is characterized by a limited number of observations\n(small T), many time series (big K) but also by featuring temporal dependence.\nNeural networks, by contrast, are designed for datasets with millions of\nobservations and covariates. In this paper, we develop Bayesian neural networks\n(BNNs) that are well-suited for handling datasets commonly used for\nmacroeconomic analysis in policy institutions. Our approach avoids extensive\nspecification searches through a novel mixture specification for the activation\nfunction that appropriately selects the form of nonlinearities. Shrinkage\npriors are used to prune the network and force irrelevant neurons to zero. To\ncope with heteroskedasticity, the BNN is augmented with a stochastic volatility\nmodel for the error term. We illustrate how the model can be used in a policy\ninstitution by first showing that our different BNNs produce precise density\nforecasts, typically better than those from other machine learning methods.\nFinally, we showcase how our model can be used to recover nonlinearities in the\nreaction of macroeconomic aggregates to financial shocks.",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Karin Klieber",
            "Massimiliano Marcellino"
        ],
        "categories": "econ.EM",
        "published": "2022-11-09T09:10:57Z",
        "updated": "2024-04-02T18:17:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.04558v3",
        "title": "Crises Do Not Cause Lower Short-Term Growth",
        "abstract": "It is commonly believed that financial crises \"lead to\" lower growth of a\ncountry during the two-year recession period, which can be reflected by their\npost-crisis GDP growth. However, by contrasting a causal model with a standard\nprediction model, this paper argues that such a belief is non-causal. To make\ncausal inferences, we design a two-stage staggered difference-in-differences\nmodel to estimate the average treatment effects. Interpreting the residuals as\nthe contribution of each crisis to the treatment effects, we astonishingly\nconclude that cross-sectional crises are often limited to providing relevant\ncausal information to policymakers.",
        "authors": [
            "Kaiwen Hou",
            "David Hou",
            "Yang Ouyang",
            "Lulu Zhang",
            "Aster Liu"
        ],
        "categories": "econ.EM",
        "published": "2022-11-08T21:07:42Z",
        "updated": "2022-11-11T17:24:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.04184v2",
        "title": "On the Past, Present, and Future of the Diebold-Yilmaz Approach to Dynamic Network Connectedness",
        "abstract": "We offer retrospective and prospective assessments of the Diebold-Yilmaz\nconnectedness research program, combined with personal recollections of its\ndevelopment. Its centerpiece in many respects is Diebold and Yilmaz (2014),\naround which our discussion is organized.",
        "authors": [
            "Francis X. Diebold",
            "Kamil Yilmaz"
        ],
        "categories": "econ.EM",
        "published": "2022-11-08T11:54:36Z",
        "updated": "2023-01-09T19:27:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.04027v3",
        "title": "Bootstraps for Dynamic Panel Threshold Models",
        "abstract": "This paper develops valid bootstrap inference methods for the dynamic short\npanel threshold regression. We demonstrate that the standard nonparametric\nbootstrap is inconsistent for the first-differenced generalized method of\nmoments (GMM) estimator. The inconsistency is due to an $n^{1/4}$-consistent\nnon-normal asymptotic distribution for the threshold estimate when the\nparameter resides within the continuity region of the parameter space. It stems\nfrom the rank deficiency of the approximate Jacobian of the sample moment\nconditions on the continuity region. To address this, we propose a grid\nbootstrap to construct confidence intervals of the threshold, a residual\nbootstrap to construct confidence intervals of the coefficients, and a\nbootstrap for testing continuity. They are shown to be valid under uncertain\ncontinuity, while the grid bootstrap is additionally shown to be uniformly\nvalid. A set of Monte Carlo experiments demonstrate that the proposed\nbootstraps perform well in the finite samples and improve upon the standard\nnonparametric bootstrap.",
        "authors": [
            "Woosik Gong",
            "Myung Hwan Seo"
        ],
        "categories": "econ.EM",
        "published": "2022-11-08T06:06:27Z",
        "updated": "2024-09-10T23:13:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.02249v3",
        "title": "Fast, Robust Inference for Linear Instrumental Variables Models using Self-Normalized Moments",
        "abstract": "We propose and implement an approach to inference in linear instrumental\nvariables models which is simultaneously robust and computationally tractable.\nInference is based on self-normalization of sample moment conditions, and\nallows for (but does not require) many (relative to the sample size), weak,\npotentially invalid or potentially endogenous instruments, as well as for many\nregressors and conditional heteroskedasticity. Our coverage results are uniform\nand can deliver a small sample guarantee. We develop a new computational\napproach based on semidefinite programming, which we show can equally be\napplied to rapidly invert existing tests (e.g,. AR, LM, CLR, etc.).",
        "authors": [
            "Eric Gautier",
            "Christiern Rose"
        ],
        "categories": "econ.EM",
        "published": "2022-11-04T03:48:55Z",
        "updated": "2022-11-28T03:44:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.02215v2",
        "title": "Boosted p-Values for High-Dimensional Vector Autoregression",
        "abstract": "Assessing the statistical significance of parameter estimates is an important\nstep in high-dimensional vector autoregression modeling. Using the\nleast-squares boosting method, we compute the p-value for each selected\nparameter at every boosting step in a linear model. The p-values are\nasymptotically valid and also adapt to the iterative nature of the boosting\nprocedure. Our simulation experiment shows that the p-values can keep false\npositive rate under control in high-dimensional vector autoregressions. In an\napplication with more than 100 macroeconomic time series, we further show that\nthe p-values can not only select a sparser model with good prediction\nperformance but also help control model stability. A companion R package\nboostvar is developed.",
        "authors": [
            "Xiao Huang"
        ],
        "categories": "econ.EM",
        "published": "2022-11-04T01:52:55Z",
        "updated": "2023-03-15T16:05:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.01921v3",
        "title": "On Estimation and Inference of Large Approximate Dynamic Factor Models via the Principal Component Analysis",
        "abstract": "We provide an alternative derivation of the asymptotic results for the\nPrincipal Components estimator of a large approximate factor model. Results are\nderived under a minimal set of assumptions and, in particular, we require only\nthe existence of 4th order moments. A special focus is given to the time series\nsetting, a case considered in almost all recent econometric applications of\nfactor models. Hence, estimation is based on the classical $n\\times n$ sample\ncovariance matrix and not on a $T\\times T$ covariance matrix often considered\nin the literature. Indeed, despite the two approaches being asymptotically\nequivalent, the former is more coherent with a time series setting and it\nimmediately allows us to write more intuitive asymptotic expansions for the\nPrincipal Component estimators showing that they are equivalent to OLS as long\nas $\\sqrt n/T\\to 0$ and $\\sqrt T/n\\to 0$, that is the loadings are estimated in\na time series regression as if the factors were known, while the factors are\nestimated in a cross-sectional regression as if the loadings were known.\nFinally, we give some alternative sets of primitive sufficient conditions for\nmean-squared consistency of the sample covariance matrix of the factors, of the\nidiosyncratic components, and of the observed time series, which is the\nstarting point for Principal Component Analysis.",
        "authors": [
            "Matteo Barigozzi"
        ],
        "categories": "econ.EM",
        "published": "2022-11-03T16:01:49Z",
        "updated": "2023-07-18T14:04:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.01575v1",
        "title": "Are Synthetic Control Weights Balancing Score?",
        "abstract": "In this short note, I outline conditions under which conditioning on\nSynthetic Control (SC) weights emulates a randomized control trial where the\ntreatment status is independent of potential outcomes. Specifically, I\ndemonstrate that if there exist SC weights such that (i) the treatment effects\nare exactly identified and (ii) these weights are uniformly and cumulatively\nbounded, then SC weights are balancing scores.",
        "authors": [
            "Harsh Parikh"
        ],
        "categories": "stat.ME",
        "published": "2022-11-03T03:52:28Z",
        "updated": "2022-11-03T03:52:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.01557v1",
        "title": "Estimating interaction effects with panel data",
        "abstract": "A common task in empirical economics is to estimate \\emph{interaction\neffects} that measure how the effect of one variable $X$ on another variable\n$Y$ depends on a third variable $H$. This paper considers the estimation of\ninteraction effects in linear panel models with a fixed number of time periods.\nThere are at least two ways to estimate interaction effects in this setting,\nboth common in applied work. Our theoretical results show that these two\napproaches are distinct, and only coincide under strong conditions on\nunobserved effect heterogeneity. Our empirical results show that the difference\nbetween the two approaches is large, leading to conflicting conclusions about\nthe sign of the interaction effect. Taken together, our findings may guide the\nchoice between the two approaches in empirical work.",
        "authors": [
            "Chris Muris",
            "Konstantin Wacker"
        ],
        "categories": "econ.EM",
        "published": "2022-11-03T02:32:34Z",
        "updated": "2022-11-03T02:32:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.01547v1",
        "title": "A Systematic Paradigm for Detecting, Surfacing, and Characterizing Heterogeneous Treatment Effects (HTE)",
        "abstract": "To effectively optimize and personalize treatments, it is necessary to\ninvestigate the heterogeneity of treatment effects. With the wide range of\nusers being treated over many online controlled experiments, the typical\napproach of manually investigating each dimension of heterogeneity becomes\noverly cumbersome and prone to subjective human biases. We need an efficient\nway to search through thousands of experiments with hundreds of target\ncovariates and hundreds of breakdown dimensions. In this paper, we propose a\nsystematic paradigm for detecting, surfacing and characterizing heterogeneous\ntreatment effects. First, we detect if treatment effect variation is present in\nan experiment, prior to specifying any breakdowns. Second, we surface the most\nrelevant dimensions for heterogeneity. Finally, we characterize the\nheterogeneity beyond just the conditional average treatment effects (CATE) by\nstudying the conditional distributions of the estimated individual treatment\neffects. We show the effectiveness of our methods using simulated data and\nempirical studies.",
        "authors": [
            "John Cai",
            "Weinan Wang"
        ],
        "categories": "stat.ME",
        "published": "2022-11-03T01:43:30Z",
        "updated": "2022-11-03T01:43:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.01537v3",
        "title": "Stochastic Treatment Choice with Empirical Welfare Updating",
        "abstract": "This paper proposes a novel method to estimate individualised treatment\nassignment rules. The method is designed to find rules that are stochastic,\nreflecting uncertainty in estimation of an assignment rule and about its\nwelfare performance. Our approach is to form a prior distribution over\nassignment rules, not over data generating processes, and to update this prior\nbased upon an empirical welfare criterion, not likelihood. The social planner\nthen assigns treatment by drawing a policy from the resulting posterior. We\nshow analytically a welfare-optimal way of updating the prior using empirical\nwelfare; this posterior is not feasible to compute, so we propose a variational\nBayes approximation for the optimal posterior. We characterise the welfare\nregret convergence of the assignment rule based upon this variational Bayes\napproximation, showing that it converges to zero at a rate of ln(n)/sqrt(n). We\napply our methods to experimental data from the Job Training Partnership Act\nStudy to illustrate the implementation of our methods.",
        "authors": [
            "Toru Kitagawa",
            "Hugo Lopez",
            "Jeff Rowley"
        ],
        "categories": "econ.EM",
        "published": "2022-11-03T01:03:04Z",
        "updated": "2023-02-21T11:51:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.01344v1",
        "title": "A New Test for Market Efficiency and Uncovered Interest Parity",
        "abstract": "We suggest a new single-equation test for Uncovered Interest Parity (UIP)\nbased on a dynamic regression approach. The method provides consistent and\nasymptotically efficient parameter estimates, and is not dependent on\nassumptions of strict exogeneity. This new approach is asymptotically more\nefficient than the common approach of using OLS with HAC robust standard errors\nin the static forward premium regression. The coefficient estimates when spot\nreturn changes are regressed on the forward premium are all positive and\nremarkably stable across currencies. These estimates are considerably larger\nthan those of previous studies, which frequently find negative coefficients.\nThe method also has the advantage of showing dynamic effects of risk premia, or\nother events that may lead to rejection of UIP or the efficient markets\nhypothesis.",
        "authors": [
            "Richard T. Baillie",
            "Francis X. Diebold",
            "George Kapetanios",
            "Kun Ho Kim"
        ],
        "categories": "econ.EM",
        "published": "2022-11-02T17:52:40Z",
        "updated": "2022-11-02T17:52:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.00873v1",
        "title": "Effects of syndication network on specialisation and performance of venture capital firms",
        "abstract": "The Chinese venture capital (VC) market is a young and rapidly expanding\nfinancial subsector. Gaining a deeper understanding of the investment\nbehaviours of VC firms is crucial for the development of a more sustainable and\nhealthier market and economy. Contrasting evidence supports that either\nspecialisation or diversification helps to achieve a better investment\nperformance. However, the impact of the syndication network is overlooked.\nSyndication network has a great influence on the propagation of information and\ntrust. By exploiting an authoritative VC dataset of thirty-five-year investment\ninformation in China, we construct a joint-investment network of VC firms and\nanalyse the effects of syndication and diversification on specialisation and\ninvestment performance. There is a clear correlation between the syndication\nnetwork degree and specialisation level of VC firms, which implies that the\nwell-connected VC firms are diversified. More connections generally bring about\nmore information or other resources, and VC firms are more likely to enter a\nnew stage or industry with some new co-investing VC firms when compared to a\nrandomised null model. Moreover, autocorrelation analysis of both\nspecialisation and success rate on the syndication network indicates that\nclustering of similar VC firms is roughly limited to the secondary\nneighbourhood. When analysing local clustering patterns, we discover that,\ncontrary to popular beliefs, there is no apparent successful club of investors.\nIn contrast, investors with low success rates are more likely to cluster. Our\ndiscoveries enrich the understanding of VC investment behaviours and can assist\npolicymakers in designing better strategies to promote the development of the\nVC industry.",
        "authors": [
            "Qing Yao",
            "Shaodong Ma",
            "Jing Liang",
            "Kim Christensen",
            "Wanru Jing",
            "Ruiqi Li"
        ],
        "categories": "physics.soc-ph",
        "published": "2022-11-02T04:42:56Z",
        "updated": "2022-11-02T04:42:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.00671v3",
        "title": "Cover It Up! Bipartite Graphs Uncover Identifiability in Sparse Factor Analysis",
        "abstract": "Despite the popularity of factor models with sparse loading matrices, little\nattention has been given to formally address identifiability of these models\nbeyond standard rotation-based identification such as the positive lower\ntriangular constraint. To fill this gap, we present a counting rule on the\nnumber of nonzero factor loadings that is sufficient for achieving generic\nuniqueness of the variance decomposition in the factor representation. This is\nformalized in the framework of sparse matrix spaces and some classical elements\nfrom graph and network theory. Furthermore, we provide a computationally\nefficient tool for verifying the counting rule. Our methodology is illustrated\nfor real data in the context of post-processing posterior draws in Bayesian\nsparse factor analysis.",
        "authors": [
            "Darjus Hosszejni",
            "Sylvia Fr\u00fchwirth-Schnatter"
        ],
        "categories": "econ.EM",
        "published": "2022-11-01T18:01:54Z",
        "updated": "2022-11-10T09:29:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.00410v1",
        "title": "Population and Technological Growth: Evidence from Roe v. Wade",
        "abstract": "We exploit the heterogeneous impact of the Roe v. Wade ruling by the US\nSupreme Court, which ruled most abortion restrictions unconstitutional. Our\nidentifying assumption is that states which had not liberalized their abortion\nlaws prior to Roe would experience a negative birth shock of greater proportion\nthan states which had undergone pre-Roe reforms. We estimate the\ndifference-in-difference in births and use estimated births as an exogenous\ntreatment variable to predict patents per capita. Our results show that one\nstandard deviation increase in cohort starting population increases per capita\npatents by 0.24 standard deviation. These results suggest that at the margins,\nincreasing fertility can increase patent production. Insofar as patent\nproduction is a sufficient proxy for technological growth, increasing births\nhas a positive impact on technological growth. This paper and its results do\nnot pertain to the issue of abortion itself.",
        "authors": [
            "John T. H. Wong",
            "Matthias Hei Man",
            "Alex Li Cheuk Hung"
        ],
        "categories": "econ.EM",
        "published": "2022-11-01T12:08:19Z",
        "updated": "2022-11-01T12:08:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.00363v3",
        "title": "Reservoir Computing for Macroeconomic Forecasting with Mixed Frequency Data",
        "abstract": "Macroeconomic forecasting has recently started embracing techniques that can\ndeal with large-scale datasets and series with unequal release periods.\nMIxed-DAta Sampling (MIDAS) and Dynamic Factor Models (DFM) are the two main\nstate-of-the-art approaches that allow modeling series with non-homogeneous\nfrequencies. We introduce a new framework called the Multi-Frequency Echo State\nNetwork (MFESN) based on a relatively novel machine learning paradigm called\nreservoir computing. Echo State Networks (ESN) are recurrent neural networks\nformulated as nonlinear state-space systems with random state coefficients\nwhere only the observation map is subject to estimation. MFESNs are\nconsiderably more efficient than DFMs and allow for incorporating many series,\nas opposed to MIDAS models, which are prone to the curse of dimensionality. All\nmethods are compared in extensive multistep forecasting exercises targeting US\nGDP growth. We find that our MFESN models achieve superior or comparable\nperformance over MIDAS and DFMs at a much lower computational cost.",
        "authors": [
            "Giovanni Ballarin",
            "Petros Dellaportas",
            "Lyudmila Grigoryeva",
            "Marcel Hirt",
            "Sophie van Huellen",
            "Juan-Pablo Ortega"
        ],
        "categories": "econ.EM",
        "published": "2022-11-01T10:25:54Z",
        "updated": "2024-01-28T08:57:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2211.00329v2",
        "title": "Weak Identification in Low-Dimensional Factor Models with One or Two Factors",
        "abstract": "This paper describes how to reparameterize low-dimensional factor models with\none or two factors to fit weak identification theory developed for generalized\nmethod of moments models. Some identification-robust tests, here called\n\"plug-in\" tests, require a reparameterization to distinguish weakly identified\nparameters from strongly identified parameters. The reparameterizations in this\npaper make plug-in tests available for subvector hypotheses in low-dimensional\nfactor models with one or two factors. Simulations show that the plug-in tests\nare less conservative than identification-robust tests that use the original\nparameterization. An empirical application to a factor model of parental\ninvestments in children is included.",
        "authors": [
            "Gregory Cox"
        ],
        "categories": "econ.EM",
        "published": "2022-11-01T08:33:54Z",
        "updated": "2024-03-07T02:08:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.17063v2",
        "title": "Shrinkage Methods for Treatment Choice",
        "abstract": "This study examines the problem of determining whether to treat individuals\nbased on observed covariates. The most common decision rule is the conditional\nempirical success (CES) rule proposed by Manski (2004), which assigns\nindividuals to treatments that yield the best experimental outcomes conditional\non the observed covariates. Conversely, using shrinkage estimators, which\nshrink unbiased but noisy preliminary estimates toward the average of these\nestimates, is a common approach in statistical estimation problems because it\nis well-known that shrinkage estimators have smaller mean squared errors than\nunshrunk estimators. Inspired by this idea, we propose a computationally\ntractable shrinkage rule that selects the shrinkage factor by minimizing the\nupper bound of the maximum regret. Then, we compare the maximum regret of the\nproposed shrinkage rule with that of CES and pooling rules when the parameter\nspace is correctly specified or misspecified. Our theoretical results\ndemonstrate that the shrinkage rule performs well in many cases and these\nfindings are further supported by numerical experiments. Specifically, we show\nthat the maximum regret of the shrinkage rule can be strictly smaller than that\nof the CES and pooling rules in certain cases when the parameter space is\ncorrectly specified. In addition, we find that the shrinkage rule is robust\nagainst misspecifications of the parameter space. Finally, we apply our method\nto experimental data from the National Job Training Partnership Act Study.",
        "authors": [
            "Takuya Ishihara",
            "Daisuke Kurisu"
        ],
        "categories": "econ.EM",
        "published": "2022-10-31T04:41:56Z",
        "updated": "2024-06-21T01:21:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.16991v2",
        "title": "Non-Robustness of the Cluster-Robust Inference: with a Proposal of a New Robust Method",
        "abstract": "The conventional cluster-robust (CR) standard errors may not be robust. They\nare vulnerable to data that contain a small number of large clusters. When a\nresearcher uses the 51 states in the U.S. as clusters, the largest cluster\n(California) consists of about 10% of the total sample. Such a case in fact\nviolates the assumptions under which the widely used CR methods are guaranteed\nto work. We formally show that the conventional CR methods fail if the\ndistribution of cluster sizes follows a power law with exponent less than two.\nBesides the example of 51 state clusters, some examples are drawn from a list\nof recent original research articles published in a top journal. In light of\nthese negative results about the existing CR methods, we propose a weighted CR\n(WCR) method as a simple fix. Simulation studies support our arguments that the\nWCR method is robust while the conventional CR methods are not.",
        "authors": [
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "categories": "econ.EM",
        "published": "2022-10-31T00:16:55Z",
        "updated": "2022-12-11T19:15:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.16547v2",
        "title": "Flexible machine learning estimation of conditional average treatment effects: a blessing and a curse",
        "abstract": "Causal inference from observational data requires untestable identification\nassumptions. If these assumptions apply, machine learning (ML) methods can be\nused to study complex forms of causal effect heterogeneity. Recently, several\nML methods were developed to estimate the conditional average treatment effect\n(CATE). If the features at hand cannot explain all heterogeneity, the\nindividual treatment effects (ITEs) can seriously deviate from the CATE. In\nthis work, we demonstrate how the distributions of the ITE and the CATE can\ndiffer when a causal random forest (CRF) is applied. We extend the CRF to\nestimate the difference in conditional variance between treated and controls.\nIf the ITE distribution equals the CATE distribution, this estimated difference\nin variance should be small. If they differ, an additional causal assumption is\nnecessary to quantify the heterogeneity not captured by the CATE distribution.\nThe conditional variance of the ITE can be identified when the individual\neffect is independent of the outcome under no treatment given the measured\nfeatures. Then, in the cases where the ITE and CATE distributions differ, the\nextended CRF can appropriately estimate the variance of the ITE distribution\nwhile the CRF fails to do so.",
        "authors": [
            "Richard Post",
            "Isabel van den Heuvel",
            "Marko Petkovic",
            "Edwin van den Heuvel"
        ],
        "categories": "stat.ME",
        "published": "2022-10-29T09:47:30Z",
        "updated": "2023-07-20T12:23:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.16525v2",
        "title": "Spectral Representation Learning for Conditional Moment Models",
        "abstract": "Many problems in causal inference and economics can be formulated in the\nframework of conditional moment models, which characterize the target function\nthrough a collection of conditional moment restrictions. For nonparametric\nconditional moment models, efficient estimation often relies on preimposed\nconditions on various measures of ill-posedness of the hypothesis space, which\nare hard to validate when flexible models are used. In this work, we address\nthis issue by proposing a procedure that automatically learns representations\nwith controlled measures of ill-posedness. Our method approximates a linear\nrepresentation defined by the spectral decomposition of a conditional\nexpectation operator, which can be used for kernelized estimators and is known\nto facilitate minimax optimal estimation in certain settings. We show this\nrepresentation can be efficiently estimated from data, and establish L2\nconsistency for the resulting estimator. We evaluate the proposed method on\nproximal causal inference tasks, exhibiting promising performance on\nhigh-dimensional, semi-synthetic data.",
        "authors": [
            "Ziyu Wang",
            "Yucen Luo",
            "Yueru Li",
            "Jun Zhu",
            "Bernhard Sch\u00f6lkopf"
        ],
        "categories": "stat.ML",
        "published": "2022-10-29T07:48:29Z",
        "updated": "2022-12-28T12:29:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.16042v1",
        "title": "Eigenvalue tests for the number of latent factors in short panels",
        "abstract": "This paper studies new tests for the number of latent factors in a large\ncross-sectional factor model with small time dimension. These tests are based\non the eigenvalues of variance-covariance matrices of (possibly weighted) asset\nreturns, and rely on either the assumption of spherical errors, or instrumental\nvariables for factor betas. We establish the asymptotic distributional results\nusing expansion theorems based on perturbation theory for symmetric matrices.\nOur framework accommodates semi-strong factors in the systematic components. We\npropose a novel statistical test for weak factors against strong or semi-strong\nfactors. We provide an empirical application to US equity data. Evidence for a\ndifferent number of latent factors according to market downturns and market\nupturns, is statistically ambiguous in the considered subperiods. In\nparticular, our results contradicts the common wisdom of a single factor model\nin bear markets.",
        "authors": [
            "Alain-Philippe Fortin",
            "Patrick Gagliardini",
            "Olivier Scaillet"
        ],
        "categories": "econ.EM",
        "published": "2022-10-28T10:24:52Z",
        "updated": "2022-10-28T10:24:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.15841v6",
        "title": "How to sample and when to stop sampling: The generalized Wald problem and minimax policies",
        "abstract": "The aim of this paper is to develop techniques for incorporating the cost of\ninformation into experimental design. Specifically, we study sequential\nexperiments where sampling is costly and a decision-maker aims to determine the\nbest treatment for full scale implementation by (1) adaptively allocating units\nto two possible treatments, and (2) stopping the experiment when the expected\nwelfare (inclusive of sampling costs) from implementing the chosen treatment is\nmaximized. Working under the diffusion limit, we describe the optimal policies\nunder the minimax regret criterion. Under small cost asymptotics, the same\npolicies are also optimal under parametric and non-parametric distributions of\noutcomes. The minimax optimal sampling rule is just the Neyman allocation; it\nis independent of sampling costs and does not adapt to previous outcomes. The\ndecision-maker stops sampling when the average difference between the treatment\noutcomes, multiplied by the number of observations collected until that point,\nexceeds a specific threshold. The results derived here also apply to best arm\nidentification with two arms.",
        "authors": [
            "Karun Adusumilli"
        ],
        "categories": "econ.EM",
        "published": "2022-10-28T02:23:43Z",
        "updated": "2024-02-09T22:10:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.15829v4",
        "title": "Estimation of Heterogeneous Treatment Effects Using a Conditional Moment Based Approach",
        "abstract": "We propose a new estimator for heterogeneous treatment effects in a partially\nlinear model (PLM) with multiple exogenous covariates and a potentially\nendogenous treatment variable. Our approach integrates a Robinson\ntransformation to handle the nonparametric component, the Smooth Minimum\nDistance (SMD) method to leverage conditional mean independence restrictions,\nand a Neyman-Orthogonalized first-order condition (FOC). By employing\nregularized model selection techniques like the Lasso method, our estimator\naccommodates numerous covariates while exhibiting reduced bias, consistency,\nand asymptotic normality. Simulations demonstrate its robust performance with\ndiverse instrument sets compared to traditional GMM-type estimators. Applying\nthis method to estimate Medicaid's heterogeneous treatment effects from the\nOregon Health Insurance Experiment reveals more robust and reliable results\nthan conventional GMM approaches.",
        "authors": [
            "Xiaolin Sun"
        ],
        "categories": "econ.EM",
        "published": "2022-10-28T01:48:26Z",
        "updated": "2024-10-03T05:55:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.14205v3",
        "title": "Unit Averaging for Heterogeneous Panels",
        "abstract": "In this work we introduce a unit averaging procedure to efficiently recover\nunit-specific parameters in a heterogeneous panel model. The procedure consists\nin estimating the parameter of a given unit using a weighted average of all the\nunit-specific parameter estimators in the panel. The weights of the average are\ndetermined by minimizing an MSE criterion we derive. We analyze the properties\nof the resulting minimum MSE unit averaging estimator in a local heterogeneity\nframework inspired by the literature on frequentist model averaging, and we\nderive the local asymptotic distribution of the estimator and the corresponding\nweights. The benefits of the procedure are showcased with an application to\nforecasting unemployment rates for a panel of German regions.",
        "authors": [
            "Christian Brownlees",
            "Vladislav Morozov"
        ],
        "categories": "econ.EM",
        "published": "2022-10-25T17:50:30Z",
        "updated": "2024-05-12T19:14:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.13843v2",
        "title": "GLS under Monotone Heteroskedasticity",
        "abstract": "The generalized least square (GLS) is one of the most basic tools in\nregression analyses. A major issue in implementing the GLS is estimation of the\nconditional variance function of the error term, which typically requires a\nrestrictive functional form assumption for parametric estimation or smoothing\nparameters for nonparametric estimation. In this paper, we propose an\nalternative approach to estimate the conditional variance function under\nnonparametric monotonicity constraints by utilizing the isotonic regression\nmethod. Our GLS estimator is shown to be asymptotically equivalent to the\ninfeasible GLS estimator with knowledge of the conditional error variance, and\ninvolves only some tuning to trim boundary observations, not only for point\nestimation but also for interval estimation or hypothesis testing. Our analysis\nextends the scope of the isotonic regression method by showing that the\nisotonic estimates, possibly with generated variables, can be employed as first\nstage estimates to be plugged in for semiparametric objects. Simulation studies\nillustrate excellent finite sample performances of the proposed method. As an\nempirical example, we revisit Acemoglu and Restrepo's (2017) study on the\nrelationship between an aging population and economic growth to illustrate how\nour GLS estimator effectively reduces estimation errors.",
        "authors": [
            "Yoichi Arai",
            "Taisuke Otsu",
            "Mengshan Xu"
        ],
        "categories": "econ.EM",
        "published": "2022-10-25T09:04:54Z",
        "updated": "2024-01-23T04:07:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.13562v3",
        "title": "Prediction intervals for economic fixed-event forecasts",
        "abstract": "The fixed-event forecasting setup is common in economic policy. It involves a\nsequence of forecasts of the same (`fixed') predictand, so that the difficulty\nof the forecasting problem decreases over time. Fixed-event point forecasts are\ntypically published without a quantitative measure of uncertainty. To construct\nsuch a measure, we consider forecast postprocessing techniques tailored to the\nfixed-event case. We develop regression methods that impose constraints\nmotivated by the problem at hand, and use these methods to construct prediction\nintervals for gross domestic product (GDP) growth in Germany and the US.",
        "authors": [
            "Fabian Kr\u00fcger",
            "Hendrik Plett"
        ],
        "categories": "econ.EM",
        "published": "2022-10-24T19:35:24Z",
        "updated": "2024-03-20T14:18:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.17529v1",
        "title": "Spatio-temporal Event Studies for Air Quality Assessment under Cross-sectional Dependence",
        "abstract": "Event Studies (ES) are statistical tools that assess whether a particular\nevent of interest has caused changes in the level of one or more relevant time\nseries. We are interested in ES applied to multivariate time series\ncharacterized by high spatial (cross-sectional) and temporal dependence. We\npursue two goals. First, we propose to extend the existing taxonomy on ES,\nmainly deriving from the financial field, by generalizing the underlying\nstatistical concepts and then adapting them to the time series analysis of\nairborne pollutant concentrations. Second, we address the spatial\ncross-sectional dependence by adopting a twofold adjustment. Initially, we use\na linear mixed spatio-temporal regression model (HDGM) to estimate the\nrelationship between the response variable and a set of exogenous factors,\nwhile accounting for the spatio-temporal dynamics of the observations. Later,\nwe apply a set of sixteen ES test statistics, both parametric and\nnonparametric, some of which directly adjusted for cross-sectional dependence.\nWe apply ES to evaluate the impact on NO2 concentrations generated by the\nlockdown restrictions adopted in the Lombardy region (Italy) during the\nCOVID-19 pandemic in 2020. The HDGM model distinctly reveals the level shift\ncaused by the event of interest, while reducing the volatility and isolating\nthe spatial dependence of the data. Moreover, all the test statistics\nunanimously suggest that the lockdown restrictions generated significant\nreductions in the average NO2 concentrations.",
        "authors": [
            "Paolo Maranzano",
            "Matteo Maria Pelagatti"
        ],
        "categories": "stat.AP",
        "published": "2022-10-24T19:10:20Z",
        "updated": "2022-10-24T19:10:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.12549v1",
        "title": "Choosing The Best Incentives for Belief Elicitation with an Application to Political Protests",
        "abstract": "Many experiments elicit subjects' prior and posterior beliefs about a random\nvariable to assess how information affects one's own actions. However, beliefs\nare multi-dimensional objects, and experimenters often only elicit a single\nresponse from subjects. In this paper, we discuss how the incentives offered by\nexperimenters map subjects' true belief distributions to what profit-maximizing\nsubjects respond in the elicitation task. In particular, we show how slightly\ndifferent incentives may induce subjects to report the mean, mode, or median of\ntheir belief distribution. If beliefs are not symmetric and unimodal, then\nusing an elicitation scheme that is mismatched with the research question may\naffect both the magnitude and the sign of identified effects, or may even make\nidentification impossible. As an example, we revisit Cantoni et al.'s (2019)\nstudy of whether political protests are strategic complements or substitutes.\nWe show that they elicit modal beliefs, while modal and mean beliefs may be\nupdated in opposite directions following their experiment. Hence, the sign of\ntheir effects may change, allowing an alternative interpretation of their\nresults.",
        "authors": [
            "Nathan Canen",
            "Anujit Chakraborty"
        ],
        "categories": "econ.EM",
        "published": "2022-10-22T20:52:10Z",
        "updated": "2022-10-22T20:52:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.11398v1",
        "title": "Allowing for weak identification when testing GARCH-X type models",
        "abstract": "In this paper, we use the results in Andrews and Cheng (2012), extended to\nallow for parameters to be near or at the boundary of the parameter space, to\nderive the asymptotic distributions of the two test statistics that are used in\nthe two-step (testing) procedure proposed by Pedersen and Rahbek (2019). The\nlatter aims at testing the null hypothesis that a GARCH-X type model, with\nexogenous covariates (X), reduces to a standard GARCH type model, while\nallowing the \"GARCH parameter\" to be unidentified. We then provide a\ncharacterization result for the asymptotic size of any test for testing this\nnull hypothesis before numerically establishing a lower bound on the asymptotic\nsize of the two-step procedure at the 5% nominal level. This lower bound\nexceeds the nominal level, revealing that the two-step procedure does not\ncontrol asymptotic size. In a simulation study, we show that this finding is\nrelevant for finite samples, in that the two-step procedure can suffer from\noverrejection in finite samples. We also propose a new test that, by\nconstruction, controls asymptotic size and is found to be more powerful than\nthe two-step procedure when the \"ARCH parameter\" is \"very small\" (in which case\nthe two-step procedure underrejects).",
        "authors": [
            "Philipp Ketz"
        ],
        "categories": "econ.EM",
        "published": "2022-10-20T16:46:20Z",
        "updated": "2022-10-20T16:46:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.11355v2",
        "title": "Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference",
        "abstract": "We propose a generalization of the synthetic controls and synthetic\ninterventions methodology to incorporate network interference. We consider the\nestimation of unit-specific potential outcomes from panel data in the presence\nof spillover across units and unobserved confounding. Key to our approach is a\nnovel latent factor model that takes into account network interference and\ngeneralizes the factor models typically used in panel data settings. We propose\nan estimator, Network Synthetic Interventions (NSI), and show that it\nconsistently estimates the mean outcomes for a unit under an arbitrary set of\ncounterfactual treatments for the network. We further establish that the\nestimator is asymptotically normal. We furnish two validity tests for whether\nthe NSI estimator reliably generalizes to produce accurate counterfactual\nestimates. We provide a novel graph-based experiment design that guarantees the\nNSI estimator produces accurate counterfactual estimates, and also analyze the\nsample complexity of the proposed design. We conclude with simulations that\ncorroborate our theoretical findings.",
        "authors": [
            "Anish Agarwal",
            "Sarah H. Cen",
            "Devavrat Shah",
            "Christina Lee Yu"
        ],
        "categories": "econ.EM",
        "published": "2022-10-20T15:44:05Z",
        "updated": "2023-10-12T00:21:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.11062v1",
        "title": "Low-rank Panel Quantile Regression: Estimation and Inference",
        "abstract": "In this paper, we propose a class of low-rank panel quantile regression\nmodels which allow for unobserved slope heterogeneity over both individuals and\ntime. We estimate the heterogeneous intercept and slope matrices via nuclear\nnorm regularization followed by sample splitting, row- and column-wise quantile\nregressions and debiasing. We show that the estimators of the factors and\nfactor loadings associated with the intercept and slope matrices are\nasymptotically normally distributed. In addition, we develop two specification\ntests: one for the null hypothesis that the slope coefficient is a constant\nover time and/or individuals under the case that true rank of slope matrix\nequals one, and the other for the null hypothesis that the slope coefficient\nexhibits an additive structure under the case that the true rank of slope\nmatrix equals two. We illustrate the finite sample performance of estimation\nand inference via Monte Carlo simulations and real datasets.",
        "authors": [
            "Yiren Wang",
            "Liangjun Su",
            "Yichong Zhang"
        ],
        "categories": "econ.EM",
        "published": "2022-10-20T07:34:15Z",
        "updated": "2022-10-20T07:34:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.11010v3",
        "title": "Efficient variational approximations for state space models",
        "abstract": "Variational Bayes methods are a potential scalable estimation approach for\nstate space models. However, existing methods are inaccurate or computationally\ninfeasible for many state space models. This paper proposes a variational\napproximation that is accurate and fast for any model with a closed-form\nmeasurement density function and a state transition distribution within the\nexponential family of distributions. We show that our method can accurately and\nquickly estimate a multivariate Skellam stochastic volatility model with\nhigh-frequency tick-by-tick discrete price changes of four stocks, and a\ntime-varying parameter vector autoregression with a stochastic volatility model\nusing eight macroeconomic variables.",
        "authors": [
            "Rub\u00e9n Loaiza-Maya",
            "Didier Nibbering"
        ],
        "categories": "econ.EM",
        "published": "2022-10-20T04:31:49Z",
        "updated": "2023-06-02T01:26:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.11003v1",
        "title": "Synthetic Blip Effects: Generalizing Synthetic Controls for the Dynamic Treatment Regime",
        "abstract": "We propose a generalization of the synthetic control and synthetic\ninterventions methodology to the dynamic treatment regime. We consider the\nestimation of unit-specific treatment effects from panel data collected via a\ndynamic treatment regime and in the presence of unobserved confounding. That\nis, each unit receives multiple treatments sequentially, based on an adaptive\npolicy, which depends on a latent endogenously time-varying confounding state\nof the treated unit. Under a low-rank latent factor model assumption and a\ntechnical overlap assumption we propose an identification strategy for any\nunit-specific mean outcome under any sequence of interventions. The latent\nfactor model we propose admits linear time-varying and time-invariant dynamical\nsystems as special cases. Our approach can be seen as an identification\nstrategy for structural nested mean models under a low-rank latent factor\nassumption on the blip effects. Our method, which we term \"synthetic blip\neffects\", is a backwards induction process, where the blip effect of a\ntreatment at each period and for a target unit is recursively expressed as\nlinear combinations of blip effects of a carefully chosen group of other units\nthat received the designated treatment. Our work avoids the combinatorial\nexplosion in the number of units that would be required by a vanilla\napplication of prior synthetic control and synthetic intervention methods in\nsuch dynamic treatment regime settings.",
        "authors": [
            "Anish Agarwal",
            "Vasilis Syrgkanis"
        ],
        "categories": "econ.EM",
        "published": "2022-10-20T04:11:20Z",
        "updated": "2022-10-20T04:11:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.10024v1",
        "title": "Linear Regression with Centrality Measures",
        "abstract": "This paper studies the properties of linear regression on centrality measures\nwhen network data is sparse -- that is, when there are many more agents than\nlinks per agent -- and when they are measured with error. We make three\ncontributions in this setting: (1) We show that OLS estimators can become\ninconsistent under sparsity and characterize the threshold at which this\noccurs, with and without measurement error. This threshold depends on the\ncentrality measure used. Specifically, regression on eigenvector is less robust\nto sparsity than on degree and diffusion. (2) We develop distributional theory\nfor OLS estimators under measurement error and sparsity, finding that OLS\nestimators are subject to asymptotic bias even when they are consistent.\nMoreover, bias can be large relative to their variances, so that bias\ncorrection is necessary for inference. (3) We propose novel bias correction and\ninference methods for OLS with sparse noisy networks. Simulation evidence\nsuggests that our theory and methods perform well, particularly in settings\nwhere the usual OLS estimators and heteroskedasticity-consistent/robust t-tests\nare deficient. Finally, we demonstrate the utility of our results in an\napplication inspired by De Weerdt and Deacon (2006), in which we consider\nconsumption smoothing and social insurance in Nyakatoke, Tanzania.",
        "authors": [
            "Yong Cai"
        ],
        "categories": "econ.EM",
        "published": "2022-10-18T17:47:43Z",
        "updated": "2022-10-18T17:47:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.09828v5",
        "title": "Modelling Large Dimensional Datasets with Markov Switching Factor Models",
        "abstract": "We study a novel large dimensional approximate factor model with regime\nchanges in the loadings driven by a latent first order Markov process. By\nexploiting the equivalent linear representation of the model, we first recover\nthe latent factors by means of Principal Component Analysis. We then cast the\nmodel in state-space form, and we estimate loadings and transition\nprobabilities through an EM algorithm based on a modified version of the\nBaum-Lindgren-Hamilton-Kim filter and smoother that makes use of the factors\npreviously estimated. Our approach is appealing as it provides closed form\nexpressions for all estimators. More importantly, it does not require knowledge\nof the true number of factors. We derive the theoretical properties of the\nproposed estimation procedure, and we show their good finite sample performance\nthrough a comprehensive set of Monte Carlo experiments. The empirical\nusefulness of our approach is illustrated through three applications to large\nU.S. datasets of stock returns, macroeconomic variables, and inflation indexes.",
        "authors": [
            "Matteo Barigozzi",
            "Daniele Massacci"
        ],
        "categories": "econ.EM",
        "published": "2022-10-18T13:17:07Z",
        "updated": "2024-12-03T17:08:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.09426v5",
        "title": "Party On: The Labor Market Returns to Social Networks in Adolescence",
        "abstract": "We investigate the returns to adolescent friendships on earnings in adulthood\nusing data from the National Longitudinal Study of Adolescent to Adult Health.\nBecause both education and friendships are jointly determined in adolescence,\nOLS estimates of their returns are likely biased. We implement a novel\nprocedure to obtain bounds on the causal returns to friendships: we assume that\nthe returns to schooling range from 5 to 15% (based on prior literature), and\ninstrument for friendships using similarity in age among peers. Having one more\nfriend in adolescence increases earnings between 7 and 14%, substantially more\nthan OLS estimates would suggest.",
        "authors": [
            "Adriana Lleras-Muney",
            "Matthew Miller",
            "Shuyang Sheng",
            "Veronica Sovero"
        ],
        "categories": "econ.EM",
        "published": "2022-10-17T20:43:21Z",
        "updated": "2024-03-26T22:41:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.09398v2",
        "title": "Concentration inequalities of MLE and robust MLE",
        "abstract": "The Maximum Likelihood Estimator (MLE) serves an important role in statistics\nand machine learning. In this article, for i.i.d. variables, we obtain\nconstant-specified and sharp concentration inequalities and oracle inequalities\nfor the MLE only under exponential moment conditions. Furthermore, in a robust\nsetting, the sub-Gaussian type oracle inequalities of the log-truncated maximum\nlikelihood estimator are derived under the second-moment condition.",
        "authors": [
            "Xiaowei Yang",
            "Xinqiao Liu",
            "Haoyu Wei"
        ],
        "categories": "math.ST",
        "published": "2022-10-17T20:05:07Z",
        "updated": "2022-12-12T16:17:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.08892v1",
        "title": "Modified Wilcoxon-Mann-Whitney tests of stochastic dominance",
        "abstract": "Given independent samples from two univariate distributions, one-sided\nWilcoxon-Mann-Whitney statistics may be used to conduct rank-based tests of\nstochastic dominance. We broaden the scope of applicability of such tests by\nshowing that the bootstrap may be used to conduct valid inference in a matched\npairs sampling framework permitting dependence between the two samples.\nFurther, we show that a modified bootstrap incorporating an implicit estimate\nof a contact set may be used to improve power. Numerical simulations indicate\nthat our test using the modified bootstrap effectively controls the null\nrejection rates and can deliver more or less power than that of the Donald-Hsu\ntest. In the course of establishing our results we obtain a weak approximation\nto the empirical ordinance dominance curve permitting its population density to\ndiverge to infinity at zero or one at arbitrary rates.",
        "authors": [
            "Brendan K. Beare",
            "Jackson D. Clarke"
        ],
        "categories": "econ.EM",
        "published": "2022-10-17T09:41:09Z",
        "updated": "2022-10-17T09:41:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.08698v2",
        "title": "A Design-Based Riesz Representation Framework for Randomized Experiments",
        "abstract": "We describe a new design-based framework for drawing causal inference in\nrandomized experiments. Causal effects in the framework are defined as linear\nfunctionals evaluated at potential outcome functions. Knowledge and assumptions\nabout the potential outcome functions are encoded as function spaces. This\nmakes the framework expressive, allowing experimenters to formulate and\ninvestigate a wide range of causal questions. We describe a class of estimators\nfor estimands defined using the framework and investigate their properties. The\nconstruction of the estimators is based on the Riesz representation theorem. We\nprovide necessary and sufficient conditions for unbiasedness and consistency.\nFinally, we provide conditions under which the estimators are asymptotically\nnormal, and describe a conservative variance estimator to facilitate the\nconstruction of confidence intervals for the estimands.",
        "authors": [
            "Christopher Harshaw",
            "Fredrik S\u00e4vje",
            "Yitan Wang"
        ],
        "categories": "stat.ME",
        "published": "2022-10-17T02:19:11Z",
        "updated": "2022-10-24T00:26:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.08524v3",
        "title": "Inference on Extreme Quantiles of Unobserved Individual Heterogeneity",
        "abstract": "We develop a methodology for conducting inference on extreme quantiles of\nunobserved individual heterogeneity (heterogeneous coefficients, heterogeneous\ntreatment effects, etc.) in a panel data or meta-analysis setting. Inference in\nsuch settings is challenging: only noisy estimates of unobserved heterogeneity\nare available, and approximations based on the central limit theorem work\npoorly for extreme quantiles. For this situation, under weak assumptions we\nderive an extreme value theorem and an intermediate order theorem for noisy\nestimates and appropriate rate and moment conditions. Both theorems are then\nused to construct confidence intervals for extremal quantiles. The intervals\nare simple to construct and require no optimization. Inference based on the\nintermediate order theorem involves a novel self-normalized intermediate order\ntheorem. In simulations, our extremal confidence intervals have favorable\ncoverage properties in the tail. Our methodology is illustrated with an\napplication to firm productivity in denser and less dense areas.",
        "authors": [
            "Vladislav Morozov"
        ],
        "categories": "econ.EM",
        "published": "2022-10-16T12:53:50Z",
        "updated": "2023-06-15T19:58:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.08338v1",
        "title": "Fair Effect Attribution in Parallel Online Experiments",
        "abstract": "A/B tests serve the purpose of reliably identifying the effect of changes\nintroduced in online services. It is common for online platforms to run a large\nnumber of simultaneous experiments by splitting incoming user traffic randomly\nin treatment and control groups. Despite a perfect randomization between\ndifferent groups, simultaneous experiments can interact with each other and\ncreate a negative impact on average population outcomes such as engagement\nmetrics. These are measured globally and monitored to protect overall user\nexperience. Therefore, it is crucial to measure these interaction effects and\nattribute their overall impact in a fair way to the respective experimenters.\nWe suggest an approach to measure and disentangle the effect of simultaneous\nexperiments by providing a cost sharing approach based on Shapley values. We\nalso provide a counterfactual perspective, that predicts shared impact based on\nconditional average treatment effects making use of causal inference\ntechniques. We illustrate our approach in real world and synthetic data\nexperiments.",
        "authors": [
            "Alexander Buchholz",
            "Vito Bellini",
            "Giuseppe Di Benedetto",
            "Yannik Stein",
            "Matteo Ruffini",
            "Fabian Moerchen"
        ],
        "categories": "cs.LG",
        "published": "2022-10-15T17:15:51Z",
        "updated": "2022-10-15T17:15:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.08147v1",
        "title": "A New Method for Generating Random Correlation Matrices",
        "abstract": "We propose a new method for generating random correlation matrices that makes\nit simple to control both location and dispersion. The method is based on a\nvector parameterization, gamma = g(C), which maps any distribution on R^d, d =\nn(n-1)/2 to a distribution on the space of non-singular nxn correlation\nmatrices. Correlation matrices with certain properties, such as being\nwell-conditioned, having block structures, and having strictly positive\nelements, are simple to generate. We compare the new method with existing\nmethods.",
        "authors": [
            "Ilya Archakov",
            "Peter Reinhard Hansen",
            "Yiyao Luo"
        ],
        "categories": "econ.EM",
        "published": "2022-10-15T00:30:24Z",
        "updated": "2022-10-15T00:30:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.07680v1",
        "title": "Conditional Likelihood Ratio Test with Many Weak Instruments",
        "abstract": "This paper extends validity of the conditional likelihood ratio (CLR) test\ndeveloped by Moreira (2003) to instrumental variable regression models with\nunknown error variance and many weak instruments. In this setting, we argue\nthat the conventional CLR test with estimated error variance loses exact\nsimilarity and is asymptotically invalid. We propose a modified critical value\nfunction for the likelihood ratio (LR) statistic with estimated error variance,\nand prove that this modified test achieves asymptotic validity under many weak\ninstrument asymptotics. Our critical value function is constructed by\nrepresenting the LR using four statistics, instead of two as in Moreira (2003).\nA simulation study illustrates the desirable properties of our test.",
        "authors": [
            "Sreevidya Ayyar",
            "Yukitoshi Matsushita",
            "Taisuke Otsu"
        ],
        "categories": "econ.EM",
        "published": "2022-10-14T10:06:55Z",
        "updated": "2022-10-14T10:06:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.07154v1",
        "title": "Fast Estimation of Bayesian State Space Models Using Amortized Simulation-Based Inference",
        "abstract": "This paper presents a fast algorithm for estimating hidden states of Bayesian\nstate space models. The algorithm is a variation of amortized simulation-based\ninference algorithms, where a large number of artificial datasets are generated\nat the first stage, and then a flexible model is trained to predict the\nvariables of interest. In contrast to those proposed earlier, the procedure\ndescribed in this paper makes it possible to train estimators for hidden states\nby concentrating only on certain characteristics of the marginal posterior\ndistributions and introducing inductive bias. Illustrations using the examples\nof the stochastic volatility model, nonlinear dynamic stochastic general\nequilibrium model, and seasonal adjustment procedure with breaks in seasonality\nshow that the algorithm has sufficient accuracy for practical use. Moreover,\nafter pretraining, which takes several hours, finding the posterior\ndistribution for any dataset takes from hundredths to tenths of a second.",
        "authors": [
            "Ramis Khabibullin",
            "Sergei Seleznev"
        ],
        "categories": "econ.EM",
        "published": "2022-10-13T16:37:05Z",
        "updated": "2022-10-13T16:37:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.06639v3",
        "title": "Robust Estimation and Inference in Panels with Interactive Fixed Effects",
        "abstract": "We consider estimation and inference for a regression coefficient in panels\nwith interactive fixed effects (i.e., with a factor structure). We demonstrate\nthat existing estimators and confidence intervals (CIs) can be heavily biased\nand size-distorted when some of the factors are weak. We propose estimators\nwith improved rates of convergence and bias-aware CIs that remain valid\nuniformly, regardless of factor strength. Our approach applies the theory of\nminimax linear estimation to form a debiased estimate, using a nuclear norm\nbound on the error of an initial estimate of the interactive fixed effects. Our\nresulting bias-aware CIs take into account the remaining bias caused by weak\nfactors. Monte Carlo experiments show substantial improvements over\nconventional methods when factors are weak, with minimal costs to estimation\naccuracy when factors are strong.",
        "authors": [
            "Timothy B. Armstrong",
            "Martin Weidner",
            "Andrei Zeleneev"
        ],
        "categories": "econ.EM",
        "published": "2022-10-13T00:32:58Z",
        "updated": "2024-12-11T15:30:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.06594v1",
        "title": "Sample Constrained Treatment Effect Estimation",
        "abstract": "Treatment effect estimation is a fundamental problem in causal inference. We\nfocus on designing efficient randomized controlled trials, to accurately\nestimate the effect of some treatment on a population of $n$ individuals. In\nparticular, we study sample-constrained treatment effect estimation, where we\nmust select a subset of $s \\ll n$ individuals from the population to experiment\non. This subset must be further partitioned into treatment and control groups.\nAlgorithms for partitioning the entire population into treatment and control\ngroups, or for choosing a single representative subset, have been well-studied.\nThe key challenge in our setting is jointly choosing a representative subset\nand a partition for that set.\n  We focus on both individual and average treatment effect estimation, under a\nlinear effects model. We give provably efficient experimental designs and\ncorresponding estimators, by identifying connections to discrepancy\nminimization and leverage-score-based sampling used in randomized numerical\nlinear algebra. Our theoretical results obtain a smooth transition to known\nguarantees when $s$ equals the population size. We also empirically demonstrate\nthe performance of our algorithms.",
        "authors": [
            "Raghavendra Addanki",
            "David Arbour",
            "Tung Mai",
            "Cameron Musco",
            "Anup Rao"
        ],
        "categories": "cs.LG",
        "published": "2022-10-12T21:13:47Z",
        "updated": "2022-10-12T21:13:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.06217v1",
        "title": "Estimating Option Pricing Models Using a Characteristic Function-Based Linear State Space Representation",
        "abstract": "We develop a novel filtering and estimation procedure for parametric option\npricing models driven by general affine jump-diffusions. Our procedure is based\non the comparison between an option-implied, model-free representation of the\nconditional log-characteristic function and the model-implied conditional\nlog-characteristic function, which is functionally affine in the model's state\nvector. We formally derive an associated linear state space representation and\nestablish the asymptotic properties of the corresponding measurement errors.\nThe state space representation allows us to use a suitably modified Kalman\nfiltering technique to learn about the latent state vector and a quasi-maximum\nlikelihood estimator of the model parameters, which brings important\ncomputational advantages. We analyze the finite-sample behavior of our\nprocedure in Monte Carlo simulations. The applicability of our procedure is\nillustrated in two case studies that analyze S&P 500 option prices and the\nimpact of exogenous state variables capturing Covid-19 reproduction and\neconomic policy uncertainty.",
        "authors": [
            "H. Peter Boswijk",
            "Roger J. A. Laeven",
            "Evgenii Vladimirov"
        ],
        "categories": "econ.EM",
        "published": "2022-10-12T14:02:59Z",
        "updated": "2022-10-12T14:02:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.05115v3",
        "title": "Bayesian analysis of mixtures of lognormal distribution with an unknown number of components from grouped data",
        "abstract": "This study proposes a reversible jump Markov chain Monte Carlo method for\nestimating parameters of lognormal distribution mixtures for income. Using\nsimulated data examples, we examined the proposed algorithm's performance and\nthe accuracy of posterior distributions of the Gini coefficients. Results\nsuggest that the parameters were estimated accurately. Therefore, the posterior\ndistributions are close to the true distributions even when the different data\ngenerating process is accounted for. Moreover, promising results for Gini\ncoefficients encouraged us to apply our method to real data from Japan. The\nempirical examples indicate two subgroups in Japan (2020) and the Gini\ncoefficients' integrity.",
        "authors": [
            "Kazuhiko Kakamu"
        ],
        "categories": "econ.EM",
        "published": "2022-10-11T03:10:29Z",
        "updated": "2023-09-21T06:33:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.05026v4",
        "title": "Uncertainty Quantification in Synthetic Controls with Staggered Treatment Adoption",
        "abstract": "We propose principled prediction intervals to quantify the uncertainty of a\nlarge class of synthetic control predictions (or estimators) in settings with\nstaggered treatment adoption, offering precise non-asymptotic coverage\nprobability guarantees. From a methodological perspective, we provide a\ndetailed discussion of different causal quantities to be predicted, which we\ncall causal predictands, allowing for multiple treated units with treatment\nadoption at possibly different points in time. From a theoretical perspective,\nour uncertainty quantification methods improve on prior literature by (i)\ncovering a large class of causal predictands in staggered adoption settings,\n(ii) allowing for synthetic control methods with possibly nonlinear\nconstraints, (iii) proposing scalable robust conic optimization methods and\nprincipled data-driven tuning parameter selection, and (iv) offering valid\nuniform inference across post-treatment periods. We illustrate our methodology\nwith an empirical application studying the effects of economic liberalization\non real GDP per capita for Sub-Saharan African countries. Companion\ngeneral-purpose software packages are provided in Python, R, and Stata.",
        "authors": [
            "Matias D. Cattaneo",
            "Yingjie Feng",
            "Filippo Palomba",
            "Rocio Titiunik"
        ],
        "categories": "econ.EM",
        "published": "2022-10-10T21:44:02Z",
        "updated": "2024-10-14T15:35:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.04703v2",
        "title": "Policy Learning with New Treatments",
        "abstract": "I study the problem of a decision maker choosing a policy which allocates\ntreatment to a heterogeneous population on the basis of experimental data that\nincludes only a subset of possible treatment values. The effects of new\ntreatments are partially identified by shape restrictions on treatment\nresponse. Policies are compared according to the minimax regret criterion, and\nI show that the empirical analog of the population decision problem has a\ntractable linear- and integer-programming formulation. I prove the maximum\nregret of the estimated policy converges to the lowest possible maximum regret\nat a rate which is the maximum of N^-1/2 and the rate at which conditional\naverage treatment effects are estimated in the experimental data. I apply my\nresults to design targeted subsidies for electrical grid connections in rural\nKenya, and estimate that 97% of the population should be given a treatment not\nimplemented in the experiment.",
        "authors": [
            "Samuel Higbee"
        ],
        "categories": "econ.EM",
        "published": "2022-10-10T14:00:51Z",
        "updated": "2023-09-27T20:06:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.04523v4",
        "title": "An identification and testing strategy for proxy-SVARs with weak proxies",
        "abstract": "When proxies (external instruments) used to identify target structural shocks\nare weak, inference in proxy-SVARs (SVAR-IVs) is nonstandard and the\nconstruction of asymptotically valid confidence sets for the impulse responses\nof interest requires weak-instrument robust methods. In the presence of\nmultiple target shocks, test inversion techniques require extra restrictions on\nthe proxy-SVAR parameters other those implied by the proxies that may be\ndifficult to interpret and test. We show that frequentist asymptotic inference\nin these situations can be conducted through Minimum Distance estimation and\nstandard asymptotic methods if the proxy-SVAR can be identified by using\n`strong' instruments for the non-target shocks; i.e. the shocks which are not\nof primary interest in the analysis. The suggested identification strategy\nhinges on a novel pre-test for the null of instrument relevance based on\nbootstrap resampling which is not subject to pre-testing issues, in the sense\nthat the validity of post-test asymptotic inferences is not affected by the\noutcomes of the test. The test is robust to conditionally heteroskedasticity\nand/or zero-censored proxies, is computationally straightforward and applicable\nregardless of the number of shocks being instrumented. Some illustrative\nexamples show the empirical usefulness of the suggested identification and\ntesting strategy.",
        "authors": [
            "Giovanni Angelini",
            "Giuseppe Cavaliere",
            "Luca Fanelli"
        ],
        "categories": "econ.EM",
        "published": "2022-10-10T09:45:35Z",
        "updated": "2023-10-19T08:17:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.04086v2",
        "title": "A Structural Equation Modeling Approach to Understand User's Perceptions of Acceptance of Ride-Sharing Services in Dhaka City",
        "abstract": "This research aims at building a multivariate statistical model for assessing\nusers' perceptions of acceptance of ride-sharing services in Dhaka City. A\nstructured questionnaire is developed based on the users' reported attitudes\nand perceived risks. A total of 350 normally distributed responses are\ncollected from ride-sharing service users and stakeholders of Dhaka City.\nRespondents are interviewed to express their experience and opinions on\nride-sharing services through the stated preference questionnaire. Structural\nEquation Modeling (SEM) is used to validate the research hypotheses.\nStatistical parameters and several trials are used to choose the best SEM. The\nresponses are also analyzed using the Relative Importance Index (RII) method,\nvalidating the chosen SEM. Inside SEM, the quality of ride-sharing services is\nmeasured by two latent and eighteen observed variables. The latent variable\n'safety & security' is more influential than 'service performance' on the\noverall quality of service index. Under 'safety & security' the other two\nvariables, i.e., 'account information' and 'personal information' are found to\nbe the most significant that impact the decision to share rides with others. In\naddition, 'risk of conflict' and 'possibility of accident' are identified using\nthe perception model as the lowest contributing variables. Factor analysis\nreveals the suitability and reliability of the proposed SEM. Identifying the\ninfluential parameters in this will help the service providers understand and\nimprove the quality of ride-sharing service for users.",
        "authors": [
            "Md. Mohaimenul Islam Sourav",
            "Mohammed Russedul Islam",
            "H M Imran Kays",
            "Md. Hadiuzzaman"
        ],
        "categories": "stat.AP",
        "published": "2022-10-08T18:47:53Z",
        "updated": "2023-06-19T15:25:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.03905v2",
        "title": "Empirical Bayes Selection for Value Maximization",
        "abstract": "We study the problem of selecting the best $m$ units from a set of $n$ as $m\n/ n \\to \\alpha \\in (0, 1)$, where noisy, heteroskedastic measurements of the\nunits' true values are available and the decision-maker wishes to maximize the\naverage true value of the units selected. Given a parametric prior\ndistribution, the empirical Bayes decision rule incurs $O_p(n^{-1})$ regret\nrelative to the Bayesian oracle that knows the true prior. More generally, if\nthe error in the estimated prior is of order $O_p(r_n)$, regret is\n$O_p(r_n^2)$. In this sense selecting the best units is easier than estimating\ntheir values. We show this regret bound is sharp in the parametric case, by\ngiving an example in which it is attained. Using priors calibrated from a\ndataset of over four thousand internet experiments, we find that empirical\nBayes methods perform well in practice for detecting the best treatments given\nonly a modest number of experiments.",
        "authors": [
            "Dominic Coey",
            "Kenneth Hung"
        ],
        "categories": "stat.ME",
        "published": "2022-10-08T04:01:42Z",
        "updated": "2023-01-03T21:46:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.03547v1",
        "title": "Order Statistics Approaches to Unobserved Heterogeneity in Auctions",
        "abstract": "We establish nonparametric identification of auction models with continuous\nand nonseparable unobserved heterogeneity using three consecutive order\nstatistics of bids. We then propose sieve maximum likelihood estimators for the\njoint distribution of unobserved heterogeneity and the private value, as well\nas their conditional and marginal distributions. Lastly, we apply our\nmethodology to a novel dataset from judicial auctions in China. Our estimates\nsuggest substantial gains from accounting for unobserved heterogeneity when\nsetting reserve prices. We propose a simple scheme that achieves nearly optimal\nrevenue by using the appraisal value as the reserve price.",
        "authors": [
            "Yao Luo",
            "Peijun Sang",
            "Ruli Xiao"
        ],
        "categories": "econ.EM",
        "published": "2022-10-07T13:31:14Z",
        "updated": "2022-10-07T13:31:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.05358v2",
        "title": "On estimating Armington elasticities for Japan's meat imports",
        "abstract": "By fully accounting for the distinct tariff regimes levied on imported meat,\nwe estimate substitution elasticities of Japan's two-stage import aggregation\nfunctions for beef, chicken and pork. While the regression analysis crucially\ndepends on the price that consumers face, the post-tariff price of imported\nmeat depends not only on ad valorem duties but also on tariff rate quotas and\ngate price system regimes. The effective tariff rate is consequently evaluated\nby utilizing monthly transaction data. To address potential endogeneity\nproblems, we apply exchange rates that we believe to be independent of the\ndemand shocks for imported meat. The panel nature of the data allows us to\nretrieve the first-stage aggregates via time dummy variables, free of demand\nshocks, to be used as part of the explanatory variable and as an instrument in\nthe second-stage regression.",
        "authors": [
            "Satoshi Nakano",
            "Kazuhiko Nishimura"
        ],
        "categories": "econ.EM",
        "published": "2022-10-06T15:03:23Z",
        "updated": "2022-10-18T10:05:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.02824v2",
        "title": "Testing the Number of Components in Finite Mixture Normal Regression Model with Panel Data",
        "abstract": "This paper develops the likelihood ratio-based test of the null hypothesis of\na M0-component model against an alternative of (M0 + 1)-component model in the\nnormal mixture panel regression by extending the Expectation-Maximization (EM)\ntest of Chen and Li (2009a) and Kasahara and Shimotsu (2015) to the case of\npanel data. We show that, unlike the cross-sectional normal mixture, the\nfirst-order derivative of the density function for the variance parameter in\nthe panel normal mixture is linearly independent of its second-order\nderivatives for the mean parameter. On the other hand, like the cross-sectional\nnormal mixture, the likelihood ratio test statistic of the panel normal mixture\nis unbounded. We consider the Penalized Maximum Likelihood Estimator to deal\nwith the unboundedness, where we obtain the data-driven penalty function via\ncomputational experiments. We derive the asymptotic distribution of the\nPenalized Likelihood Ratio Test (PLRT) and EM test statistics by expanding the\nlog-likelihood function up to five times for the reparameterized parameters.\nThe simulation experiment indicates good finite sample performance of the\nproposed EM test. We apply our EM test to estimate the number of production\ntechnology types for the finite mixture Cobb-Douglas production function model\nstudied by Kasahara et al. (2022) used the panel data of the Japanese and\nChilean manufacturing firms. We find the evidence of heterogeneity in\nelasticities of output for intermediate goods, suggesting that production\nfunction is heterogeneous across firms beyond their Hicks-neutral productivity\nterms.",
        "authors": [
            "Yu Hao",
            "Hiroyuki Kasahara"
        ],
        "categories": "econ.EM",
        "published": "2022-10-06T11:25:35Z",
        "updated": "2023-06-02T23:17:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.02599v3",
        "title": "The Local to Unity Dynamic Tobit Model",
        "abstract": "This paper considers highly persistent time series that are subject to\nnonlinearities in the form of censoring or an occasionally binding constraint,\nsuch as are regularly encountered in macroeconomics. A tractable candidate\nmodel for such series is the dynamic Tobit with a root local to unity. We show\nthat this model generates a process that converges weakly to a non-standard\nlimiting process, that is constrained (regulated) to be positive. Surprisingly,\ndespite the presence of censoring, the OLS estimators of the model parameters\nare consistent. We show that this allows OLS-based inferences to be drawn on\nthe overall persistence of the process (as measured by the sum of the\nautoregressive coefficients), and for the null of a unit root to be tested in\nthe presence of censoring. Our simulations illustrate that the conventional ADF\ntest substantially over-rejects when the data is generated by a dynamic Tobit\nwith a unit root, whereas our proposed test is correctly sized. We provide an\napplication of our methods to testing for a unit root in the Swiss franc / euro\nexchange rate, during a period when this was subject to an occasionally binding\nlower bound.",
        "authors": [
            "Anna Bykhovskaya",
            "James A. Duffy"
        ],
        "categories": "econ.EM",
        "published": "2022-10-05T23:21:03Z",
        "updated": "2024-05-08T15:22:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.02548v1",
        "title": "Regression discontinuity design with right-censored survival data",
        "abstract": "In this paper the regression discontinuity design is adapted to the survival\nanalysis setting with right-censored data, studied in an intensity based\ncounting process framework. In particular, a local polynomial regression\nversion of the Aalen additive hazards estimator is introduced as an estimator\nof the difference between two covariate dependent cumulative hazard rate\nfunctions. Large-sample theory for this estimator is developed, including\nconfidence intervals that take into account the uncertainty associated with\nbias correction. As is standard in the causality literature, the models and the\ntheory are embedded in the potential outcomes framework. Two general results\nconcerning potential outcomes and the multiplicative hazards model for survival\ndata are presented.",
        "authors": [
            "Emil Aas Stoltenberg"
        ],
        "categories": "stat.ME",
        "published": "2022-10-05T20:23:04Z",
        "updated": "2022-10-05T20:23:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.02504v2",
        "title": "Bikeability and the induced demand for cycling",
        "abstract": "To what extent is the volume of urban bicycle traffic affected by the\nprovision of bicycle infrastructure? In this study, we exploit a large dataset\nof observed bicycle trajectories in combination with a fine-grained\nrepresentation of the Copenhagen bicycle-relevant network. We apply a novel\nmodel for bicyclists' choice of route from origin to destination that takes the\ncomplete network into account. This enables us to determine bicyclists'\npreferences for a range of infrastructure and land-use types. We use the\nestimated preferences to compute a subjective cost of bicycle travel, which we\ncorrelate with the number of bicycle trips across a large number of\norigin-destination pairs. Simulations suggest that the extensive Copenhagen\nbicycle lane network has caused the number of bicycle trips and the bicycle\nkilometers traveled to increase by 60% and 90%, respectively, compared with a\ncounterfactual without the bicycle lane network. This translates into an annual\nbenefit of EUR 0.4M per km of bicycle lane owing to changes in subjective\ntravel cost, health, and accidents. Our results thus strongly support the\nprovision of bicycle infrastructure.",
        "authors": [
            "Mogens Fosgerau",
            "Miroslawa Lukawska",
            "Mads Paulsen",
            "Thomas Kj\u00e6r Rasmussen"
        ],
        "categories": "econ.EM",
        "published": "2022-10-05T18:42:21Z",
        "updated": "2022-12-05T16:44:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.01938v6",
        "title": "Probability of Causation with Sample Selection: A Reanalysis of the Impacts of J\u00f3venes en Acci\u00f3n on Formality",
        "abstract": "This paper identifies the probability of causation when there is sample\nselection. We show that the probability of causation is partially identified\nfor individuals who are always observed regardless of treatment status and\nderive sharp bounds under three increasingly restrictive sets of assumptions.\nThe first set imposes an exogenous treatment and a monotone sample selection\nmechanism. To tighten these bounds, the second set also imposes the monotone\ntreatment response assumption, while the third set additionally imposes a\nstochastic dominance assumption. Finally, we use experimental data from the\nColombian job training program J\\'ovenes en Acci\\'on to empirically illustrate\nour approach's usefulness. We find that, among always-employed women, at least\n10.2% and at most 13.4% transitioned to the formal labor market because of the\nprogram. However, our 90%-confidence region does not reject the null hypothesis\nthat the lower bound is equal to zero.",
        "authors": [
            "Vitor Possebom",
            "Flavio Riva"
        ],
        "categories": "econ.EM",
        "published": "2022-10-04T22:20:58Z",
        "updated": "2024-07-03T18:38:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.01300v1",
        "title": "Revealing Unobservables by Deep Learning: Generative Element Extraction Networks (GEEN)",
        "abstract": "Latent variable models are crucial in scientific research, where a key\nvariable, such as effort, ability, and belief, is unobserved in the sample but\nneeds to be identified. This paper proposes a novel method for estimating\nrealizations of a latent variable $X^*$ in a random sample that contains its\nmultiple measurements. With the key assumption that the measurements are\nindependent conditional on $X^*$, we provide sufficient conditions under which\nrealizations of $X^*$ in the sample are locally unique in a class of\ndeviations, which allows us to identify realizations of $X^*$. To the best of\nour knowledge, this paper is the first to provide such identification in\nobservation. We then use the Kullback-Leibler distance between the two\nprobability densities with and without the conditional independence as the loss\nfunction to train a Generative Element Extraction Networks (GEEN) that maps\nfrom the observed measurements to realizations of $X^*$ in the sample. The\nsimulation results imply that this proposed estimator works quite well and the\nestimated values are highly correlated with realizations of $X^*$. Our\nestimator can be applied to a large class of latent variable models and we\nexpect it will change how people deal with latent variables.",
        "authors": [
            "Yingyao Hu",
            "Yang Liu",
            "Jiaxiong Yao"
        ],
        "categories": "stat.ML",
        "published": "2022-10-04T01:09:05Z",
        "updated": "2022-10-04T01:09:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.01282v3",
        "title": "Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees",
        "abstract": "We consider the task of estimating a structural model of dynamic decisions by\na human agent based upon the observable history of implemented actions and\nvisited states. This problem has an inherent nested structure: in the inner\nproblem, an optimal policy for a given reward function is identified while in\nthe outer problem, a measure of fit is maximized. Several approaches have been\nproposed to alleviate the computational burden of this nested-loop structure,\nbut these methods still suffer from high complexity when the state space is\neither discrete with large cardinality or continuous in high dimensions. Other\napproaches in the inverse reinforcement learning (IRL) literature emphasize\npolicy estimation at the expense of reduced reward estimation accuracy. In this\npaper we propose a single-loop estimation algorithm with finite time guarantees\nthat is equipped to deal with high-dimensional state spaces without\ncompromising reward estimation accuracy. In the proposed algorithm, each policy\nimprovement step is followed by a stochastic gradient step for likelihood\nmaximization. We show that the proposed algorithm converges to a stationary\nsolution with a finite-time guarantee. Further, if the reward is parameterized\nlinearly, we show that the algorithm approximates the maximum likelihood\nestimator sublinearly. Finally, by using robotics control problems in MuJoCo\nand their transfer settings, we show that the proposed algorithm achieves\nsuperior performance compared with other IRL and imitation learning benchmarks.",
        "authors": [
            "Siliang Zeng",
            "Mingyi Hong",
            "Alfredo Garcia"
        ],
        "categories": "cs.LG",
        "published": "2022-10-04T00:11:38Z",
        "updated": "2024-03-01T18:31:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.01179v3",
        "title": "Reconciling econometrics with continuous maximum-entropy network models",
        "abstract": "In the study of economic networks, econometric approaches interpret the\ntraditional Gravity Model specification as the expected link weight coming from\na probability distribution whose functional form can be chosen arbitrarily,\nwhile statistical-physics approaches construct maximum-entropy distributions of\nweighted graphs, constrained to satisfy a given set of measurable network\nproperties. In a recent, companion paper, we integrated the two approaches and\napplied them to the World Trade Web, i.e. the network of international trade\namong world countries. While the companion paper dealt only with\ndiscrete-valued link weights, the present paper extends the theoretical\nframework to continuous-valued link weights. In particular, we construct two\nbroad classes of maximum-entropy models, namely the integrated and the\nconditional ones, defined by different criteria to derive and combine the\nprobabilistic rules for placing links and loading them with weights. In the\nintegrated models, both rules follow from a single, constrained optimization of\nthe continuous Kullback-Leibler divergence; in the conditional models, the two\nrules are disentangled and the functional form of the weight distribution\nfollows from a conditional, optimization procedure. After deriving the general\nfunctional form of the two classes, we turn each of them into a proper family\nof econometric models via a suitable identification of the econometric function\nrelating the corresponding, expected link weights to macroeconomic factors.\nAfter testing the two classes of models on World Trade Web data, we discuss\ntheir strengths and weaknesses.",
        "authors": [
            "Marzio Di Vece",
            "Diego Garlaschelli",
            "Tiziano Squartini"
        ],
        "categories": "physics.soc-ph",
        "published": "2022-10-03T18:52:46Z",
        "updated": "2022-12-20T05:31:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.00624v4",
        "title": "Conditional Distribution Model Specification Testing Using Chi-Square Goodness-of-Fit Tests",
        "abstract": "This paper introduces chi-square goodness-of-fit tests to check for\nconditional distribution model specification. The data is cross-classified\naccording to the Rosenblatt transform of the dependent variable and the\nexplanatory variables, resulting in a contingency table with expected joint\nfrequencies equal to the product of the row and column marginals, which are\nindependent of the model parameters. The test statistics assess whether the\ndifference between observed and expected frequencies is due to chance. We\npropose three types of test statistics: the classical trinity of tests based on\nthe likelihood of grouped data, and two statistics based on the efficient raw\ndata estimator -- namely, a Chernoff-Lehmann and a generalized Wald statistic.\nThe asymptotic distribution of these statistics is invariant to\nsample-dependent partitions. Monte Carlo experiments demonstrate the good\nperformance of the proposed tests.",
        "authors": [
            "Miguel A. Delgado",
            "Julius Vainora"
        ],
        "categories": "econ.EM",
        "published": "2022-10-02T20:52:15Z",
        "updated": "2023-09-22T14:36:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.00563v3",
        "title": "AI-Assisted Discovery of Quantitative and Formal Models in Social Science",
        "abstract": "In social science, formal and quantitative models, such as ones describing\neconomic growth and collective action, are used to formulate mechanistic\nexplanations, provide predictions, and uncover questions about observed\nphenomena. Here, we demonstrate the use of a machine learning system to aid the\ndiscovery of symbolic models that capture nonlinear and dynamical relationships\nin social science datasets. By extending neuro-symbolic methods to find compact\nfunctions and differential equations in noisy and longitudinal data, we show\nthat our system can be used to discover interpretable models from real-world\ndata in economics and sociology. Augmenting existing workflows with symbolic\nregression can help uncover novel relationships and explore counterfactual\nmodels during the scientific process. We propose that this AI-assisted\nframework can bridge parametric and non-parametric models commonly employed in\nsocial science research by systematically exploring the space of nonlinear\nmodels and enabling fine-grained control over expressivity and\ninterpretability.",
        "authors": [
            "Julia Balla",
            "Sihao Huang",
            "Owen Dugan",
            "Rumen Dangovski",
            "Marin Soljacic"
        ],
        "categories": "cs.SC",
        "published": "2022-10-02T16:25:47Z",
        "updated": "2023-08-16T17:45:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.00463v1",
        "title": "Large-Scale Allocation of Personalized Incentives",
        "abstract": "We consider a regulator willing to drive individual choices towards\nincreasing social welfare by providing incentives to a large population of\nindividuals.\n  For that purpose, we formalize and solve the problem of finding an optimal\npersonalized-incentive policy: optimal in the sense that it maximizes social\nwelfare under an incentive budget constraint, personalized in the sense that\nthe incentives proposed depend on the alternatives available to each\nindividual, as well as her preferences.\n  We propose a polynomial time approximation algorithm that computes a policy\nwithin few seconds and we analytically prove that it is boundedly close to the\noptimum.\n  We then extend the problem to efficiently calculate the Maximum Social\nWelfare Curve, which gives the maximum social welfare achievable for a range of\nincentive budgets (not just one value).\n  This curve is a valuable practical tool for the regulator to determine the\nright incentive budget to invest.\n  Finally, we simulate a large-scale application to mode choice in a French\ndepartment (about 200 thousands individuals) and illustrate the effectiveness\nof the proposed personalized-incentive policy in reducing CO2 emissions.",
        "authors": [
            "Lucas Javaudin",
            "Andrea Araldo",
            "Andr\u00e9 de Palma"
        ],
        "categories": "econ.EM",
        "published": "2022-10-02T08:45:14Z",
        "updated": "2022-10-02T08:45:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.00362v3",
        "title": "Yurinskii's Coupling for Martingales",
        "abstract": "Yurinskii's coupling is a popular theoretical tool for non-asymptotic\ndistributional analysis in mathematical statistics and applied probability,\noffering a Gaussian strong approximation with an explicit error bound under\neasily verifiable conditions. Originally stated in $\\ell^2$-norm for sums of\nindependent random vectors, it has recently been extended both to the\n$\\ell^p$-norm, for $1 \\leq p \\leq \\infty$, and to vector-valued martingales in\n$\\ell^2$-norm, under some strong conditions. We present as our main result a\nYurinskii coupling for approximate martingales in $\\ell^p$-norm, under\nsubstantially weaker conditions than those previously imposed. Our formulation\nfurther allows for the coupling variable to follow a more general Gaussian\nmixture distribution, and we provide a novel third-order coupling method which\ngives tighter approximations in certain settings. We specialize our main result\nto mixingales, martingales, and independent data, and derive uniform Gaussian\nmixture strong approximations for martingale empirical processes. Applications\nto nonparametric partitioning-based and local polynomial regression procedures\nare provided, alongside central limit theorems for high-dimensional martingale\nvectors.",
        "authors": [
            "Matias D. Cattaneo",
            "Ricardo P. Masini",
            "William G. Underwood"
        ],
        "categories": "math.ST",
        "published": "2022-10-01T20:40:52Z",
        "updated": "2024-09-23T09:48:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.15212v1",
        "title": "A Posteriori Risk Classification and Ratemaking with Random Effects in the Mixture-of-Experts Model",
        "abstract": "A well-designed framework for risk classification and ratemaking in\nautomobile insurance is key to insurers' profitability and risk management,\nwhile also ensuring that policyholders are charged a fair premium according to\ntheir risk profile. In this paper, we propose to adapt a flexible regression\nmodel, called the Mixed LRMoE, to the problem of a posteriori risk\nclassification and ratemaking, where policyholder-level random effects are\nincorporated to better infer their risk profile reflected by the claim history.\nWe also develop a stochastic variational Expectation-Conditional-Maximization\nalgorithm for estimating model parameters and inferring the posterior\ndistribution of random effects, which is numerically efficient and scalable to\nlarge insurance portfolios. We then apply the Mixed LRMoE model to a real,\nmultiyear automobile insurance dataset, where the proposed framework is shown\nto offer better fit to data and produce posterior premium which accurately\nreflects policyholders' claim history.",
        "authors": [
            "Spark C. Tseung",
            "Ian Weng Chan",
            "Tsz Chai Fung",
            "Andrei L. Badescu",
            "X. Sheldon Lin"
        ],
        "categories": "stat.AP",
        "published": "2022-09-30T03:53:48Z",
        "updated": "2022-09-30T03:53:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.15422v1",
        "title": "Statistical Inference for Fisher Market Equilibrium",
        "abstract": "Statistical inference under market equilibrium effects has attracted\nincreasing attention recently. In this paper we focus on the specific case of\nlinear Fisher markets. They have been widely use in fair resource allocation of\nfood/blood donations and budget management in large-scale Internet ad auctions.\nIn resource allocation, it is crucial to quantify the variability of the\nresource received by the agents (such as blood banks and food banks) in\naddition to fairness and efficiency properties of the systems. For ad auction\nmarkets, it is important to establish statistical properties of the platform's\nrevenues in addition to their expected values. To this end, we propose a\nstatistical framework based on the concept of infinite-dimensional Fisher\nmarkets. In our framework, we observe a market formed by a finite number of\nitems sampled from an underlying distribution (the \"observed market\") and aim\nto infer several important equilibrium quantities of the underlying long-run\nmarket. These equilibrium quantities include individual utilities, social\nwelfare, and pacing multipliers. Through the lens of sample average\napproximation (SSA), we derive a collection of statistical results and show\nthat the observed market provides useful statistical information of the\nlong-run market. In other words, the equilibrium quantities of the observed\nmarket converge to the true ones of the long-run market with strong statistical\nguarantees. These include consistency, finite sample bounds, asymptotics, and\nconfidence. As an extension, we discuss revenue inference in quasilinear Fisher\nmarkets.",
        "authors": [
            "Luofeng Liao",
            "Yuan Gao",
            "Christian Kroer"
        ],
        "categories": "econ.EM",
        "published": "2022-09-29T15:45:47Z",
        "updated": "2022-09-29T15:45:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.14611v1",
        "title": "With big data come big problems: pitfalls in measuring basis risk for crop index insurance",
        "abstract": "New satellite sensors will soon make it possible to estimate field-level crop\nyields, showing a great potential for agricultural index insurance. This paper\nidentifies an important threat to better insurance from these new technologies:\ndata with many fields and few years can yield downward biased estimates of\nbasis risk, a fundamental metric in index insurance. To demonstrate this bias,\nwe use state-of-the-art satellite-based data on agricultural yields in the US\nand in Kenya to estimate and simulate basis risk. We find a substantive\ndownward bias leading to a systematic overestimation of insurance quality.\n  In this paper, we argue that big data in crop insurance can lead to a new\nsituation where the number of variables $N$ largely exceeds the number of\nobservations $T$. In such a situation where $T\\ll N$, conventional asymptotics\nbreak, as evidenced by the large bias we find in simulations. We show how the\nhigh-dimension, low-sample-size (HDLSS) asymptotics, together with the spiked\ncovariance model, provide a more relevant framework for the $T\\ll N$ case\nencountered in index insurance. More precisely, we derive the asymptotic\ndistribution of the relative share of the first eigenvalue of the covariance\nmatrix, a measure of systematic risk in index insurance. Our formula accurately\napproximates the empirical bias simulated from the satellite data, and provides\na useful tool for practitioners to quantify bias in insurance quality.",
        "authors": [
            "Matthieu Stigler",
            "Apratim Dey",
            "Andrew Hobbs",
            "David Lobell"
        ],
        "categories": "econ.EM",
        "published": "2022-09-29T08:08:04Z",
        "updated": "2022-09-29T08:08:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.14502v5",
        "title": "Fast Inference for Quantile Regression with Tens of Millions of Observations",
        "abstract": "Big data analytics has opened new avenues in economic research, but the\nchallenge of analyzing datasets with tens of millions of observations is\nsubstantial. Conventional econometric methods based on extreme estimators\nrequire large amounts of computing resources and memory, which are often not\nreadily available. In this paper, we focus on linear quantile regression\napplied to \"ultra-large\" datasets, such as U.S. decennial censuses. A fast\ninference framework is presented, utilizing stochastic subgradient descent\n(S-subGD) updates. The inference procedure handles cross-sectional data\nsequentially: (i) updating the parameter estimate with each incoming \"new\nobservation\", (ii) aggregating it as a $\\textit{Polyak-Ruppert}$ average, and\n(iii) computing a pivotal statistic for inference using only a solution path.\nThe methodology draws from time-series regression to create an asymptotically\npivotal statistic through random scaling. Our proposed test statistic is\ncalculated in a fully online fashion and critical values are calculated without\nresampling. We conduct extensive numerical studies to showcase the\ncomputational merits of our proposed inference. For inference problems as large\nas $(n, d) \\sim (10^7, 10^3)$, where $n$ is the sample size and $d$ is the\nnumber of regressors, our method generates new insights, surpassing current\ninference methods in computation. Our method specifically reveals trends in the\ngender gap in the U.S. college wage premium using millions of observations,\nwhile controlling over $10^3$ covariates to mitigate confounding effects.",
        "authors": [
            "Sokbae Lee",
            "Yuan Liao",
            "Myung Hwan Seo",
            "Youngki Shin"
        ],
        "categories": "econ.EM",
        "published": "2022-09-29T01:39:53Z",
        "updated": "2023-10-31T23:42:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.14430v3",
        "title": "Minimax Optimal Kernel Operator Learning via Multilevel Training",
        "abstract": "Learning mappings between infinite-dimensional function spaces has achieved\nempirical success in many disciplines of machine learning, including generative\nmodeling, functional data analysis, causal inference, and multi-agent\nreinforcement learning. In this paper, we study the statistical limit of\nlearning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev\nreproducing kernel Hilbert spaces. We establish the information-theoretic lower\nbound in terms of the Sobolev Hilbert-Schmidt norm and show that a\nregularization that learns the spectral components below the bias contour and\nignores the ones that are above the variance contour can achieve the optimal\nlearning rate. At the same time, the spectral components between the bias and\nvariance contours give us flexibility in designing computationally feasible\nmachine learning algorithms. Based on this observation, we develop a multilevel\nkernel operator learning algorithm that is optimal when learning linear\noperators between infinite-dimensional function spaces.",
        "authors": [
            "Jikai Jin",
            "Yiping Lu",
            "Jose Blanchet",
            "Lexing Ying"
        ],
        "categories": "cs.LG",
        "published": "2022-09-28T21:31:43Z",
        "updated": "2023-07-24T09:15:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.14391v1",
        "title": "The Network Propensity Score: Spillovers, Homophily, and Selection into Treatment",
        "abstract": "I establish primitive conditions for unconfoundedness in a coherent model\nthat features heterogeneous treatment effects, spillovers,\nselection-on-observables, and network formation. I identify average partial\neffects under minimal exchangeability conditions. If social interactions are\nalso anonymous, I derive a three-dimensional network propensity score,\ncharacterize its support conditions, relate it to recent work on network\npseudo-metrics, and study extensions. I propose a two-step semiparametric\nestimator for a random coefficients model which is consistent and\nasymptotically normal as the number and size of the networks grows. I apply my\nestimator to a political participation intervention Uganda and a microfinance\napplication in India.",
        "authors": [
            "Alejandro Sanchez-Becerra"
        ],
        "categories": "econ.EM",
        "published": "2022-09-28T19:31:26Z",
        "updated": "2022-09-28T19:31:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.14748v1",
        "title": "Economic effects of Chile FTAs and an eventual CTPP accession",
        "abstract": "In this article, we show the benefits derived from the Chile-USA (in-force\nJan, 2004) and Chile-China (in-force Oct, 2006) FTA on GDP consumer and\nproducers to conclude that Chile improved its welfare improved after its\nsubscription. From that point, we extrapolate to show the direct and indirect\nbenefits of CTPP accession.",
        "authors": [
            "Vargas Sepulveda",
            "Mauricio \"Pacha\""
        ],
        "categories": "econ.EM",
        "published": "2022-09-28T17:34:13Z",
        "updated": "2022-09-28T17:34:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.14181v5",
        "title": "Linear estimation of average global effects",
        "abstract": "We study the problem of estimating the average causal effect of treating\nevery member of a population, as opposed to none, using an experiment that\ntreats only some. This is the policy-relevant estimand when deciding whether to\nscale up an intervention based on the results of an RCT, for example, but\ndiffers from the usual average treatment effect in the presence of spillovers.\nWe consider both estimation and experimental design given a bound (parametrized\nby $\\eta > 0$) on the rate at which spillovers decay with the ``distance''\nbetween units, defined in a generalized way to encompass spatial and\nquasi-spatial settings, e.g. where the economically relevant concept of\ndistance is a gravity equation. Over all estimators linear in the outcomes and\nall cluster-randomized designs the optimal geometric rate of convergence is\n$n^{-\\frac{1}{2+\\frac{1}{\\eta}}}$, and this rate can be achieved using a\ngeneralized ``Scaling Clusters'' design that we provide. We then introduce the\nadditional assumption, implicit in the OLS estimators used in recent applied\nstudies, that potential outcomes are linear in population treatment\nassignments. These estimators are inconsistent for our estimand, but a refined\nOLS estimator is consistent and rate optimal, and performs better than IPW\nestimators when clusters must be small. Its finite-sample performance can be\nimproved by incorporating prior information about the structure of spillovers.\nAs a robust alternative to the linear approach we also provide a method to\nselect estimator-design pairs that minimize a notion of worst-case risk when\nthe data generating process is unknown. Finally, we provide asymptotically\nvalid inference methods.",
        "authors": [
            "Stefan Faridani",
            "Paul Niehaus"
        ],
        "categories": "econ.EM",
        "published": "2022-09-28T15:54:20Z",
        "updated": "2024-12-11T21:56:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.14737v2",
        "title": "Sentiment Analysis on Inflation after Covid-19",
        "abstract": "We implement traditional machine learning and deep learning methods for\nglobal tweets from 2017-2022 to build a high-frequency measure of the public's\nsentiment index on inflation and analyze its correlation with other online data\nsources such as google trend and market-oriented inflation index. We use\nmanually labeled trigrams to test the prediction performance of several machine\nlearning models(logistic regression,random forest etc.) and choose Bert model\nfor final demonstration. Later, we sum daily tweets' sentiment scores gained\nfrom Bert model to obtain the predicted inflation sentiment index, and we\nfurther analyze the regional and pre/post covid patterns of these inflation\nindexes. Lastly, we take other empirical inflation-related data as references\nand prove that twitter-based inflation sentiment analysis method has an\noutstanding capability to predict inflation. The results suggest that Twitter\ncombined with deep learning methods can be a novel and timely method to utilize\nexisting abundant data sources on inflation expectations and provide daily\nindicators of consumers' perception on inflation.",
        "authors": [
            "Xinyu Li",
            "Zihan Tang"
        ],
        "categories": "econ.EM",
        "published": "2022-09-25T20:37:54Z",
        "updated": "2022-12-04T21:03:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.11970v3",
        "title": "Bayesian Modeling of TVP-VARs Using Regression Trees",
        "abstract": "In light of widespread evidence of parameter instability in macroeconomic\nmodels, many time-varying parameter (TVP) models have been proposed. This paper\nproposes a nonparametric TVP-VAR model using Bayesian additive regression trees\n(BART) that models the TVPs as an unknown function of effect modifiers. The\nnovelty of this model arises from the fact that the law of motion driving the\nparameters is treated nonparametrically. This leads to great flexibility in the\nnature and extent of parameter change, both in the conditional mean and in the\nconditional variance. Parsimony is achieved through adopting nonparametric\nfactor structures and use of shrinkage priors. In an application to US\nmacroeconomic data, we illustrate the use of our model in tracking both the\nevolving nature of the Phillips curve and how the effects of business cycle\nshocks on inflation measures vary nonlinearly with changes in the effect\nmodifiers.",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Gary Koop",
            "James Mitchell"
        ],
        "categories": "econ.EM",
        "published": "2022-09-24T09:40:41Z",
        "updated": "2023-05-05T07:02:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.11840v6",
        "title": "Revisiting the Analysis of Matched-Pair and Stratified Experiments in the Presence of Attrition",
        "abstract": "In this paper we revisit some common recommendations regarding the analysis\nof matched-pair and stratified experimental designs in the presence of\nattrition. Our main objective is to clarify a number of well-known claims about\nthe practice of dropping pairs with an attrited unit when analyzing\nmatched-pair designs. Contradictory advice appears in the literature about\nwhether or not dropping pairs is beneficial or harmful, and stratifying into\nlarger groups has been recommended as a resolution to the issue. To address\nthese claims, we derive the estimands obtained from the difference-in-means\nestimator in a matched-pair design both when the observations from pairs with\nan attrited unit are retained and when they are dropped. We find limited\nevidence to support the claims that dropping pairs helps recover the average\ntreatment effect, but we find that it may potentially help in recovering a\nconvex weighted average of conditional average treatment effects. We report\nsimilar findings for stratified designs when studying the estimands obtained\nfrom a regression of outcomes on treatment with and without strata fixed\neffects.",
        "authors": [
            "Yuehao Bai",
            "Meng Hsuan Hsieh",
            "Jizhou Liu",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2022-09-23T20:06:18Z",
        "updated": "2023-10-18T22:43:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.11837v1",
        "title": "Doubly Fair Dynamic Pricing",
        "abstract": "We study the problem of online dynamic pricing with two types of fairness\nconstraints: a \"procedural fairness\" which requires the proposed prices to be\nequal in expectation among different groups, and a \"substantive fairness\" which\nrequires the accepted prices to be equal in expectation among different groups.\nA policy that is simultaneously procedural and substantive fair is referred to\nas \"doubly fair\". We show that a doubly fair policy must be random to have\nhigher revenue than the best trivial policy that assigns the same price to\ndifferent groups. In a two-group setting, we propose an online learning\nalgorithm for the 2-group pricing problems that achieves $\\tilde{O}(\\sqrt{T})$\nregret, zero procedural unfairness and $\\tilde{O}(\\sqrt{T})$ substantive\nunfairness over $T$ rounds of learning. We also prove two lower bounds showing\nthat these results on regret and unfairness are both information-theoretically\noptimal up to iterated logarithmic factors. To the best of our knowledge, this\nis the first dynamic pricing algorithm that learns to price while satisfying\ntwo fairness constraints at the same time.",
        "authors": [
            "Jianyu Xu",
            "Dan Qiao",
            "Yu-Xiang Wang"
        ],
        "categories": "cs.LG",
        "published": "2022-09-23T20:02:09Z",
        "updated": "2022-09-23T20:02:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.11691v4",
        "title": "Linear multidimensional regression with interactive fixed-effects",
        "abstract": "This paper studies a linear and additively separable model for\nmultidimensional panel data of three or more dimensions with unobserved\ninteractive fixed effects. Two approaches are considered to account for these\nunobserved interactive fixed-effects when estimating coefficients on the\nobserved covariates. First, the model is embedded within the standard two\ndimensional panel framework and restrictions are formed under which the factor\nstructure methods in Bai (2009) lead to consistent estimation of model\nparameters, but at slow rates of convergence. The second approach develops a\nkernel weighted fixed-effects method that is more robust to the\nmultidimensional nature of the problem and can achieve the parametric rate of\nconsistency under certain conditions. Theoretical results and simulations show\nsome benefits to standard two-dimensional panel methods when the structure of\nthe interactive fixed-effect term is known, but also highlight how the kernel\nweighted method performs well without knowledge of this structure. The methods\nare implemented to estimate the demand elasticity for beer.",
        "authors": [
            "Hugo Freeman"
        ],
        "categories": "econ.EM",
        "published": "2022-09-23T16:11:09Z",
        "updated": "2024-08-26T02:33:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.11444v4",
        "title": "Treatment Effects with Multidimensional Unobserved Heterogeneity: Identification of the Marginal Treatment Effect",
        "abstract": "This paper establishes sufficient conditions for the identification of the\nmarginal treatment effects with multivalued treatments. Our model is based on a\nmultinomial choice model with utility maximization. Our MTE generalizes the MTE\ndefined in Heckman and Vytlacil (2005) in binary treatment models. As in the\nbinary case, we can interpret the MTE as the treatment effect for persons who\nare indifferent between two treatments at a particular level. Our MTE enables\none to obtain the treatment effects of those with specific preference orders\nover the choice set. Further, our results can identify other parameters such as\nthe marginal distribution of potential outcomes.",
        "authors": [
            "Toshiki Tsuda"
        ],
        "categories": "econ.EM",
        "published": "2022-09-23T07:06:43Z",
        "updated": "2024-02-01T01:27:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2210.00883v1",
        "title": "Forecasting Cryptocurrencies Log-Returns: a LASSO-VAR and Sentiment Approach",
        "abstract": "Cryptocurrencies have become a trendy topic recently, primarily due to their\ndisruptive potential and reports of unprecedented returns. In addition,\nacademics increasingly acknowledge the predictive power of Social Media in many\nfields and, more specifically, for financial markets and economics. In this\npaper, we leverage the predictive power of Twitter and Reddit sentiment\ntogether with Google Trends indexes and volume to forecast the log returns of\nten cryptocurrencies. Specifically, we consider $Bitcoin$, $Ethereum$,\n$Tether$, $Binance Coin$, $Litecoin$, $Enjin Coin$, $Horizen$, $Namecoin$,\n$Peercoin$, and $Feathercoin$. We evaluate the performance of LASSO-VAR using\ndaily data from January 2018 to January 2022. In a 30 days recursive forecast,\nwe can retrieve the correct direction of the actual series more than 50% of the\ntime. We compare this result with the main benchmarks, and we see a 10%\nimprovement in Mean Directional Accuracy (MDA). The use of sentiment and\nattention variables as predictors increase significantly the forecast accuracy\nin terms of MDA but not in terms of Root Mean Squared Errors. We perform a\nGranger causality test using a post-double LASSO selection for high-dimensional\nVARs. Results show no \"causality\" from Social Media sentiment to\ncryptocurrencies returns",
        "authors": [
            "Federico D'Amario",
            "Milos Ciganovic"
        ],
        "categories": "q-fin.ST",
        "published": "2022-09-22T14:26:32Z",
        "updated": "2022-09-22T14:26:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.10841v1",
        "title": "Multiscale Comparison of Nonparametric Trend Curves",
        "abstract": "We develop new econometric methods for the comparison of nonparametric time\ntrends. In many applications, practitioners are interested in whether the\nobserved time series all have the same time trend. Moreover, they would often\nlike to know which trends are different and in which time intervals they\ndiffer. We design a multiscale test to formally approach these questions.\nSpecifically, we develop a test which allows to make rigorous confidence\nstatements about which time trends are different and where (that is, in which\ntime intervals) they differ. Based on our multiscale test, we further develop a\nclustering algorithm which allows to cluster the observed time series into\ngroups with the same trend. We derive asymptotic theory for our test and\nclustering methods. The theory is complemented by a simulation study and two\napplications to GDP growth data and house pricing data.",
        "authors": [
            "Marina Khismatullina",
            "Michael Vogt"
        ],
        "categories": "econ.EM",
        "published": "2022-09-22T08:05:16Z",
        "updated": "2022-09-22T08:05:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.10664v1",
        "title": "Modelling the Frequency of Home Deliveries: An Induced Travel Demand Contribution of Aggrandized E-shopping in Toronto during COVID-19 Pandemics",
        "abstract": "The COVID-19 pandemic dramatically catalyzed the proliferation of e-shopping.\nThe dramatic growth of e-shopping will undoubtedly cause significant impacts on\ntravel demand. As a result, transportation modeller's ability to model\ne-shopping demand is becoming increasingly important. This study developed\nmodels to predict household' weekly home delivery frequencies. We used both\nclassical econometric and machine learning techniques to obtain the best model.\nIt is found that socioeconomic factors such as having an online grocery\nmembership, household members' average age, the percentage of male household\nmembers, the number of workers in the household and various land use factors\ninfluence home delivery demand. This study also compared the interpretations\nand performances of the machine learning models and the classical econometric\nmodel. Agreement is found in the variable's effects identified through the\nmachine learning and econometric models. However, with similar recall accuracy,\nthe ordered probit model, a classical econometric model, can accurately predict\nthe aggregate distribution of household delivery demand. In contrast, both\nmachine learning models failed to match the observed distribution.",
        "authors": [
            "Yicong Liu",
            "Kaili Wang",
            "Patrick Loa",
            "Khandker Nurul Habib"
        ],
        "categories": "econ.EM",
        "published": "2022-09-21T21:18:25Z",
        "updated": "2022-09-21T21:18:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.10128v3",
        "title": "Efficient Integrated Volatility Estimation in the Presence of Infinite Variation Jumps via Debiased Truncated Realized Variations",
        "abstract": "Statistical inference for stochastic processes based on high-frequency\nobservations has been an active research area for more than two decades. One of\nthe most well-known and widely studied problems has been the estimation of the\nquadratic variation of the continuous component of an It\\^o semimartingale with\njumps. Several rate- and variance-efficient estimators have been proposed in\nthe literature when the jump component is of bounded variation. However, to\ndate, very few methods can deal with jumps of unbounded variation. By\ndeveloping new high-order expansions of the truncated moments of a locally\nstable L\\'evy process, we propose a new rate- and variance-efficient volatility\nestimator for a class of It\\^o semimartingales whose jumps behave locally like\nthose of a stable L\\'evy process with Blumenthal-Getoor index $Y\\in (1,8/5)$\n(hence, of unbounded variation). The proposed method is based on a two-step\ndebiasing procedure for the truncated realized quadratic variation of the\nprocess and can also cover the case $Y<1$. Our Monte Carlo experiments indicate\nthat the method outperforms other efficient alternatives in the literature in\nthe setting covered by our theoretical framework.",
        "authors": [
            "B. Cooper Boniece",
            "Jos\u00e9 E. Figueroa-L\u00f3pez",
            "Yuchen Han"
        ],
        "categories": "econ.EM",
        "published": "2022-09-21T05:36:17Z",
        "updated": "2024-04-20T11:53:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.09810v2",
        "title": "The boosted HP filter is more general than you might think",
        "abstract": "The global financial crisis and Covid recession have renewed discussion\nconcerning trend-cycle discovery in macroeconomic data, and boosting has\nrecently upgraded the popular HP filter to a modern machine learning device\nsuited to data-rich and rapid computational environments. This paper extends\nboosting's trend determination capability to higher order integrated processes\nand time series with roots that are local to unity. The theory is established\nby understanding the asymptotic effect of boosting on a simple exponential\nfunction. Given a universe of time series in FRED databases that exhibit\nvarious dynamic patterns, boosting timely captures downturns at crises and\nrecoveries that follow.",
        "authors": [
            "Ziwei Mei",
            "Peter C. B. Phillips",
            "Zhentao Shi"
        ],
        "categories": "econ.EM",
        "published": "2022-09-20T15:58:37Z",
        "updated": "2024-04-12T18:36:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.09354v1",
        "title": "A Dynamic Stochastic Block Model for Multi-Layer Networks",
        "abstract": "We propose a flexible stochastic block model for multi-layer networks, where\nlayer-specific hidden Markov-chain processes drive the changes in the formation\nof communities. The changes in block membership of a node in a given layer may\nbe influenced by its own past membership in other layers. This allows for\nclustering overlap, clustering decoupling, or more complex relationships\nbetween layers including settings of unidirectional, or bidirectional, block\ncausality. We cope with the overparameterization issue of a saturated\nspecification by assuming a Multi-Laplacian prior distribution within a\nBayesian framework. Data augmentation and Gibbs sampling are used to make the\ninference problem more tractable. Through simulations, we show that the\nstandard linear models are not able to detect the block causality under the\ngreat majority of scenarios. As an application to trade networks, we show that\nour model provides a unified framework including community detection and\nGravity equation. The model is used to study the causality between trade\nagreements and trade looking at the global topological properties of the\nnetworks as opposed to the main existent approaches which focus on local\nbilateral relationships. We are able to provide new evidence of unidirectional\ncausality from the free trade agreements network to the non-observable trade\nbarriers network structure for 159 countries in the period 1995-2017.",
        "authors": [
            "Ovielt Baltodano L\u00f3pez",
            "Roberto Casarin"
        ],
        "categories": "stat.ME",
        "published": "2022-09-19T21:23:17Z",
        "updated": "2022-09-19T21:23:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.09077v2",
        "title": "Statistical Treatment Rules under Social Interaction",
        "abstract": "In this paper we study treatment assignment rules in the presence of social\ninteraction. We construct an analytical framework under the anonymous\ninteraction assumption, where the decision problem becomes choosing a treatment\nfraction. We propose a multinomial empirical success (MES) rule that includes\nthe empirical success rule of Manski (2004) as a special case. We investigate\nthe non-asymptotic bounds of the expected utility based on the MES rule.\nFinally, we prove that the MES rule achieves the asymptotic optimality with the\nminimax regret criterion.",
        "authors": [
            "Seungjin Han",
            "Julius Owusu",
            "Youngki Shin"
        ],
        "categories": "econ.EM",
        "published": "2022-09-19T15:07:25Z",
        "updated": "2022-11-09T22:09:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.08885v2",
        "title": "Causal Effect Estimation with Global Probabilistic Forecasting: A Case Study of the Impact of Covid-19 Lockdowns on Energy Demand",
        "abstract": "The electricity industry is heavily implementing smart grid technologies to\nimprove reliability, availability, security, and efficiency. This\nimplementation needs technological advancements, the development of standards\nand regulations, as well as testing and planning. Smart grid load forecasting\nand management are critical for reducing demand volatility and improving the\nmarket mechanism that connects generators, distributors, and retailers. During\npolicy implementations or external interventions, it is necessary to analyse\nthe uncertainty of their impact on the electricity demand to enable a more\naccurate response of the system to fluctuating demand. This paper analyses the\nuncertainties of external intervention impacts on electricity demand. It\nimplements a framework that combines probabilistic and global forecasting\nmodels using a deep learning approach to estimate the causal impact\ndistribution of an intervention. The causal effect is assessed by predicting\nthe counterfactual distribution outcome for the affected instances and then\ncontrasting it to the real outcomes. We consider the impact of Covid-19\nlockdowns on energy usage as a case study to evaluate the non-uniform effect of\nthis intervention on the electricity demand distribution. We could show that\nduring the initial lockdowns in Australia and some European countries, there\nwas often a more significant decrease in the troughs than in the peaks, while\nthe mean remained almost unaffected.",
        "authors": [
            "Ankitha Nandipura Prasanna",
            "Priscila Grecov",
            "Angela Dieyu Weng",
            "Christoph Bergmeir"
        ],
        "categories": "cs.LG",
        "published": "2022-09-19T09:39:29Z",
        "updated": "2022-10-20T06:59:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.08793v1",
        "title": "A Generalized Argmax Theorem with Applications",
        "abstract": "The argmax theorem is a useful result for deriving the limiting distribution\nof estimators in many applications. The conclusion of the argmax theorem states\nthat the argmax of a sequence of stochastic processes converges in distribution\nto the argmax of a limiting stochastic process. This paper generalizes the\nargmax theorem to allow the maximization to take place over a sequence of\nsubsets of the domain. If the sequence of subsets converges to a limiting\nsubset, then the conclusion of the argmax theorem continues to hold. We\ndemonstrate the usefulness of this generalization in three applications:\nestimating a structural break, estimating a parameter on the boundary of the\nparameter space, and estimating a weakly identified parameter. The generalized\nargmax theorem simplifies the proofs for existing results and can be used to\nprove new results in these literatures.",
        "authors": [
            "Gregory Cox"
        ],
        "categories": "econ.EM",
        "published": "2022-09-19T06:53:56Z",
        "updated": "2022-09-19T06:53:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.08380v2",
        "title": "A Structural Model for Detecting Communities in Networks",
        "abstract": "The objective of this paper is to identify and analyze the response actions\nof a set of players embedded in sub-networks in the context of interaction and\nlearning. We characterize strategic network formation as a static game of\ninteractions where players maximize their utility depending on the connections\nthey establish and multiple interdependent actions that permit group-specific\nparameters of players. It is challenging to apply this type of model to\nreal-life scenarios for two reasons: The computation of the Bayesian Nash\nEquilibrium is highly demanding and the identification of social influence\nrequires the use of excluded variables that are oftentimes unavailable. Based\non the theoretical proposal, we propose a set of simulant equations and discuss\nthe identification of the social interaction effect employing multi-modal\nnetwork autoregressive.",
        "authors": [
            "Alex Centeno"
        ],
        "categories": "econ.TH",
        "published": "2022-09-17T17:58:01Z",
        "updated": "2022-10-27T13:28:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.07330v4",
        "title": "Best Arm Identification with Contextual Information under a Small Gap",
        "abstract": "We study the best-arm identification (BAI) problem with a fixed budget and\ncontextual (covariate) information. In each round of an adaptive experiment,\nafter observing contextual information, we choose a treatment arm using past\nobservations and current context. Our goal is to identify the best treatment\narm, which is a treatment arm with the maximal expected reward marginalized\nover the contextual distribution, with a minimal probability of\nmisidentification. In this study, we consider a class of nonparametric bandit\nmodels that converge to location-shift models when the gaps go to zero. First,\nwe derive lower bounds of the misidentification probability for a certain class\nof strategies and bandit models (probabilistic models of potential outcomes)\nunder a small-gap regime. A small-gap regime is a situation where gaps of the\nexpected rewards between the best and suboptimal treatment arms go to zero,\nwhich corresponds to one of the worst cases in identifying the best treatment\narm. We then develop the ``Random Sampling (RS)-Augmented Inverse Probability\nweighting (AIPW) strategy,'' which is asymptotically optimal in the sense that\nthe probability of misidentification under the strategy matches the lower bound\nwhen the budget goes to infinity in the small-gap regime. The RS-AIPW strategy\nconsists of the RS rule tracking a target sample allocation ratio and the\nrecommendation rule using the AIPW estimator.",
        "authors": [
            "Masahiro Kato",
            "Masaaki Imaizumi",
            "Takuya Ishihara",
            "Toru Kitagawa"
        ],
        "categories": "cs.LG",
        "published": "2022-09-15T14:38:47Z",
        "updated": "2023-01-04T18:56:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.07111v2",
        "title": "$\u03c1$-GNF: A Copula-based Sensitivity Analysis to Unobserved Confounding Using Normalizing Flows",
        "abstract": "We propose a novel sensitivity analysis to unobserved confounding in\nobservational studies using copulas and normalizing flows. Using the idea of\ninterventional equivalence of structural causal models, we develop $\\rho$-GNF\n($\\rho$-graphical normalizing flow), where $\\rho{\\in}[-1,+1]$ is a bounded\nsensitivity parameter. This parameter represents the back-door non-causal\nassociation due to unobserved confounding, and which is encoded with a Gaussian\ncopula. In other words, the $\\rho$-GNF enables scholars to estimate the average\ncausal effect (ACE) as a function of $\\rho$, while accounting for various\nassumed strengths of the unobserved confounding. The output of the $\\rho$-GNF\nis what we denote as the $\\rho_{curve}$ that provides the bounds for the ACE\ngiven an interval of assumed $\\rho$ values. In particular, the $\\rho_{curve}$\nenables scholars to identify the confounding strength required to nullify the\nACE, similar to other sensitivity analysis methods (e.g., the E-value).\nLeveraging on experiments from simulated and real-world data, we show the\nbenefits of $\\rho$-GNF. One benefit is that the $\\rho$-GNF uses a Gaussian\ncopula to encode the distribution of the unobserved causes, which is commonly\nused in many applied settings. This distributional assumption produces narrower\nACE bounds compared to other popular sensitivity analysis methods.",
        "authors": [
            "Sourabh Balgi",
            "Jose M. Pe\u00f1a",
            "Adel Daoud"
        ],
        "categories": "stat.ME",
        "published": "2022-09-15T07:49:23Z",
        "updated": "2024-08-22T04:23:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.06870v2",
        "title": "Do shared e-scooter services cause traffic accidents? Evidence from six European countries",
        "abstract": "We estimate the causal effect of shared e-scooter services on traffic\naccidents by exploiting variation in availability of e-scooter services,\ninduced by the staggered rollout across 93 cities in six countries.\nPolice-reported accidents in the average month increased by around 8.2% after\nshared e-scooters were introduced. For cities with limited cycling\ninfrastructure and where mobility relies heavily on cars, estimated effects are\nlargest. In contrast, no effects are detectable in cities with high bike-lane\ndensity. This heterogeneity suggests that public policy can play a crucial role\nin mitigating accidents related to e-scooters and, more generally, to changes\nin urban mobility.",
        "authors": [
            "Cannon Cloud",
            "Simon He\u00df",
            "Johannes Kasinger"
        ],
        "categories": "econ.EM",
        "published": "2022-09-14T18:32:34Z",
        "updated": "2022-09-16T10:16:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.06631v1",
        "title": "Sample Fit Reliability",
        "abstract": "Researchers frequently test and improve model fit by holding a sample\nconstant and varying the model. We propose methods to test and improve sample\nfit by holding a model constant and varying the sample. Much as the bootstrap\nis a well-known method to re-sample data and estimate the uncertainty of the\nfit of parameters in a model, we develop Sample Fit Reliability (SFR) as a set\nof computational methods to re-sample data and estimate the reliability of the\nfit of observations in a sample. SFR uses Scoring to assess the reliability of\neach observation in a sample, Annealing to check the sensitivity of results to\nremoving unreliable data, and Fitting to re-weight observations for more robust\nanalysis. We provide simulation evidence to demonstrate the advantages of using\nSFR, and we replicate three empirical studies with treatment effects to\nillustrate how SFR reveals new insights about each study.",
        "authors": [
            "Gabriel Okasa",
            "Kenneth A. Younge"
        ],
        "categories": "econ.EM",
        "published": "2022-09-14T13:31:23Z",
        "updated": "2022-09-14T13:31:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.06086v1",
        "title": "Carbon Monitor-Power: near-real-time monitoring of global power generation on hourly to daily scales",
        "abstract": "We constructed a frequently updated, near-real-time global power generation\ndataset: Carbon Monitor-Power since January, 2016 at national levels with\nnear-global coverage and hourly-to-daily time resolution. The data presented\nhere are collected from 37 countries across all continents for eight source\ngroups, including three types of fossil sources (coal, gas, and oil), nuclear\nenergy and four groups of renewable energy sources (solar energy, wind energy,\nhydro energy and other renewables including biomass, geothermal, etc.). The\nglobal near-real-time power dataset shows the dynamics of the global power\nsystem, including its hourly, daily, weekly and seasonal patterns as influenced\nby daily periodical activities, weekends, seasonal cycles, regular and\nirregular events (i.e., holidays) and extreme events (i.e., the COVID-19\npandemic). The Carbon Monitor-Power dataset reveals that the COVID-19 pandemic\ncaused strong disruptions in some countries (i.e., China and India), leading to\na temporary or long-lasting shift to low carbon intensity, while it had only\nlittle impact in some other countries (i.e., Australia). This dataset offers a\nlarge range of opportunities for power-related scientific research and\npolicy-making.",
        "authors": [
            "Biqing Zhu",
            "Xuanren Song",
            "Zhu Deng",
            "Wenli Zhao",
            "Da Huo",
            "Taochun Sun",
            "Piyu Ke",
            "Duo Cui",
            "Chenxi Lu",
            "Haiwang Zhong",
            "Chaopeng Hong",
            "Jian Qiu",
            "Steven J. Davis",
            "Pierre Gentine",
            "Philippe Ciais",
            "Zhu Liu"
        ],
        "categories": "physics.data-an",
        "published": "2022-09-13T15:35:34Z",
        "updated": "2022-09-13T15:35:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.05914v1",
        "title": "Estimation of Average Derivatives of Latent Regressors: With an Application to Inference on Buffer-Stock Saving",
        "abstract": "This paper proposes a density-weighted average derivative estimator based on\ntwo noisy measures of a latent regressor. Both measures have classical errors\nwith possibly asymmetric distributions. We show that the proposed estimator\nachieves the root-n rate of convergence, and derive its asymptotic normal\ndistribution for statistical inference. Simulation studies demonstrate\nexcellent small-sample performance supporting the root-n asymptotic normality.\nBased on the proposed estimator, we construct a formal test on the sub-unity of\nthe marginal propensity to consume out of permanent income (MPCP) under a\nnonparametric consumption model and a permanent-transitory model of income\ndynamics with nonparametric distribution. Applying the test to four recent\nwaves of U.S. Panel Study of Income Dynamics (PSID), we reject the null\nhypothesis of the unit MPCP in favor of a sub-unit MPCP, supporting the\nbuffer-stock model of saving.",
        "authors": [
            "Hao Dong",
            "Yuya Sasaki"
        ],
        "categories": "econ.EM",
        "published": "2022-09-13T11:57:37Z",
        "updated": "2022-09-13T11:57:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.05767v1",
        "title": "Bayesian Functional Emulation of CO2 Emissions on Future Climate Change Scenarios",
        "abstract": "We propose a statistical emulator for a climate-economy deterministic\nintegrated assessment model ensemble, based on a functional regression\nframework. Inference on the unknown parameters is carried out through a mixed\neffects hierarchical model using a fully Bayesian framework with a prior\ndistribution on the vector of all parameters. We also suggest an autoregressive\nparameterization of the covariance matrix of the error, with matching marginal\nprior. In this way, we allow for a functional framework for the discretized\noutput of the simulators that allows their time continuous evaluation.",
        "authors": [
            "Luca Aiello",
            "Matteo Fontana",
            "Alessandra Guglielmi"
        ],
        "categories": "stat.ME",
        "published": "2022-09-13T07:07:49Z",
        "updated": "2022-09-13T07:07:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.05563v1",
        "title": "Testing Endogeneity of Spatial Weights Matrices in Spatial Dynamic Panel Data Models",
        "abstract": "I propose Robust Rao's Score (RS) test statistic to determine endogeneity of\nspatial weights matrices in a spatial dynamic panel data (SDPD) model (Qu, Lee,\nand Yu, 2017). I firstly introduce the bias-corrected score function since the\nscore function is not centered around zero due to the two-way fixed effects. I\nfurther adjust score functions to rectify the over-rejection of the null\nhypothesis under a presence of local misspecification in contemporaneous\ndependence over space, dependence over time, or spatial time dependence. I then\nderive the explicit forms of our test statistic. A Monte Carlo simulation\nsupports the analytics and shows nice finite sample properties. Finally, an\nempirical illustration is provided using data from Penn World Table version\n6.1.",
        "authors": [
            "Jieun Lee"
        ],
        "categories": "econ.EM",
        "published": "2022-09-12T19:35:56Z",
        "updated": "2022-09-12T19:35:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.05562v1",
        "title": "Evidence and Strategy on Economic Distance in Spatially Augmented Solow-Swan Growth Model",
        "abstract": "Economists' interests in growth theory have a very long history (Harrod,\n1939; Domar, 1946; Solow, 1956; Swan 1956; Mankiw, Romer, and Weil, 1992).\nRecently, starting from the neoclassical growth model, Ertur and Koch (2007)\ndeveloped the spatially augmented Solow-Swan growth model with the exogenous\nspatial weights matrices ($W$). While the exogenous $W$ assumption could be\ntrue only with the geographical/physical distance, it may not be true when\neconomic/social distances play a role. Using Penn World Table version 7.1,\nwhich covers year 1960-2010, I conducted the robust Rao's score test (Bera,\nDogan, and Taspinar, 2018) to determine if $W$ is endogeonus and used the\nmaximum likelihood estimation (Qu and Lee, 2015). The key finding is that the\nsignificance and positive effects of physical capital externalities and spatial\nexternalities (technological interdependence) in Ertur and Koch (2007) were no\nlonger found with the exogenous $W$, but still they were with the endogenous\n$W$ models. I also found an empirical strategy on which economic distance to\nuse when the data recently has been under heavy shocks of the worldwide\nfinancial crises during year 1996-2010.",
        "authors": [
            "Jieun Lee"
        ],
        "categories": "econ.EM",
        "published": "2022-09-12T19:35:07Z",
        "updated": "2022-09-12T19:35:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.04770v2",
        "title": "Testing the martingale difference hypothesis in high dimension",
        "abstract": "In this paper, we consider testing the martingale difference hypothesis for\nhigh-dimensional time series. Our test is built on the sum of squares of the\nelement-wise max-norm of the proposed matrix-valued nonlinear dependence\nmeasure at different lags. To conduct the inference, we approximate the null\ndistribution of our test statistic by Gaussian approximation and provide a\nsimulation-based approach to generate critical values. The asymptotic behavior\nof the test statistic under the alternative is also studied. Our approach is\nnonparametric as the null hypothesis only assumes the time series concerned is\nmartingale difference without specifying any parametric forms of its\nconditional moments. As an advantage of Gaussian approximation, our test is\nrobust to the cross-series dependence of unknown magnitude. To the best of our\nknowledge, this is the first valid test for the martingale difference\nhypothesis that not only allows for large dimension but also captures nonlinear\nserial dependence. The practical usefulness of our test is illustrated via\nsimulation and a real data analysis. The test is implemented in a user-friendly\nR-function.",
        "authors": [
            "Jinyuan Chang",
            "Qing Jiang",
            "Xiaofeng Shao"
        ],
        "categories": "econ.EM",
        "published": "2022-09-11T02:59:39Z",
        "updated": "2022-10-01T02:10:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.04329v5",
        "title": "Heterogeneous Treatment Effect Bounds under Sample Selection with an Application to the Effects of Social Media on Political Polarization",
        "abstract": "We propose a method for estimation and inference for bounds for heterogeneous\ncausal effect parameters in general sample selection models where the treatment\ncan affect whether an outcome is observed and no exclusion restrictions are\navailable. The method provides conditional effect bounds as functions of policy\nrelevant pre-treatment variables. It allows for conducting valid statistical\ninference on the unidentified conditional effects. We use a flexible\ndebiased/double machine learning approach that can accommodate non-linear\nfunctional forms and high-dimensional confounders. Easily verifiable high-level\nconditions for estimation, misspecification robust confidence intervals, and\nuniform confidence bands are provided as well. We re-analyze data from a large\nscale field experiment on Facebook on counter-attitudinal news subscription\nwith attrition. Our method yields substantially tighter effect bounds compared\nto conventional methods and suggests depolarization effects for younger users.",
        "authors": [
            "Phillip Heiler"
        ],
        "categories": "econ.EM",
        "published": "2022-09-09T14:42:03Z",
        "updated": "2024-07-26T08:15:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.03945v1",
        "title": "W-Transformers : A Wavelet-based Transformer Framework for Univariate Time Series Forecasting",
        "abstract": "Deep learning utilizing transformers has recently achieved a lot of success\nin many vital areas such as natural language processing, computer vision,\nanomaly detection, and recommendation systems, among many others. Among several\nmerits of transformers, the ability to capture long-range temporal dependencies\nand interactions is desirable for time series forecasting, leading to its\nprogress in various time series applications. In this paper, we build a\ntransformer model for non-stationary time series. The problem is challenging\nyet crucially important. We present a novel framework for univariate time\nseries representation learning based on the wavelet-based transformer encoder\narchitecture and call it W-Transformer. The proposed W-Transformers utilize a\nmaximal overlap discrete wavelet transformation (MODWT) to the time series data\nand build local transformers on the decomposed datasets to vividly capture the\nnonstationarity and long-range nonlinear dependencies in the time series.\nEvaluating our framework on several publicly available benchmark time series\ndatasets from various domains and with diverse characteristics, we demonstrate\nthat it performs, on average, significantly better than the baseline\nforecasters for short-term and long-term forecasting, even for datasets that\nconsist of only a few hundred training samples.",
        "authors": [
            "Lena Sasal",
            "Tanujit Chakraborty",
            "Abdenour Hadid"
        ],
        "categories": "cs.LG",
        "published": "2022-09-08T17:39:38Z",
        "updated": "2022-09-08T17:39:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.03744v1",
        "title": "Modified Causal Forest",
        "abstract": "Uncovering the heterogeneity of causal effects of policies and business\ndecisions at various levels of granularity provides substantial value to\ndecision makers. This paper develops estimation and inference procedures for\nmultiple treatment models in a selection-on-observed-variables framework by\nmodifying the Causal Forest approach (Wager and Athey, 2018) in several\ndimensions. The new estimators have desirable theoretical, computational, and\npractical properties for various aggregation levels of the causal effects.\nWhile an Empirical Monte Carlo study suggests that they outperform previously\nsuggested estimators, an application to the evaluation of an active labour\nmarket pro-gramme shows their value for applied research.",
        "authors": [
            "Michael Lechner",
            "Jana Mareckova"
        ],
        "categories": "econ.EM",
        "published": "2022-09-08T12:03:46Z",
        "updated": "2022-09-08T12:03:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.03259v2",
        "title": "A Ridge-Regularised Jackknifed Anderson-Rubin Test",
        "abstract": "We consider hypothesis testing in instrumental variable regression models\nwith few included exogenous covariates but many instruments -- possibly more\nthan the number of observations. We show that a ridge-regularised version of\nthe jackknifed Anderson Rubin (1949, henceforth AR) test controls asymptotic\nsize in the presence of heteroskedasticity, and when the instruments may be\narbitrarily weak. Asymptotic size control is established under weaker\nassumptions than those imposed for recently proposed jackknifed AR tests in the\nliterature. Furthermore, ridge-regularisation extends the scope of jackknifed\nAR tests to situations in which there are more instruments than observations.\nMonte-Carlo simulations indicate that our method has favourable finite-sample\nsize and power properties compared to recently proposed alternative approaches\nin the literature. An empirical application on the elasticity of substitution\nbetween immigrants and natives in the US illustrates the usefulness of the\nproposed method for practitioners.",
        "authors": [
            "Max-Sebastian Dov\u00ec",
            "Anders Bredahl Kock",
            "Sophocles Mavroeidis"
        ],
        "categories": "econ.EM",
        "published": "2022-09-07T16:08:51Z",
        "updated": "2023-11-06T18:42:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.03218v3",
        "title": "Local Projection Inference in High Dimensions",
        "abstract": "In this paper, we estimate impulse responses by local projections in\nhigh-dimensional settings. We use the desparsified (de-biased) lasso to\nestimate the high-dimensional local projections, while leaving the impulse\nresponse parameter of interest unpenalized. We establish the uniform asymptotic\nnormality of the proposed estimator under general conditions. Finally, we\ndemonstrate small sample performance through a simulation study and consider\ntwo canonical applications in macroeconomic research on monetary policy and\ngovernment spending.",
        "authors": [
            "Robert Adamek",
            "Stephan Smeekes",
            "Ines Wilms"
        ],
        "categories": "econ.EM",
        "published": "2022-09-07T15:23:28Z",
        "updated": "2024-04-16T19:04:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.03199v1",
        "title": "An Assessment Tool for Academic Research Managers in the Third World",
        "abstract": "The academic evaluation of the publication record of researchers is relevant\nfor identifying talented candidates for promotion and funding. A key tool for\nthis is the use of the indexes provided by Web of Science and SCOPUS, costly\ndatabases that sometimes exceed the possibilities of academic institutions in\nmany parts of the world. We show here how the data in one of the bases can be\nused to infer the main index of the other one. Methods of data analysis used in\nMachine Learning allow us to select just a few of the hundreds of variables in\na database, which later are used in a panel regression, yielding a good\napproximation to the main index in the other database. Since the information of\nSCOPUS can be freely scraped from the Web, this approach allows to infer for\nfree the Impact Factor of publications, the main index used in research\nassessments around the globe.",
        "authors": [
            "Fernando Delbianco",
            "Andres Fioriti",
            "Fernando Tohm\u00e9"
        ],
        "categories": "econ.EM",
        "published": "2022-09-07T14:59:25Z",
        "updated": "2022-09-07T14:59:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.05225v1",
        "title": "Rethinking Generalized Beta Family of Distributions",
        "abstract": "We approach the Generalized Beta (GB) family of distributions using a\nmean-reverting stochastic differential equation (SDE) for a power of the\nvariable, whose steady-state (stationary) probability density function (PDF) is\na modified GB (mGB) distribution. The SDE approach allows for a lucid\nexplanation of Generalized Beta Prime (GB2) and Generalized Beta (GB1) limits\nof GB distribution and, further down, of Generalized Inverse Gamma (GIGa) and\nGeneralized Gamma (GGa) limits, as well as describe the transition between the\nlatter two. We provide an alternative form to the \"traditional\" GB PDF to\nunderscore that a great deal of usefulness of GB distribution lies in its\nallowing a long-range power-law behavior to be ultimately terminated at a\nfinite value. We derive the cumulative distribution function (CDF) of the\n\"traditional\" GB, which belongs to the family generated by the regularized beta\nfunction and is crucial for analysis of the tails of the distribution. We\nanalyze fifty years of historical data on realized market volatility,\nspecifically for S\\&P500, as a case study of the use of GB/mGB distributions\nand show that its behavior is consistent with that of negative Dragon Kings.",
        "authors": [
            "Jiong Liu",
            "R. A. Serota"
        ],
        "categories": "q-fin.ST",
        "published": "2022-09-05T21:57:30Z",
        "updated": "2022-09-05T21:57:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.01910v1",
        "title": "Bayesian Mixed-Frequency Quantile Vector Autoregression: Eliciting tail risks of Monthly US GDP",
        "abstract": "Timely characterizations of risks in economic and financial systems play an\nessential role in both economic policy and private sector decisions. However,\nthe informational content of low-frequency variables and the results from\nconditional mean models provide only limited evidence to investigate this\nproblem. We propose a novel mixed-frequency quantile vector autoregression\n(MF-QVAR) model to address this issue. Inspired by the univariate Bayesian\nquantile regression literature, the multivariate asymmetric Laplace\ndistribution is exploited under the Bayesian framework to form the likelihood.\nA data augmentation approach coupled with a precision sampler efficiently\nestimates the missing low-frequency variables at higher frequencies under the\nstate-space representation. The proposed methods allow us to nowcast\nconditional quantiles for multiple variables of interest and to derive\nquantile-related risk measures at high frequency, thus enabling timely policy\ninterventions. The main application of the model is to nowcast conditional\nquantiles of the US GDP, which is strictly related to the quantification of\nValue-at-Risk and the Expected Shortfall.",
        "authors": [
            "Matteo Iacopini",
            "Aubrey Poon",
            "Luca Rossini",
            "Dan Zhu"
        ],
        "categories": "econ.EM",
        "published": "2022-09-05T11:17:40Z",
        "updated": "2022-09-05T11:17:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.01805v1",
        "title": "Robust Causal Learning for the Estimation of Average Treatment Effects",
        "abstract": "Many practical decision-making problems in economics and healthcare seek to\nestimate the average treatment effect (ATE) from observational data. The\nDouble/Debiased Machine Learning (DML) is one of the prevalent methods to\nestimate ATE in the observational study. However, the DML estimators can suffer\nan error-compounding issue and even give an extreme estimate when the\npropensity scores are misspecified or very close to 0 or 1. Previous studies\nhave overcome this issue through some empirical tricks such as propensity score\ntrimming, yet none of the existing literature solves this problem from a\ntheoretical standpoint. In this paper, we propose a Robust Causal Learning\n(RCL) method to offset the deficiencies of the DML estimators. Theoretically,\nthe RCL estimators i) are as consistent and doubly robust as the DML\nestimators, and ii) can get rid of the error-compounding issue. Empirically,\nthe comprehensive experiments show that i) the RCL estimators give more stable\nestimations of the causal parameters than the DML estimators, and ii) the RCL\nestimators outperform the traditional estimators and their variants when\napplying different machine learning models on both simulation and benchmark\ndatasets.",
        "authors": [
            "Yiyan Huang",
            "Cheuk Hang Leung",
            "Xing Yan",
            "Qi Wu",
            "Shumin Ma",
            "Zhiri Yuan",
            "Dongdong Wang",
            "Zhixiang Huang"
        ],
        "categories": "econ.EM",
        "published": "2022-09-05T07:35:58Z",
        "updated": "2022-09-05T07:35:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.01697v2",
        "title": "Combining Forecasts under Structural Breaks Using Graphical LASSO",
        "abstract": "In this paper we develop a novel method of combining many forecasts based on\na machine learning algorithm called Graphical LASSO (GL). We visualize forecast\nerrors from different forecasters as a network of interacting entities and\ngeneralize network inference in the presence of common factor structure and\nstructural breaks. First, we note that forecasters often use common information\nand hence make common mistakes, which makes the forecast errors exhibit common\nfactor structures. We use the Factor Graphical LASSO (FGL, Lee and Seregina\n(2023)) to separate common forecast errors from the idiosyncratic errors and\nexploit sparsity of the precision matrix of the latter. Second, since the\nnetwork of experts changes over time as a response to unstable environments\nsuch as recessions, it is unreasonable to assume constant forecast combination\nweights. Hence, we propose Regime-Dependent Factor Graphical LASSO (RD-FGL)\nthat allows factor loadings and idiosyncratic precision matrix to be\nregime-dependent. We develop its scalable implementation using the Alternating\nDirection Method of Multipliers (ADMM) to estimate regime-dependent forecast\ncombination weights. The empirical application to forecasting macroeconomic\nseries using the data of the European Central Bank's Survey of Professional\nForecasters (ECB SPF) demonstrates superior performance of a combined forecast\nusing FGL and RD-FGL.",
        "authors": [
            "Tae-Hwy Lee",
            "Ekaterina Seregina"
        ],
        "categories": "econ.EM",
        "published": "2022-09-04T21:22:32Z",
        "updated": "2023-09-27T02:07:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.01429v2",
        "title": "Instrumental variable quantile regression under random right censoring",
        "abstract": "This paper studies a semiparametric quantile regression model with endogenous\nvariables and random right censoring. The endogeneity issue is solved using\ninstrumental variables. It is assumed that the structural quantile of the\nlogarithm of the outcome variable is linear in the covariates and censoring is\nindependent. The regressors and instruments can be either continuous or\ndiscrete. The specification generates a continuum of equations of which the\nquantile regression coefficients are a solution. Identification is obtained\nwhen this system of equations has a unique solution. Our estimation procedure\nsolves an empirical analogue of the system of equations. We derive conditions\nunder which the estimator is asymptotically normal and prove the validity of a\nbootstrap procedure for inference. The finite sample performance of the\napproach is evaluated through numerical simulations. An application to the\nnational Job Training Partnership Act study illustrates the method.",
        "authors": [
            "Jad Beyhum",
            "Lorenzo Tedesco",
            "Ingrid Van Keilegom"
        ],
        "categories": "econ.EM",
        "published": "2022-09-03T14:08:13Z",
        "updated": "2023-02-02T08:34:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.00417v3",
        "title": "Instrumental variables with unordered treatments: Theory and evidence from returns to fields of study",
        "abstract": "We revisit the identification argument of Kirkeboen et al. (2016) who showed\nhow one may combine instruments for multiple unordered treatments with\ninformation about individuals' ranking of these treatments to achieve\nidentification while allowing for both observed and unobserved heterogeneity in\ntreatment effects. We show that the key assumptions underlying their\nidentification argument have testable implications. We also provide a new\ncharacterization of the bias that may arise if these assumptions are violated.\nTaken together, these results allow researchers not only to test the underlying\nassumptions, but also to argue whether the bias from violation of these\nassumptions are likely to be economically meaningful. Guided and motivated by\nthese results, we estimate and compare the earnings payoffs to post-secondary\nfields of study in Norway and Denmark. In each country, we apply the\nidentification argument of Kirkeboen et al. (2016) to data on individuals'\nranking of fields of study and field-specific instruments from discontinuities\nin the admission systems. We empirically examine whether and why the payoffs to\nfields of study differ across the two countries. We find strong cross-country\ncorrelation in the payoffs to fields of study, especially after removing fields\nwith violations of the assumptions underlying the identification argument.",
        "authors": [
            "Eskil Heinesen",
            "Christian Hvid",
            "Lars Kirkeb\u00f8en",
            "Edwin Leuven",
            "Magne Mogstad"
        ],
        "categories": "econ.EM",
        "published": "2022-09-01T12:47:16Z",
        "updated": "2022-10-11T11:55:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.00391v1",
        "title": "A Unified Framework for Estimation of High-dimensional Conditional Factor Models",
        "abstract": "This paper develops a general framework for estimation of high-dimensional\nconditional factor models via nuclear norm regularization. We establish large\nsample properties of the estimators, and provide an efficient computing\nalgorithm for finding the estimators as well as a cross validation procedure\nfor choosing the regularization parameter. The general framework allows us to\nestimate a variety of conditional factor models in a unified way and quickly\ndeliver new asymptotic results. We apply the method to analyze the cross\nsection of individual US stock returns, and find that imposing homogeneity may\nimprove the model's out-of-sample predictability.",
        "authors": [
            "Qihui Chen"
        ],
        "categories": "econ.EM",
        "published": "2022-09-01T12:10:29Z",
        "updated": "2022-09-01T12:10:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.00197v3",
        "title": "Switchback Experiments under Geometric Mixing",
        "abstract": "The switchback is an experimental design that measures treatment effects by\nrepeatedly turning an intervention on and off for a whole system. Switchback\nexperiments are a robust way to overcome cross-unit spillover effects; however,\nthey are vulnerable to bias from temporal carryovers. In this paper, we\nconsider properties of switchback experiments in Markovian systems that mix at\na geometric rate. We find that, in this setting, standard switchback designs\nsuffer considerably from carryover bias: Their estimation error decays as\n$T^{-1/3}$ in terms of the experiment horizon $T$, whereas in the absence of\ncarryovers a faster rate of $T^{-1/2}$ would have been possible. We also show,\nhowever, that judicious use of burn-in periods can considerably improve the\nsituation, and enables errors that decay as $\\log(T)^{1/2}T^{-1/2}$. Our formal\nresults are mirrored in an empirical evaluation.",
        "authors": [
            "Yuchen Hu",
            "Stefan Wager"
        ],
        "categories": "stat.ME",
        "published": "2022-09-01T03:29:58Z",
        "updated": "2024-04-02T21:02:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.14311v4",
        "title": "Modeling Volatility and Dependence of European Carbon and Energy Prices",
        "abstract": "We study the prices of European Emission Allowances (EUA), whereby we analyze\ntheir uncertainty and dependencies on related energy prices (natural gas, coal,\nand oil). We propose a probabilistic multivariate conditional time series model\nwith a VECM-Copula-GARCH structure which exploits key characteristics of the\ndata. Data are normalized with respect to inflation and carbon emissions to\nallow for proper cross-series evaluation. The forecasting performance is\nevaluated in an extensive rolling-window forecasting study, covering eight\nyears out-of-sample. We discuss our findings for both levels- and\nlog-transformed data, focusing on time-varying correlations, and in view of the\nRussian invasion of Ukraine.",
        "authors": [
            "Jonathan Berrisch",
            "Sven Pappert",
            "Florian Ziel",
            "Antonia Arsova"
        ],
        "categories": "q-fin.ST",
        "published": "2022-08-30T14:50:25Z",
        "updated": "2023-02-10T14:38:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.13370v2",
        "title": "A Consistent ICM-based $\u03c7^2$ Specification Test",
        "abstract": "In spite of the omnibus property of Integrated Conditional Moment (ICM)\nspecification tests, they are not commonly used in empirical practice owing to,\ne.g., the non-pivotality of the test and the high computational cost of\navailable bootstrap schemes especially in large samples. This paper proposes\nspecification and mean independence tests based on a class of ICM metrics\ntermed the generalized martingale difference divergence (GMDD). The proposed\ntests exhibit consistency, asymptotic $\\chi^2$-distribution under the null\nhypothesis, and computational efficiency. Moreover, they demonstrate robustness\nto heteroskedasticity of unknown form and can be adapted to enhance power\ntowards specific alternatives. A power comparison with classical\nbootstrap-based ICM tests using Bahadur slopes is also provided. Monte Carlo\nsimulations are conducted to showcase the proposed tests' excellent size\ncontrol and competitive power.",
        "authors": [
            "Feiyu Jiang",
            "Emmanuel Selorm Tsyawo"
        ],
        "categories": "econ.EM",
        "published": "2022-08-29T05:11:11Z",
        "updated": "2024-05-16T08:48:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.13323v4",
        "title": "Safe Policy Learning under Regression Discontinuity Designs with Multiple Cutoffs",
        "abstract": "The regression discontinuity (RD) design is widely used for program\nevaluation with observational data. The primary focus of the existing\nliterature has been the estimation of the local average treatment effect at the\nexisting treatment cutoff. In contrast, we consider policy learning under the\nRD design. Because the treatment assignment mechanism is deterministic,\nlearning better treatment cutoffs requires extrapolation. We develop a robust\noptimization approach to finding optimal treatment cutoffs that improve upon\nthe existing ones. We first decompose the expected utility into\npoint-identifiable and unidentifiable components. We then propose an efficient\ndoubly-robust estimator for the identifiable parts. To account for the\nunidentifiable components, we leverage the existence of multiple cutoffs that\nare common under the RD design. Specifically, we assume that the heterogeneity\nin the conditional expectations of potential outcomes across different groups\nvary smoothly along the running variable. Under this assumption, we minimize\nthe worst case utility loss relative to the status quo policy. The resulting\nnew treatment cutoffs have a safety guarantee that they will not yield a worse\noverall outcome than the existing cutoffs. Finally, we establish the asymptotic\nregret bounds for the learned policy using semi-parametric efficiency theory.\nWe apply the proposed methodology to empirical and simulated data sets.",
        "authors": [
            "Yi Zhang",
            "Eli Ben-Michael",
            "Kosuke Imai"
        ],
        "categories": "stat.ME",
        "published": "2022-08-29T01:27:23Z",
        "updated": "2024-09-04T15:14:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.13255v1",
        "title": "Comparing Stochastic Volatility Specifications for Large Bayesian VARs",
        "abstract": "Large Bayesian vector autoregressions with various forms of stochastic\nvolatility have become increasingly popular in empirical macroeconomics. One\nmain difficulty for practitioners is to choose the most suitable stochastic\nvolatility specification for their particular application. We develop Bayesian\nmodel comparison methods -- based on marginal likelihood estimators that\ncombine conditional Monte Carlo and adaptive importance sampling -- to choose\namong a variety of stochastic volatility specifications. The proposed methods\ncan also be used to select an appropriate shrinkage prior on the VAR\ncoefficients, which is a critical component for avoiding over-fitting in\nhigh-dimensional settings. Using US quarterly data of different dimensions, we\nfind that both the Cholesky stochastic volatility and factor stochastic\nvolatility outperform the common stochastic volatility specification. Their\nsuperior performance, however, can mostly be attributed to the more flexible\npriors that accommodate cross-variable shrinkage.",
        "authors": [
            "Joshua C. C. Chan"
        ],
        "categories": "econ.EM",
        "published": "2022-08-28T17:28:56Z",
        "updated": "2022-08-28T17:28:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.13254v3",
        "title": "An agent-based modeling approach for real-world economic systems: Example and calibration with a Social Accounting Matrix of Spain",
        "abstract": "The global economy is one of today's major challenges, with increasing\nrelevance in recent decades. A frequent observation by policy makers is the\nlack of tools that help at least to understand, if not predict, economic\ncrises. Currently, macroeconomic modeling is dominated by Dynamic Stochastic\nGeneral Equilibrium (DSGE) models. The limitations of DSGE in coping with the\ncomplexity of today's global economy are often recognized and are the subject\nof intense research to find possible solutions. As an alternative or complement\nto DSGE, the last two decades have seen the rise of agent-based models (ABM).\nAn attractive feature of ABM is that it can model very complex systems because\nit is a bottom-up approach that can describe the specific behavior of\nheterogeneous agents. The main obstacle, however, is the large number of\nparameters that need to be known or calibrated. To enable the use of ABM with\ndata from the real-world economy, this paper describes an agent-based\nmacroeconomic modeling approach that can read a Social Accounting Matrix (SAM)\nand deploy from scratch an economic system (labor, activity sectors operating\nas firms, a central bank, the government, external sectors...) whose structure\nand activity produce a SAM with values very close to those of the actual SAM\nsnapshot. This approach paves the way for unleashing the expected high\nperformance of ABM models to deal with the complexities of current global\nmacroeconomics, including other layers of interest like ecology, epidemiology,\nor social networks among others.",
        "authors": [
            "Martin Jaraiz"
        ],
        "categories": "q-fin.GN",
        "published": "2022-08-28T17:09:19Z",
        "updated": "2023-05-14T18:05:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.13012v1",
        "title": "A Descriptive Method of Firm Size Transition Dynamics Using Markov Chain",
        "abstract": "Social employment, which is mostly carried by firms of different types,\ndetermines the prosperity and stability of a country. As time passing, the\nfluctuations of firm employment can reflect the process of creating or\ndestroying jobs. Therefore, it is instructive to investigate the firm\nemployment (size) dynamics. Drawing on the firm-level panel data extracted from\nthe Chinese Industrial Enterprises Database 1998-2013, this paper proposes a\nMarkov-chain-based descriptive approach to clearly demonstrate the firm size\ntransfer dynamics between different size categories. With this method, any firm\nsize transition path in a short time period can be intuitively demonstrated.\nFurthermore, by utilizing the properties of Markov transfer matrices, the\ndefinition of transition trend and the transition entropy are introduced and\nestimated. As a result, the tendency of firm size transfer between small,\nmedium and large can be exactly revealed, and the uncertainty of size change\ncan be quantified. Generally from the evidence of this paper, it can be\ninferred that small and medium manufacturing firms in China have greater job\ncreation potentials compared to large firms over this time period.",
        "authors": [
            "Boyang You",
            "Kerry Papps"
        ],
        "categories": "econ.EM",
        "published": "2022-08-27T13:30:33Z",
        "updated": "2022-08-27T13:30:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.12990v1",
        "title": "A restricted eigenvalue condition for unit-root non-stationary data",
        "abstract": "In this paper, we develop a restricted eigenvalue condition for unit-root\nnon-stationary data and derive its validity under the assumption of independent\nGaussian innovations that may be contemporaneously correlated. The method of\nproof relies on matrix concentration inequalities and offers sufficient\nflexibility to enable extensions of our results to alternative time series\nsettings. As an application of this result, we show the consistency of the\nlasso estimator on ultra high-dimensional cointegrated data in which the number\nof integrated regressors may grow exponentially in relation to the sample size.",
        "authors": [
            "Etienne Wijler"
        ],
        "categories": "econ.EM",
        "published": "2022-08-27T11:43:26Z",
        "updated": "2022-08-27T11:43:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.12323v2",
        "title": "Large Volatility Matrix Analysis Using Global and National Factor Models",
        "abstract": "Several large volatility matrix inference procedures have been developed,\nbased on the latent factor model. They often assumed that there are a few of\ncommon factors, which can account for volatility dynamics. However, several\nstudies have demonstrated the presence of local factors. In particular, when\nanalyzing the global stock market, we often observe that nation-specific\nfactors explain their own country's volatility dynamics. To account for this,\nwe propose the Double Principal Orthogonal complEment Thresholding\n(Double-POET) method, based on multi-level factor models, and also establish\nits asymptotic properties. Furthermore, we demonstrate the drawback of using\nthe regular principal orthogonal component thresholding (POET) when the local\nfactor structure exists. We also describe the blessing of dimensionality using\nDouble-POET for local covariance matrix estimation. Finally, we investigate the\nperformance of the Double-POET estimator in an out-of-sample portfolio\nallocation study using international stocks from 20 financial markets.",
        "authors": [
            "Sung Hoon Choi",
            "Donggyu Kim"
        ],
        "categories": "econ.EM",
        "published": "2022-08-25T19:46:56Z",
        "updated": "2022-12-18T01:32:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.11828v2",
        "title": "What Impulse Response Do Instrumental Variables Identify?",
        "abstract": "Macro shocks are often composites, yet overlooked in the impulse response\nanalysis. When an instrumental variable (IV) is used to identify a composite\nshock, it violates the common IV exclusion restriction. We show that the Local\nProjection-IV estimand is represented as a weighted average of component-wise\nimpulse responses but with possibly negative weights, which occur when the IV\nand shock components have opposite correlations. We further develop alternative\n(set-) identification strategies for the LP-IV based on sign restrictions or\nadditional granular information. Our applications confirm the composite nature\nof monetary policy shocks and reveal a non-defense spending multiplier\nexceeding one.",
        "authors": [
            "Bonsoo Koo",
            "Seojeong Lee",
            "Myung Hwan Seo"
        ],
        "categories": "econ.EM",
        "published": "2022-08-25T02:18:56Z",
        "updated": "2023-08-24T02:51:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.11281v2",
        "title": "Robust Tests of Model Incompleteness in the Presence of Nuisance Parameters",
        "abstract": "Economic models may exhibit incompleteness depending on whether or not they\nadmit certain policy-relevant features such as strategic interaction,\nself-selection, or state dependence. We develop a novel test of model\nincompleteness and analyze its asymptotic properties. A key observation is that\none can identify the least-favorable parametric model that represents the most\nchallenging scenario for detecting local alternatives without knowledge of the\nselection mechanism. We build a robust test of incompleteness on a score\nfunction constructed from such a model. The proposed procedure remains\ncomputationally tractable even with nuisance parameters because it suffices to\nestimate them only under the null hypothesis of model completeness. We\nillustrate the test by applying it to a market entry model and a triangular\nmodel with a set-valued control function.",
        "authors": [
            "Shuowen Chen",
            "Hiroaki Kaido"
        ],
        "categories": "econ.EM",
        "published": "2022-08-24T02:51:05Z",
        "updated": "2023-09-07T17:17:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.10974v3",
        "title": "Beta-Sorted Portfolios",
        "abstract": "Beta-sorted portfolios -- portfolios comprised of assets with similar\ncovariation to selected risk factors -- are a popular tool in empirical finance\nto analyze models of (conditional) expected returns. Despite their widespread\nuse, little is known of their statistical properties in contrast to comparable\nprocedures such as two-pass regressions. We formally investigate the properties\nof beta-sorted portfolio returns by casting the procedure as a two-step\nnonparametric estimator with a nonparametric first step and a beta-adaptive\nportfolios construction. Our framework rationalize the well-known estimation\nalgorithm with precise economic and statistical assumptions on the general data\ngenerating process and characterize its key features. We study beta-sorted\nportfolios for both a single cross-section as well as for aggregation over time\n(e.g., the grand mean), offering conditions that ensure consistency and\nasymptotic normality along with new uniform inference procedures allowing for\nuncertainty quantification and testing of various relevant hypotheses in\nfinancial applications. We also highlight some limitations of current empirical\npractices and discuss what inferences can and cannot be drawn from returns to\nbeta-sorted portfolios for either a single cross-section or across the whole\nsample. Finally, we illustrate the functionality of our new procedures in an\nempirical application.",
        "authors": [
            "Matias D. Cattaneo",
            "Richard K. Crump",
            "Weining Wang"
        ],
        "categories": "econ.EM",
        "published": "2022-08-23T13:45:20Z",
        "updated": "2024-11-10T22:25:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.10896v2",
        "title": "pystacked: Stacking generalization and machine learning in Stata",
        "abstract": "pystacked implements stacked generalization (Wolpert, 1992) for regression\nand binary classification via Python's scikit-learn. Stacking combines multiple\nsupervised machine learners -- the \"base\" or \"level-0\" learners -- into a\nsingle learner. The currently supported base learners include regularized\nregression, random forest, gradient boosted trees, support vector machines, and\nfeed-forward neural nets (multi-layer perceptron). pystacked can also be used\nwith as a `regular' machine learning program to fit a single base learner and,\nthus, provides an easy-to-use API for scikit-learn's machine learning\nalgorithms.",
        "authors": [
            "Achim Ahrens",
            "Christian B. Hansen",
            "Mark E. Schaffer"
        ],
        "categories": "econ.EM",
        "published": "2022-08-23T12:03:04Z",
        "updated": "2023-03-06T16:25:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.09638v3",
        "title": "Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability",
        "abstract": "What is the purpose of pre-analysis plans, and how should they be designed?\nWe model the interaction between an agent who analyzes data and a principal who\nmakes a decision based on agent reports. The agent could be the manufacturer of\na new drug, and the principal a regulator deciding whether the drug is\napproved. Or the agent could be a researcher submitting a research paper, and\nthe principal an editor deciding whether it is published. The agent decides\nwhich statistics to report to the principal. The principal cannot verify\nwhether the analyst reported selectively. Absent a pre-analysis message, if\nthere are conflicts of interest, then many desirable decision rules cannot be\nimplemented. Allowing the agent to send a message before seeing the data\nincreases the set of decision rules that can be implemented, and allows the\nprincipal to leverage agent expertise. The optimal mechanisms that we\ncharacterize require pre-analysis plans. Applying these results to hypothesis\ntesting, we show that optimal rejection rules pre-register a valid test, and\nmake worst-case assumptions about unreported statistics. Optimal tests can be\nfound as a solution to a linear-programming problem.",
        "authors": [
            "Maximilian Kasy",
            "Jann Spiess"
        ],
        "categories": "econ.EM",
        "published": "2022-08-20T08:54:39Z",
        "updated": "2024-07-29T07:38:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.09325v1",
        "title": "Deep Learning for Choice Modeling",
        "abstract": "Choice modeling has been a central topic in the study of individual\npreference or utility across many fields including economics, marketing,\noperations research, and psychology. While the vast majority of the literature\non choice models has been devoted to the analytical properties that lead to\nmanagerial and policy-making insights, the existing methods to learn a choice\nmodel from empirical data are often either computationally intractable or\nsample inefficient. In this paper, we develop deep learning-based choice models\nunder two settings of choice modeling: (i) feature-free and (ii) feature-based.\nOur model captures both the intrinsic utility for each candidate choice and the\neffect that the assortment has on the choice probability. Synthetic and real\ndata experiments demonstrate the performances of proposed models in terms of\nthe recovery of the existing choice models, sample complexity, assortment\neffect, architecture design, and model interpretation.",
        "authors": [
            "Zhongze Cai",
            "Hanzhao Wang",
            "Kalyan Talluri",
            "Xiaocheng Li"
        ],
        "categories": "stat.ML",
        "published": "2022-08-19T13:10:17Z",
        "updated": "2022-08-19T13:10:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.09148v1",
        "title": "Understanding Volatility Spillover Relationship Among G7 Nations And India During Covid-19",
        "abstract": "Purpose: In the context of a COVID pandemic in 2020-21, this paper attempts\nto capture the interconnectedness and volatility transmission dynamics. The\nnature of change in volatility spillover effects and time-varying conditional\ncorrelation among the G7 countries and India is investigated. Methodology: To\nassess the volatility spillover effects, the bivariate BEKK and t- DCC (1,1)\nGARCH (1,1) models have been used. Our research shows how the dynamics of\nvolatility spillover between India and the G7 countries shift before and during\nCOVID-19. Findings: The findings reveal that the extent of volatility spillover\nhas altered during COVID compared to the pre-COVID environment. During this\npandemic, a sharp increase in conditional correlation indicates an increase in\nsystematic risk between countries. Originality: The study contributes to a\nbetter understanding of the dynamics of volatility spillover between G7\ncountries and India. Asset managers and foreign corporations can use the\nchanging spillover dynamics to improve investment decisions and implement\neffective hedging measures to protect their interests. Furthermore, this\nresearch will assist financial regulators in assessing market risk in the\nfuture owing to crises like as COVID-19.",
        "authors": [
            "Avik Das",
            "Devanjali Nandi Das"
        ],
        "categories": "econ.EM",
        "published": "2022-08-19T04:51:06Z",
        "updated": "2022-08-19T04:51:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.09102v1",
        "title": "On the Estimation of Peer Effects for Sampled Networks",
        "abstract": "This paper deals with the estimation of exogeneous peer effects for partially\nobserved networks under the new inferential paradigm of design identification,\nwhich characterizes the missing data challenge arising with sampled networks\nwith the central idea that two full data versions which are topologically\ncompatible with the observed data may give rise to two different probability\ndistributions. We show that peer effects cannot be identified by design when\nnetwork links between sampled and unsampled units are not observed. Under\nrealistic modeling conditions, and under the assumption that sampled units\nreport on the size of their network of contacts, the asymptotic bias arising\nfrom estimating peer effects with incomplete network data is characterized, and\na bias-corrected estimator is proposed. The finite sample performance of our\nmethodology is investigated via simulations.",
        "authors": [
            "Mamadou Yauck"
        ],
        "categories": "econ.EM",
        "published": "2022-08-19T00:30:43Z",
        "updated": "2022-08-19T00:30:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.08693v3",
        "title": "Matrix Quantile Factor Model",
        "abstract": "This paper introduces a matrix quantile factor model for matrix-valued data\nwith low-rank structure. We estimate the row and column factor spaces via\nminimizing the empirical check loss function with orthogonal rotation\nconstraints. We show that the estimates converge at rate\n$(\\min\\{p_1p_2,p_2T,p_1T\\})^{-1/2}$ in the average Frobenius norm, where $p_1$,\n$p_2$ and $T$ are the row dimensionality, column dimensionality and length of\nthe matrix sequence, respectively. This rate is faster than that of the\nquantile estimates via ``flattening\" the matrix model into a large vector\nmodel. To derive the central limit theorem, we introduce a novel augmented\nLagrangian function, which is equivalent to the original constrained empirical\ncheck loss minimization problem. Via the equivalence, we prove that the Hessian\nmatrix of the augmented Lagrangian function is locally positive definite,\nresulting in a locally convex penalized loss function around the true factors\nand their loadings. This easily leads to a feasible second-order expansion of\nthe score function and readily established central limit theorems of the\nsmoothed estimates of the loadings. We provide three consistent criteria to\ndetermine the pair of row and column factor numbers. Extensive simulation\nstudies and an empirical study justify our theory.",
        "authors": [
            "Xin-Bing Kong",
            "Yong-Xin Liu",
            "Long Yu",
            "Peng Zhao"
        ],
        "categories": "stat.ME",
        "published": "2022-08-18T08:15:56Z",
        "updated": "2024-08-20T00:58:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.08291v3",
        "title": "Inference on Strongly Identified Functionals of Weakly Identified Functions",
        "abstract": "In a variety of applications, including nonparametric instrumental variable\n(NPIV) analysis, proximal causal inference under unmeasured confounding, and\nmissing-not-at-random data with shadow variables, we are interested in\ninference on a continuous linear functional (e.g., average causal effects) of\nnuisance function (e.g., NPIV regression) defined by conditional moment\nrestrictions. These nuisance functions are generally weakly identified, in that\nthe conditional moment restrictions can be severely ill-posed as well as admit\nmultiple solutions. This is sometimes resolved by imposing strong conditions\nthat imply the function can be estimated at rates that make inference on the\nfunctional possible. In this paper, we study a novel condition for the\nfunctional to be strongly identified even when the nuisance function is not;\nthat is, the functional is amenable to asymptotically-normal estimation at\n$\\sqrt{n}$-rates. The condition implies the existence of debiasing nuisance\nfunctions, and we propose penalized minimax estimators for both the primary and\ndebiasing nuisance functions. The proposed nuisance estimators can accommodate\nflexible function classes, and importantly they can converge to fixed limits\ndetermined by the penalization regardless of the identifiability of the\nnuisances. We use the penalized nuisance estimators to form a debiased\nestimator for the functional of interest and prove its asymptotic normality\nunder generic high-level conditions, which provide for asymptotically valid\nconfidence intervals. We also illustrate our method in a novel partially linear\nproximal causal inference problem and a partially linear instrumental variable\nregression problem.",
        "authors": [
            "Andrew Bennett",
            "Nathan Kallus",
            "Xiaojie Mao",
            "Whitney Newey",
            "Vasilis Syrgkanis",
            "Masatoshi Uehara"
        ],
        "categories": "stat.ME",
        "published": "2022-08-17T13:38:31Z",
        "updated": "2023-07-01T01:08:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.08169v1",
        "title": "Time is limited on the road to asymptopia",
        "abstract": "One challenge in the estimation of financial market agent-based models\n(FABMs) is to infer reliable insights using numerical simulations validated by\nonly a single observed time series. Ergodicity (besides stationarity) is a\nstrong precondition for any estimation, however it has not been systematically\nexplored and is often simply presumed. For finite-sample lengths and limited\ncomputational resources empirical estimation always takes place in\npre-asymptopia. Thus broken ergodicity must be considered the rule, but it\nremains largely unclear how to deal with the remaining uncertainty in\nnon-ergodic observables. Here we show how an understanding of the ergodic\nproperties of moment functions can help to improve the estimation of (F)ABMs.\nWe run Monte Carlo experiments and study the convergence behaviour of moment\nfunctions of two prototype models. We find infeasibly-long convergence times\nfor most. Choosing an efficient mix of ensemble size and simulated time length\nguided our estimation and might help in general.",
        "authors": [
            "Ivonne Schwartz",
            "Mark Kirstein"
        ],
        "categories": "econ.EM",
        "published": "2022-08-17T09:15:07Z",
        "updated": "2022-08-17T09:15:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.08108v1",
        "title": "Characterizing M-estimators",
        "abstract": "We characterize the full classes of M-estimators for semiparametric models of\ngeneral functionals by formally connecting the theory of consistent loss\nfunctions from forecast evaluation with the theory of M-estimation. This novel\ncharacterization result opens up the possibility for theoretical research on\nefficient and equivariant M-estimation and, more generally, it allows to\nleverage existing results on loss functions known from the literature of\nforecast evaluation in estimation theory.",
        "authors": [
            "Timo Dimitriadis",
            "Tobias Fissler",
            "Johanna Ziegel"
        ],
        "categories": "math.ST",
        "published": "2022-08-17T06:48:48Z",
        "updated": "2022-08-17T06:48:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.06729v3",
        "title": "Optimal Recovery for Causal Inference",
        "abstract": "Problems in causal inference can be fruitfully addressed using signal\nprocessing techniques. As an example, it is crucial to successfully quantify\nthe causal effects of an intervention to determine whether the intervention\nachieved desired outcomes. We present a new geometric signal processing\napproach to classical synthetic control called ellipsoidal optimal recovery\n(EOpR), for estimating the unobservable outcome of a treatment unit. EOpR\nprovides policy evaluators with both worst-case and typical outcomes to help in\ndecision making. It is an approximation-theoretic technique that relates to the\ntheory of principal components, which recovers unknown observations given a\nlearned signal class and a set of known observations. We show EOpR can improve\npre-treatment fit and mitigate bias of the post-treatment estimate relative to\nother methods in causal inference. Beyond recovery of the unit of interest, an\nadvantage of EOpR is that it produces worst-case limits over the estimates\nproduced. We assess our approach on artificially-generated data, on datasets\ncommonly used in the econometrics literature, and in the context of the\nCOVID-19 pandemic, showing better performance than baseline techniques",
        "authors": [
            "Ibtihal Ferwana",
            "Lav R. Varshney"
        ],
        "categories": "stat.ME",
        "published": "2022-08-13T20:59:59Z",
        "updated": "2023-12-20T01:24:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.06675v1",
        "title": "From the historical Roman road network to modern infrastructure in Italy",
        "abstract": "An integrated and widespread road system, like the one built during the Roman\nEmpire in Italy, plays an important role today in facilitating the construction\nof new infrastructure. This paper investigates the historical path of Roman\nroads as main determinant of both motorways and railways in the country. The\nempirical analysis shows how the modern Italian transport infrastructure\nfollowed the path traced in ancient times by the Romans in constructing their\nroads. Being paved and connecting Italy from North to South, consular\ntrajectories lasted in time, representing the starting physical capital for\ndeveloping the new transport networks.",
        "authors": [
            "Luca De Benedictis",
            "Vania Licio",
            "Anna Pinna"
        ],
        "categories": "econ.GN",
        "published": "2022-08-13T15:34:15Z",
        "updated": "2022-08-13T15:34:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.06115v5",
        "title": "A Nonparametric Approach with Marginals for Modeling Consumer Choice",
        "abstract": "Given data on the choices made by consumers for different offer sets, a key\nchallenge is to develop parsimonious models that describe and predict consumer\nchoice behavior while being amenable to prescriptive tasks such as pricing and\nassortment optimization. The marginal distribution model (MDM) is one such\nmodel, which requires only the specification of marginal distributions of the\nrandom utilities. This paper aims to establish necessary and sufficient\nconditions for given choice data to be consistent with the MDM hypothesis,\ninspired by the usefulness of similar characterizations for the random utility\nmodel (RUM). This endeavor leads to an exact characterization of the set of\nchoice probabilities that the MDM can represent. Verifying the consistency of\nchoice data with this characterization is equivalent to solving a\npolynomial-sized linear program. Since the analogous verification task for RUM\nis computationally intractable and neither of these models subsumes the other,\nMDM is helpful in striking a balance between tractability and representational\npower. The characterization is then used with robust optimization for making\ndata-driven sales and revenue predictions for new unseen assortments. When the\nchoice data lacks consistency with the MDM hypothesis, finding the best-fitting\nMDM choice probabilities reduces to solving a mixed integer convex program.\nNumerical results using real world data and synthetic data demonstrate that MDM\nexhibits competitive representational power and prediction performance compared\nto RUM and parametric models while being significantly faster in computation\nthan RUM.",
        "authors": [
            "Yanqiu Ruan",
            "Xiaobo Li",
            "Karthyek Murthy",
            "Karthik Natarajan"
        ],
        "categories": "stat.ML",
        "published": "2022-08-12T04:43:26Z",
        "updated": "2024-11-05T05:12:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.05344v4",
        "title": "Testing for homogeneous treatment effects in linear and nonparametric instrumental variable models",
        "abstract": "The hypothesis of homogeneous treatment effects is central to the\ninstrumental variables literature. This assumption signifies that treatment\neffects are constant across all subjects. It allows to interpret instrumental\nvariable estimates as average treatment effects over the whole population of\nthe study. When this assumption does not hold, the bias of instrumental\nvariable estimators can be larger than that of naive estimators ignoring\nendogeneity. This paper develops two tests for the assumption of homogeneous\ntreatment effects when the treatment is endogenous and an instrumental variable\nis available. The tests leverage a covariable that is (jointly with the error\nterms) independent of a coordinate of the instrument. This covariate does not\nneed to be exogenous. The first test assumes that the potential outcomes are\nlinear in the regressors and is computationally simple. The second test is\nnonparametric and relies on Tikhonov regularization. The treatment can be\neither discrete or continuous. We show that the tests have asymptotically\ncorrect level and asymptotic power equal to one against a range of\nalternatives. Simulations demonstrate that the proposed tests attain excellent\nfinite sample performances. The methodology is also applied to the evaluation\nof returns to schooling and the effect of price on demand in a fish market.",
        "authors": [
            "Jad Beyhum",
            "Jean-Pierre Florens",
            "Elia Lapenta",
            "Ingrid Van Keilegom"
        ],
        "categories": "econ.EM",
        "published": "2022-08-10T13:38:57Z",
        "updated": "2023-04-14T16:14:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.05278v1",
        "title": "Selecting Valid Instrumental Variables in Linear Models with Multiple Exposure Variables: Adaptive Lasso and the Median-of-Medians Estimator",
        "abstract": "In a linear instrumental variables (IV) setting for estimating the causal\neffects of multiple confounded exposure/treatment variables on an outcome, we\ninvestigate the adaptive Lasso method for selecting valid instrumental\nvariables from a set of available instruments that may contain invalid ones. An\ninstrument is invalid if it fails the exclusion conditions and enters the model\nas an explanatory variable. We extend the results as developed in Windmeijer et\nal. (2019) for the single exposure model to the multiple exposures case. In\nparticular we propose a median-of-medians estimator and show that the\nconditions on the minimum number of valid instruments under which this\nestimator is consistent for the causal effects are only moderately stronger\nthan the simple majority rule that applies to the median estimator for the\nsingle exposure case. The adaptive Lasso method which uses the initial\nmedian-of-medians estimator for the penalty weights achieves consistent\nselection with oracle properties of the resulting IV estimator. This is\nconfirmed by some Monte Carlo simulation results. We apply the method to\nestimate the causal effects of educational attainment and cognitive ability on\nbody mass index (BMI) in a Mendelian Randomization setting.",
        "authors": [
            "Xiaoran Liang",
            "Eleanor Sanderson",
            "Frank Windmeijer"
        ],
        "categories": "stat.ME",
        "published": "2022-08-10T11:23:42Z",
        "updated": "2022-08-10T11:23:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.05047v1",
        "title": "Endogeneity in Weakly Separable Models without Monotonicity",
        "abstract": "We identify and estimate treatment effects when potential outcomes are weakly\nseparable with a binary endogenous treatment. Vytlacil and Yildiz (2007)\nproposed an identification strategy that exploits the mean of observed\noutcomes, but their approach requires a monotonicity condition. In comparison,\nwe exploit full information in the entire outcome distribution, instead of just\nits mean. As a result, our method does not require monotonicity and is also\napplicable to general settings with multiple indices. We provide examples where\nour approach can identify treatment effect parameters of interest whereas\nexisting methods would fail. These include models where potential outcomes\ndepend on multiple unobserved disturbance terms, such as a Roy model, a\nmultinomial choice model, as well as a model with endogenous random\ncoefficients. We establish consistency and asymptotic normality of our\nestimators.",
        "authors": [
            "Songnian Chen",
            "Shakeeb Khan",
            "Xun Tang"
        ],
        "categories": "econ.EM",
        "published": "2022-08-09T21:34:09Z",
        "updated": "2022-08-09T21:34:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.03737v5",
        "title": "Finite Tests from Functional Characterizations",
        "abstract": "Classically, testing whether decision makers belong to specific preference\nclasses involves two main approaches. The first, known as the functional\napproach, assumes access to an entire demand function. The second, the revealed\npreference approach, constructs inequalities to test finite demand data. This\npaper bridges these methods by using the functional approach to test finite\ndata through preference learnability results. We develop a computationally\nefficient algorithm that generates tests for choice data based on functional\ncharacterizations of preference families. We provide these restrictions for\nvarious applications, including homothetic and weakly separable preferences,\nwhere the latter's revealed preference characterization is provably NP-Hard. We\nalso address choice under uncertainty, offering tests for betweenness\npreferences. Lastly, we perform a simulation exercise demonstrating that our\ntests are effective in finite samples and accurately reject demands not\nbelonging to a specified class.",
        "authors": [
            "Charles Gauthier",
            "Raghav Malhotra",
            "Agustin Troccoli Moretti"
        ],
        "categories": "econ.TH",
        "published": "2022-08-07T14:29:18Z",
        "updated": "2024-07-04T14:15:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.03719v1",
        "title": "Strategic differences between regional investments into graphene technology and how corporations and universities manage patent portfolios",
        "abstract": "Nowadays, patenting activities are essential in converting applied science to\ntechnology in the prevailing innovation model. To gain strategic advantages in\nthe technological competitions between regions, nations need to leverage the\ninvestments of public and private funds to diversify over all technologies or\nspecialize in a small number of technologies. In this paper, we investigated\nwho the leaders are at the regional and assignee levels, how they attained\ntheir leadership positions, and whether they adopted diversification or\nspecialization strategies, using a dataset of 176,193 patent records on\ngraphene between 1986 and 2017 downloaded from Derwent Innovation. By applying\na co-clustering method to the IPC subclasses in the patents and using a z-score\nmethod to extract keywords from their titles and abstracts, we identified seven\ngraphene technology areas emerging in the sequence synthesis - composites -\nsensors - devices - catalyst - batteries - water treatment. We then examined\nthe top regions in their investment preferences and their changes in rankings\nover time and found that they invested in all seven technology areas. In\ncontrast, at the assignee level, some were diversified while others were\nspecialized. We found that large entities diversified their portfolios across\nmultiple technology areas, while small entities specialized around their core\ncompetencies. In addition, we found that universities had higher entropy values\nthan corporations on average, leading us to the hypothesis that corporations\nfile, buy, or sell patents to enable product development. In contrast,\nuniversities focus only on licensing their patents. We validated this\nhypothesis through an aggregate analysis of reassignment and licensing and a\nmore detailed analysis of three case studies - SAMSUNG, RICE UNIVERSITY, and\nDYSON.",
        "authors": [
            "Ai Linh Nguyen",
            "Wenyuan Liu",
            "Khiam Aik Khor",
            "Andrea Nanetti",
            "Siew Ann Cheong"
        ],
        "categories": "cs.DL",
        "published": "2022-08-07T13:28:07Z",
        "updated": "2022-08-07T13:28:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.03632v3",
        "title": "Quantile Random-Coefficient Regression with Interactive Fixed Effects: Heterogeneous Group-Level Policy Evaluation",
        "abstract": "We propose a quantile random-coefficient regression with interactive fixed\neffects to study the effects of group-level policies that are heterogeneous\nacross individuals. Our approach is the first to use a latent factor structure\nto handle the unobservable heterogeneities in the random coefficient. The\nasymptotic properties and an inferential method for the policy estimators are\nestablished. The model is applied to evaluate the effect of the minimum wage\npolicy on earnings between 1967 and 1980 in the United States. Our results\nsuggest that the minimum wage policy has significant and persistent positive\neffects on black workers and female workers up to the median. Our results also\nindicate that the policy helps reduce income disparity up to the median between\ntwo groups: black, female workers versus white, male workers. However, the\npolicy is shown to have little effect on narrowing the income gap between low-\nand high-income workers within the subpopulations.",
        "authors": [
            "Ruofan Xu",
            "Jiti Gao",
            "Tatsushi Oka",
            "Yoon-Jae Whang"
        ],
        "categories": "econ.EM",
        "published": "2022-08-07T03:56:12Z",
        "updated": "2024-11-05T02:57:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.03489v3",
        "title": "Forecasting Algorithms for Causal Inference with Panel Data",
        "abstract": "Conducting causal inference with panel data is a core challenge in social\nscience research. We adapt a deep neural architecture for time series\nforecasting (the N-BEATS algorithm) to more accurately impute the\ncounterfactual evolution of a treated unit had treatment not occurred. Across a\nrange of settings, the resulting estimator (``SyNBEATS'') significantly\noutperforms commonly employed methods (synthetic controls, two-way fixed\neffects), and attains comparable or more accurate performance compared to\nrecently proposed methods (synthetic difference-in-differences, matrix\ncompletion). An implementation of this estimator is available for public use.\nOur results highlight how advances in the forecasting literature can be\nharnessed to improve causal inference in panel data settings.",
        "authors": [
            "Jacob Goldin",
            "Julian Nyarko",
            "Justin Young"
        ],
        "categories": "econ.EM",
        "published": "2022-08-06T10:23:38Z",
        "updated": "2024-04-16T19:08:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.03381v2",
        "title": "Partial Identification of Personalized Treatment Response with Trial-reported Analyses of Binary Subgroups",
        "abstract": "Medical journals have adhered to a reporting practice that seriously limits\nthe usefulness of published trial findings. Medical decision makers commonly\nobserve many patient covariates and seek to use this information to personalize\ntreatment choices. Yet standard summaries of trial findings only partition\nsubjects into broad subgroups, typically into binary categories. Given this\nreporting practice, we study the problem of inference on long mean treatment\noutcomes E[y(t)|x], where t is a treatment, y(t) is a treatment outcome, and\nthe covariate vector x has length K, each component being a binary variable.\nThe available data are estimates of {E[y(t)|xk = 0], E[y(t)|xk = 1], P(xk)}, k\n= 1, . . . , K reported in journal articles. We show that reported trial\nfindings partially identify {E[y(t)|x], P(x)}. Illustrative computations\ndemonstrate that the summaries of trial findings in journal articles may imply\nonly wide bounds on long mean outcomes. One can realistically tighten\ninferences if one can combine reported trial findings with credible assumptions\nhaving identifying power, such as bounded-variation assumptions.",
        "authors": [
            "Sheyu Li",
            "Valentyn Litvin",
            "Charles F. Manski"
        ],
        "categories": "econ.EM",
        "published": "2022-08-05T20:39:27Z",
        "updated": "2022-09-03T18:46:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.02925v4",
        "title": "Factor Network Autoregressions",
        "abstract": "We propose a factor network autoregressive (FNAR) model for time series with\ncomplex network structures. The coefficients of the model reflect many\ndifferent types of connections between economic agents (``multilayer network\"),\nwhich are summarized into a smaller number of network matrices (``network\nfactors\") through a novel tensor-based principal component approach. We provide\nconsistency and asymptotic normality results for the estimation of the factors\nand the coefficients of the FNAR. Our approach combines two different\ndimension-reduction techniques and can be applied to ultra-high-dimensional\ndatasets. Simulation results show the goodness of our approach in finite\nsamples. In an empirical application, we use the FNAR to investigate the\ncross-country interdependence of GDP growth rates based on a variety of\ninternational trade and financial linkages. The model provides a rich\ncharacterization of macroeconomic network effects.",
        "authors": [
            "Matteo Barigozzi",
            "Giuseppe Cavaliere",
            "Graziano Moramarco"
        ],
        "categories": "econ.EM",
        "published": "2022-08-04T23:01:37Z",
        "updated": "2024-02-26T13:01:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.02516v2",
        "title": "Weak convergence to derivatives of fractional Brownian motion",
        "abstract": "It is well known that, under suitable regularity conditions, the normalized\nfractional process with fractional parameter $d$ converges weakly to fractional\nBrownian motion for $d>1/2$. We show that, for any non-negative integer $M$,\nderivatives of order $m=0,1,\\dots,M$ of the normalized fractional process with\nrespect to the fractional parameter $d$, jointly converge weakly to the\ncorresponding derivatives of fractional Brownian motion. As an illustration we\napply the results to the asymptotic distribution of the score vectors in the\nmultifractional vector autoregressive model.",
        "authors": [
            "S\u00f8ren Johansen",
            "Morten \u00d8rregaard Nielsen"
        ],
        "categories": "math.PR",
        "published": "2022-08-04T08:04:54Z",
        "updated": "2022-10-03T13:46:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.02412v1",
        "title": "Difference-in-Differences with a Misclassified Treatment",
        "abstract": "This paper studies identification and estimation of the average treatment\neffect on the treated (ATT) in difference-in-difference (DID) designs when the\nvariable that classifies individuals into treatment and control groups\n(treatment status, D) is endogenously misclassified. We show that\nmisclassification in D hampers consistent estimation of ATT because 1) it\nrestricts us from identifying the truly treated from those misclassified as\nbeing treated and 2) differential misclassification in counterfactual trends\nmay result in parallel trends being violated with D even when they hold with\nthe true but unobserved D*. We propose a solution to correct for endogenous\none-sided misclassification in the context of a parametric DID regression which\nallows for considerable heterogeneity in treatment effects and establish its\nasymptotic properties in panel and repeated cross section settings.\nFurthermore, we illustrate the method by using it to estimate the insurance\nimpact of a large-scale in-kind food transfer program in India which is known\nto suffer from large targeting errors.",
        "authors": [
            "Akanksha Negi",
            "Digvijay Singh Negi"
        ],
        "categories": "econ.EM",
        "published": "2022-08-04T02:42:49Z",
        "updated": "2022-08-04T02:42:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.02098v3",
        "title": "The Econometrics of Financial Duration Modeling",
        "abstract": "We establish new results for estimation and inference in financial durations\nmodels, where events are observed over a given time span, such as a trading\nday, or a week. For the classical autoregressive conditional duration (ACD)\nmodels by Engle and Russell (1998, Econometrica 66, 1127-1162), we show that\nthe large sample behavior of likelihood estimators is highly sensitive to the\ntail behavior of the financial durations. In particular, even under\nstationarity, asymptotic normality breaks down for tail indices smaller than\none or, equivalently, when the clustering behaviour of the observed events is\nsuch that the unconditional distribution of the durations has no finite mean.\nInstead, we find that estimators are mixed Gaussian and have non-standard rates\nof convergence. The results are based on exploiting the crucial fact that for\nduration data the number of observations within any given time span is random.\nOur results apply to general econometric models where the number of observed\nevents is random.",
        "authors": [
            "Giuseppe Cavaliere",
            "Thomas Mikosch",
            "Anders Rahbek",
            "Frederik Vilandt"
        ],
        "categories": "econ.EM",
        "published": "2022-08-03T14:28:03Z",
        "updated": "2022-12-01T10:31:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.02038v1",
        "title": "Bayesian ranking and selection with applications to field studies, economic mobility, and forecasting",
        "abstract": "Decision-making often involves ranking and selection. For example, to\nassemble a team of political forecasters, we might begin by narrowing our\nchoice set to the candidates we are confident rank among the top 10% in\nforecasting ability. Unfortunately, we do not know each candidate's true\nability but observe a noisy estimate of it. This paper develops new Bayesian\nalgorithms to rank and select candidates based on noisy estimates. Using\nsimulations based on empirical data, we show that our algorithms often\noutperform frequentist ranking and selection algorithms. Our Bayesian ranking\nalgorithms yield shorter rank confidence intervals while maintaining\napproximately correct coverage. Our Bayesian selection algorithms select more\ncandidates while maintaining correct error rates. We apply our ranking and\nselection procedures to field experiments, economic mobility, forecasting, and\nsimilar problems. Finally, we implement our ranking and selection techniques in\na user-friendly Python package documented here:\nhttps://dsbowen-conditional-inference.readthedocs.io/en/latest/.",
        "authors": [
            "Dillon Bowen"
        ],
        "categories": "stat.ME",
        "published": "2022-08-03T13:02:34Z",
        "updated": "2022-08-03T13:02:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.02028v3",
        "title": "Bootstrap inference in the presence of bias",
        "abstract": "We consider bootstrap inference for estimators which are (asymptotically)\nbiased. We show that, even when the bias term cannot be consistently estimated,\nvalid inference can be obtained by proper implementations of the bootstrap.\nSpecifically, we show that the prepivoting approach of Beran (1987, 1988),\noriginally proposed to deliver higher-order refinements, restores bootstrap\nvalidity by transforming the original bootstrap p-value into an asymptotically\nuniform random variable. We propose two different implementations of\nprepivoting (plug-in and double bootstrap), and provide general high-level\nconditions that imply validity of bootstrap inference. To illustrate the\npractical relevance and implementation of our results, we discuss five\nexamples: (i) inference on a target parameter based on model averaging; (ii)\nridge-type regularized estimators; (iii) nonparametric regression; (iv) a\nlocation model for infinite variance data; and (v) dynamic panel data models.",
        "authors": [
            "Giuseppe Cavaliere",
            "S\u00edlvia Gon\u00e7alves",
            "Morten \u00d8rregaard Nielsen",
            "Edoardo Zanelli"
        ],
        "categories": "econ.EM",
        "published": "2022-08-03T12:50:07Z",
        "updated": "2023-11-08T09:01:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.01967v1",
        "title": "Weak Instruments, First-Stage Heteroskedasticity, the Robust F-Test and a GMM Estimator with the Weight Matrix Based on First-Stage Residuals",
        "abstract": "This paper is concerned with the findings related to the robust first-stage\nF-statistic in the Monte Carlo analysis of Andrews (2018), who found in a\nheteroskedastic grouped-data design that even for very large values of the\nrobust F-statistic, the standard 2SLS confidence intervals had large coverage\ndistortions. This finding appears to discredit the robust F-statistic as a test\nfor underidentification. However, it is shown here that large values of the\nrobust F-statistic do imply that there is first-stage information, but this may\nnot be utilized well by the 2SLS estimator, or the standard GMM estimator. An\nestimator that corrects for this is a robust GMM estimator, denoted GMMf, with\nthe robust weight matrix not based on the structural residuals, but on the\nfirst-stage residuals. For the grouped-data setting of Andrews (2018), this\nGMMf estimator gives the weights to the group specific estimators according to\nthe group specific concentration parameters in the same way as 2SLS does under\nhomoskedasticity, which is formally shown using weak instrument asymptotics.\nThe GMMf estimator is much better behaved than the 2SLS estimator in the\nAndrews (2018) design, behaving well in terms of relative bias and Wald-test\nsize distortion at more standard values of the robust F-statistic. We show that\nthe same patterns can occur in a dynamic panel data model when the error\nvariance is heteroskedastic over time. We further derive the conditions under\nwhich the Stock and Yogo (2005) weak instruments critical values apply to the\nrobust F-statistic in relation to the behaviour of the GMMf estimator.",
        "authors": [
            "Frank Windmeijer"
        ],
        "categories": "econ.EM",
        "published": "2022-08-03T10:34:20Z",
        "updated": "2022-08-03T10:34:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.01445v1",
        "title": "Multifractal cross-correlations of bitcoin and ether trading characteristics in the post-COVID-19 time",
        "abstract": "Unlike price fluctuations, the temporal structure of cryptocurrency trading\nhas seldom been a subject of systematic study. In order to fill this gap, we\nanalyse detrended correlations of the price returns, the average number of\ntrades in time unit, and the traded volume based on high-frequency data\nrepresenting two major cryptocurrencies: bitcoin and ether. We apply the\nmultifractal detrended cross-correlation analysis, which is considered the most\nreliable method for identifying nonlinear correlations in time series. We find\nthat all the quantities considered in our study show an unambiguous\nmultifractal structure from both the univariate (auto-correlation) and\nbivariate (cross-correlation) perspectives. We looked at the bitcoin--ether\ncross-correlations in simultaneously recorded signals, as well as in\ntime-lagged signals, in which a time series for one of the cryptocurrencies is\nshifted with respect to the other. Such a shift suppresses the\ncross-correlations partially for short time scales, but does not remove them\ncompletely. We did not observe any qualitative asymmetry in the results for the\ntwo choices of a leading asset. The cross-correlations for the simultaneous and\nlagged time series became the same in magnitude for the sufficiently long\nscales.",
        "authors": [
            "Marcin W\u0105torek",
            "Jaros\u0142aw Kwapie\u0144",
            "Stanis\u0142aw Dro\u017cd\u017c"
        ],
        "categories": "q-fin.ST",
        "published": "2022-08-02T13:24:28Z",
        "updated": "2022-08-02T13:24:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.01300v2",
        "title": "Doubly Robust Estimation of Local Average Treatment Effects Using Inverse Probability Weighted Regression Adjustment",
        "abstract": "We revisit the problem of estimating the local average treatment effect\n(LATE) and the local average treatment effect on the treated (LATT) when\ncontrol variables are available, either to render the instrumental variable\n(IV) suitably exogenous or to improve precision. Unlike previous approaches,\nour doubly robust (DR) estimation procedures use quasi-likelihood methods\nweighted by the inverse of the IV propensity score - so-called inverse\nprobability weighted regression adjustment (IPWRA) estimators. By properly\nchoosing models for the propensity score and outcome models, fitted values are\nensured to be in the logical range determined by the response variable,\nproducing DR estimators of LATE and LATT with appealing small sample\nproperties. Inference is relatively straightforward both analytically and using\nthe nonparametric bootstrap. Our DR LATE and DR LATT estimators work well in\nsimulations. We also propose a DR version of the Hausman test that can be used\nto assess the unconfoundedness assumption through a comparison of different\nestimates of the average treatment effect on the treated (ATT) under one-sided\nnoncompliance. Unlike the usual test that compares OLS and IV estimates, this\nprocedure is robust to treatment effect heterogeneity.",
        "authors": [
            "Tymon S\u0142oczy\u0144ski",
            "S. Derya Uysal",
            "Jeffrey M. Wooldridge"
        ],
        "categories": "econ.EM",
        "published": "2022-08-02T08:09:47Z",
        "updated": "2022-11-15T04:07:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.00972v1",
        "title": "A penalized two-pass regression to predict stock returns with time-varying risk premia",
        "abstract": "We develop a penalized two-pass regression with time-varying factor loadings.\nThe penalization in the first pass enforces sparsity for the time-variation\ndrivers while also maintaining compatibility with the no-arbitrage restrictions\nby regularizing appropriate groups of coefficients. The second pass delivers\nrisk premia estimates to predict equity excess returns. Our Monte Carlo results\nand our empirical results on a large cross-sectional data set of US individual\nstocks show that penalization without grouping can yield to nearly all\nestimated time-varying models violating the no-arbitrage restrictions.\nMoreover, our results demonstrate that the proposed method reduces the\nprediction errors compared to a penalized approach without appropriate grouping\nor a time-invariant factor model.",
        "authors": [
            "Gaetan Bakalli",
            "St\u00e9phane Guerrier",
            "Olivier Scaillet"
        ],
        "categories": "econ.EM",
        "published": "2022-08-01T16:21:20Z",
        "updated": "2022-08-01T16:21:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.00552v2",
        "title": "The Effect of Omitted Variables on the Sign of Regression Coefficients",
        "abstract": "Omitted variables are a common concern in empirical research. We distinguish\nbetween two ways omitted variables can affect baseline estimates: by driving\nthem to zero or by reversing their sign. We show that, depending on how the\nimpact of omitted variables is measured, it can be substantially easier for\nomitted variables to flip coefficient signs than to drive them to zero.\nConsequently, results which are considered robust to being \"explained away\" by\nomitted variables are not necessarily robust to sign changes. We show that this\nbehavior occurs with \"Oster's delta\" (Oster 2019), a commonly reported measure\nof regression coefficient robustness to the presence of omitted variables.\nSpecifically, we show that any time this measure is large--suggesting that\nomitted variables may be unimportant--a much smaller value reverses the sign of\nthe parameter of interest. Relatedly, we show that selection bias adjusted\nestimands can be extremely sensitive to the choice of the sensitivity\nparameter. Specifically, researchers commonly compute a bias adjustment under\nthe assumption that Oster's delta equals one. Under the alternative assumption\nthat delta is very close to one, but not exactly equal to one, we show that the\nbias can instead be arbitrarily large. To address these concerns, we propose a\nmodified measure of robustness that accounts for such sign changes, and discuss\nbest practices for assessing sensitivity to omitted variables. We demonstrate\nthis sign flipping behavior in an empirical application to social capital and\nthe rise of the Nazi party, where we show how it can overturn conclusions about\nrobustness, and how our proposed modifications can be used to regain\nrobustness. We analyze three additional empirical applications as well. We\nimplement our proposed methods in the companion Stata module regsensitivity for\neasy use in practice.",
        "authors": [
            "Matthew A. Masten",
            "Alexandre Poirier"
        ],
        "categories": "econ.EM",
        "published": "2022-08-01T00:50:42Z",
        "updated": "2023-02-12T03:08:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2209.05998v1",
        "title": "Interpreting and predicting the economy flows: A time-varying parameter global vector autoregressive integrated the machine learning model",
        "abstract": "The paper proposes a time-varying parameter global vector autoregressive\n(TVP-GVAR) framework for predicting and analysing developed region economic\nvariables. We want to provide an easily accessible approach for the economy\napplication settings, where a variety of machine learning models can be\nincorporated for out-of-sample prediction. The LASSO-type technique for\nnumerically efficient model selection of mean squared errors (MSEs) is\nselected. We show the convincing in-sample performance of our proposed model in\nall economic variables and relatively high precision out-of-sample predictions\nwith different-frequency economic inputs. Furthermore, the time-varying\northogonal impulse responses provide novel insights into the connectedness of\neconomic variables at critical time points across developed regions. We also\nderive the corresponding asymptotic bands (the confidence intervals) for\northogonal impulse responses function under standard assumptions.",
        "authors": [
            "Yukang Jiang",
            "Xueqin Wang",
            "Zhixi Xiong",
            "Haisheng Yang",
            "Ting Tian"
        ],
        "categories": "econ.EM",
        "published": "2022-07-31T06:24:15Z",
        "updated": "2022-07-31T06:24:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2208.00057v1",
        "title": "Compact representations of structured BFGS matrices",
        "abstract": "For general large-scale optimization problems compact representations exist\nin which recursive quasi-Newton update formulas are represented as compact\nmatrix factorizations. For problems in which the objective function contains\nadditional structure, so-called structured quasi-Newton methods exploit\navailable second-derivative information and approximate unavailable second\nderivatives. This article develops the compact representations of two\nstructured Broyden-Fletcher-Goldfarb-Shanno update formulas. The compact\nrepresentations enable efficient limited memory and initialization strategies.\nTwo limited memory line search algorithms are described and tested on a\ncollection of problems, including a real world large scale imaging application.",
        "authors": [
            "Johannes J. Brust",
            "Zichao",
            "Di",
            "Sven Leyffer",
            "Cosmin G. Petra"
        ],
        "categories": "math.OC",
        "published": "2022-07-29T20:09:16Z",
        "updated": "2022-07-29T20:09:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.14727v2",
        "title": "Tangential Wasserstein Projections",
        "abstract": "We develop a notion of projections between sets of probability measures using\nthe geometric properties of the 2-Wasserstein space. It is designed for general\nmultivariate probability measures, is computationally efficient to implement,\nand provides a unique solution in regular settings. The idea is to work on\nregular tangent cones of the Wasserstein space using generalized geodesics. Its\nstructure and computational properties make the method applicable in a variety\nof settings, from causal inference to the analysis of object data. An\napplication to estimating causal effects yields a generalization of the notion\nof synthetic controls to multivariate data with individual-level heterogeneity,\nas well as a way to estimate optimal weights jointly over all time periods.",
        "authors": [
            "Florian Gunsilius",
            "Meng Hsuan Hsieh",
            "Myung Jin Lee"
        ],
        "categories": "stat.ML",
        "published": "2022-07-29T14:59:58Z",
        "updated": "2022-08-02T18:13:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.14481v2",
        "title": "Same Root Different Leaves: Time Series and Cross-Sectional Methods in Panel Data",
        "abstract": "A central goal in social science is to evaluate the causal effect of a\npolicy. One dominant approach is through panel data analysis in which the\nbehaviors of multiple units are observed over time. The information across time\nand space motivates two general approaches: (i) horizontal regression (i.e.,\nunconfoundedness), which exploits time series patterns, and (ii) vertical\nregression (e.g., synthetic controls), which exploits cross-sectional patterns.\nConventional wisdom states that the two approaches are fundamentally different.\nWe establish this position to be partly false for estimation but generally true\nfor inference. In particular, we prove that both approaches yield identical\npoint estimates under several standard settings. For the same point estimate,\nhowever, each approach quantifies uncertainty with respect to a distinct\nestimand. In turn, the confidence interval developed for one estimand may have\nincorrect coverage for another. This emphasizes that the source of randomness\nthat researchers assume has direct implications for the accuracy of inference.",
        "authors": [
            "Dennis Shen",
            "Peng Ding",
            "Jasjeet Sekhon",
            "Bin Yu"
        ],
        "categories": "econ.EM",
        "published": "2022-07-29T05:12:32Z",
        "updated": "2022-10-08T23:41:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.13939v4",
        "title": "Stable Matching with Mistaken Agents",
        "abstract": "Motivated by growing evidence of agents' mistakes in strategically simple\nenvironments, we propose a solution concept -- robust equilibrium -- that\nrequires only an asymptotically optimal behavior. We use it to study large\nrandom matching markets operated by the applicant-proposing Deferred Acceptance\n(DA). Although truth-telling is a dominant strategy, almost all applicants may\nbe non-truthful in robust equilibrium; however, the outcome must be arbitrarily\nclose to the stable matching. Our results imply that one can assume truthful\nagents to study DA outcomes, theoretically or counterfactually. However, to\nestimate the preferences of mistaken agents, one should assume stable matching\nbut not truth-telling.",
        "authors": [
            "Georgy Artemov",
            "Yeon-Koo Che",
            "YingHua He"
        ],
        "categories": "econ.TH",
        "published": "2022-07-28T08:04:12Z",
        "updated": "2022-10-10T21:52:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.13797v1",
        "title": "Identification and Inference with Min-over-max Estimators for the Measurement of Labor Market Fairness",
        "abstract": "These notes shows how to do inference on the Demographic Parity (DP) metric.\nAlthough the metric is a complex statistic involving min and max computations,\nwe propose a smooth approximation of those functions and derive its asymptotic\ndistribution. The limit of these approximations and their gradients converge to\nthose of the true max and min functions, wherever they exist. More importantly,\nwhen the true max and min functions are not differentiable, the approximations\nstill are, and they provide valid asymptotic inference everywhere in the\ndomain. We conclude with some directions on how to compute confidence intervals\nfor DP, how to test if it is under 0.8 (the U.S. Equal Employment Opportunity\nCommission fairness threshold), and how to do inference in an A/B test.",
        "authors": [
            "Karthik Rajkumar"
        ],
        "categories": "stat.ME",
        "published": "2022-07-27T21:16:01Z",
        "updated": "2022-07-27T21:16:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.13656v2",
        "title": "Conformal Prediction Bands for Two-Dimensional Functional Time Series",
        "abstract": "Time evolving surfaces can be modeled as two-dimensional Functional time\nseries, exploiting the tools of Functional data analysis. Leveraging this\napproach, a forecasting framework for such complex data is developed. The main\nfocus revolves around Conformal Prediction, a versatile nonparametric paradigm\nused to quantify uncertainty in prediction problems. Building upon recent\nvariations of Conformal Prediction for Functional time series, a probabilistic\nforecasting scheme for two-dimensional functional time series is presented,\nwhile providing an extension of Functional Autoregressive Processes of order\none to this setting. Estimation techniques for the latter process are\nintroduced and their performance are compared in terms of the resulting\nprediction regions. Finally, the proposed forecasting procedure and the\nuncertainty quantification technique are applied to a real dataset, collecting\ndaily observations of Sea Level Anomalies of the Black Sea",
        "authors": [
            "Niccol\u00f2 Ajroldi",
            "Jacopo Diquigiovanni",
            "Matteo Fontana",
            "Simone Vantini"
        ],
        "categories": "stat.ME",
        "published": "2022-07-27T17:23:14Z",
        "updated": "2023-07-18T09:38:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.12602v1",
        "title": "Differentially Private Estimation via Statistical Depth",
        "abstract": "Constructing a differentially private (DP) estimator requires deriving the\nmaximum influence of an observation, which can be difficult in the absence of\nexogenous bounds on the input data or the estimator, especially in high\ndimensional settings. This paper shows that standard notions of statistical\ndepth, i.e., halfspace depth and regression depth, are particularly\nadvantageous in this regard, both in the sense that the maximum influence of a\nsingle observation is easy to analyze and that this value is typically low.\nThis is used to motivate new approximate DP location and regression estimators\nusing the maximizers of these two notions of statistical depth. A more\ncomputationally efficient variant of the approximate DP regression estimator is\nalso provided. Also, to avoid requiring that users specify a priori bounds on\nthe estimates and/or the observations, variants of these DP mechanisms are\ndescribed that satisfy random differential privacy (RDP), which is a relaxation\nof differential privacy provided by Hall, Wasserman, and Rinaldo (2013). We\nalso provide simulations of the two DP regression methods proposed here. The\nproposed estimators appear to perform favorably relative to the existing DP\nregression methods we consider in these simulations when either the sample size\nis at least 100-200 or the privacy-loss budget is sufficiently high.",
        "authors": [
            "Ryan Cumings-Menon"
        ],
        "categories": "stat.ML",
        "published": "2022-07-26T01:59:07Z",
        "updated": "2022-07-26T01:59:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.12225v1",
        "title": "Forecasting euro area inflation using a huge panel of survey expectations",
        "abstract": "In this paper, we forecast euro area inflation and its main components using\nan econometric model which exploits a massive number of time series on survey\nexpectations for the European Commission's Business and Consumer Survey. To\nmake estimation of such a huge model tractable, we use recent advances in\ncomputational statistics to carry out posterior simulation and inference. Our\nfindings suggest that the inclusion of a wide range of firms and consumers'\nopinions about future economic developments offers useful information to\nforecast prices and assess tail risks to inflation. These predictive\nimprovements do not only arise from surveys related to expected inflation but\nalso from other questions related to the general economic environment. Finally,\nwe find that firms' expectations about the future seem to have more predictive\ncontent than consumer expectations.",
        "authors": [
            "Florian Huber",
            "Luca Onorante",
            "Michael Pfarrhofer"
        ],
        "categories": "econ.EM",
        "published": "2022-07-25T14:24:32Z",
        "updated": "2022-07-25T14:24:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.12147v1",
        "title": "Sparse Bayesian State-Space and Time-Varying Parameter Models",
        "abstract": "In this chapter, we review variance selection for time-varying parameter\n(TVP) models for univariate and multivariate time series within a Bayesian\nframework. We show how both continuous as well as discrete spike-and-slab\nshrinkage priors can be transferred from variable selection for regression\nmodels to variance selection for TVP models by using a non-centered\nparametrization. We discuss efficient MCMC estimation and provide an\napplication to US inflation modeling.",
        "authors": [
            "Sylvia Fr\u00fchwirth-Schnatter",
            "Peter Knaus"
        ],
        "categories": "econ.EM",
        "published": "2022-07-25T12:49:13Z",
        "updated": "2022-07-25T12:49:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.11890v2",
        "title": "Misclassification in Difference-in-differences Models",
        "abstract": "The difference-in-differences (DID) design is one of the most popular methods\nused in empirical economics research. However, there is almost no work\nexamining what the DID method identifies in the presence of a misclassified\ntreatment variable. This paper studies the identification of treatment effects\nin DID designs when the treatment is misclassified. Misclassification arises in\nvarious ways, including when the timing of a policy intervention is ambiguous\nor when researchers need to infer treatment from auxiliary data. We show that\nthe DID estimand is biased and recovers a weighted average of the average\ntreatment effects on the treated (ATT) in two subpopulations -- the correctly\nclassified and misclassified groups. In some cases, the DID estimand may yield\nthe wrong sign and is otherwise attenuated. We provide bounds on the ATT when\nthe researcher has access to information on the extent of misclassification in\nthe data. We demonstrate our theoretical results using simulations and provide\ntwo empirical applications to guide researchers in performing sensitivity\nanalysis using our proposed methods.",
        "authors": [
            "Augustine Denteh",
            "D\u00e9sir\u00e9 K\u00e9dagni"
        ],
        "categories": "econ.EM",
        "published": "2022-07-25T03:37:10Z",
        "updated": "2022-07-30T11:25:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.11557v1",
        "title": "Detecting common bubbles in multivariate mixed causal-noncausal models",
        "abstract": "This paper proposes methods to investigate whether the bubble patterns\nobserved in individual series are common to various series. We detect the\nnon-linear dynamics using the recent mixed causal and noncausal models. Both a\nlikelihood ratio test and information criteria are investigated, the former\nhaving better performances in our Monte Carlo simulations. Implementing our\napproach on three commodity prices we do not find evidence of commonalities\nalthough some series look very similar.",
        "authors": [
            "Gianluca Cubadda",
            "Alain Hecq",
            "Elisa Voisin"
        ],
        "categories": "econ.EM",
        "published": "2022-07-23T17:27:35Z",
        "updated": "2022-07-23T17:27:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.11137v3",
        "title": "A Conditional Linear Combination Test with Many Weak Instruments",
        "abstract": "We consider a linear combination of jackknife Anderson-Rubin (AR), jackknife\nLagrangian multiplier (LM), and orthogonalized jackknife LM tests for inference\nin IV regressions with many weak instruments and heteroskedasticity. Following\nI.Andrews (2016), we choose the weights in the linear combination based on a\ndecision-theoretic rule that is adaptive to the identification strength. Under\nboth weak and strong identifications, the proposed test controls asymptotic\nsize and is admissible among certain class of tests. Under strong\nidentification, our linear combination test has optimal power against local\nalternatives among the class of invariant or unbiased tests which are\nconstructed based on jackknife AR and LM tests. Simulations and an empirical\napplication to Angrist and Krueger's (1991) dataset confirm the good power\nproperties of our test.",
        "authors": [
            "Dennis Lim",
            "Wenjie Wang",
            "Yichong Zhang"
        ],
        "categories": "econ.EM",
        "published": "2022-07-22T15:19:00Z",
        "updated": "2023-04-20T06:36:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.11003v1",
        "title": "Time-Varying Poisson Autoregression",
        "abstract": "In this paper we propose a new time-varying econometric model, called\nTime-Varying Poisson AutoRegressive with eXogenous covariates (TV-PARX), suited\nto model and forecast time series of counts. {We show that the score-driven\nframework is particularly suitable to recover the evolution of time-varying\nparameters and provides the required flexibility to model and forecast time\nseries of counts characterized by convoluted nonlinear dynamics and structural\nbreaks.} We study the asymptotic properties of the TV-PARX model and prove\nthat, under mild conditions, maximum likelihood estimation (MLE) yields\nstrongly consistent and asymptotically normal parameter estimates.\nFinite-sample performance and forecasting accuracy are evaluated through Monte\nCarlo simulations. The empirical usefulness of the time-varying specification\nof the proposed TV-PARX model is shown by analyzing the number of new daily\nCOVID-19 infections in Italy and the number of corporate defaults in the US.",
        "authors": [
            "Giovanni Angelini",
            "Giuseppe Cavaliere",
            "Enzo D'Innocenzo",
            "Luca De Angelis"
        ],
        "categories": "econ.EM",
        "published": "2022-07-22T10:43:18Z",
        "updated": "2022-07-22T10:43:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.10076v1",
        "title": "Testing for a Threshold in Models with Endogenous Regressors",
        "abstract": "We show by simulation that the test for an unknown threshold in models with\nendogenous regressors - proposed in Caner and Hansen (2004) - can exhibit\nsevere size distortions both in small and in moderately large samples,\npertinent to empirical applications. We propose three new tests that rectify\nthese size distortions. The first test is based on GMM estimators. The other\ntwo are based on unconventional 2SLS estimators, that use additional\ninformation about the linearity (or lack of linearity) of the first stage. Just\nlike the test in Caner and Hansen (2004), our tests are non-pivotal, and we\nprove their bootstrap validity. The empirical application revisits the question\nin Ramey and Zubairy (2018) whether government spending multipliers are larger\nin recessions, but using tests for an unknown threshold. Consistent with Ramey\nand Zubairy (2018), we do not find strong evidence that these multipliers are\nlarger in recessions.",
        "authors": [
            "Mario P. Rothfelder",
            "Otilia Boldea"
        ],
        "categories": "econ.EM",
        "published": "2022-07-20T17:59:47Z",
        "updated": "2022-07-20T17:59:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.09943v4",
        "title": "Efficient Bias Correction for Cross-section and Panel Data",
        "abstract": "Bias correction can often improve the finite sample performance of\nestimators. We show that the choice of bias correction method has no effect on\nthe higher-order variance of semiparametrically efficient parametric\nestimators, so long as the estimate of the bias is asymptotically linear. It is\nalso shown that bootstrap, jackknife, and analytical bias estimates are\nasymptotically linear for estimators with higher-order expansions of a standard\nform. In particular, we find that for a variety of estimators the\nstraightforward bootstrap bias correction gives the same higher-order variance\nas more complicated analytical or jackknife bias corrections. In contrast, bias\ncorrections that do not estimate the bias at the parametric rate, such as the\nsplit-sample jackknife, result in larger higher-order variances in the i.i.d.\nsetting we focus on. For both a cross-sectional MLE and a panel model with\nindividual fixed effects, we show that the split-sample jackknife has a\nhigher-order variance term that is twice as large as that of the\n`leave-one-out' jackknife.",
        "authors": [
            "Jinyong Hahn",
            "David W. Hughes",
            "Guido Kuersteiner",
            "Whitney K. Newey"
        ],
        "categories": "econ.EM",
        "published": "2022-07-20T14:39:31Z",
        "updated": "2024-01-26T18:25:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.09246v3",
        "title": "Asymptotic Properties of Endogeneity Corrections Using Nonlinear Transformations",
        "abstract": "This paper considers a linear regression model with an endogenous regressor\nwhich arises from a nonlinear transformation of a latent variable. It is shown\nthat the corresponding coefficient can be consistently estimated without\nexternal instruments by adding a rank-based transformation of the regressor to\nthe model and performing standard OLS estimation. In contrast to other\napproaches, our nonparametric control function approach does not rely on a\nconformably specified copula. Furthermore, the approach allows for the presence\nof additional exogenous regressors which may be (linearly) correlated with the\nendogenous regressor(s). Consistency and asymptotic normality of the estimator\nare proved and the estimator is compared with copula based approaches by means\nof Monte Carlo simulations. An empirical application on wage data of the US\ncurrent population survey demonstrates the usefulness of our method.",
        "authors": [
            "J\u00f6rg Breitung",
            "Alexander Mayer",
            "Dominik Wied"
        ],
        "categories": "econ.EM",
        "published": "2022-07-19T12:59:26Z",
        "updated": "2023-11-07T15:41:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.09016v1",
        "title": "The role of the geometric mean in case-control studies",
        "abstract": "Historically used in settings where the outcome is rare or data collection is\nexpensive, outcome-dependent sampling is relevant to many modern settings where\ndata is readily available for a biased sample of the target population, such as\npublic administrative data. Under outcome-dependent sampling, common effect\nmeasures such as the average risk difference and the average risk ratio are not\nidentified, but the conditional odds ratio is. Aggregation of the conditional\nodds ratio is challenging since summary measures are generally not identified.\nFurthermore, the marginal odds ratio can be larger (or smaller) than all\nconditional odds ratios. This so-called non-collapsibility of the odds ratio is\navoidable if we use an alternative aggregation to the standard arithmetic mean.\nWe provide a new definition of collapsibility that makes this choice of\naggregation method explicit, and we demonstrate that the odds ratio is\ncollapsible under geometric aggregation. We describe how to partially identify,\nestimate, and do inference on the geometric odds ratio under outcome-dependent\nsampling. Our proposed estimator is based on the efficient influence function\nand therefore has doubly robust-style properties.",
        "authors": [
            "Amanda Coston",
            "Edward H. Kennedy"
        ],
        "categories": "stat.ME",
        "published": "2022-07-19T01:42:52Z",
        "updated": "2022-07-19T01:42:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.09004v1",
        "title": "Bias correction and uniform inference for the quantile density function",
        "abstract": "For the kernel estimator of the quantile density function (the derivative of\nthe quantile function), I show how to perform the boundary bias correction,\nestablish the rate of strong uniform consistency of the bias-corrected\nestimator, and construct the confidence bands that are asymptotically exact\nuniformly over the entire domain $[0,1]$. The proposed procedures rely on the\npivotality of the studentized bias-corrected estimator and known\nanti-concentration properties of the Gaussian approximation for its supremum.",
        "authors": [
            "Grigory Franguridi"
        ],
        "categories": "econ.EM",
        "published": "2022-07-19T01:09:02Z",
        "updated": "2022-07-19T01:09:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.08868v2",
        "title": "Isotonic propensity score matching",
        "abstract": "We propose a one-to-many matching estimator of the average treatment effect\nbased on propensity scores estimated by isotonic regression. The method relies\non the monotonicity assumption on the propensity score function, which can be\njustified in many applications in economics. We show that the nature of the\nisotonic estimator can help us to fix many problems of existing matching\nmethods, including efficiency, choice of the number of matches, choice of\ntuning parameters, robustness to propensity score misspecification, and\nbootstrap validity. As a by-product, a uniformly consistent isotonic estimator\nis developed for our proposed matching method.",
        "authors": [
            "Mengshan Xu",
            "Taisuke Otsu"
        ],
        "categories": "econ.EM",
        "published": "2022-07-18T18:27:01Z",
        "updated": "2024-08-30T10:01:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.08789v2",
        "title": "Estimating Continuous Treatment Effects in Panel Data using Machine Learning with a Climate Application",
        "abstract": "This paper introduces and proves asymptotic normality for a new\nsemi-parametric estimator of continuous treatment effects in panel data.\nSpecifically, we estimate the average derivative. Our estimator uses the panel\nstructure of data to account for unobservable time-invariant heterogeneity and\nmachine learning (ML) methods to preserve statistical power while modeling\nhigh-dimensional relationships. We construct our estimator using tools from\ndouble de-biased machine learning (DML) literature. Monte Carlo simulations in\na nonlinear panel setting show that our method estimates the average derivative\nwith low bias and variance relative to other approaches. Lastly, we use our\nestimator to measure the impact of extreme heat on United States (U.S.) corn\nproduction, after flexibly controlling for precipitation and other weather\nfeatures. Our approach yields extreme heat effect estimates that are 50% larger\nthan estimates using linear regression. This difference in estimates\ncorresponds to an additional $3.17 billion in annual damages by 2050 under\nmedian climate scenarios. We also estimate a dose-response curve, which shows\nthat damages from extreme heat decline somewhat in counties with more extreme\nheat exposure.",
        "authors": [
            "Sylvia Klosin",
            "Max Vilgalys"
        ],
        "categories": "econ.EM",
        "published": "2022-07-18T17:44:35Z",
        "updated": "2023-09-13T22:13:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.08249v1",
        "title": "Testing for explosive bubbles: a review",
        "abstract": "This review discusses methods of testing for explosive bubbles in time\nseries. A large number of recently developed testing methods under various\nassumptions about innovation of errors are covered. The review also considers\nthe methods for dating explosive (bubble) regimes. Special attention is devoted\nto time-varying volatility in the errors. Moreover, the modelling of possible\nrelationships between time series with explosive regimes is discussed.",
        "authors": [
            "Anton Skrobotov"
        ],
        "categories": "econ.EM",
        "published": "2022-07-17T18:15:07Z",
        "updated": "2022-07-17T18:15:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.07343v2",
        "title": "Simultaneity in Binary Outcome Models with an Application to Employment for Couples",
        "abstract": "Two of Peter Schmidt's many contributions to econometrics have been to\nintroduce a simultaneous logit model for bivariate binary outcomes and to study\nestimation of dynamic linear fixed effects panel data models using short\npanels. In this paper, we study a dynamic panel data version of the bivariate\nmodel introduced in Schmidt and Strauss (1975) that allows for lagged dependent\nvariables and fixed effects as in Ahn and Schmidt (1995). We combine a\nconditional likelihood approach with a method of moments approach to obtain an\nestimation strategy for the resulting model. We apply this estimation strategy\nto a simple model for the intra-household relationship in employment. Our main\nconclusion is that the within-household dependence in employment differs\nsignificantly by the ethnicity composition of the couple even after one allows\nfor unobserved household specific heterogeneity.",
        "authors": [
            "Bo E. Honor\u00e9",
            "Luojia Hu",
            "Ekaterini Kyriazidou",
            "Martin Weidner"
        ],
        "categories": "econ.EM",
        "published": "2022-07-15T08:39:29Z",
        "updated": "2023-03-17T14:21:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.07318v3",
        "title": "Flexible global forecast combinations",
        "abstract": "Forecast combination -- the aggregation of individual forecasts from multiple\nexperts or models -- is a proven approach to economic forecasting. To date,\nresearch on economic forecasting has concentrated on local combination methods,\nwhich handle separate but related forecasting tasks in isolation. Yet, it has\nbeen known for over two decades in the machine learning community that global\nmethods, which exploit task-relatedness, can improve on local methods that\nignore it. Motivated by the possibility for improvement, this paper introduces\na framework for globally combining forecasts while being flexible to the level\nof task-relatedness. Through our framework, we develop global versions of\nseveral existing forecast combinations. To evaluate the efficacy of these new\nglobal forecast combinations, we conduct extensive comparisons using synthetic\nand real data. Our real data comparisons, which involve forecasts of core\neconomic indicators in the Eurozone, provide empirical evidence that the\naccuracy of global combinations of economic forecasts can surpass local\ncombinations.",
        "authors": [
            "Ryan Thompson",
            "Yilin Qian",
            "Andrey L. Vasnev"
        ],
        "categories": "econ.EM",
        "published": "2022-07-15T07:23:06Z",
        "updated": "2024-03-09T05:05:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.07055v4",
        "title": "High Dimensional Generalised Penalised Least Squares",
        "abstract": "In this paper we develop inference for high dimensional linear models, with\nserially correlated errors. We examine Lasso under the assumption of strong\nmixing in the covariates and error process, allowing for fatter tails in their\ndistribution. While the Lasso estimator performs poorly under such\ncircumstances, we estimate via GLS Lasso the parameters of interest and extend\nthe asymptotic properties of the Lasso under more general conditions. Our\ntheoretical results indicate that the non-asymptotic bounds for stationary\ndependent processes are sharper, while the rate of Lasso under general\nconditions appears slower as $T,p\\to \\infty$. Further we employ the debiased\nLasso to perform inference uniformly on the parameters of interest. Monte Carlo\nresults support the proposed estimator, as it has significant efficiency gains\nover traditional methods.",
        "authors": [
            "Ilias Chronopoulos",
            "Katerina Chrysikou",
            "George Kapetanios"
        ],
        "categories": "econ.EM",
        "published": "2022-07-14T16:57:01Z",
        "updated": "2023-10-04T14:30:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.06564v3",
        "title": "Parallel Trends and Dynamic Choices",
        "abstract": "Difference-in-differences is a common method for estimating treatment\neffects, and the parallel trends condition is its main identifying assumption:\nthe trend in mean untreated outcomes is independent of the observed treatment\nstatus. In observational settings, treatment is often a dynamic choice made or\ninfluenced by rational actors, such as policy-makers, firms, or individual\nagents. This paper relates parallel trends to economic models of dynamic\nchoice. We clarify the implications of parallel trends on agent behavior and\nstudy when dynamic selection motives lead to violations of parallel trends.\nFinally, we consider identification under alternative assumptions that\naccommodate features of dynamic choice.",
        "authors": [
            "Philip Marx",
            "Elie Tamer",
            "Xun Tang"
        ],
        "categories": "econ.EM",
        "published": "2022-07-14T00:02:41Z",
        "updated": "2023-08-08T17:46:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.06558v1",
        "title": "Parametric quantile regression for income data",
        "abstract": "Univariate normal regression models are statistical tools widely applied in\nmany areas of economics. Nevertheless, income data have asymmetric behavior and\nare best modeled by non-normal distributions. The modeling of income plays an\nimportant role in determining workers' earnings, as well as being an important\nresearch topic in labor economics. Thus, the objective of this work is to\npropose parametric quantile regression models based on two important asymmetric\nincome distributions, namely, Dagum and Singh-Maddala distributions. The\nproposed quantile models are based on reparameterizations of the original\ndistributions by inserting a quantile parameter. We present the\nreparameterizations, some properties of the distributions, and the quantile\nregression models with their inferential aspects. We proceed with Monte Carlo\nsimulation studies, considering the maximum likelihood estimation performance\nevaluation and an analysis of the empirical distribution of two residuals. The\nMonte Carlo results show that both models meet the expected outcomes. We apply\nthe proposed quantile regression models to a household income data set provided\nby the National Institute of Statistics of Chile. We showed that both proposed\nmodels had a good performance both in terms of model fitting. Thus, we conclude\nthat results were favorable to the use of Singh-Maddala and Dagum quantile\nregression models for positive asymmetric data, such as income data.",
        "authors": [
            "Helton Saulo",
            "Roberto Vila",
            "Giovanna V. Borges",
            "Marcelo Bourguignon"
        ],
        "categories": "stat.ME",
        "published": "2022-07-13T23:42:15Z",
        "updated": "2022-07-13T23:42:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.05943v1",
        "title": "Two-stage differences in differences",
        "abstract": "A recent literature has shown that when adoption of a treatment is staggered\nand average treatment effects vary across groups and over time,\ndifference-in-differences regression does not identify an easily interpretable\nmeasure of the typical effect of the treatment. In this paper, I extend this\nliterature in two ways. First, I provide some simple underlying intuition for\nwhy difference-in-differences regression does not identify a\ngroup$\\times$period average treatment effect. Second, I propose an alternative\ntwo-stage estimation framework, motivated by this intuition. In this framework,\ngroup and period effects are identified in a first stage from the sample of\nuntreated observations, and average treatment effects are identified in a\nsecond stage by comparing treated and untreated outcomes, after removing these\ngroup and period effects. The two-stage approach is robust to treatment-effect\nheterogeneity under staggered adoption, and can be used to identify a host of\ndifferent average treatment effect measures. It is also simple, intuitive, and\neasy to implement. I establish the theoretical properties of the two-stage\napproach and demonstrate its effectiveness and applicability using Monte-Carlo\nevidence and an example from the literature.",
        "authors": [
            "John Gardner"
        ],
        "categories": "econ.EM",
        "published": "2022-07-13T03:44:58Z",
        "updated": "2022-07-13T03:44:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.04481v2",
        "title": "Detecting Grouped Local Average Treatment Effects and Selecting True Instruments",
        "abstract": "Under an endogenous binary treatment with heterogeneous effects and multiple\ninstruments, we propose a two-step procedure for identifying complier groups\nwith identical local average treatment effects (LATE) despite relying on\ndistinct instruments, even if several instruments violate the identifying\nassumptions. We use the fact that the LATE is homogeneous for instruments which\n(i) satisfy the LATE assumptions (instrument validity and treatment\nmonotonicity in the instrument) and (ii) generate identical complier groups in\nterms of treatment propensities given the respective instruments. We propose a\ntwo-step procedure, where we first cluster the propensity scores in the first\nstep and find groups of IVs with the same reduced form parameters in the second\nstep. Under the plurality assumption that within each set of instruments with\nidentical treatment propensities, instruments truly satisfying the LATE\nassumptions are the largest group, our procedure permits identifying these true\ninstruments in a data driven way. We show that our procedure is consistent and\nprovides consistent and asymptotically normal estimators of underlying LATEs.\nWe also provide a simulation study investigating the finite sample properties\nof our approach and an empirical application investigating the effect of\nincarceration on recidivism in the US with judge assignments serving as\ninstruments.",
        "authors": [
            "Nicolas Apfel",
            "Helmut Farbmacher",
            "Rebecca Groh",
            "Martin Huber",
            "Henrika Langen"
        ],
        "categories": "econ.EM",
        "published": "2022-07-10T15:08:39Z",
        "updated": "2023-10-31T11:19:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.04314v1",
        "title": "Identification and Inference for Welfare Gains without Unconfoundedness",
        "abstract": "This paper studies identification and inference of the welfare gain that\nresults from switching from one policy (such as the status quo policy) to\nanother policy. The welfare gain is not point identified in general when data\nare obtained from an observational study or a randomized experiment with\nimperfect compliance. I characterize the sharp identified region of the welfare\ngain and obtain bounds under various assumptions on the unobservables with and\nwithout instrumental variables. Estimation and inference of the lower and upper\nbounds are conducted using orthogonalized moment conditions to deal with the\npresence of infinite-dimensional nuisance parameters. I illustrate the analysis\nby considering hypothetical policies of assigning individuals to job training\nprograms using experimental data from the National Job Training Partnership Act\nStudy. Monte Carlo simulations are conducted to assess the finite sample\nperformance of the estimators.",
        "authors": [
            "Undral Byambadalai"
        ],
        "categories": "econ.EM",
        "published": "2022-07-09T18:06:08Z",
        "updated": "2022-07-09T18:06:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.04299v1",
        "title": "Model diagnostics of discrete data regression: a unifying framework using functional residuals",
        "abstract": "Model diagnostics is an indispensable component of regression analysis, yet\nit is not well addressed in standard textbooks on generalized linear models.\nThe lack of exposition is attributed to the fact that when outcome data are\ndiscrete, classical methods (e.g., Pearson/deviance residual analysis and\ngoodness-of-fit tests) have limited utility in model diagnostics and treatment.\nThis paper establishes a novel framework for model diagnostics of discrete data\nregression. Unlike the literature defining a single-valued quantity as the\nresidual, we propose to use a function as a vehicle to retain the residual\ninformation. In the presence of discreteness, we show that such a functional\nresidual is appropriate for summarizing the residual randomness that cannot be\ncaptured by the structural part of the model. We establish its theoretical\nproperties, which leads to the innovation of new diagnostic tools including the\nfunctional-residual-vs covariate plot and Function-to-Function (Fn-Fn) plot.\nOur numerical studies demonstrate that the use of these tools can reveal a\nvariety of model misspecifications, such as not properly including a\nhigher-order term, an explanatory variable, an interaction effect, a dispersion\nparameter, or a zero-inflation component. The functional residual yields, as a\nbyproduct, Liu-Zhang's surrogate residual mainly developed for cumulative link\nmodels for ordinal data (Liu and Zhang, 2018, JASA). As a general notion, it\nconsiderably broadens the diagnostic scope as it applies to virtually all\nparametric models for binary, ordinal and count data, all in a unified\ndiagnostic scheme.",
        "authors": [
            "Zewei Lin",
            "Dungang Liu"
        ],
        "categories": "stat.ME",
        "published": "2022-07-09T17:00:18Z",
        "updated": "2022-07-09T17:00:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.04082v1",
        "title": "Spatial Econometrics for Misaligned Data",
        "abstract": "We produce methodology for regression analysis when the geographic locations\nof the independent and dependent variables do not coincide, in which case we\nspeak of misaligned data. We develop and investigate two complementary methods\nfor regression analysis with misaligned data that circumvent the need to\nestimate or specify the covariance of the regression errors. We carry out a\ndetailed reanalysis of Maccini and Yang (2009) and find economically\nsignificant quantitative differences but sustain most qualitative conclusions.",
        "authors": [
            "Guillaume Allaire Pouliot"
        ],
        "categories": "econ.EM",
        "published": "2022-07-08T18:12:50Z",
        "updated": "2022-07-08T18:12:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.03988v1",
        "title": "Large Bayesian VARs with Factor Stochastic Volatility: Identification, Order Invariance and Structural Analysis",
        "abstract": "Vector autoregressions (VARs) with multivariate stochastic volatility are\nwidely used for structural analysis. Often the structural model identified\nthrough economically meaningful restrictions--e.g., sign restrictions--is\nsupposed to be independent of how the dependent variables are ordered. But\nsince the reduced-form model is not order invariant, results from the\nstructural analysis depend on the order of the variables. We consider a VAR\nbased on the factor stochastic volatility that is constructed to be order\ninvariant. We show that the presence of multivariate stochastic volatility\nallows for statistical identification of the model. We further prove that, with\na suitable set of sign restrictions, the corresponding structural model is\npoint-identified. An additional appeal of the proposed approach is that it can\neasily handle a large number of dependent variables as well as sign\nrestrictions. We demonstrate the methodology through a structural analysis in\nwhich we use a 20-variable VAR with sign restrictions to identify 5 structural\nshocks.",
        "authors": [
            "Joshua Chan",
            "Eric Eisenstat",
            "Xuewen Yu"
        ],
        "categories": "econ.EM",
        "published": "2022-07-08T16:22:26Z",
        "updated": "2022-07-08T16:22:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.03035v2",
        "title": "On the instrumental variable estimation with many weak and invalid instruments",
        "abstract": "We discuss the fundamental issue of identification in linear instrumental\nvariable (IV) models with unknown IV validity. With the assumption of the\n\"sparsest rule\", which is equivalent to the plurality rule but becomes\noperational in computation algorithms, we investigate and prove the advantages\nof non-convex penalized approaches over other IV estimators based on two-step\nselections, in terms of selection consistency and accommodation for\nindividually weak IVs. Furthermore, we propose a surrogate sparsest penalty\nthat aligns with the identification condition and provides oracle sparse\nstructure simultaneously. Desirable theoretical properties are derived for the\nproposed estimator with weaker IV strength conditions compared to the previous\nliterature. Finite sample properties are demonstrated using simulations and the\nselection and estimation method is applied to an empirical study concerning the\neffect of BMI on diastolic blood pressure.",
        "authors": [
            "Yiqi Lin",
            "Frank Windmeijer",
            "Xinyuan Song",
            "Qingliang Fan"
        ],
        "categories": "stat.ME",
        "published": "2022-07-07T01:31:34Z",
        "updated": "2023-12-05T15:06:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.02943v1",
        "title": "Degrees of Freedom and Information Criteria for the Synthetic Control Method",
        "abstract": "We provide an analytical characterization of the model flexibility of the\nsynthetic control method (SCM) in the familiar form of degrees of freedom. We\nobtain estimable information criteria. These may be used to circumvent\ncross-validation when selecting either the weighting matrix in the SCM with\ncovariates, or the tuning parameter in model averaging or penalized variants of\nSCM. We assess the impact of car license rationing in Tianjin and make a novel\nuse of SCM; while a natural match is available, it and other donors are noisy,\ninviting the use of SCM to average over approximately matching donors. The very\nlarge number of candidate donors calls for model averaging or penalized\nvariants of SCM and, with short pre-treatment series, model selection per\ninformation criteria outperforms that per cross-validation.",
        "authors": [
            "Guillaume Allaire Pouliot",
            "Zhen Xie"
        ],
        "categories": "econ.EM",
        "published": "2022-07-06T19:52:03Z",
        "updated": "2022-07-06T19:52:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.01533v2",
        "title": "csa2sls: A complete subset approach for many instruments using Stata",
        "abstract": "We develop a Stata command $\\texttt{csa2sls}$ that implements the complete\nsubset averaging two-stage least squares (CSA2SLS) estimator in Lee and Shin\n(2021). The CSA2SLS estimator is an alternative to the two-stage least squares\nestimator that remedies the bias issue caused by many correlated instruments.\nWe conduct Monte Carlo simulations and confirm that the CSA2SLS estimator\nreduces both the mean squared error and the estimation bias substantially when\ninstruments are correlated. We illustrate the usage of $\\texttt{csa2sls}$ in\nStata by an empirical application.",
        "authors": [
            "Seojeong Lee",
            "Siha Lee",
            "Julius Owusu",
            "Youngki Shin"
        ],
        "categories": "econ.EM",
        "published": "2022-07-04T15:54:15Z",
        "updated": "2023-04-04T20:51:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.00683v1",
        "title": "A Comparison of Methods for Adaptive Experimentation",
        "abstract": "We use a simulation study to compare three methods for adaptive\nexperimentation: Thompson sampling, Tempered Thompson sampling, and Exploration\nsampling. We gauge the performance of each in terms of social welfare and\nestimation accuracy, and as a function of the number of experimental waves. We\nfurther construct a set of novel \"hybrid\" loss measures to identify which\nmethods are optimal for researchers pursuing a combination of experimental\naims. Our main results are: 1) the relative performance of Thompson sampling\ndepends on the number of experimental waves, 2) Tempered Thompson sampling\nuniquely distributes losses across multiple experimental aims, and 3) in most\ncases, Exploration sampling performs similarly to random assignment.",
        "authors": [
            "Samantha Horn",
            "Sabina J. Sloman"
        ],
        "categories": "stat.ME",
        "published": "2022-07-01T23:12:52Z",
        "updated": "2022-07-01T23:12:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2207.00206v1",
        "title": "Valid and Unobtrusive Measurement of Returns to Advertising through Asymmetric Budget Split",
        "abstract": "Ad platforms require reliable measurement of advertising returns: what\nincrease in performance (such as clicks or conversions) can an advertiser\nexpect in return for additional budget on the platform? Even from the\nperspective of the platform, accurately measuring advertising returns is hard.\nSelection and omitted variable biases make estimates from observational methods\nunreliable, and straightforward experimentation is often costly or infeasible.\nWe introduce Asymmetric Budget Split, a novel methodology for valid measurement\nof ad returns from the perspective of the platform. Asymmetric budget split\ncreates small asymmetries in ad budget allocation across comparable partitions\nof the platform's userbase. By observing performance of the same ad at\ndifferent budget levels while holding all other factors constant, the platform\ncan obtain a valid measure of ad returns. The methodology is unobtrusive and\ncost-effective in that it does not require holdout groups or sacrifices in ad\nor marketplace performance. We discuss a successful deployment of asymmetric\nbudget split to LinkedIn's Jobs Marketplace, an ad marketplace where it is used\nto measure returns from promotion budgets in terms of incremental job\napplicants. We outline operational considerations for practitioners and discuss\nfurther use cases such as budget-aware performance forecasting.",
        "authors": [
            "Johannes Hermle",
            "Giorgio Martini"
        ],
        "categories": "econ.EM",
        "published": "2022-07-01T05:13:05Z",
        "updated": "2022-07-01T05:13:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.15039v1",
        "title": "Unique futures in China: studys on volatility spillover effects of ferrous metal futures",
        "abstract": "Ferrous metal futures have become unique commodity futures with Chinese\ncharacteristics. Due to the late listing time, it has received less attention\nfrom scholars. Our research focuses on the volatility spillover effects,\ndefined as the intensity of price volatility in financial instruments. We use\nDCC-GARCH, BEKK-GARCH, and DY(2012) index methods to conduct empirical tests on\nthe volatility spillover effects of the Chinese ferrous metal futures market\nand other parts of the Chinese commodity futures market, as well as industries\nrelated to the steel industry chain in stock markets. It can be seen that there\nis a close volatility spillover relationship between ferrous metal futures and\nnonferrous metal futures. Energy futures and chemical futures have a\nsignificant transmission effect on the fluctuations of ferrous metals. In\naddition, ferrous metal futures have a significant spillover effect on the\nstock index of the steel industry, real estate industry, building materials\nindustry, machinery equipment industry, and household appliance industry.\nStudying the volatility spillover effect of the ferrous metal futures market\ncan reveal the operating laws of this field and provide ideas and theoretical\nreferences for investors to hedge their risks. It shows that the ferrous metal\nfutures market has an essential role as a \"barometer\" for the Chinese commodity\nfutures market and the stock market.",
        "authors": [
            "Tingting Cao",
            "Weiqing Sun",
            "Cuiping Sun",
            "Lin Hao"
        ],
        "categories": "econ.EM",
        "published": "2022-06-30T06:09:06Z",
        "updated": "2022-06-30T06:09:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.14275v3",
        "title": "Dynamic CoVaR Modeling",
        "abstract": "The popular systemic risk measure CoVaR (conditional Value-at-Risk) is widely\nused in economics and finance. Formally, it is defined as a large quantile of\none variable (e.g., losses in the financial system) conditional on some other\nvariable (e.g., losses in a bank's shares) being in distress. In this article,\nwe propose joint dynamic forecasting models for the Value-at-Risk (VaR) and\nCoVaR. We also introduce a two-step M-estimator for the model parameters\ndrawing on recently proposed bivariate scoring functions for the pair (VaR,\nCoVaR). We prove consistency and asymptotic normality of our parameter\nestimator and analyze its finite-sample properties in simulations. Finally, we\napply a specific subclass of our dynamic forecasting models, which we call\nCoCAViaR models, to log-returns of large US banks. It is shown that our\nCoCAViaR models generate CoVaR predictions that are superior to forecasts\nissued from current benchmark models.",
        "authors": [
            "Timo Dimitriadis",
            "Yannick Hoga"
        ],
        "categories": "econ.EM",
        "published": "2022-06-28T20:09:35Z",
        "updated": "2024-02-23T11:02:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.14128v1",
        "title": "Business Cycle Synchronization in the EU: A Regional-Sectoral Look through Soft-Clustering and Wavelet Decomposition",
        "abstract": "This paper elaborates on the sectoral-regional view of the business cycle\nsynchronization in the EU -- a necessary condition for the optimal currency\narea. We argue that complete and tidy clustering of the data improves the\ndecision maker's understanding of the business cycle and, by extension, the\nquality of economic decisions. We define the business cycles by applying a\nwavelet approach to drift-adjusted gross value added data spanning over 2000Q1\nto 2021Q2. For the application of the synchronization analysis, we propose the\nnovel soft-clustering approach, which adjusts hierarchical clustering in\nseveral aspects. First, the method relies on synchronicity dissimilarity\nmeasures, noting that, for time series data, the feature space is the set of\nall points in time. Then, the ``soft'' part of the approach strengthens the\nsynchronization signal by using silhouette measures. Finally, we add a\nprobabilistic sparsity algorithm to drop out the most asynchronous ``noisy''\ndata improving the silhouette scores of the most and less synchronous groups.\nThe method, hence, splits the sectoral-regional data into three groups: the\nsynchronous group that shapes the EU business cycle; the less synchronous group\nthat may hint at cycle forecasting relevant information; the asynchronous group\nthat may help investors to diversify through-the-cycle risks of the investment\nportfolios. The results support the core-periphery hypothesis.",
        "authors": [
            "Saulius Jokubaitis",
            "Dmitrij Celov"
        ],
        "categories": "econ.EM",
        "published": "2022-06-28T16:42:02Z",
        "updated": "2022-06-28T16:42:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.13751v4",
        "title": "Estimating the Currency Composition of Foreign Exchange Reserves",
        "abstract": "Central banks manage about \\$12 trillion in foreign exchange reserves,\ninfluencing global exchange rates and asset prices. However, some of the\nlargest holders of reserves report minimal information about their currency\ncomposition, hindering empirical analysis. I describe a Hidden Markov Model to\nestimate the composition of a central bank's reserves by relating the\nfluctuation in the portfolio's valuation to the exchange rates of major reserve\ncurrencies. I apply the model to China and Singapore, two countries that\ncollectively hold about \\$3.4 trillion in reserves and conceal their\ncomposition. I find that both China's reserve composition likely resembles the\nglobal average, while Singapore probably holds fewer US dollars.",
        "authors": [
            "Matthew Ferranti"
        ],
        "categories": "q-fin.ST",
        "published": "2022-06-28T04:34:00Z",
        "updated": "2023-05-08T00:43:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.13600v1",
        "title": "Misspecification and Weak Identification in Asset Pricing",
        "abstract": "The widespread co-existence of misspecification and weak identification in\nasset pricing has led to an overstated performance of risk factors. Because the\nconventional Fama and MacBeth (1973) methodology is jeopardized by\nmisspecification and weak identification, we infer risk premia by using a\ndouble robust Lagrange multiplier test that remains reliable in the presence of\nthese two empirically relevant issues. Moreover, we show how the\nidentification, and the resulting appropriate interpretation, of the risk\npremia is governed by the relative magnitudes of the misspecification\nJ-statistic and the identification IS-statistic. We revisit several prominent\nempirical applications and all specifications with one to six factors from the\nfactor zoo of Feng, Giglio, and Xiu (2020) to emphasize the widespread\noccurrence of misspecification and weak identification.",
        "authors": [
            "Frank Kleibergen",
            "Zhaoguo Zhan"
        ],
        "categories": "econ.EM",
        "published": "2022-06-27T19:26:32Z",
        "updated": "2022-06-27T19:26:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.12919v2",
        "title": "Instrumented Common Confounding",
        "abstract": "Causal inference is difficult in the presence of unobserved confounders. We\nintroduce the instrumented common confounding (ICC) approach to\n(nonparametrically) identify causal effects with instruments, which are\nexogenous only conditional on some unobserved common confounders. The ICC\napproach is most useful in rich observational data with multiple sources of\nunobserved confounding, where instruments are at most exogenous conditional on\nsome unobserved common confounders. Suitable examples of this setting are\nvarious identification problems in the social sciences, nonlinear dynamic\npanels, and problems with multiple endogenous confounders. The ICC identifying\nassumptions are closely related to those in mixture models, negative control\nand IV. Compared to mixture models [Bonhomme et al., 2016], we require less\nconditionally independent variables and do not need to model the unobserved\nconfounder. Compared to negative control [Cui et al., 2020], we allow for\nnon-common confounders, with respect to which the instruments are exogenous.\nCompared to IV [Newey and Powell, 2003], we allow instruments to be exogenous\nconditional on some unobserved common confounders, for which a set of relevant\nobserved variables exists. We prove point identification with outcome model and\nalternatively first stage restrictions. We provide a practical step-by-step\nguide to the ICC model assumptions and present the causal effect of education\non income as a motivating example.",
        "authors": [
            "Christian Tien"
        ],
        "categories": "econ.EM",
        "published": "2022-06-26T16:31:38Z",
        "updated": "2022-09-19T09:50:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.12152v2",
        "title": "Estimation and Inference in High-Dimensional Panel Data Models with Interactive Fixed Effects",
        "abstract": "We develop new econometric methods for estimation and inference in\nhigh-dimensional panel data models with interactive fixed effects. Our approach\ncan be regarded as a non-trivial extension of the very popular common\ncorrelated effects (CCE) approach. Roughly speaking, we proceed as follows: We\nfirst construct a projection device to eliminate the unobserved factors from\nthe model by applying a dimensionality reduction transform to the matrix of\ncross-sectionally averaged covariates. The unknown parameters are then\nestimated by applying lasso techniques to the projected model. For inference\npurposes, we derive a desparsified version of our lasso-type estimator. While\nthe original CCE approach is restricted to the low-dimensional case where the\nnumber of regressors is small and fixed, our methods can deal with both low-\nand high-dimensional situations where the number of regressors is large and may\neven exceed the overall sample size. We derive theory for our estimation and\ninference methods both in the large-T-case, where the time series length T\ntends to infinity, and in the small-T-case, where T is a fixed natural number.\nSpecifically, we derive the convergence rate of our estimator and show that its\ndesparsified version is asymptotically normal under suitable regularity\nconditions. The theoretical analysis of the paper is complemented by a\nsimulation study and an empirical application to characteristic based asset\npricing.",
        "authors": [
            "Oliver Linton",
            "Maximilian Ruecker",
            "Michael Vogt",
            "Christopher Walsh"
        ],
        "categories": "econ.EM",
        "published": "2022-06-24T08:30:39Z",
        "updated": "2024-11-28T08:51:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.10721v2",
        "title": "Assessing and Comparing Fixed-Target Forecasts of Arctic Sea Ice: Glide Charts for Feature-Engineered Linear Regression and Machine Learning Models",
        "abstract": "We use \"glide charts\" (plots of sequences of root mean squared forecast\nerrors as the target date is approached) to evaluate and compare fixed-target\nforecasts of Arctic sea ice. We first use them to evaluate the simple\nfeature-engineered linear regression (FELR) forecasts of Diebold and Goebel\n(2021), and to compare FELR forecasts to naive pure-trend benchmark forecasts.\nThen we introduce a much more sophisticated feature-engineered machine learning\n(FEML) model, and we use glide charts to evaluate FEML forecasts and compare\nthem to a FELR benchmark. Our substantive results include the frequent\nappearance of predictability thresholds, which differ across months, meaning\nthat accuracy initially fails to improve as the target date is approached but\nthen increases progressively once a threshold lead time is crossed. Also, we\nfind that FEML can improve appreciably over FELR when forecasting \"turning\npoint\" months in the annual cycle at horizons of one to three months ahead.",
        "authors": [
            "Francis X. Diebold",
            "Maximilian Goebel",
            "Philippe Goulet Coulombe"
        ],
        "categories": "econ.EM",
        "published": "2022-06-21T20:43:48Z",
        "updated": "2023-06-08T19:58:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.10475v6",
        "title": "New possibilities in identification of binary choice models with fixed effects",
        "abstract": "We study the identification of binary choice models with fixed effects. We\nprovide a condition called sign saturation and show that this condition is\nsufficient for the identification of the model. In particular, we can guarantee\nidentification even when all the regressors are bounded, including multiple\ndiscrete regressors. We also show that without this condition, the model is not\nidentified unless the error distribution belongs to a special class. The same\nsign saturation condition is also essential for identifying the sign of\ntreatment effects. A test is provided to check the sign saturation condition\nand can be implemented using existing algorithms for the maximum score\nestimator.",
        "authors": [
            "Yinchu Zhu"
        ],
        "categories": "econ.EM",
        "published": "2022-06-21T15:40:14Z",
        "updated": "2024-12-09T17:40:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.10054v1",
        "title": "Symmetric generalized Heckman models",
        "abstract": "The sample selection bias problem arises when a variable of interest is\ncorrelated with a latent variable, and involves situations in which the\nresponse variable had part of its observations censored. Heckman (1976)\nproposed a sample selection model based on the bivariate normal distribution\nthat fits both the variable of interest and the latent variable. Recently, this\nassumption of normality has been relaxed by more flexible models such as the\nStudent-t distribution (Marchenko and Genton, 2012; Lachos et al., 2021). The\naim of this work is to propose generalized Heckman sample selection models\nbased on symmetric distributions (Fang et al., 1990). This is a new class of\nsample selection models, in which variables are added to the dispersion and\ncorrelation parameters. A Monte Carlo simulation study is performed to assess\nthe behavior of the parameter estimation method. Two real data sets are\nanalyzed to illustrate the proposed approach.",
        "authors": [
            "Helton Saulo",
            "Roberto Vila",
            "Shayane S. Cordeiro"
        ],
        "categories": "stat.ME",
        "published": "2022-06-21T00:29:24Z",
        "updated": "2022-06-21T00:29:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.09883v3",
        "title": "Policy Learning under Endogeneity Using Instrumental Variables",
        "abstract": "This paper studies the identification and estimation of individualized\nintervention policies in observational data settings characterized by\nendogenous treatment selection and the availability of instrumental variables.\nWe introduce encouragement rules that manipulate an instrument. Incorporating\nthe marginal treatment effects (MTE) as policy invariant structural parameters,\nwe establish the identification of the social welfare criterion for the optimal\nencouragement rule. Focusing on binary encouragement rules, we propose to\nestimate the optimal policy via the Empirical Welfare Maximization (EWM) method\nand derive convergence rates of the regret (welfare loss). We consider\nextensions to accommodate multiple instruments and budget constraints. Using\ndata from the Indonesian Family Life Survey, we apply the EWM encouragement\nrule to advise on the optimal tuition subsidy assignment. Our framework offers\ninterpretability regarding why a certain subpopulation is targeted.",
        "authors": [
            "Yan Liu"
        ],
        "categories": "econ.EM",
        "published": "2022-06-20T16:37:30Z",
        "updated": "2024-03-01T22:35:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.09644v1",
        "title": "Unbiased estimation of the OLS covariance matrix when the errors are clustered",
        "abstract": "When data are clustered, common practice has become to do OLS and use an\nestimator of the covariance matrix of the OLS estimator that comes close to\nunbiasedness. In this paper we derive an estimator that is unbiased when the\nrandom-effects model holds. We do the same for two more general structures. We\nstudy the usefulness of these estimators against others by simulation, the size\nof the $t$-test being the criterion. Our findings suggest that the choice of\nestimator hardly matters when the regressor has the same distribution over the\nclusters. But when the regressor is a cluster-specific treatment variable, the\nchoice does matter and the unbiased estimator we propose for the random-effects\nmodel shows excellent performance, even when the clusters are highly\nunbalanced.",
        "authors": [
            "Tom Boot",
            "Gianmaria Niccodemi",
            "Tom Wansbeek"
        ],
        "categories": "econ.EM",
        "published": "2022-06-20T08:51:35Z",
        "updated": "2022-06-20T08:51:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.09300v1",
        "title": "Optimal data-driven hiring with equity for underrepresented groups",
        "abstract": "We present a data-driven prescriptive framework for fair decisions, motivated\nby hiring. An employer evaluates a set of applicants based on their observable\nattributes. The goal is to hire the best candidates while avoiding bias with\nregard to a certain protected attribute. Simply ignoring the protected\nattribute will not eliminate bias due to correlations in the data. We present a\nhiring policy that depends on the protected attribute functionally, but not\nstatistically, and we prove that, among all possible fair policies, ours is\noptimal with respect to the firm's objective. We test our approach on both\nsynthetic and real data, and find that it shows great practical potential to\nimprove equity for underrepresented and historically marginalized groups.",
        "authors": [
            "Yinchu Zhu",
            "Ilya O. Ryzhov"
        ],
        "categories": "econ.EM",
        "published": "2022-06-19T01:05:31Z",
        "updated": "2022-06-19T01:05:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.09073v1",
        "title": "Interpretable and Actionable Vehicular Greenhouse Gas Emission Prediction at Road link-level",
        "abstract": "To help systematically lower anthropogenic Greenhouse gas (GHG) emissions,\naccurate and precise GHG emission prediction models have become a key focus of\nthe climate research. The appeal is that the predictive models will inform\npolicymakers, and hopefully, in turn, they will bring about systematic changes.\nSince the transportation sector is constantly among the top GHG emission\ncontributors, especially in populated urban areas, substantial effort has been\ngoing into building more accurate and informative GHG prediction models to help\ncreate more sustainable urban environments. In this work, we seek to establish\na predictive framework of GHG emissions at the urban road segment or link level\nof transportation networks. The key theme of the framework centers around model\ninterpretability and actionability for high-level decision-makers using\neconometric Discrete Choice Modelling (DCM). We illustrate that DCM is capable\nof predicting link-level GHG emission levels on urban road networks in a\nparsimonious and effective manner. Our results show up to 85.4% prediction\naccuracy in the DCM models' performances. We also argue that since the goal of\nmost GHG emission prediction models focuses on involving high-level\ndecision-makers to make changes and curb emissions, the DCM-based GHG emission\nprediction framework is the most suitable framework.",
        "authors": [
            "S. Roderick Zhang",
            "Bilal Farooq"
        ],
        "categories": "econ.EM",
        "published": "2022-06-18T00:49:51Z",
        "updated": "2022-06-18T00:49:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.08503v3",
        "title": "Semiparametric Single-Index Estimation for Average Treatment Effects",
        "abstract": "We propose a semiparametric method to estimate the average treatment effect\nunder the assumption of unconfoundedness given observational data. Our\nestimation method alleviates misspecification issues of the propensity score\nfunction by estimating the single-index link function involved through Hermite\npolynomials. Our approach is computationally tractable and allows for\nmoderately large dimension covariates. We provide the large sample properties\nof the estimator and show its validity. Also, the average treatment effect\nestimator achieves the parametric rate and asymptotic normality. Our extensive\nMonte Carlo study shows that the proposed estimator is valid in finite samples.\nWe also provide an empirical analysis on the effect of maternal smoking on\nbabies' birth weight and the effect of job training program on future earnings.",
        "authors": [
            "Difang Huang",
            "Jiti Gao",
            "Tatsushi Oka"
        ],
        "categories": "econ.EM",
        "published": "2022-06-17T01:44:53Z",
        "updated": "2024-04-14T14:41:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.08438v1",
        "title": "Fast and Accurate Variational Inference for Large Bayesian VARs with Stochastic Volatility",
        "abstract": "We propose a new variational approximation of the joint posterior\ndistribution of the log-volatility in the context of large Bayesian VARs. In\ncontrast to existing approaches that are based on local approximations, the new\nproposal provides a global approximation that takes into account the entire\nsupport of the joint distribution. In a Monte Carlo study we show that the new\nglobal approximation is over an order of magnitude more accurate than existing\nalternatives. We illustrate the proposed methodology with an application of a\n96-variable VAR with stochastic volatility to measure global bank network\nconnectedness.",
        "authors": [
            "Joshua C. C. Chan",
            "Xuewen Yu"
        ],
        "categories": "econ.EM",
        "published": "2022-06-16T20:42:27Z",
        "updated": "2022-06-16T20:42:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.08052v2",
        "title": "Likelihood ratio test for structural changes in factor models",
        "abstract": "A factor model with a break in its factor loadings is observationally\nequivalent to a model without changes in the loadings but a change in the\nvariance of its factors. This effectively transforms a structural change\nproblem of high dimension into a problem of low dimension. This paper considers\nthe likelihood ratio (LR) test for a variance change in the estimated factors.\nThe LR test implicitly explores a special feature of the estimated factors: the\npre-break and post-break variances can be a singular matrix under the\nalternative hypothesis, making the LR test diverging faster and thus more\npowerful than Wald-type tests. The better power property of the LR test is also\nconfirmed by simulations. We also consider mean changes and multiple breaks. We\napply the procedure to the factor modelling and structural change of the US\nemployment using monthly industry-level-data.",
        "authors": [
            "Jushan Bai",
            "Jiangtao Duan",
            "Xu Han"
        ],
        "categories": "econ.EM",
        "published": "2022-06-16T10:08:39Z",
        "updated": "2023-12-05T13:36:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.07845v1",
        "title": "Optimality of Matched-Pair Designs in Randomized Controlled Trials",
        "abstract": "In randomized controlled trials (RCTs), treatment is often assigned by\nstratified randomization. I show that among all stratified randomization\nschemes which treat all units with probability one half, a certain matched-pair\ndesign achieves the maximum statistical precision for estimating the average\ntreatment effect (ATE). In an important special case, the optimal design pairs\nunits according to the baseline outcome. In a simulation study based on\ndatasets from 10 RCTs, this design lowers the standard error for the estimator\nof the ATE by 10% on average, and by up to 34%, relative to the original\ndesigns.",
        "authors": [
            "Yuehao Bai"
        ],
        "categories": "econ.EM",
        "published": "2022-06-15T23:21:13Z",
        "updated": "2022-06-15T23:21:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.07386v1",
        "title": "Finite-Sample Guarantees for High-Dimensional DML",
        "abstract": "Debiased machine learning (DML) offers an attractive way to estimate\ntreatment effects in observational settings, where identification of causal\nparameters requires a conditional independence or unconfoundedness assumption,\nsince it allows to control flexibly for a potentially very large number of\ncovariates. This paper gives novel finite-sample guarantees for joint inference\non high-dimensional DML, bounding how far the finite-sample distribution of the\nestimator is from its asymptotic Gaussian approximation. These guarantees are\nuseful to applied researchers, as they are informative about how far off the\ncoverage of joint confidence bands can be from the nominal level. There are\nmany settings where high-dimensional causal parameters may be of interest, such\nas the ATE of many treatment profiles, or the ATE of a treatment on many\noutcomes. We also cover infinite-dimensional parameters, such as impacts on the\nentire marginal distribution of potential outcomes. The finite-sample\nguarantees in this paper complement the existing results on consistency and\nasymptotic normality of DML estimators, which are either asymptotic or treat\nonly the one-dimensional case.",
        "authors": [
            "Victor Quintas-Martinez"
        ],
        "categories": "econ.EM",
        "published": "2022-06-15T08:48:58Z",
        "updated": "2022-06-15T08:48:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.06892v1",
        "title": "A new algorithm for structural restrictions in Bayesian vector autoregressions",
        "abstract": "A comprehensive methodology for inference in vector autoregressions (VARs)\nusing sign and other structural restrictions is developed. The reduced-form VAR\ndisturbances are driven by a few common factors and structural identification\nrestrictions can be incorporated in their loadings in the form of parametric\nrestrictions. A Gibbs sampler is derived that allows for reduced-form\nparameters and structural restrictions to be sampled efficiently in one step. A\nkey benefit of the proposed approach is that it allows for treating parameter\nestimation and structural inference as a joint problem. An additional benefit\nis that the methodology can scale to large VARs with multiple shocks, and it\ncan be extended to accommodate non-linearities, asymmetries, and numerous other\ninteresting empirical features. The excellent properties of the new algorithm\nfor inference are explored using synthetic data experiments, and by revisiting\nthe role of financial factors in economic fluctuations using identification\nbased on sign restrictions.",
        "authors": [
            "Dimitris Korobilis"
        ],
        "categories": "econ.EM",
        "published": "2022-06-14T14:48:24Z",
        "updated": "2022-06-14T14:48:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.06823v1",
        "title": "Nowcasting the Portuguese GDP with Monthly Data",
        "abstract": "In this article, we present a method to forecast the Portuguese gross\ndomestic product (GDP) in each current quarter (nowcasting). It combines bridge\nequations of the real GDP on readily available monthly data like the Economic\nSentiment Indicator (ESI), industrial production index, cement sales or exports\nand imports, with forecasts for the jagged missing values computed with the\nwell-known Hodrick and Prescott (HP) filter. As shown, this simple multivariate\napproach can perform as well as a Targeted Diffusion Index (TDI) model and\nslightly better than the univariate Theta method in terms of out-of-sample mean\nerrors.",
        "authors": [
            "Jo\u00e3o B. Assun\u00e7\u00e3o",
            "Pedro Afonso Fernandes"
        ],
        "categories": "econ.EM",
        "published": "2022-06-14T13:15:57Z",
        "updated": "2022-06-14T13:15:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.06493v1",
        "title": "A novel reconstruction attack on foreign-trade official statistics, with a Brazilian case study",
        "abstract": "In this paper we describe, formalize, implement, and experimentally evaluate\na novel transaction re-identification attack against official foreign-trade\nstatistics releases in Brazil. The attack's goal is to re-identify the\nimporters of foreign-trade transactions (by revealing the identity of the\ncompany performing that transaction), which consequently violates those\nimporters' fiscal secrecy (by revealing sensitive information: the value and\nvolume of traded goods). We provide a mathematical formalization of this fiscal\nsecrecy problem using principles from the framework of quantitative information\nflow (QIF), then carefully identify the main sources of imprecision in the\nofficial data releases used as auxiliary information in the attack, and model\ntransaction re-construction as a linear optimization problem solvable through\ninteger linear programming (ILP). We show that this problem is NP-complete, and\nprovide a methodology to identify tractable instances. We exemplify the\nfeasibility of our attack by performing 2,003 transaction re-identifications\nthat in total amount to more than \\$137M, and affect 348 Brazilian companies.\nFurther, since similar statistics are produced by other statistical agencies,\nour attack is of broader concern.",
        "authors": [
            "Danilo Fabrino Favato",
            "Gabriel Coutinho",
            "M\u00e1rio S. Alvim",
            "Natasha Fernandes"
        ],
        "categories": "cs.CR",
        "published": "2022-06-13T21:48:17Z",
        "updated": "2022-06-13T21:48:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.06309v2",
        "title": "Clustering coefficients as measures of the complex interactions in a directed weighted multilayer network",
        "abstract": "In this paper, we provide novel definitions of clustering coefficient for\nweighted and directed multilayer networks. We extend in the multilayer\ntheoretical context the clustering coefficients proposed in the literature for\nweighted directed monoplex networks. We quantify how deeply a node is involved\nin a choesive structure focusing on a single node, on a single layer or on the\nentire system. The coefficients convey several characteristics inherent to the\ncomplex topology of the multilayer network. We test their effectiveness\napplying them to a particularly complex structure such as the international\ntrade network. The trade data integrate different aspects and they can be\ndescribed by a directed and weighted multilayer network, where each layer\nrepresents import and export relationships between countries for a given\nsector. The proposed coefficients find successful application in describing the\ninterrelations of the trade network, allowing to disentangle the effects of\ncountries and sectors and jointly consider the interactions between them.",
        "authors": [
            "Paolo Bartesaghi",
            "Gian Paolo Clemente",
            "Rosanna Grassi"
        ],
        "categories": "econ.EM",
        "published": "2022-06-13T16:48:03Z",
        "updated": "2022-12-23T10:26:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.06116v1",
        "title": "A Constructive GAN-based Approach to Exact Estimate Treatment Effect without Matching",
        "abstract": "Matching has become the mainstream in counterfactual inference, with which\nselection bias between sample groups can be significantly eliminated. However\nin practice, when estimating average treatment effect on the treated (ATT) via\nmatching, no matter which method, the trade-off between estimation accuracy and\ninformation loss constantly exist. Attempting to completely replace the\nmatching process, this paper proposes the GAN-ATT estimator that integrates\ngenerative adversarial network (GAN) into counterfactual inference framework.\nThrough GAN machine learning, the probability density functions (PDFs) of\nsamples in both treatment group and control group can be approximated. By\ndifferentiating conditional PDFs of the two groups with identical input\ncondition, the conditional average treatment effect (CATE) can be estimated,\nand the ensemble average of corresponding CATEs over all treatment group\nsamples is the estimate of ATT. Utilizing GAN-based infinite sample\naugmentations, problems in the case of insufficient samples or lack of common\nsupport domains can be easily solved. Theoretically, when GAN could perfectly\nlearn the PDFs, our estimators can provide exact estimate of ATT.\n  To check the performance of the GAN-ATT estimator, three sets of data are\nused for ATT estimations: Two toy data sets with 1/2 dimensional covariate\ninputs and constant/covariate-dependent treatment effect are tested. The\nestimates of GAN-ATT are proved close to the ground truth and are better than\ntraditional matching approaches; A real firm-level data set with\nhigh-dimensional input is tested and the applicability towards real data sets\nis evaluated by comparing matching approaches. Through the evidences obtained\nfrom the three tests, we believe that the GAN-ATT estimator has significant\nadvantages over traditional matching methods in estimating ATT.",
        "authors": [
            "Boyang You",
            "Kerry Papps"
        ],
        "categories": "econ.EM",
        "published": "2022-06-13T12:54:29Z",
        "updated": "2022-06-13T12:54:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.06026v1",
        "title": "Robust Knockoffs for Controlling False Discoveries With an Application to Bond Recovery Rates",
        "abstract": "We address challenges in variable selection with highly correlated data that\nare frequently present in finance, economics, but also in complex natural\nsystems as e.g. weather. We develop a robustified version of the knockoff\nframework, which addresses challenges with high dependence among possibly many\ninfluencing factors and strong time correlation. In particular, the repeated\nsubsampling strategy tackles the variability of the knockoffs and the\ndependency of factors. Simultaneously, we also control the proportion of false\ndiscoveries over a grid of all possible values, which mitigates variability of\nselected factors from ad-hoc choices of a specific false discovery level. In\nthe application for corporate bond recovery rates, we identify new important\ngroups of relevant factors on top of the known standard drivers. But we also\nshow that out-of-sample, the resulting sparse model has similar predictive\npower to state-of-the-art machine learning models that use the entire set of\npredictors.",
        "authors": [
            "Konstantin G\u00f6rgen",
            "Abdolreza Nazemi",
            "Melanie Schienle"
        ],
        "categories": "econ.EM",
        "published": "2022-06-13T10:25:57Z",
        "updated": "2022-06-13T10:25:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.05235v3",
        "title": "Machine Learning Inference on Inequality of Opportunity",
        "abstract": "Equality of opportunity has emerged as an important ideal of distributive\njustice. Empirically, Inequality of Opportunity (IOp) is measured in two steps:\nfirst, an outcome (e.g., income) is predicted given individual circumstances;\nand second, an inequality index (e.g., Gini) of the predictions is computed.\nMachine Learning (ML) methods are tremendously useful in the first step.\nHowever, they can cause sizable biases in IOp since the bias-variance trade-off\nallows the bias to creep in the second step. We propose a simple debiased IOp\nestimator robust to such ML biases and provide the first valid inferential\ntheory for IOp. We demonstrate improved performance in simulations and report\nthe first unbiased measures of income IOp in Europe. Mother's education and\nfather's occupation are the circumstances that explain the most. Plug-in\nestimators are very sensitive to the ML algorithm, while debiased IOp\nestimators are robust. These results are extended to a general U-statistics\nsetting.",
        "authors": [
            "Juan Carlos Escanciano",
            "Jo\u00ebl Robert Terschuur"
        ],
        "categories": "econ.EM",
        "published": "2022-06-10T17:16:36Z",
        "updated": "2023-10-04T16:28:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.04902v4",
        "title": "Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!",
        "abstract": "Vector autogressions (VARs) are widely applied when it comes to modeling and\nforecasting macroeconomic variables. In high dimensions, however, they are\nprone to overfitting. Bayesian methods, more concretely shrinkage priors, have\nshown to be successful in improving prediction performance. In the present\npaper, we introduce the semi-global framework, in which we replace the\ntraditional global shrinkage parameter with group-specific shrinkage\nparameters. We show how this framework can be applied to various shrinkage\npriors, such as global-local priors and stochastic search variable selection\npriors. We demonstrate the virtues of the proposed framework in an extensive\nsimulation study and in an empirical application forecasting data of the US\neconomy. Further, we shed more light on the ongoing ``Illusion of Sparsity''\ndebate, finding that forecasting performances under sparse/dense priors vary\nacross evaluated economic variables and across time frames. Dynamic model\naveraging, however, can combine the merits of both worlds.",
        "authors": [
            "Luis Gruber",
            "Gregor Kastner"
        ],
        "categories": "econ.EM",
        "published": "2022-06-10T06:57:53Z",
        "updated": "2024-11-13T14:09:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.04643v4",
        "title": "On the Performance of the Neyman Allocation with Small Pilots",
        "abstract": "The Neyman Allocation is used in many papers on experimental design, which\ntypically assume that researchers have access to large pilot studies. This may\nbe unrealistic. To understand the properties of the Neyman Allocation with\nsmall pilots, we study its behavior in an asymptotic framework that takes pilot\nsize to be fixed even as the size of the main wave tends to infinity. Our\nanalysis shows that the Neyman Allocation can lead to estimates of the ATE with\nhigher asymptotic variance than with (non-adaptive) balanced randomization. In\nparticular, this happens when the outcome variable is relatively homoskedastic\nwith respect to treatment status or when it exhibits high kurtosis. We provide\na series of empirical examples showing that such situations can arise in\npractice. Our results suggest that researchers with small pilots should not use\nthe Neyman Allocation if they believe that outcomes are homoskedastic or\nheavy-tailed. Finally, we examine some potential methods for improving the\nfinite sample performance of the FNA via simulations.",
        "authors": [
            "Yong Cai",
            "Ahnaf Rafi"
        ],
        "categories": "econ.EM",
        "published": "2022-06-09T17:38:41Z",
        "updated": "2024-06-04T15:24:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.04605v6",
        "title": "A Two-Ball Ellsberg Paradox: An Experiment",
        "abstract": "We conduct an incentivized experiment on a nationally representative US\nsample \\\\ (N=708) to test whether people prefer to avoid ambiguity even when it\nmeans choosing dominated options. In contrast to the literature, we find that\n55\\% of subjects prefer a risky act to an ambiguous act that always provides a\nlarger probability of winning. Our experimental design shows that such a\npreference is not mainly due to a lack of understanding. We conclude that\nsubjects avoid ambiguity \\textit{per se} rather than avoiding ambiguity because\nit may yield a worse outcome. Such behavior cannot be reconciled with existing\nmodels of ambiguity aversion in a straightforward manner.",
        "authors": [
            "Brian Jabarian",
            "Simon Lazarus"
        ],
        "categories": "econ.TH",
        "published": "2022-06-09T16:50:38Z",
        "updated": "2022-11-21T11:37:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.04157v5",
        "title": "Inference for Matched Tuples and Fully Blocked Factorial Designs",
        "abstract": "This paper studies inference in randomized controlled trials with multiple\ntreatments, where treatment status is determined according to a \"matched\ntuples\" design. Here, by a matched tuples design, we mean an experimental\ndesign where units are sampled i.i.d. from the population of interest, grouped\ninto \"homogeneous\" blocks with cardinality equal to the number of treatments,\nand finally, within each block, each treatment is assigned exactly once\nuniformly at random. We first study estimation and inference for matched tuples\ndesigns in the general setting where the parameter of interest is a vector of\nlinear contrasts over the collection of average potential outcomes for each\ntreatment. Parameters of this form include standard average treatment effects\nused to compare one treatment relative to another, but also include parameters\nwhich may be of interest in the analysis of factorial designs. We first\nestablish conditions under which a sample analogue estimator is asymptotically\nnormal and construct a consistent estimator of its corresponding asymptotic\nvariance. Combining these results establishes the asymptotic exactness of tests\nbased on these estimators. In contrast, we show that, for two common testing\nprocedures based on t-tests constructed from linear regressions, one test is\ngenerally conservative while the other generally invalid. We go on to apply our\nresults to study the asymptotic properties of what we call \"fully-blocked\" 2^K\nfactorial designs, which are simply matched tuples designs applied to a full\nfactorial experiment. Leveraging our previous results, we establish that our\nestimator achieves a lower asymptotic variance under the fully-blocked design\nthan that under any stratified factorial design which stratifies the\nexperimental sample into a finite number of \"large\" strata. A simulation study\nand empirical application illustrate the practical relevance of our results.",
        "authors": [
            "Yuehao Bai",
            "Jizhou Liu",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2022-06-08T20:41:06Z",
        "updated": "2023-11-02T18:22:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.03187v2",
        "title": "Economic activity and climate change",
        "abstract": "In this paper, we survey recent econometric contributions to measure the\nrelationship between economic activity and climate change. Due to the critical\nrelevance of these effects for the well-being of future generations, there is\nan explosion of publications devoted to measuring this relationship and its\nmain channels. The relation between economic activity and climate change is\ncomplex with the possibility of causality running in both directions. Starting\nfrom economic activity, the channels that relate economic activity and climate\nchange are energy consumption and the consequent pollution. Hence, we first\ndescribe the main econometric contributions about the interactions between\neconomic activity and energy consumption, moving then to describing the\ncontributions on the interactions between economic activity and pollution.\nFinally, we look at the main results on the relationship between climate change\nand economic activity. An important consequence of climate change is the\nincreasing occurrence of extreme weather phenomena. Therefore, we also survey\ncontributions on the economic effects of catastrophic climate phenomena.",
        "authors": [
            "Ar\u00e1nzazu de Juan",
            "Pilar Poncela",
            "Vladimir Rodr\u00edguez-Caballero",
            "Esther Ruiz"
        ],
        "categories": "econ.EM",
        "published": "2022-06-07T11:18:15Z",
        "updated": "2022-06-15T14:38:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.02376v1",
        "title": "The Impact of Sampling Variability on Estimated Combinations of Distributional Forecasts",
        "abstract": "We investigate the performance and sampling variability of estimated forecast\ncombinations, with particular attention given to the combination of forecast\ndistributions. Unknown parameters in the forecast combination are optimized\naccording to criterion functions based on proper scoring rules, which are\nchosen to reward the form of forecast accuracy that matters for the problem at\nhand, and forecast performance is measured using the out-of-sample expectation\nof said scoring rule. Our results provide novel insights into the behavior of\nestimated forecast combinations. Firstly, we show that, asymptotically, the\nsampling variability in the performance of standard forecast combinations is\ndetermined solely by estimation of the constituent models, with estimation of\nthe combination weights contributing no sampling variability whatsoever, at\nfirst order. Secondly, we show that, if computationally feasible, forecast\ncombinations produced in a single step -- in which the constituent model and\ncombination function parameters are estimated jointly -- have superior\npredictive accuracy and lower sampling variability than standard forecast\ncombinations -- where constituent model and combination function parameters are\nestimated in two steps. These theoretical insights are demonstrated\nnumerically, both in simulation settings and in an extensive empirical\nillustration using a time series of S&P500 returns.",
        "authors": [
            "Ryan Zischke",
            "Gael M. Martin",
            "David T. Frazier",
            "D. S. Poskitt"
        ],
        "categories": "stat.ME",
        "published": "2022-06-06T06:11:03Z",
        "updated": "2022-06-06T06:11:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.02371v2",
        "title": "Markovian Interference in Experiments",
        "abstract": "We consider experiments in dynamical systems where interventions on some\nexperimental units impact other units through a limiting constraint (such as a\nlimited inventory). Despite outsize practical importance, the best estimators\nfor this `Markovian' interference problem are largely heuristic in nature, and\ntheir bias is not well understood. We formalize the problem of inference in\nsuch experiments as one of policy evaluation. Off-policy estimators, while\nunbiased, apparently incur a large penalty in variance relative to\nstate-of-the-art heuristics. We introduce an on-policy estimator: the\nDifferences-In-Q's (DQ) estimator. We show that the DQ estimator can in general\nhave exponentially smaller variance than off-policy evaluation. At the same\ntime, its bias is second order in the impact of the intervention. This yields a\nstriking bias-variance tradeoff so that the DQ estimator effectively dominates\nstate-of-the-art alternatives. From a theoretical perspective, we introduce\nthree separate novel techniques that are of independent interest in the theory\nof Reinforcement Learning (RL). Our empirical evaluation includes a set of\nexperiments on a city-scale ride-hailing simulator.",
        "authors": [
            "Vivek F. Farias",
            "Andrew A. Li",
            "Tianyi Peng",
            "Andrew Zheng"
        ],
        "categories": "cs.LG",
        "published": "2022-06-06T05:53:36Z",
        "updated": "2022-06-09T14:13:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.02303v4",
        "title": "Assessing Omitted Variable Bias when the Controls are Endogenous",
        "abstract": "Omitted variables are one of the most important threats to the identification\nof causal effects. Several widely used approaches, including Oster (2019),\nassess the impact of omitted variables on empirical conclusions by comparing\nmeasures of selection on observables with measures of selection on\nunobservables. These approaches either (1) assume the omitted variables are\nuncorrelated with the included controls, an assumption that is often considered\nstrong and implausible, or (2) use a method called residualization to avoid\nthis assumption. In our first contribution, we develop a framework for\nobjectively comparing sensitivity parameters. We use this framework to formally\nprove that the residualization method generally leads to incorrect conclusions\nabout robustness. In our second contribution, we then provide a new approach to\nsensitivity analysis that avoids this critique, allows the omitted variables to\nbe correlated with the included controls, and lets researchers calibrate\nsensitivity parameters by comparing the magnitude of selection on observables\nwith the magnitude of selection on unobservables as in previous methods. We\nillustrate our results in an empirical study of the effect of historical\nAmerican frontier life on modern cultural beliefs. Finally, we implement these\nmethods in the companion Stata module regsensitivity for easy use in practice.",
        "authors": [
            "Paul Diegert",
            "Matthew A. Masten",
            "Alexandre Poirier"
        ],
        "categories": "econ.EM",
        "published": "2022-06-06T01:08:12Z",
        "updated": "2023-07-03T18:56:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.02122v1",
        "title": "Causal impact of severe events on electricity demand: The case of COVID-19 in Japan",
        "abstract": "As of May 2022, the coronavirus disease 2019 (COVID-19) still has a severe\nglobal impact on people's lives. Previous studies have reported that COVID-19\ndecreased the electricity demand in early 2020. However, our study found that\nthe electricity demand increased in summer and winter even when the infection\nwas widespread. The fact that the event has continued over two years suggests\nthat it is essential to introduce the method which can estimate the impact of\nthe event for long period considering seasonal fluctuations. We employed the\nBayesian structural time-series model to estimate the causal impact of COVID-19\non electricity demand in Japan. The results indicate that behavioral\nrestrictions due to COVID-19 decreased the daily electricity demand (-5.1% in\nweekdays, -6.1% in holidays) in April and May 2020 as indicated by previous\nstudies. However, even in 2020, the results show that the demand increases in\nthe hot summer and cold winter (the increasing rate is +14% in the period from\n1st August to 15th September 2020, and +7.6% from 16th December 2020 to 15th\nJanuary 2021). This study shows that the significant decrease in electricity\ndemand for the business sector exceeded the increase in demand for the\nhousehold sector in April and May 2020; however, the increase in demand for the\nhouseholds exceeded the decrease in demand for the business in hot summer and\ncold winter periods. Our result also implies that it is possible to run out of\nelectricity when people's behavior changes even if they are less active.",
        "authors": [
            "Yasunobu Wakashiro"
        ],
        "categories": "econ.EM",
        "published": "2022-06-05T08:31:01Z",
        "updated": "2022-06-05T08:31:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.01825v2",
        "title": "Debiased Machine Learning without Sample-Splitting for Stable Estimators",
        "abstract": "Estimation and inference on causal parameters is typically reduced to a\ngeneralized method of moments problem, which involves auxiliary functions that\ncorrespond to solutions to a regression or classification problem. Recent line\nof work on debiased machine learning shows how one can use generic machine\nlearning estimators for these auxiliary problems, while maintaining asymptotic\nnormality and root-$n$ consistency of the target parameter of interest, while\nonly requiring mean-squared-error guarantees from the auxiliary estimation\nalgorithms. The literature typically requires that these auxiliary problems are\nfitted on a separate sample or in a cross-fitting manner. We show that when\nthese auxiliary estimation algorithms satisfy natural leave-one-out stability\nproperties, then sample splitting is not required. This allows for sample\nre-use, which can be beneficial in moderately sized sample regimes. For\ninstance, we show that the stability properties that we propose are satisfied\nfor ensemble bagged estimators, built via sub-sampling without replacement, a\npopular technique in machine learning practice.",
        "authors": [
            "Qizhao Chen",
            "Vasilis Syrgkanis",
            "Morgane Austern"
        ],
        "categories": "econ.EM",
        "published": "2022-06-03T21:31:28Z",
        "updated": "2022-11-14T20:52:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.01779v3",
        "title": "Bayesian and Frequentist Inference for Synthetic Controls",
        "abstract": "The synthetic control method has become a widely popular tool to estimate\ncausal effects with observational data. Despite this, inference for synthetic\ncontrol methods remains challenging. Often, inferential results rely on linear\nfactor model data generating processes. In this paper, we characterize the\nconditions on the factor model primitives (the factor loadings) for which the\nstatistical risk minimizers are synthetic controls (in the simplex). Then, we\npropose a Bayesian alternative to the synthetic control method that preserves\nthe main features of the standard method and provides a new way of doing valid\ninference. We explore a Bernstein-von Mises style result to link our Bayesian\ninference to the frequentist inference. For linear factor model frameworks we\nshow that a maximum likelihood estimator (MLE) of the synthetic control weights\ncan consistently estimate the predictive function of the potential outcomes for\nthe treated unit and that our Bayes estimator is asymptotically close to the\nMLE in the total variation sense. Through simulations, we show that there is\nconvergence between the Bayes and frequentist approach even in sparse settings.\nFinally, we apply the method to re-visit the study of the economic costs of the\nGerman re-unification and the Catalan secession movement. The Bayesian\nsynthetic control method is available in the bsynth R-package.",
        "authors": [
            "Ignacio Martinez",
            "Jaume Vives-i-Bastida"
        ],
        "categories": "stat.ME",
        "published": "2022-06-03T18:48:25Z",
        "updated": "2024-07-07T15:42:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.03278v1",
        "title": "Cointegration and ARDL specification between the Dubai crude oil and the US natural gas market",
        "abstract": "This paper examines the relationship between the price of the Dubai crude oil\nand the price of the US natural gas using an updated monthly dataset from 1992\nto 2018, incorporating the latter events in the energy markets. After employing\na variety of unit root and cointegration tests, the long-run relationship is\nexamined via the autoregressive distributed lag (ARDL) cointegration technique,\nalong with the Toda-Yamamoto (1995) causality test. Our results indicate that\nthere is a long-run relationship with a unidirectional causality running from\nthe Dubai crude oil market to the US natural gas market. A variety of post\nspecification tests indicate that the selected ARDL model is well-specified,\nand the results of the Toda-Yamamoto approach via impulse response functions,\nforecast error variance decompositions, and historical decompositions with\ngeneralized weights, show that the Dubai crude oil price retains a positive\nrelationship and affects the US natural gas price.",
        "authors": [
            "Stavros Stavroyiannis"
        ],
        "categories": "econ.EM",
        "published": "2022-06-03T11:57:02Z",
        "updated": "2022-06-03T11:57:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.00999v1",
        "title": "Randomization Inference Tests for Shift-Share Designs",
        "abstract": "We consider the problem of inference in shift-share research designs. The\nchoice between existing approaches that allow for unrestricted spatial\ncorrelation involves tradeoffs, varying in terms of their validity when there\nare relatively few or concentrated shocks, and in terms of the assumptions on\nthe shock assignment process and treatment effects heterogeneity. We propose\nalternative randomization inference methods that combine the advantages of\ndifferent approaches. These methods are valid in finite samples under\nrelatively stronger assumptions, while asymptotically valid under weaker\nassumptions.",
        "authors": [
            "Luis Alvarez",
            "Bruno Ferman",
            "Raoni Oliveira"
        ],
        "categories": "econ.EM",
        "published": "2022-06-02T11:52:03Z",
        "updated": "2022-06-02T11:52:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.00574v1",
        "title": "Human Wellbeing and Machine Learning",
        "abstract": "There is a vast literature on the determinants of subjective wellbeing.\nInternational organisations and statistical offices are now collecting such\nsurvey data at scale. However, standard regression models explain surprisingly\nlittle of the variation in wellbeing, limiting our ability to predict it. In\nresponse, we here assess the potential of Machine Learning (ML) to help us\nbetter understand wellbeing. We analyse wellbeing data on over a million\nrespondents from Germany, the UK, and the United States. In terms of predictive\npower, our ML approaches do perform better than traditional models. Although\nthe size of the improvement is small in absolute terms, it turns out to be\nsubstantial when compared to that of key variables like health. We moreover\nfind that drastically expanding the set of explanatory variables doubles the\npredictive power of both OLS and the ML approaches on unseen data. The\nvariables identified as important by our ML algorithms - $i.e.$ material\nconditions, health, and meaningful social relations - are similar to those that\nhave already been identified in the literature. In that sense, our data-driven\nML results validate the findings from conventional approaches.",
        "authors": [
            "Ekaterina Oparina",
            "Caspar Kaiser",
            "Niccol\u00f2 Gentile",
            "Alexandre Tkatchenko",
            "Andrew E. Clark",
            "Jan-Emmanuel De Neve",
            "Conchita D'Ambrosio"
        ],
        "categories": "econ.EM",
        "published": "2022-06-01T15:35:50Z",
        "updated": "2022-06-01T15:35:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.00409v1",
        "title": "Time-Varying Multivariate Causal Processes",
        "abstract": "In this paper, we consider a wide class of time-varying multivariate causal\nprocesses which nests many classic and new examples as special cases. We first\nprove the existence of a weakly dependent stationary approximation for our\nmodel which is the foundation to initiate the theoretical development.\nAfterwards, we consider the QMLE estimation approach, and provide both\npoint-wise and simultaneous inferences on the coefficient functions. In\naddition, we demonstrate the theoretical findings through both simulated and\nreal data examples. In particular, we show the empirical relevance of our study\nusing an application to evaluate the conditional correlations between the stock\nmarkets of China and U.S. We find that the interdependence between the two\nstock markets is increasing over time.",
        "authors": [
            "Jiti Gao",
            "Bin Peng",
            "Wei Biao Wu",
            "Yayi Yan"
        ],
        "categories": "econ.EM",
        "published": "2022-06-01T11:21:01Z",
        "updated": "2022-06-01T11:21:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.15853v2",
        "title": "Predicting Day-Ahead Stock Returns using Search Engine Query Volumes: An Application of Gradient Boosted Decision Trees to the S&P 100",
        "abstract": "The internet has changed the way we live, work and take decisions. As it is\nthe major modern resource for research, detailed data on internet usage\nexhibits vast amounts of behavioral information. This paper aims to answer the\nquestion whether this information can be facilitated to predict future returns\nof stocks on financial capital markets. In an empirical analysis it implements\ngradient boosted decision trees to learn relationships between abnormal returns\nof stocks within the S&P 100 index and lagged predictors derived from\nhistorical financial data, as well as search term query volumes on the internet\nsearch engine Google. Models predict the occurrence of day-ahead stock returns\nin excess of the index median. On a time frame from 2005 to 2017, all disparate\ndatasets exhibit valuable information. Evaluated models have average areas\nunder the receiver operating characteristic between 54.2% and 56.7%, clearly\nindicating a classification better than random guessing. Implementing a simple\nstatistical arbitrage strategy, models are used to create daily trading\nportfolios of ten stocks and result in annual performances of more than 57%\nbefore transaction costs. With ensembles of different data sets topping up the\nperformance ranking, the results further question the weak form and semi-strong\nform efficiency of modern financial capital markets. Even though transaction\ncosts are not included, the approach adds to the existing literature. It gives\nguidance on how to use and transform data on internet usage behavior for\nfinancial and economic modeling and forecasting.",
        "authors": [
            "Christopher Bockel-Rickermann"
        ],
        "categories": "econ.EM",
        "published": "2022-05-31T14:58:46Z",
        "updated": "2022-06-01T09:16:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.15750v3",
        "title": "Variable importance without impossible data",
        "abstract": "The most popular methods for measuring importance of the variables in a black\nbox prediction algorithm make use of synthetic inputs that combine predictor\nvariables from multiple subjects. These inputs can be unlikely, physically\nimpossible, or even logically impossible. As a result, the predictions for such\ncases can be based on data very unlike any the black box was trained on. We\nthink that users cannot trust an explanation of the decision of a prediction\nalgorithm when the explanation uses such values. Instead we advocate a method\ncalled Cohort Shapley that is grounded in economic game theory and unlike most\nother game theoretic methods, it uses only actually observed data to quantify\nvariable importance. Cohort Shapley works by narrowing the cohort of subjects\njudged to be similar to a target subject on one or more features. We illustrate\nit on an algorithmic fairness problem where it is essential to attribute\nimportance to protected variables that the model was not trained on.",
        "authors": [
            "Masayoshi Mase",
            "Art B. Owen",
            "Benjamin B. Seiler"
        ],
        "categories": "cs.LG",
        "published": "2022-05-31T12:36:18Z",
        "updated": "2023-04-13T03:06:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.15738v2",
        "title": "Estimating spot volatility under infinite variation jumps with dependent market microstructure noise",
        "abstract": "Jumps and market microstructure noise are stylized features of high-frequency\nfinancial data. It is well known that they introduce bias in the estimation of\nvolatility (including integrated and spot volatilities) of assets, and many\nmethods have been proposed to deal with this problem. When the jumps are\nintensive with infinite variation, the efficient estimation of spot volatility\nunder serially dependent noise is not available and is thus in need. For this\npurpose, we propose a novel estimator of spot volatility with a hybrid use of\nthe pre-averaging technique and the empirical characteristic function. Under\nmild assumptions, the results of consistency and asymptotic normality of our\nestimator are established. Furthermore, we show that our estimator achieves an\nalmost efficient convergence rate with optimal variance when the jumps are\neither less active or active with symmetric structure. Simulation studies\nverify our theoretical conclusions. We apply our proposed estimator to\nempirical analyses, such as estimating the weekly volatility curve using\nsecond-by-second transaction price data.",
        "authors": [
            "Qiang Liu",
            "Zhi Liu"
        ],
        "categories": "econ.EM",
        "published": "2022-05-31T12:26:03Z",
        "updated": "2023-02-17T10:58:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.15420v3",
        "title": "Fast Two-Stage Variational Bayesian Approach to Estimating Panel Spatial Autoregressive Models with Unrestricted Spatial Weights Matrices",
        "abstract": "This paper proposes a fast two-stage variational Bayesian (VB) algorithm to\nestimate unrestricted panel spatial autoregressive models. Using\nDirichlet-Laplace priors, we are able to uncover the spatial relationships\nbetween cross-sectional units without imposing any a priori restrictions. Monte\nCarlo experiments show that our approach works well for both long and short\npanels. We are also the first in the literature to develop VB methods to\nestimate large covariance matrices with unrestricted sparsity patterns, which\nare useful for popular large data models such as Bayesian vector\nautoregressions. In empirical applications, we examine the spatial\ninterdependence between euro area sovereign bond ratings and spreads. We find\nmarked differences between the spillover behaviours of the northern euro area\ncountries and those of the south.",
        "authors": [
            "Deborah Gefang",
            "Stephen G. Hall",
            "George S. Tavlas"
        ],
        "categories": "econ.EM",
        "published": "2022-05-30T20:34:44Z",
        "updated": "2023-08-21T22:17:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.14758v1",
        "title": "Credible, Strategyproof, Optimal, and Bounded Expected-Round Single-Item Auctions for all Distributions",
        "abstract": "We consider a revenue-maximizing seller with a single item for sale to\nmultiple buyers with i.i.d. valuations. Akbarpour and Li (2020) show that the\nonly optimal, credible, strategyproof auction is the ascending price auction\nwith reserves which has unbounded communication complexity. Recent work of\nFerreira and Weinberg (2020) circumvents their impossibility result assuming\nthe existence of cryptographically secure commitment schemes, and designs a\ntwo-round credible, strategyproof, optimal auction. However, their auction is\nonly credible when buyers' valuations are MHR or $\\alpha$-strongly regular:\nthey show their auction might not be credible even when there is a single buyer\ndrawn from a non-MHR distribution. In this work, under the same cryptographic\nassumptions, we identify a new single-item auction that is credible,\nstrategyproof, revenue optimal, and terminates in constant rounds in\nexpectation for all distributions with finite monopoly price.",
        "authors": [
            "Meryem Essaidi",
            "Matheus V. X. Ferreira",
            "S. Matthew Weinberg"
        ],
        "categories": "cs.GT",
        "published": "2022-05-29T20:10:02Z",
        "updated": "2022-05-29T20:10:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.14284v2",
        "title": "Provably Auditing Ordinary Least Squares in Low Dimensions",
        "abstract": "Measuring the stability of conclusions derived from Ordinary Least Squares\nlinear regression is critically important, but most metrics either only measure\nlocal stability (i.e. against infinitesimal changes in the data), or are only\ninterpretable under statistical assumptions. Recent work proposes a simple,\nglobal, finite-sample stability metric: the minimum number of samples that need\nto be removed so that rerunning the analysis overturns the conclusion,\nspecifically meaning that the sign of a particular coefficient of the estimated\nregressor changes. However, besides the trivial exponential-time algorithm, the\nonly approach for computing this metric is a greedy heuristic that lacks\nprovable guarantees under reasonable, verifiable assumptions; the heuristic\nprovides a loose upper bound on the stability and also cannot certify lower\nbounds on it.\n  We show that in the low-dimensional regime where the number of covariates is\na constant but the number of samples is large, there are efficient algorithms\nfor provably estimating (a fractional version of) this metric. Applying our\nalgorithms to the Boston Housing dataset, we exhibit regression analyses where\nwe can estimate the stability up to a factor of $3$ better than the greedy\nheuristic, and analyses where we can certify stability to dropping even a\nmajority of the samples.",
        "authors": [
            "Ankur Moitra",
            "Dhruv Rohatgi"
        ],
        "categories": "stat.ML",
        "published": "2022-05-28T00:45:10Z",
        "updated": "2022-06-05T13:44:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.14048v2",
        "title": "Average Adjusted Association: Efficient Estimation with High Dimensional Confounders",
        "abstract": "The log odds ratio is a well-established metric for evaluating the\nassociation between binary outcome and exposure variables. Despite its\nwidespread use, there has been limited discussion on how to summarize the log\nodds ratio as a function of confounders through averaging. To address this\nissue, we propose the Average Adjusted Association (AAA), which is a summary\nmeasure of association in a heterogeneous population, adjusted for observed\nconfounders. To facilitate the use of it, we also develop efficient\ndouble/debiased machine learning (DML) estimators of the AAA. Our DML\nestimators use two equivalent forms of the efficient influence function, and\nare applicable in various sampling scenarios, including random sampling,\noutcome-based sampling, and exposure-based sampling. Through real data and\nsimulations, we demonstrate the practicality and effectiveness of our proposed\nestimators in measuring the AAA.",
        "authors": [
            "Sung Jae Jun",
            "Sokbae Lee"
        ],
        "categories": "stat.ME",
        "published": "2022-05-27T15:36:12Z",
        "updated": "2023-04-02T22:54:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.12917v2",
        "title": "Identification of Auction Models Using Order Statistics",
        "abstract": "Auction data often contain information on only the most competitive bids as\nopposed to all bids. The usual measurement error approaches to unobserved\nheterogeneity are inapplicable due to dependence among order statistics. We\nbridge this gap by providing a set of positive identification results. First,\nwe show that symmetric auctions with discrete unobserved heterogeneity are\nidentifiable using two consecutive order statistics and an instrument. Second,\nwe extend the results to ascending auctions with unknown competition and\nunobserved heterogeneity.",
        "authors": [
            "Yao Luo",
            "Ruli Xiao"
        ],
        "categories": "econ.EM",
        "published": "2022-05-25T17:11:05Z",
        "updated": "2023-04-23T20:07:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.12746v2",
        "title": "Machine learning method for return direction forecasting of Exchange Traded Funds using classification and regression models",
        "abstract": "This article aims to propose and apply a machine learning method to analyze\nthe direction of returns from Exchange Traded Funds (ETFs) using the historical\nreturn data of its components, helping to make investment strategy decisions\nthrough a trading algorithm. In methodological terms, regression and\nclassification models were applied, using standard datasets from Brazilian and\nAmerican markets, in addition to algorithmic error metrics. In terms of\nresearch results, they were analyzed and compared to those of the Na\\\"ive\nforecast and the returns obtained by the buy & hold technique in the same\nperiod of time. In terms of risk and return, the models mostly performed better\nthan the control metrics, with emphasis on the linear regression model and the\nclassification models by logistic regression, support vector machine (using the\nLinearSVC model), Gaussian Naive Bayes and K-Nearest Neighbors, where in\ncertain datasets the returns exceeded by two times and the Sharpe ratio by up\nto four times those of the buy & hold control model.",
        "authors": [
            "Raphael P. B. Piovezan",
            "Pedro Paulo de Andrade Junior"
        ],
        "categories": "q-fin.CP",
        "published": "2022-05-25T12:54:46Z",
        "updated": "2022-06-13T13:07:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.12126v2",
        "title": "Estimation and Inference for High Dimensional Factor Model with Regime Switching",
        "abstract": "This paper proposes maximum (quasi)likelihood estimation for high dimensional\nfactor models with regime switching in the loadings. The model parameters are\nestimated jointly by the EM (expectation maximization) algorithm, which in the\ncurrent context only requires iteratively calculating regime probabilities and\nprincipal components of the weighted sample covariance matrix. When regime\ndynamics are taken into account, smoothed regime probabilities are calculated\nusing a recursive algorithm. Consistency, convergence rates and limit\ndistributions of the estimated loadings and the estimated factors are\nestablished under weak cross-sectional and temporal dependence as well as\nheteroscedasticity. It is worth noting that due to high dimension, regime\nswitching can be identified consistently after the switching point with only\none observation. Simulation results show good performance of the proposed\nmethod. An application to the FRED-MD dataset illustrates the potential of the\nproposed method for detection of business cycle turning points.",
        "authors": [
            "Giovanni Urga",
            "Fa Wang"
        ],
        "categories": "econ.EM",
        "published": "2022-05-24T14:57:58Z",
        "updated": "2023-04-10T15:50:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.11953v2",
        "title": "Subgeometrically ergodic autoregressions with autoregressive conditional heteroskedasticity",
        "abstract": "In this paper, we consider subgeometric (specifically, polynomial) ergodicity\nof univariate nonlinear autoregressions with autoregressive conditional\nheteroskedasticity (ARCH). The notion of subgeometric ergodicity was introduced\nin the Markov chain literature in 1980s and it means that the transition\nprobability measures converge to the stationary measure at a rate slower than\ngeometric; this rate is also closely related to the convergence rate of\n$\\beta$-mixing coefficients. While the existing literature on subgeometrically\nergodic autoregressions assumes a homoskedastic error term, this paper provides\nan extension to the case of conditionally heteroskedastic ARCH-type errors,\nconsiderably widening the scope of potential applications. Specifically, we\nconsider suitably defined higher-order nonlinear autoregressions with possibly\nnonlinear ARCH errors and show that they are, under appropriate conditions,\nsubgeometrically ergodic at a polynomial rate. An empirical example using\nenergy sector volatility index data illustrates the use of subgeometrically\nergodic AR-ARCH models.",
        "authors": [
            "Mika Meitz",
            "Pentti Saikkonen"
        ],
        "categories": "econ.EM",
        "published": "2022-05-24T10:34:28Z",
        "updated": "2023-04-20T11:57:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.11568v3",
        "title": "Quasi Black-Box Variational Inference with Natural Gradients for Bayesian Learning",
        "abstract": "We develop an optimization algorithm suitable for Bayesian learning in\ncomplex models. Our approach relies on natural gradient updates within a\ngeneral black-box framework for efficient training with limited model-specific\nderivations. It applies within the class of exponential-family variational\nposterior distributions, for which we extensively discuss the Gaussian case for\nwhich the updates have a rather simple form. Our Quasi Black-box Variational\nInference (QBVI) framework is readily applicable to a wide class of Bayesian\ninference problems and is of simple implementation as the updates of the\nvariational posterior do not involve gradients with respect to the model\nparameters, nor the prescription of the Fisher information matrix. We develop\nQBVI under different hypotheses for the posterior covariance matrix, discuss\ndetails about its robust and feasible implementation, and provide a number of\nreal-world applications to demonstrate its effectiveness.",
        "authors": [
            "Martin Magris",
            "Mostafa Shabani",
            "Alexandros Iosifidis"
        ],
        "categories": "stat.ML",
        "published": "2022-05-23T18:54:27Z",
        "updated": "2022-12-12T15:00:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.11486v2",
        "title": "Robust and Agnostic Learning of Conditional Distributional Treatment Effects",
        "abstract": "The conditional average treatment effect (CATE) is the best measure of\nindividual causal effects given baseline covariates. However, the CATE only\ncaptures the (conditional) average, and can overlook risks and tail events,\nwhich are important to treatment choice. In aggregate analyses, this is usually\naddressed by measuring the distributional treatment effect (DTE), such as\ndifferences in quantiles or tail expectations between treatment groups.\nHypothetically, one can similarly fit conditional quantile regressions in each\ntreatment group and take their difference, but this would not be robust to\nmisspecification or provide agnostic best-in-class predictions. We provide a\nnew robust and model-agnostic methodology for learning the conditional DTE\n(CDTE) for a class of problems that includes conditional quantile treatment\neffects, conditional super-quantile treatment effects, and conditional\ntreatment effects on coherent risk measures given by $f$-divergences. Our\nmethod is based on constructing a special pseudo-outcome and regressing it on\ncovariates using any regression learner. Our method is model-agnostic in that\nit can provide the best projection of CDTE onto the regression model class. Our\nmethod is robust in that even if we learn these nuisances nonparametrically at\nvery slow rates, we can still learn CDTEs at rates that depend on the class\ncomplexity and even conduct inferences on linear projections of CDTEs. We\ninvestigate the behavior of our proposal in simulations, as well as in a case\nstudy of 401(k) eligibility effects on wealth.",
        "authors": [
            "Nathan Kallus",
            "Miruna Oprescu"
        ],
        "categories": "stat.ML",
        "published": "2022-05-23T17:40:31Z",
        "updated": "2023-02-24T16:26:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.11439v1",
        "title": "Probabilistic forecasting of German electricity imbalance prices",
        "abstract": "The exponential growth of renewable energy capacity has brought much\nuncertainty to electricity prices and to electricity generation. To address\nthis challenge, the energy exchanges have been developing further trading\npossibilities, especially the intraday and balancing markets. For an energy\ntrader participating in both markets, the forecasting of imbalance prices is of\nparticular interest. Therefore, in this manuscript we conduct a very short-term\nprobabilistic forecasting of imbalance prices, contributing to the scarce\nliterature in this novel subject. The forecasting is performed 30 minutes\nbefore the delivery, so that the trader might still choose the trading place.\nThe distribution of the imbalance prices is modelled and forecasted using\nmethods well-known in the electricity price forecasting literature: lasso with\nbootstrap, gamlss, and probabilistic neural networks. The methods are compared\nwith a naive benchmark in a meaningful rolling window study. The results\nprovide evidence of the efficiency between the intraday and balancing markets\nas the sophisticated methods do not substantially overperform the intraday\ncontinuous price index. On the other hand, they significantly improve the\nempirical coverage. The analysis was conducted on the German market, however it\ncould be easily applied to any other market of similar structure.",
        "authors": [
            "Micha\u0142 Narajewski"
        ],
        "categories": "q-fin.ST",
        "published": "2022-05-23T16:32:20Z",
        "updated": "2022-05-23T16:32:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.15115v3",
        "title": "A Novel Control-Oriented Cell Transmission Model Including Service Stations on Highways",
        "abstract": "In this paper, we propose a novel model that describes how the traffic\nevolution on a highway stretch is affected by the presence of a service\nstation. The presented model enhances the classical CTM dynamics by adding the\ndynamics associated with the service stations, where the vehicles may stop\nbefore merging back into the mainstream. We name it CTMs. We discuss its\nflexibility in describing different complex scenarios where multiple stations\nare characterized by different drivers' average stopping times corresponding to\ndifferent services. The model has been developed to help design control\nstrategies aimed at decreasing traffic congestion. Thus, we discuss how\nclassical control schemes can interact with the proposed \\gls{CTMs}. Finally,\nwe validate the proposed model through numerical simulations and assess the\neffects of service stations on traffic evolution, which appear to be\nbeneficial, especially for relatively short congested periods.",
        "authors": [
            "Carlo Cenedese",
            "Michele Cucuzzella",
            "Antonella Ferrara",
            "John Lygeros"
        ],
        "categories": "eess.SY",
        "published": "2022-05-23T15:54:24Z",
        "updated": "2022-09-13T19:21:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.11365v2",
        "title": "Graph-Based Methods for Discrete Choice",
        "abstract": "Choices made by individuals have widespread impacts--for instance, people\nchoose between political candidates to vote for, between social media posts to\nshare, and between brands to purchase--moreover, data on these choices are\nincreasingly abundant. Discrete choice models are a key tool for learning\nindividual preferences from such data. Additionally, social factors like\nconformity and contagion influence individual choice. Traditional methods for\nincorporating these factors into choice models do not account for the entire\nsocial network and require hand-crafted features. To overcome these\nlimitations, we use graph learning to study choice in networked contexts. We\nidentify three ways in which graph learning techniques can be used for discrete\nchoice: learning chooser representations, regularizing choice model parameters,\nand directly constructing predictions from a network. We design methods in each\ncategory and test them on real-world choice datasets, including county-level\n2016 US election results and Android app installation and usage data. We show\nthat incorporating social network structure can improve the predictions of the\nstandard econometric choice model, the multinomial logit. We provide evidence\nthat app installations are influenced by social context, but we find no such\neffect on app usage among the same participants, which instead is habit-driven.\nIn the election data, we highlight the additional insights a discrete choice\nframework provides over classification or regression, the typical approaches.\nOn synthetic data, we demonstrate the sample complexity benefit of using social\ninformation in choice models.",
        "authors": [
            "Kiran Tomlinson",
            "Austin R. Benson"
        ],
        "categories": "cs.LG",
        "published": "2022-05-23T14:51:23Z",
        "updated": "2023-11-17T22:12:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.11189v1",
        "title": "Regime and Treatment Effects in Duration Models: Decomposing Expectation and Transplant Effects on the Kidney Waitlist",
        "abstract": "This paper proposes a causal decomposition framework for settings in which an\ninitial regime randomization influences the timing of a treatment duration. The\ninitial randomization and treatment affect in turn a duration outcome of\ninterest. Our empirical application considers the survival of individuals on\nthe kidney transplant waitlist. Upon entering the waitlist, individuals with an\nAB blood type, who are universal recipients, are effectively randomized to a\nregime with a higher propensity to rapidly receive a kidney transplant. Our\ndynamic potential outcomes framework allows us to identify the pre-transplant\neffect of the blood type, and the transplant effects depending on blood type.\nWe further develop dynamic assumptions which build on the LATE framework and\nallow researchers to separate effects for different population substrata. Our\nmain empirical result is that AB blood type candidates display a higher\npre-transplant mortality. We provide evidence that this effect is due to\nbehavioural changes rather than biological differences.",
        "authors": [
            "Stephen Kastoryano"
        ],
        "categories": "econ.EM",
        "published": "2022-05-23T10:47:41Z",
        "updated": "2022-05-23T10:47:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.10772v2",
        "title": "Fast Instrument Learning with Faster Rates",
        "abstract": "We investigate nonlinear instrumental variable (IV) regression given\nhigh-dimensional instruments. We propose a simple algorithm which combines\nkernelized IV methods and an arbitrary, adaptive regression algorithm, accessed\nas a black box. Our algorithm enjoys faster-rate convergence and adapts to the\ndimensionality of informative latent features, while avoiding an expensive\nminimax optimization procedure, which has been necessary to establish similar\nguarantees. It further brings the benefit of flexible machine learning models\nto quasi-Bayesian uncertainty quantification, likelihood-based model selection,\nand model averaging. Simulation studies demonstrate the competitive performance\nof our method.",
        "authors": [
            "Ziyu Wang",
            "Yuhao Zhou",
            "Jun Zhu"
        ],
        "categories": "stat.ML",
        "published": "2022-05-22T08:06:54Z",
        "updated": "2022-10-22T07:43:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.14186v2",
        "title": "The Effect of Increased Access to IVF on Women's Careers",
        "abstract": "Motherhood is the main contributor to gender gaps in the labor market. IVF is\na method of assisted reproduction that can delay fertility, which results in\ndecreased motherhood income penalty. In this research, I estimate the effects\nof expanded access to in vitro fertilization (IVF) arising from state insurance\nmandates. I use a difference-in-differences model to estimate the effect of\nincreased IVF accessibility for delaying childbirth and decreasing the\nmotherhood income penalty. Using the fertility supplement dataset from the\nCurrent Population Survey (CPS), I estimate how outcomes change in states when\nthey implement their mandates compared to how outcomes change in states that\nare not changing their policies. The results indicate that IVF mandates\nincrease the probability of motherhood by 38 by 3.1 percentage points (p<0.01).\nHowever, the results provide no evidence that IVF insurance mandates impact\nwomen's earnings.",
        "authors": [
            "Lingxi Chen"
        ],
        "categories": "econ.GN",
        "published": "2022-05-21T23:25:04Z",
        "updated": "2022-06-02T01:44:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.10478v1",
        "title": "Conditional Balance Tests: Increasing Sensitivity and Specificity With Prognostic Covariates",
        "abstract": "Researchers often use covariate balance tests to assess whether a treatment\nvariable is assigned \"as-if\" at random. However, standard tests may shed no\nlight on a key condition for causal inference: the independence of treatment\nassignment and potential outcomes. We focus on a key factor that affects the\nsensitivity and specificity of balance tests: the extent to which covariates\nare prognostic, that is, predictive of potential outcomes. We propose a\n\"conditional balance test\" based on the weighted sum of covariate differences\nof means, where the weights are coefficients from a standardized regression of\nobserved outcomes on covariates. Our theory and simulations show that this\napproach increases power relative to other global tests when potential outcomes\nare imbalanced, while limiting spurious rejections due to imbalance on\nirrelevant covariates.",
        "authors": [
            "Clara Bicalho",
            "Adam Bouyamourn",
            "Thad Dunning"
        ],
        "categories": "stat.ME",
        "published": "2022-05-21T01:12:24Z",
        "updated": "2022-05-21T01:12:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.10327v2",
        "title": "What's the Harm? Sharp Bounds on the Fraction Negatively Affected by Treatment",
        "abstract": "The fundamental problem of causal inference -- that we never observe\ncounterfactuals -- prevents us from identifying how many might be negatively\naffected by a proposed intervention. If, in an A/B test, half of users click\n(or buy, or watch, or renew, etc.), whether exposed to the standard experience\nA or a new one B, hypothetically it could be because the change affects no one,\nbecause the change positively affects half the user population to go from\nno-click to click while negatively affecting the other half, or something in\nbetween. While unknowable, this impact is clearly of material importance to the\ndecision to implement a change or not, whether due to fairness, long-term,\nsystemic, or operational considerations. We therefore derive the\ntightest-possible (i.e., sharp) bounds on the fraction negatively affected (and\nother related estimands) given data with only factual observations, whether\nexperimental or observational. Naturally, the more we can stratify individuals\nby observable covariates, the tighter the sharp bounds. Since these bounds\ninvolve unknown functions that must be learned from data, we develop a robust\ninference algorithm that is efficient almost regardless of how and how fast\nthese functions are learned, remains consistent when some are mislearned, and\nstill gives valid conservative bounds when most are mislearned. Our methodology\naltogether therefore strongly supports credible conclusions: it avoids\nspuriously point-identifying this unknowable impact, focusing on the best\nbounds instead, and it permits exceedingly robust inference on these. We\ndemonstrate our method in simulation studies and in a case study of career\ncounseling for the unemployed.",
        "authors": [
            "Nathan Kallus"
        ],
        "categories": "stat.ME",
        "published": "2022-05-20T17:36:33Z",
        "updated": "2022-11-21T03:04:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.10310v4",
        "title": "Treatment Effects in Bunching Designs: The Impact of Mandatory Overtime Pay on Hours",
        "abstract": "This paper studies the identifying power of bunching at kinks when the\nresearcher does not assume a parametric choice model. I find that in a general\nchoice model, identifying the average causal response to the policy switch at a\nkink amounts to confronting two extrapolation problems, each about the\ndistribution of a counterfactual choice that is observed only in a censored\nmanner. I apply this insight to partially identify the effect of overtime pay\nregulation on the hours of U.S. workers using administrative payroll data,\nassuming that each distribution satisfies a weak non-parametric shape\nconstraint in the region where it is not observed. The resulting bounds are\ninformative and indicate a relatively small elasticity of demand for weekly\nhours, addressing a long-standing question about the causal effects of the\novertime mandate.",
        "authors": [
            "Leonard Goff"
        ],
        "categories": "econ.EM",
        "published": "2022-05-20T17:22:57Z",
        "updated": "2024-06-20T06:17:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.10256v2",
        "title": "The Forecasting performance of the Factor model with Martingale Difference errors",
        "abstract": "This paper analyses the forecasting performance of a new class of factor\nmodels with martingale difference errors (FMMDE) recently introduced by Lee and\nShao (2018). The FMMDE makes it possible to retrieve a transformation of the\noriginal series so that the resulting variables can be partitioned according to\nwhether they are conditionally mean-independent with respect to past\ninformation. We contribute to the literature in two respects. First, we propose\na novel methodology for selecting the number of factors in FMMDE. Through\nsimulation experiments, we show the good performance of our approach for finite\nsamples for various panel data specifications. Second, we compare the\nforecasting performance of FMMDE with alternative factor model specifications\nby conducting an extensive forecasting exercise using FRED-MD, a comprehensive\nmonthly macroeconomic database for the US economy. Our empirical findings\nindicate that FMMDE provides an advantage in predicting the evolution of the\nreal sector of the economy when the novel methodology for factor selection is\nadopted. These results are confirmed for key aggregates such as Production and\nIncome, the Labor Market, and Consumption.",
        "authors": [
            "Luca Mattia Rolla",
            "Alessandro Giovannelli"
        ],
        "categories": "econ.EM",
        "published": "2022-05-20T15:38:23Z",
        "updated": "2023-06-22T11:51:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.10198v3",
        "title": "A New Central Limit Theorem for the Augmented IPW Estimator: Variance Inflation, Cross-Fit Covariance and Beyond",
        "abstract": "Estimation of the average treatment effect (ATE) is a central problem in\ncausal inference. In recent times, inference for the ATE in the presence of\nhigh-dimensional covariates has been extensively studied. Among the diverse\napproaches that have been proposed, augmented inverse probability weighting\n(AIPW) with cross-fitting has emerged a popular choice in practice. In this\nwork, we study this cross-fit AIPW estimator under well-specified outcome\nregression and propensity score models in a high-dimensional regime where the\nnumber of features and samples are both large and comparable. Under assumptions\non the covariate distribution, we establish a new central limit theorem for the\nsuitably scaled cross-fit AIPW that applies without any sparsity assumptions on\nthe underlying high-dimensional parameters. Our CLT uncovers two crucial\nphenomena among others: (i) the AIPW exhibits a substantial variance inflation\nthat can be precisely quantified in terms of the signal-to-noise ratio and\nother problem parameters, (ii) the asymptotic covariance between the\npre-cross-fit estimators is non-negligible even on the root-n scale. These\nfindings are strikingly different from their classical counterparts. On the\ntechnical front, our work utilizes a novel interplay between three distinct\ntools--approximate message passing theory, the theory of deterministic\nequivalents, and the leave-one-out approach. We believe our proof techniques\nshould be useful for analyzing other two-stage estimators in this\nhigh-dimensional regime. Finally, we complement our theoretical results with\nsimulations that demonstrate both the finite sample efficacy of our CLT and its\nrobustness to our assumptions.",
        "authors": [
            "Kuanhao Jiang",
            "Rajarshi Mukherjee",
            "Subhabrata Sen",
            "Pragya Sur"
        ],
        "categories": "math.ST",
        "published": "2022-05-20T14:17:53Z",
        "updated": "2022-10-28T20:50:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.09922v3",
        "title": "Nonlinear Fore(Back)casting and Innovation Filtering for Causal-Noncausal VAR Models",
        "abstract": "We introduce closed-form formulas of out-of-sample predictive densities for\nforecasting and backcasting of mixed causal-noncausal (Structural) Vector\nAutoregressive VAR models. These nonlinear and time irreversible non-Gaussian\nVAR processes are shown to satisfy the Markov property in both calendar and\nreverse time. A post-estimation inference method for assessing the forecast\ninterval uncertainty due to the preliminary estimation step is introduced too.\nThe nonlinear past-dependent innovations of a mixed causal-noncausal VAR model\nare defined and their filtering and identification methods are discussed. Our\napproach is illustrated by a simulation study, and an application to\ncryptocurrency prices.",
        "authors": [
            "Christian Gourieroux",
            "Joann Jasiak"
        ],
        "categories": "econ.EM",
        "published": "2022-05-20T01:32:07Z",
        "updated": "2024-04-06T22:56:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.09691v1",
        "title": "High-dimensional Data Bootstrap",
        "abstract": "This article reviews recent progress in high-dimensional bootstrap. We first\nreview high-dimensional central limit theorems for distributions of sample mean\nvectors over the rectangles, bootstrap consistency results in high dimensions,\nand key techniques used to establish those results. We then review selected\napplications of high-dimensional bootstrap: construction of simultaneous\nconfidence sets for high-dimensional vector parameters, multiple hypothesis\ntesting via stepdown, post-selection inference, intersection bounds for\npartially identified parameters, and inference on best policies in policy\nevaluation. Finally, we also comment on a couple of future research directions.",
        "authors": [
            "Victor Chernozhukov",
            "Denis Chetverikov",
            "Kengo Kato",
            "Yuta Koike"
        ],
        "categories": "math.ST",
        "published": "2022-05-19T16:52:00Z",
        "updated": "2022-05-19T16:52:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2206.07132v1",
        "title": "Dynamics of a Binary Option Market with Exogenous Information and Price Sensitivity",
        "abstract": "In this paper, we derive and analyze a continuous of a binary option market\nwith exogenous information. The resulting non-linear system has a discontinuous\nright hand side, which can be analyzed using zero-dimensional Filippov\nsurfaces. Under general assumptions on purchasing rules, we show that when\nexogenous information is constant in the binary asset market, the price always\nconverges. We then investigate market prices in the case of changing\ninformation, showing empirically that price sensitivity has a strong effect on\nprice lag vs. information. We conclude with open questions on general $n$-ary\noption markets. As a by-product of the analysis, we show that these markets are\nequivalent to a simple recurrent neural network, helping to explain some of the\npredictive power associated with prediction markets, which are usually designed\nas $n$-ary option markets.",
        "authors": [
            "Hannah Gampe",
            "Christopher Griffin"
        ],
        "categories": "q-fin.TR",
        "published": "2022-05-18T14:39:16Z",
        "updated": "2022-05-18T14:39:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.08586v6",
        "title": "Treatment Choice with Nonlinear Regret",
        "abstract": "The literature focuses on the mean of welfare regret, which can lead to\nundesirable treatment choice due to sensitivity to sampling uncertainty. We\npropose to minimize the mean of a nonlinear transformation of regret and show\nthat singleton rules are not essentially complete for nonlinear regret.\nFocusing on mean square regret, we derive closed-form fractions for\nfinite-sample Bayes and minimax optimal rules. Our approach is grounded in\ndecision theory and extends to limit experiments. The treatment fractions can\nbe viewed as the strength of evidence favoring treatment. We apply our\nframework to a normal regression model and sample size calculation.",
        "authors": [
            "Toru Kitagawa",
            "Sokbae Lee",
            "Chen Qiu"
        ],
        "categories": "econ.EM",
        "published": "2022-05-17T19:06:12Z",
        "updated": "2024-10-01T20:01:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.07950v3",
        "title": "The Power of Tests for Detecting $p$-Hacking",
        "abstract": "$p$-Hacking undermines the validity of empirical studies. A flourishing\nempirical literature investigates the prevalence of $p$-hacking based on the\ndistribution of $p$-values across studies. Interpreting results in this\nliterature requires a careful understanding of the power of methods for\ndetecting $p$-hacking. We theoretically study the implications of likely forms\nof $p$-hacking on the distribution of $p$-values to understand the power of\ntests for detecting it. Power depends crucially on the $p$-hacking strategy and\nthe distribution of true effects. Publication bias can enhance the power for\ntesting the joint null of no $p$-hacking and no publication bias.",
        "authors": [
            "Graham Elliott",
            "Nikolay Kudrin",
            "Kaspar W\u00fcthrich"
        ],
        "categories": "econ.EM",
        "published": "2022-05-16T19:18:55Z",
        "updated": "2024-04-22T20:20:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.07836v11",
        "title": "2SLS with Multiple Treatments",
        "abstract": "We study what two-stage least squares (2SLS) identifies in models with\nmultiple treatments under treatment effect heterogeneity. Two conditions are\nshown to be necessary and sufficient for the 2SLS to identify positively\nweighted sums of agent-specific effects of each treatment: average conditional\nmonotonicity and no cross effects. Our identification analysis allows for any\nnumber of treatments, any number of continuous or discrete instruments, and the\ninclusion of covariates. We provide testable implications and present\ncharacterizations of choice behavior implied by our identification conditions.",
        "authors": [
            "Manudeep Bhuller",
            "Henrik Sigstad"
        ],
        "categories": "econ.EM",
        "published": "2022-05-16T17:45:46Z",
        "updated": "2024-05-23T16:33:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.07719v1",
        "title": "HARNet: A Convolutional Neural Network for Realized Volatility Forecasting",
        "abstract": "Despite the impressive success of deep neural networks in many application\nareas, neural network models have so far not been widely adopted in the context\nof volatility forecasting. In this work, we aim to bridge the conceptual gap\nbetween established time series approaches, such as the Heterogeneous\nAutoregressive (HAR) model, and state-of-the-art deep neural network models.\nThe newly introduced HARNet is based on a hierarchy of dilated convolutional\nlayers, which facilitates an exponential growth of the receptive field of the\nmodel in the number of model parameters. HARNets allow for an explicit\ninitialization scheme such that before optimization, a HARNet yields identical\npredictions as the respective baseline HAR model. Particularly when considering\nthe QLIKE error as a loss function, we find that this approach significantly\nstabilizes the optimization of HARNets. We evaluate the performance of HARNets\nwith respect to three different stock market indexes. Based on this evaluation,\nwe formulate clear guidelines for the optimization of HARNets and show that\nHARNets can substantially improve upon the forecasting accuracy of their\nrespective HAR baseline models. In a qualitative analysis of the filter weights\nlearnt by a HARNet, we report clear patterns regarding the predictive power of\npast information. Among information from the previous week, yesterday and the\nday before, yesterday's volatility makes by far the most contribution to\ntoday's realized volatility forecast. Moroever, within the previous month, the\nimportance of single weeks diminishes almost linearly when moving further into\nthe past.",
        "authors": [
            "Rafael Reisenhofer",
            "Xandro Bayer",
            "Nikolaus Hautsch"
        ],
        "categories": "econ.EM",
        "published": "2022-05-16T14:33:32Z",
        "updated": "2022-05-16T14:33:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.07579v3",
        "title": "Is climate change time reversible?",
        "abstract": "This paper proposes strategies to detect time reversibility in stationary\nstochastic processes by using the properties of mixed causal and noncausal\nmodels. It shows that they can also be used for non-stationary processes when\nthe trend component is computed with the Hodrick-Prescott filter rendering a\ntime-reversible closed-form solution. This paper also links the concept of an\nenvironmental tipping point to the statistical property of time irreversibility\nand assesses fourteen climate indicators. We find evidence of time\nirreversibility in $GHG$ emissions, global temperature, global sea levels, sea\nice area, and some natural oscillation indices. While not conclusive, our\nfindings urge the implementation of correction policies to avoid the worst\nconsequences of climate change and not miss the opportunity window, which might\nstill be available, despite closing quickly.",
        "authors": [
            "Francesco Giancaterini",
            "Alain Hecq",
            "Claudio Morana"
        ],
        "categories": "econ.EM",
        "published": "2022-05-16T11:29:23Z",
        "updated": "2022-11-22T09:49:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.07388v1",
        "title": "Inference with Imputed Data: The Allure of Making Stuff Up",
        "abstract": "Incomplete observability of data generates an identification problem. There\nis no panacea for missing data. What one can learn about a population parameter\ndepends on the assumptions one finds credible to maintain. The credibility of\nassumptions varies with the empirical setting. No specific assumptions can\nprovide a realistic general solution to the problem of inference with missing\ndata. Yet Rubin has promoted random multiple imputation (RMI) as a general way\nto deal with missing values in public-use data. This recommendation has been\ninfluential to empirical researchers who seek a simple fix to the nuisance of\nmissing data. This paper adds to my earlier critiques of imputation. It\nprovides a transparent assessment of the mix of Bayesian and frequentist\nthinking used by Rubin to argue for RMI. It evaluates random imputation to\nreplace missing outcome or covariate data when the objective is to learn a\nconditional expectation. It considers steps that might help combat the allure\nof making stuff up.",
        "authors": [
            "Charles F. Manski"
        ],
        "categories": "econ.EM",
        "published": "2022-05-15T22:03:45Z",
        "updated": "2022-05-15T22:03:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.07345v2",
        "title": "Joint Location and Cost Planning in Maximum Capture Facility Location under Multiplicative Random Utility Maximization",
        "abstract": "We study a joint facility location and cost planning problem in a competitive\nmarket under random utility maximization (RUM) models. The objective is to\nlocate new facilities and make decisions on the costs (or budgets) to spend on\nthe new facilities, aiming to maximize an expected captured customer demand,\nassuming that customers choose a facility among all available facilities\naccording to a RUM model. We examine two RUM frameworks in the discrete choice\nliterature, namely, the additive and multiplicative RUM. While the former has\nbeen widely used in facility location problems, we are the first to explore the\nlatter in the context. We numerically show that the two RUM frameworks can well\napproximate each other in the context of the cost optimization problem. In\naddition, we show that, under the additive RUM framework, the resultant cost\noptimization problem becomes highly non-convex and may have several local\noptima. In contrast, the use of the multiplicative RUM brings several\nadvantages to the competitive facility location problem. For instance, the cost\noptimization problem under the multiplicative RUM can be solved efficiently by\na general convex optimization solver or can be reformulated as a conic\nquadratic program and handled by a conic solver available in some off-the-shelf\nsolvers such as CPLEX or GUROBI. Furthermore, we consider a joint location and\ncost optimization problem under the multiplicative RUM and propose three\napproaches to solve the problem, namely, an equivalent conic reformulation, a\nmulti-cut outer-approximation algorithm, and a local search heuristic. We\nprovide numerical experiments based on synthetic instances of various sizes to\nevaluate the performances of the proposed algorithms in solving the cost\noptimization, and the joint location and cost optimization problems.",
        "authors": [
            "Ngan Ha Duong",
            "Tien Thanh Dam",
            "Thuy Anh Ta",
            "Tien Mai"
        ],
        "categories": "math.OC",
        "published": "2022-05-15T17:45:38Z",
        "updated": "2023-02-11T17:20:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.06866v1",
        "title": "How do Bounce Rates vary according to product sold?",
        "abstract": "Bounce Rate of different E-commerce websites depends on the different factors\nbased upon the different devices through which traffic share is observed. This\nresearch paper focuses on how the type of products sold by different E-commerce\nwebsites affects the bounce rate obtained through Mobile/Desktop. It tries to\nexplain the observations which counter the general trend of positive relation\nbetween Mobile traffic share and bounce rate and how this is different for the\nDesktop. To estimate the differences created by the types of products sold by\nE-commerce websites on the bounce rate according to the data observed for\ndifferent time, fixed effect model (within group method) is used to determine\nthe difference created by the factors. Along with the effect of the type of\nproducts sold by the E-commerce website on bounce rate, the effect of\nindividual website is also compared to verify the results obtained for type of\nproducts.",
        "authors": [
            "Himanshu Sharma"
        ],
        "categories": "econ.EM",
        "published": "2022-05-13T19:52:51Z",
        "updated": "2022-05-13T19:52:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.06713v4",
        "title": "A Robust Permutation Test for Subvector Inference in Linear Regressions",
        "abstract": "We develop a new permutation test for inference on a subvector of\ncoefficients in linear models. The test is exact when the regressors and the\nerror terms are independent. Then, we show that the test is asymptotically of\ncorrect level, consistent and has power against local alternatives when the\nindependence condition is relaxed, under two main conditions. The first is a\nslight reinforcement of the usual absence of correlation between the regressors\nand the error term. The second is that the number of strata, defined by values\nof the regressors not involved in the subvector test, is small compared to the\nsample size. The latter implies that the vector of nuisance regressors is\ndiscrete. Simulations and empirical illustrations suggest that the test has\ngood power in practice if, indeed, the number of strata is small compared to\nthe sample size.",
        "authors": [
            "Xavier D'Haultf\u0153uille",
            "Purevdorj Tuvaandorj"
        ],
        "categories": "econ.EM",
        "published": "2022-05-13T15:35:06Z",
        "updated": "2023-09-12T07:32:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.06363v1",
        "title": "Causal Estimation of Position Bias in Recommender Systems Using Marketplace Instruments",
        "abstract": "Information retrieval systems, such as online marketplaces, news feeds, and\nsearch engines, are ubiquitous in today's digital society. They facilitate\ninformation discovery by ranking retrieved items on predicted relevance, i.e.\nlikelihood of interaction (click, share) between users and items. Typically\nmodeled using past interactions, such rankings have a major drawback:\ninteraction depends on the attention items receive. A highly-relevant item\nplaced outside a user's attention could receive little interaction. This\ndiscrepancy between observed interaction and true relevance is termed the\nposition bias. Position bias degrades relevance estimation and when it\ncompounds over time, it can silo users into false relevant items, causing\nmarketplace inefficiencies. Position bias may be identified with randomized\nexperiments, but such an approach can be prohibitive in cost and feasibility.\nPast research has also suggested propensity score methods, which do not\nadequately address unobserved confounding; and regression discontinuity\ndesigns, which have poor external validity. In this work, we address these\nconcerns by leveraging the abundance of A/B tests in ranking evaluations as\ninstrumental variables. Historical A/B tests allow us to access exogenous\nvariation in rankings without manually introducing them, harming user\nexperience and platform revenue. We demonstrate our methodology in two distinct\napplications at LinkedIn - feed ads and the People-You-May-Know (PYMK)\nrecommender. The marketplaces comprise users and campaigns on the ads side, and\ninvite senders and recipients on PYMK. By leveraging prior experimentation, we\nobtain quasi-experimental variation in item rankings that is orthogonal to user\nrelevance. Our method provides robust position effect estimates that handle\nunobserved confounding well, greater generalizability, and easily extends to\nother information retrieval systems.",
        "authors": [
            "Rina Friedberg",
            "Karthik Rajkumar",
            "Jialiang Mao",
            "Qian Yao",
            "YinYin Yu",
            "Min Liu"
        ],
        "categories": "econ.EM",
        "published": "2022-05-12T20:58:25Z",
        "updated": "2022-05-12T20:58:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.06087v1",
        "title": "A single risk approach to the semiparametric copula competing risks model",
        "abstract": "A typical situation in competing risks analysis is that the researcher is\nonly interested in a subset of risks. This paper considers a depending\ncompeting risks model with the distribution of one risk being a parametric or\nsemi-parametric model, while the model for the other risks being unknown.\nIdentifiability is shown for popular classes of parametric models and the\nsemiparametric proportional hazards model. The identifiability of the\nparametric models does not require a covariate, while the semiparametric model\nrequires at least one. Estimation approaches are suggested which are shown to\nbe $\\sqrt{n}$-consistent. Applicability and attractive finite sample\nperformance are demonstrated with the help of simulations and data examples.",
        "authors": [
            "Simon M. S. Lo",
            "Ralf A. Wilke"
        ],
        "categories": "stat.ME",
        "published": "2022-05-12T13:44:49Z",
        "updated": "2022-05-12T13:44:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.05779v2",
        "title": "Multivariate ordered discrete response models",
        "abstract": "We introduce multivariate ordered discrete response models with general\nrectangular structures. From the perspective of behavioral economics, these\nnon-lattice models correspond to broad bracketing in decision making, whereas\nlattice models, which researchers typically estimate in practice, correspond to\nnarrow bracketing. In these models, we specify latent processes as a sum of an\nindex of covariates and an unobserved error, with unobservables for different\nlatent processes potentially correlated. We provide conditions that are\nsufficient for identification under the independence of errors and covariates\nand outline an estimation approach. We present simulations and empirical\nexamples, with a particular focus on probit specifications.",
        "authors": [
            "Tatiana Komarova",
            "William Matcham"
        ],
        "categories": "econ.EM",
        "published": "2022-05-11T21:29:53Z",
        "updated": "2023-03-13T16:22:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.05561v3",
        "title": "Externally Valid Policy Choice",
        "abstract": "We consider the problem of learning personalized treatment policies that are\nexternally valid or generalizable: they perform well in other target\npopulations besides the experimental (or training) population from which data\nare sampled. We first show that welfare-maximizing policies for the\nexperimental population are robust to shifts in the distribution of outcomes\n(but not characteristics) between the experimental and target populations. We\nthen develop new methods for learning policies that are robust to shifts in\noutcomes and characteristics. In doing so, we highlight how treatment effect\nheterogeneity within the experimental population affects the generalizability\nof policies. Our methods may be used with experimental or observational data\n(where treatment is endogenous). Many of our methods can be implemented with\nlinear programming.",
        "authors": [
            "Christopher Adjaho",
            "Timothy Christensen"
        ],
        "categories": "econ.EM",
        "published": "2022-05-11T15:19:22Z",
        "updated": "2023-07-02T16:16:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.05052v2",
        "title": "On learning agent-based models from data",
        "abstract": "Agent-Based Models (ABMs) are used in several fields to study the evolution\nof complex systems from micro-level assumptions. However, ABMs typically can\nnot estimate agent-specific (or \"micro\") variables: this is a major limitation\nwhich prevents ABMs from harnessing micro-level data availability and which\ngreatly limits their predictive power. In this paper, we propose a protocol to\nlearn the latent micro-variables of an ABM from data. The first step of our\nprotocol is to reduce an ABM to a probabilistic model, characterized by a\ncomputationally tractable likelihood. This reduction follows two general design\nprinciples: balance of stochasticity and data availability, and replacement of\nunobservable discrete choices with differentiable approximations. Then, our\nprotocol proceeds by maximizing the likelihood of the latent variables via a\ngradient-based expectation maximization algorithm. We demonstrate our protocol\nby applying it to an ABM of the housing market, in which agents with different\nincomes bid higher prices to live in high-income neighborhoods. We demonstrate\nthat the obtained model allows accurate estimates of the latent variables,\nwhile preserving the general behavior of the ABM. We also show that our\nestimates can be used for out-of-sample forecasting. Our protocol can be seen\nas an alternative to black-box data assimilation methods, that forces the\nmodeler to lay bare the assumptions of the model, to think about the\ninferential process, and to spot potential identification problems.",
        "authors": [
            "Corrado Monti",
            "Marco Pangallo",
            "Gianmarco De Francisci Morales",
            "Francesco Bonchi"
        ],
        "categories": "physics.soc-ph",
        "published": "2022-05-10T17:08:26Z",
        "updated": "2022-11-23T18:05:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.05002v4",
        "title": "Estimating Discrete Games of Complete Information: Bringing Logit Back in the Game",
        "abstract": "Estimating discrete games of complete information is often computationally\ndifficult due to partial identification and the absence of closed-form moment\ncharacterizations. This paper proposes computationally tractable approaches to\nestimation and inference that remove the computational burden associated with\nequilibria enumeration, numerical simulation, and grid search. Separately for\nunordered and ordered-actions games, I construct an identified set\ncharacterized by a finite set of generalized likelihood-based conditional\nmoment inequalities that are convex in (a subvector of) structural model\nparameters under the standard logit assumption on unobservables. I use\nsimulation and empirical examples to show that the proposed approaches generate\ninformative identified sets and can be several orders of magnitude faster than\nexisting estimation methods.",
        "authors": [
            "Paul S. Koh"
        ],
        "categories": "econ.EM",
        "published": "2022-05-10T16:17:55Z",
        "updated": "2024-08-07T16:10:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.04990v2",
        "title": "Stable Outcomes and Information in Games: An Empirical Framework",
        "abstract": "Empirically, many strategic settings are characterized by stable outcomes in\nwhich players' decisions are publicly observed, yet no player takes the\nopportunity to deviate. To analyze such situations in the presence of\nincomplete information, we build an empirical framework by introducing a novel\nsolution concept that we call Bayes stable equilibrium. Our framework allows\nthe researcher to be agnostic about players' information and the equilibrium\nselection rule. The Bayes stable equilibrium identified set collapses to the\ncomplete information pure strategy Nash equilibrium identified set under strong\nassumptions on players' information. Furthermore, all else equal, it is weakly\ntighter than the Bayes correlated equilibrium identified set. We also propose\ncomputationally tractable approaches for estimation and inference. In an\napplication, we study the strategic entry decisions of McDonald's and Burger\nKing in the US. Our results highlight the identifying power of informational\nassumptions and show that the Bayes stable equilibrium identified set can be\nsubstantially tighter than the Bayes correlated equilibrium identified set. In\na counterfactual experiment, we examine the impact of increasing access to\nhealthy food on the market structures in Mississippi food deserts.",
        "authors": [
            "Paul S. Koh"
        ],
        "categories": "econ.EM",
        "published": "2022-05-10T15:56:50Z",
        "updated": "2023-05-18T22:30:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.04637v2",
        "title": "Distributionally Robust Policy Learning with Wasserstein Distance",
        "abstract": "The effects of treatments are often heterogeneous, depending on the\nobservable characteristics, and it is necessary to exploit such heterogeneity\nto devise individualized treatment rules (ITRs). Existing estimation methods of\nsuch ITRs assume that the available experimental or observational data are\nderived from the target population in which the estimated policy is\nimplemented. However, this assumption often fails in practice because of\nlimited useful data. In this case, policymakers must rely on the data generated\nin the source population, which differs from the target population.\nUnfortunately, existing estimation methods do not necessarily work as expected\nin the new setting, and strategies that can achieve a reasonable goal in such a\nsituation are required. This study examines the application of distributionally\nrobust optimization (DRO), which formalizes an ambiguity about the target\npopulation and adapts to the worst-case scenario in the set. It is shown that\nDRO with Wasserstein distance-based characterization of ambiguity provides\nsimple intuitions and a simple estimation method. I then develop an estimator\nfor the distributionally robust ITR and evaluate its theoretical performance.\nAn empirical application shows that the proposed approach outperforms the naive\napproach in the target population.",
        "authors": [
            "Daido Kido"
        ],
        "categories": "econ.EM",
        "published": "2022-05-10T02:51:46Z",
        "updated": "2022-08-06T20:11:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.04573v1",
        "title": "Robust Data-Driven Decisions Under Model Uncertainty",
        "abstract": "When sample data are governed by an unknown sequence of independent but\npossibly non-identical distributions, the data-generating process (DGP) in\ngeneral cannot be perfectly identified from the data. For making decisions\nfacing such uncertainty, this paper presents a novel approach by studying how\nthe data can best be used to robustly improve decisions. That is, no matter\nwhich DGP governs the uncertainty, one can make a better decision than without\nusing the data. I show that common inference methods, e.g., maximum likelihood\nand Bayesian updating cannot achieve this goal. To address, I develop new\nupdating rules that lead to robustly better decisions either asymptotically\nalmost surely or in finite sample with a pre-specified probability. Especially,\nthey are easy to implement as are given by simple extensions of the standard\nstatistical procedures in the case where the possible DGPs are all independent\nand identically distributed. Finally, I show that the new updating rules also\nlead to more intuitive conclusions in existing economic models such as asset\npricing under ambiguity.",
        "authors": [
            "Xiaoyu Cheng"
        ],
        "categories": "econ.TH",
        "published": "2022-05-09T21:36:20Z",
        "updated": "2022-05-09T21:36:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.04345v4",
        "title": "A unified diagnostic test for regression discontinuity designs",
        "abstract": "Diagnostic tests for regression discontinuity design face a size-control\nproblem. We document a massive over-rejection of the identifying restriction\namong empirical studies in the top five economics journals. At least one\ndiagnostic test was rejected for 21 out of 60 studies, whereas less than 5% of\nthe collected 799 tests rejected the null hypotheses. In other words, more than\none-third of the studies rejected at least one of their diagnostic tests,\nwhereas their underlying identifying restrictions appear valid. Multiple\ntesting causes this problem because the median number of tests per study was as\nhigh as 12. Therefore, we offer unified tests to overcome the size-control\nproblem. Our procedure is based on the new joint asymptotic normality of local\npolynomial mean and density estimates. In simulation studies, our unified tests\noutperformed the Bonferroni correction. We implement the procedure as an R\npackage rdtest with two empirical examples in its vignettes.",
        "authors": [
            "Koki Fusejima",
            "Takuya Ishihara",
            "Masayuki Sawada"
        ],
        "categories": "econ.EM",
        "published": "2022-05-09T14:46:29Z",
        "updated": "2024-07-22T07:28:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.03970v4",
        "title": "Policy Choice in Time Series by Empirical Welfare Maximization",
        "abstract": "This paper develops a novel method for policy choice in a dynamic setting\nwhere the available data is a multi-variate time series. Building on the\nstatistical treatment choice framework, we propose Time-series Empirical\nWelfare Maximization (T-EWM) methods to estimate an optimal policy rule by\nmaximizing an empirical welfare criterion constructed using nonparametric\npotential outcome time series. We characterize conditions under which T-EWM\nconsistently learns a policy choice that is optimal in terms of conditional\nwelfare given the time-series history. We derive a nonasymptotic upper bound\nfor conditional welfare regret. To illustrate the implementation and uses of\nT-EWM, we perform simulation studies and apply the method to estimate optimal\nrestriction rules against Covid-19.",
        "authors": [
            "Toru Kitagawa",
            "Weining Wang",
            "Mengshan Xu"
        ],
        "categories": "econ.EM",
        "published": "2022-05-08T23:22:35Z",
        "updated": "2024-12-09T05:11:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.03948v2",
        "title": "Dynamic demand for differentiated products with fixed-effects unobserved heterogeneity",
        "abstract": "This paper studies identification and estimation of a dynamic discrete choice\nmodel of demand for differentiated product using consumer-level panel data with\nfew purchase events per consumer (i.e., short panel). Consumers are\nforward-looking and their preferences incorporate two sources of dynamics: last\nchoice dependence due to habits and switching costs, and duration dependence\ndue to inventory, depreciation, or learning. A key distinguishing feature of\nthe model is that consumer unobserved heterogeneity has a Fixed Effects (FE)\nstructure -- that is, its probability distribution conditional on the initial\nvalues of endogenous state variables is unrestricted. I apply and extend recent\nresults to establish the identification of all the structural parameters as\nlong as the dataset includes four or more purchase events per household. The\nparameters can be estimated using a sufficient statistic - conditional maximum\nlikelihood (CML) method. An attractive feature of CML in this model is that the\nsufficient statistic controls for the forward-looking value of the consumer's\ndecision problem such that the method does not require solving dynamic\nprogramming problems or calculating expected present values.",
        "authors": [
            "Victor Aguirregabiria"
        ],
        "categories": "econ.EM",
        "published": "2022-05-08T20:35:57Z",
        "updated": "2022-08-18T09:53:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.03706v4",
        "title": "Identification and Estimation of Dynamic Games with Unknown Information Structure",
        "abstract": "This paper studies the identification and estimation of dynamic games when\nthe underlying information structure is unknown to the researcher. To tractably\ncharacterize the set of Markov perfect equilibrium predictions while\nmaintaining weak assumptions on players' information, we introduce\n\\textit{Markov correlated equilibrium}, a dynamic analog of Bayes correlated\nequilibrium. The set of Markov correlated equilibrium predictions coincides\nwith the set of Markov perfect equilibrium predictions that can arise when the\nplayers can observe more signals than assumed by the analyst. Using Markov\ncorrelated equilibrium as the solution concept, we propose tractable\ncomputational strategies for informationally robust estimation, inference, and\ncounterfactual analysis that deal with the non-convexities arising in dynamic\nenvironments. We use our method to analyze the dynamic competition between\nStarbucks and Dunkin' in the US and the role of informational assumptions.",
        "authors": [
            "Konan Hara",
            "Yuki Ito",
            "Paul Koh"
        ],
        "categories": "econ.EM",
        "published": "2022-05-07T19:19:44Z",
        "updated": "2024-10-11T20:42:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.03318v1",
        "title": "Benchmarking Econometric and Machine Learning Methodologies in Nowcasting",
        "abstract": "Nowcasting can play a key role in giving policymakers timelier insight to\ndata published with a significant time lag, such as final GDP figures.\nCurrently, there are a plethora of methodologies and approaches for\npractitioners to choose from. However, there lacks a comprehensive comparison\nof these disparate approaches in terms of predictive performance and\ncharacteristics. This paper addresses that deficiency by examining the\nperformance of 12 different methodologies in nowcasting US quarterly GDP\ngrowth, including all the methods most commonly employed in nowcasting, as well\nas some of the most popular traditional machine learning approaches.\nPerformance was assessed on three different tumultuous periods in US economic\nhistory: the early 1980s recession, the 2008 financial crisis, and the COVID\ncrisis. The two best performing methodologies in the analysis were long\nshort-term memory artificial neural networks (LSTM) and Bayesian vector\nautoregression (BVAR). To facilitate further application and testing of each of\nthe examined methodologies, an open-source repository containing boilerplate\ncode that can be applied to different datasets is published alongside the\npaper, available at: github.com/dhopp1/nowcasting_benchmark.",
        "authors": [
            "Daniel Hopp"
        ],
        "categories": "stat.ML",
        "published": "2022-05-06T15:51:31Z",
        "updated": "2022-05-06T15:51:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.03288v3",
        "title": "Leverage, Influence, and the Jackknife in Clustered Regression Models: Reliable Inference Using summclust",
        "abstract": "We introduce a new Stata package called summclust that summarizes the cluster\nstructure of the dataset for linear regression models with clustered\ndisturbances. The key unit of observation for such a model is the cluster. We\ntherefore propose cluster-level measures of leverage, partial leverage, and\ninfluence and show how to compute them quickly in most cases. The measures of\nleverage and partial leverage can be used as diagnostic tools to identify\ndatasets and regression designs in which cluster-robust inference is likely to\nbe challenging. The measures of influence can provide valuable information\nabout how the results depend on the data in the various clusters. We also show\nhow to calculate two jackknife variance matrix estimators efficiently as a\nbyproduct of our other computations. These estimators, which are already\navailable in Stata, are generally more conservative than conventional variance\nmatrix estimators. The summclust package computes all the quantities that we\ndiscuss.",
        "authors": [
            "James G. MacKinnon",
            "Morten \u00d8rregaard Nielsen",
            "Matthew D. Webb"
        ],
        "categories": "econ.EM",
        "published": "2022-05-06T15:14:29Z",
        "updated": "2023-11-23T14:50:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.03285v1",
        "title": "Cluster-Robust Inference: A Guide to Empirical Practice",
        "abstract": "Methods for cluster-robust inference are routinely used in economics and many\nother disciplines. However, it is only recently that theoretical foundations\nfor the use of these methods in many empirically relevant situations have been\ndeveloped. In this paper, we use these theoretical results to provide a guide\nto empirical practice. We do not attempt to present a comprehensive survey of\nthe (very large) literature. Instead, we bridge theory and practice by\nproviding a thorough guide on what to do and why, based on recently available\neconometric theory and simulation evidence. To practice what we preach, we\ninclude an empirical analysis of the effects of the minimum wage on labor\nsupply of teenagers using individual data.",
        "authors": [
            "James G. MacKinnon",
            "Morten \u00d8rregaard Nielsen",
            "Matthew D. Webb"
        ],
        "categories": "econ.EM",
        "published": "2022-05-06T15:13:28Z",
        "updated": "2022-05-06T15:13:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.03254v1",
        "title": "Estimation and Inference by Stochastic Optimization",
        "abstract": "In non-linear estimations, it is common to assess sampling uncertainty by\nbootstrap inference. For complex models, this can be computationally intensive.\nThis paper combines optimization with resampling: turning stochastic\noptimization into a fast resampling device. Two methods are introduced: a\nresampled Newton-Raphson (rNR) and a resampled quasi-Newton (rqN) algorithm.\nBoth produce draws that can be used to compute consistent estimates, confidence\nintervals, and standard errors in a single run. The draws are generated by a\ngradient and Hessian (or an approximation) computed from batches of data that\nare resampled at each iteration. The proposed methods transition quickly from\noptimization to resampling when the objective is smooth and strictly convex.\nSimulated and empirical applications illustrate the properties of the methods\non large scale and computationally intensive problems. Comparisons with\nfrequentist and Bayesian methods highlight the features of the algorithms.",
        "authors": [
            "Jean-Jacques Forneron"
        ],
        "categories": "econ.EM",
        "published": "2022-05-06T14:17:23Z",
        "updated": "2022-05-06T14:17:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.02288v1",
        "title": "Choosing Exogeneity Assumptions in Potential Outcome Models",
        "abstract": "There are many kinds of exogeneity assumptions. How should researchers choose\namong them? When exogeneity is imposed on an unobservable like a potential\noutcome, we argue that the form of exogeneity should be chosen based on the\nkind of selection on unobservables it allows. Consequently, researchers can\nassess the plausibility of any exogeneity assumption by studying the\ndistributions of treatment given the unobservables that are consistent with\nthat assumption. We use this approach to study two common exogeneity\nassumptions: quantile and mean independence. We show that both assumptions\nrequire a kind of non-monotonic relationship between treatment and the\npotential outcomes. We discuss how to assess the plausibility of this kind of\ntreatment selection. We also show how to define a new and weaker version of\nquantile independence that allows for monotonic treatment selection. We then\nshow the implications of the choice of exogeneity assumption for\nidentification. We apply these results in an empirical illustration of the\neffect of child soldiering on wages.",
        "authors": [
            "Matthew A. Masten",
            "Alexandre Poirier"
        ],
        "categories": "econ.EM",
        "published": "2022-05-04T18:47:17Z",
        "updated": "2022-05-04T18:47:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.02274v4",
        "title": "Reducing Marketplace Interference Bias Via Shadow Prices",
        "abstract": "Marketplace companies rely heavily on experimentation when making changes to\nthe design or operation of their platforms. The workhorse of experimentation is\nthe randomized controlled trial (RCT), or A/B test, in which users are randomly\nassigned to treatment or control groups. However, marketplace interference\ncauses the Stable Unit Treatment Value Assumption (SUTVA) to be violated,\nleading to bias in the standard RCT metric. In this work, we propose techniques\nfor platforms to run standard RCTs and still obtain meaningful estimates\ndespite the presence of marketplace interference. We specifically consider a\ngeneralized matching setting, in which the platform explicitly matches supply\nwith demand via a linear programming algorithm. Our first proposal is for the\nplatform to estimate the value of global treatment and global control via\noptimization. We prove that this approach is unbiased in the fluid limit. Our\nsecond proposal is to compare the average shadow price of the treatment and\ncontrol groups rather than the total value accrued by each group. We prove that\nthis technique corresponds to the correct first-order approximation (in a\nTaylor series sense) of the value function of interest even in a finite-size\nsystem. We then use this result to prove that, under reasonable assumptions,\nour estimator is less biased than the RCT estimator. At the heart of our result\nis the idea that it is relatively easy to model interference in matching-driven\nmarketplaces since, in such markets, the platform mediates the spillover.",
        "authors": [
            "Ido Bright",
            "Arthur Delarue",
            "Ilan Lobel"
        ],
        "categories": "math.OC",
        "published": "2022-05-04T18:22:52Z",
        "updated": "2024-03-14T02:21:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.01882v4",
        "title": "Approximating Choice Data by Discrete Choice Models",
        "abstract": "We obtain a necessary and sufficient condition under which random-coefficient\ndiscrete choice models, such as mixed-logit models, are rich enough to\napproximate any nonparametric random utility models arbitrarily well across\nchoice sets. The condition turns out to be the affine-independence of the set\nof characteristic vectors. When the condition fails, resulting in some random\nutility models that cannot be closely approximated, we identify preferences and\nsubstitution patterns that are challenging to approximate accurately. We also\npropose algorithms to quantify the magnitude of approximation errors.",
        "authors": [
            "Haoge Chang",
            "Yusuke Narita",
            "Kota Saito"
        ],
        "categories": "econ.TH",
        "published": "2022-05-04T04:07:16Z",
        "updated": "2023-12-10T20:02:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.01875v2",
        "title": "Machine Learning based Framework for Robust Price-Sensitivity Estimation with Application to Airline Pricing",
        "abstract": "We consider the problem of dynamic pricing of a product in the presence of\nfeature-dependent price sensitivity. Developing practical algorithms that can\nestimate price elasticities robustly, especially when information about no\npurchases (losses) is not available, to drive such automated pricing systems is\na challenge faced by many industries. Based on the Poisson semi-parametric\napproach, we construct a flexible yet interpretable demand model where the\nprice related part is parametric while the remaining (nuisance) part of the\nmodel is non-parametric and can be modeled via sophisticated machine learning\n(ML) techniques. The estimation of price-sensitivity parameters of this model\nvia direct one-stage regression techniques may lead to biased estimates due to\nregularization. To address this concern, we propose a two-stage estimation\nmethodology which makes the estimation of the price-sensitivity parameters\nrobust to biases in the estimators of the nuisance parameters of the model. In\nthe first-stage we construct estimators of observed purchases and prices given\nthe feature vector using sophisticated ML estimators such as deep neural\nnetworks. Utilizing the estimators from the first-stage, in the second-stage we\nleverage a Bayesian dynamic generalized linear model to estimate the\nprice-sensitivity parameters. We test the performance of the proposed\nestimation schemes on simulated and real sales transaction data from the\nAirline industry. Our numerical studies demonstrate that our proposed two-stage\napproach reduces the estimation error in price-sensitivity parameters from 25\\%\nto 4\\% in realistic simulation settings. The two-stage estimation techniques\nproposed in this work allows practitioners to leverage modern ML techniques to\nrobustly estimate price-sensitivities while still maintaining interpretability\nand allowing ease of validation of its various constituent parts.",
        "authors": [
            "Ravi Kumar",
            "Shahin Boluki",
            "Karl Isler",
            "Jonas Rauch",
            "Darius Walczak"
        ],
        "categories": "stat.ML",
        "published": "2022-05-04T03:35:12Z",
        "updated": "2022-12-19T22:20:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.01565v1",
        "title": "Efficient Score Computation and Expectation-Maximization Algorithm in Regime-Switching Models",
        "abstract": "This study proposes an efficient algorithm for score computation for\nregime-switching models, and derived from which, an efficient\nexpectation-maximization (EM) algorithm. Different from existing algorithms,\nthis algorithm does not rely on the forward-backward filtering for smoothed\nregime probabilities, and only involves forward computation. Moreover, the\nalgorithm to compute score is readily extended to compute the Hessian matrix.",
        "authors": [
            "Chaojun Li",
            "Shi Qiu"
        ],
        "categories": "econ.EM",
        "published": "2022-05-03T15:42:08Z",
        "updated": "2022-05-03T15:42:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.01246v2",
        "title": "Heterogeneous Treatment Effects for Networks, Panels, and other Outcome Matrices",
        "abstract": "We are interested in the distribution of treatment effects for an experiment\nwhere units are randomized to a treatment but outcomes are measured for pairs\nof units. For example, we might measure risk sharing links between households\nenrolled in a microfinance program, employment relationships between workers\nand firms exposed to a trade shock, or bids from bidders to items assigned to\nan auction format. Such a double randomized experimental design may be\nappropriate when there are social interactions, market externalities, or other\nspillovers across units assigned to the same treatment. Or it may describe a\nnatural or quasi experiment given to the researcher. In this paper, we propose\na new empirical strategy that compares the eigenvalues of the outcome matrices\nassociated with each treatment. Our proposal is based on a new matrix analog of\nthe Fr\\'echet-Hoeffding bounds that play a key role in the standard theory. We\nfirst use this result to bound the distribution of treatment effects. We then\npropose a new matrix analog of quantile treatment effects that is given by a\ndifference in the eigenvalues. We call this analog spectral treatment effects.",
        "authors": [
            "Eric Auerbach",
            "Yong Cai"
        ],
        "categories": "econ.EM",
        "published": "2022-05-02T23:39:12Z",
        "updated": "2022-10-07T23:57:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.00924v2",
        "title": "A short term credibility index for central banks under inflation targeting: an application to Brazil",
        "abstract": "This paper uses predictive densities obtained via mixed causal-noncausal\nautoregressive models to evaluate the statistical sustainability of Brazilian\ninflation targeting system with the tolerance bounds. The probabilities give an\nindication of the short-term credibility of the targeting system without\nrequiring modelling people's beliefs. We employ receiver operating\ncharacteristic curves to determine the optimal probability threshold from which\nthe bank is predicted to be credible. We also investigate the added value of\nincluding experts predictions of key macroeconomic variables.",
        "authors": [
            "Alain Hecq",
            "Joao Issler",
            "Elisa Voisin"
        ],
        "categories": "econ.EM",
        "published": "2022-05-02T14:08:35Z",
        "updated": "2022-07-23T11:50:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.00852v1",
        "title": "A Note on \"A survey of preference estimation with unobserved choice set heterogeneity\" by Gregory S. Crawford, Rachel Griffith, and Alessandro Iaria",
        "abstract": "Crawford's et al. (2021) article on estimation of discrete choice models with\nunobserved or latent consideration sets, presents a unified framework to\naddress the problem in practice by using \"sufficient sets\", defined as a\ncombination of past observed choices. The proposed approach is sustained in a\nre-interpretation of a consistency result by McFadden (1978) for the problem of\nsampling of alternatives, but the usage of that result in Crawford et al.\n(2021) is imprecise in an important matter. It is stated that consistency would\nbe attained if any subset of the true consideration set is used for estimation,\nbut McFadden (1978) shows that, in general, one needs to do a sampling\ncorrection that depends on the protocol used to draw the choice set. This note\nderives the sampling correction that is required when the choice set for\nestimation is built from past choices. Then, it formalizes the conditions under\nwhich such correction would fulfill the uniform condition property and can\ntherefore be ignored when building practical estimators, such as the ones\nanalyzed by Crawford et al. (2021).",
        "authors": [
            "C. Angelo Guevara"
        ],
        "categories": "econ.EM",
        "published": "2022-05-02T12:33:21Z",
        "updated": "2022-05-02T12:33:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.00577v2",
        "title": "Higher-order Expansions and Inference for Panel Data Models",
        "abstract": "In this paper, we propose a simple inferential method for a wide class of\npanel data models with a focus on such cases that have both serial correlation\nand cross-sectional dependence. In order to establish an asymptotic theory to\nsupport the inferential method, we develop some new and useful higher-order\nexpansions, such as Berry-Esseen bound and Edgeworth Expansion, under a set of\nsimple and general conditions. We further demonstrate the usefulness of these\ntheoretical results by explicitly investigating a panel data model with\ninteractive effects which nests many traditional panel data models as special\ncases. Finally, we show the superiority of our approach over several natural\ncompetitors using extensive numerical studies.",
        "authors": [
            "Jiti Gao",
            "Bin Peng",
            "Yayi Yan"
        ],
        "categories": "econ.EM",
        "published": "2022-05-01T23:04:40Z",
        "updated": "2023-06-08T11:12:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.00295v1",
        "title": "Greenhouse Gas Emissions and its Main Drivers: a Panel Assessment for EU-27 Member States",
        "abstract": "This paper assesses the effects of greenhouse gas emissions drivers in EU-27\nover the period 2010-2019, using a Panel EGLS model with period fixed effects.\nIn particular, we focused our research on studying the effects of GDP,\nrenewable energy, households energy consumption and waste on the greenhouse gas\nemissions. In this regard, we found a positive relationship between three\nindependent variables (real GDP per capita, households final consumption per\ncapita and waste generation per capita) and greenhouse gas emissions per\ncapita, while the effect of the share of renewable energy in gross final energy\nconsumption on the dependent variable proved to be negative, but quite low. In\naddition, we demonstrate that the main challenge that affects greenhouse gas\nemissions is related to the structure of households energy consumption, which\nis generally composed by environmentally harmful fuels. This suggests the need\nto make greater efforts to support the shift to a green economy based on a\nhigher energy efficiency.",
        "authors": [
            "I. Jianu",
            "S. M. Jeloaica",
            "M. D. Tudorache"
        ],
        "categories": "econ.EM",
        "published": "2022-04-30T15:43:46Z",
        "updated": "2022-04-30T15:43:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2205.00171v3",
        "title": "A Heteroskedasticity-Robust Overidentifying Restriction Test with High-Dimensional Covariates",
        "abstract": "This paper proposes an overidentifying restriction test for high-dimensional\nlinear instrumental variable models. The novelty of the proposed test is that\nit allows the number of covariates and instruments to be larger than the sample\nsize. The test is scale-invariant and is robust to heteroskedastic errors. To\nconstruct the final test statistic, we first introduce a test based on the\nmaximum norm of multiple parameters that could be high-dimensional. The\ntheoretical power based on the maximum norm is higher than that in the modified\nCragg-Donald test (Koles\\'{a}r, 2018), the only existing test allowing for\nlarge-dimensional covariates. Second, following the principle of power\nenhancement (Fan et al., 2015), we introduce the power-enhanced test, with an\nasymptotically zero component used to enhance the power to detect some extreme\nalternatives with many locally invalid instruments. Finally, an empirical\nexample of the trade and economic growth nexus demonstrates the usefulness of\nthe proposed test.",
        "authors": [
            "Qingliang Fan",
            "Zijian Guo",
            "Ziwei Mei"
        ],
        "categories": "econ.EM",
        "published": "2022-04-30T06:13:15Z",
        "updated": "2024-05-06T20:35:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.13815v2",
        "title": "Controlling for Latent Confounding with Triple Proxies",
        "abstract": "We present new results for nonparametric identification of causal effects\nusing noisy proxies for unobserved confounders. Our approach builds on the\nresults of \\citet{Hu2008} who tackle the problem of general measurement error.\nWe call this the `triple proxy' approach because it requires three proxies that\nare jointly independent conditional on unobservables. We consider three\ndifferent choices for the third proxy: it may be an outcome, a vector of\ntreatments, or a collection of auxiliary variables. We compare to an\nalternative identification strategy introduced by \\citet{Miao2018a} in which\ncausal effects are identified using two conditionally independent proxies. We\nrefer to this as the `double proxy' approach. The triple proxy approach\nidentifies objects that are not identified by the double proxy approach,\nincluding some that capture the variation in average treatment effects between\nstrata of the unobservables. Moreover, the conditional independence assumptions\nin the double and triple proxy approaches are non-nested.",
        "authors": [
            "Ben Deaner"
        ],
        "categories": "econ.EM",
        "published": "2022-04-28T23:12:10Z",
        "updated": "2023-05-04T06:20:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.13488v1",
        "title": "Penalized Sieve Estimation of Structural Models",
        "abstract": "Estimating structural models is an essential tool for economists. However,\nexisting methods are often inefficient either computationally or statistically,\ndepending on how equilibrium conditions are imposed. We propose a class of\npenalized sieve estimators that are consistent, asymptotic normal, and\nasymptotically efficient. Instead of solving the model repeatedly, we\napproximate the solution with a linear combination of basis functions and\nimpose equilibrium conditions as a penalty in searching for the best fitting\ncoefficients. We apply our method to an entry game between Walmart and Kmart.",
        "authors": [
            "Yao Luo",
            "Peijun Sang"
        ],
        "categories": "econ.EM",
        "published": "2022-04-28T13:31:31Z",
        "updated": "2022-04-28T13:31:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.13424v3",
        "title": "From prediction markets to interpretable collective intelligence",
        "abstract": "We outline how to create a mechanism that provides an optimal way to elicit,\nfrom an arbitrary group of experts, the probability of the truth of an\narbitrary logical proposition together with collective information that has an\nexplicit form and interprets this probability. Namely, we provide strong\narguments for the possibility of the development of a self-resolving prediction\nmarket with play money that incentivizes direct information exchange between\nexperts. Such a system could, in particular, motivate simultaneously many\nexperts to collectively solve scientific or medical problems in a very\nefficient manner. We also note that in our considerations, experts are not\nassumed to be Bayesian.",
        "authors": [
            "Alexey V. Osipov",
            "Nikolay N. Osipov"
        ],
        "categories": "cs.GT",
        "published": "2022-04-28T11:44:29Z",
        "updated": "2023-09-01T16:37:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.13150v1",
        "title": "Impulse response estimation via flexible local projections",
        "abstract": "This paper introduces a flexible local projection that generalizes the model\nby Jord\\'a (2005) to a non-parametric setting using Bayesian Additive\nRegression Trees. Monte Carlo experiments show that our BART-LP model is able\nto capture non-linearities in the impulse responses. Our first application\nshows that the fiscal multiplier is stronger in recession than in expansion\nonly in response to contractionary fiscal shocks, but not in response to\nexpansionary fiscal shocks. We then show that financial shocks generate effects\non the economy that increase more than proportionately in the size of the shock\nwhen the shock is negative, but not when the shock is positive.",
        "authors": [
            "Haroon Mumtaz",
            "Michele Piffer"
        ],
        "categories": "econ.EM",
        "published": "2022-04-27T19:14:13Z",
        "updated": "2022-04-27T19:14:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.12992v1",
        "title": "Estimation of Recursive Route Choice Models with Incomplete Trip Observations",
        "abstract": "This work concerns the estimation of recursive route choice models in the\nsituation that the trip observations are incomplete, i.e., there are\nunconnected links (or nodes) in the observations. A direct approach to handle\nthis issue would be intractable because enumerating all paths between\nunconnected links (or nodes) in a real network is typically not possible. We\nexploit an expectation-maximization (EM) method that allows to deal with the\nmissing-data issue by alternatively performing two steps of sampling the\nmissing segments in the observations and solving maximum likelihood estimation\nproblems. Moreover, observing that the EM method would be expensive, we propose\na new estimation method based on the idea that the choice probabilities of\nunconnected link observations can be exactly computed by solving systems of\nlinear equations. We further design a new algorithm, called as\ndecomposition-composition (DC), that helps reduce the number of systems of\nlinear equations to be solved and speed up the estimation. We compare our\nproposed algorithms with some standard baselines using a dataset from a real\nnetwork and show that the DC algorithm outperforms the other approaches in\nrecovering missing information in the observations. Our methods work with most\nof the recursive route choice models proposed in the literature, including the\nrecursive logit, nested recursive logit, or discounted recursive models.",
        "authors": [
            "Tien Mai",
            "The Viet Bui",
            "Quoc Phong Nguyen",
            "Tho V. Le"
        ],
        "categories": "econ.EM",
        "published": "2022-04-27T15:01:29Z",
        "updated": "2022-04-27T15:01:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.12472v1",
        "title": "A Multivariate Spatial and Spatiotemporal ARCH Model",
        "abstract": "This paper introduces a multivariate spatiotemporal autoregressive\nconditional heteroscedasticity (ARCH) model based on a vec-representation. The\nmodel includes instantaneous spatial autoregressive spill-over effects in the\nconditional variance, as they are usually present in spatial econometric\napplications. Furthermore, spatial and temporal cross-variable effects are\nexplicitly modelled. We transform the model to a multivariate spatiotemporal\nautoregressive model using a log-squared transformation and derive a consistent\nquasi-maximum-likelihood estimator (QMLE). For finite samples and different\nerror distributions, the performance of the QMLE is analysed in a series of\nMonte-Carlo simulations. In addition, we illustrate the practical usage of the\nnew model with a real-world example. We analyse the monthly real-estate price\nreturns for three different property types in Berlin from 2002 to 2014. We find\nweak (instantaneous) spatial interactions, while the temporal autoregressive\nstructure in the market risks is of higher importance. Interactions between the\ndifferent property types only occur in the temporally lagged variables. Thus,\nwe see mainly temporal volatility clusters and weak spatial volatility\nspill-overs.",
        "authors": [
            "Philipp Otto"
        ],
        "categories": "stat.ME",
        "published": "2022-04-26T17:43:21Z",
        "updated": "2022-04-26T17:43:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.12462v3",
        "title": "GMM is Inadmissible Under Weak Identification",
        "abstract": "We consider estimation in moment condition models and show that under any\nbound on identification strength, asymptotically admissible (i.e. undominated)\nestimators in a wide class of estimation problems must be uniformly continuous\nin the sample moment function. GMM estimators are in general discontinuous in\nthe sample moments, and are thus inadmissible. We show, by contrast, that\nbagged, or bootstrap aggregated, GMM estimators as well as quasi-Bayes\nposterior means have superior continuity properties, while results in the\nliterature imply that they are equivalent to GMM when identification is strong.\nIn simulations calibrated to published instrumental variables specifications,\nwe find that these alternatives often outperform GMM.",
        "authors": [
            "Isaiah Andrews",
            "Anna Mikusheva"
        ],
        "categories": "econ.EM",
        "published": "2022-04-26T17:33:42Z",
        "updated": "2023-05-09T18:10:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.12023v3",
        "title": "A One-Covariate-at-a-Time Method for Nonparametric Additive Models",
        "abstract": "This paper proposes a one-covariate-at-a-time multiple testing (OCMT)\napproach to choose significant variables in high-dimensional nonparametric\nadditive regression models. Similarly to Chudik, Kapetanios and Pesaran (2018),\nwe consider the statistical significance of individual nonparametric additive\ncomponents one at a time and take into account the multiple testing nature of\nthe problem. One-stage and multiple-stage procedures are both considered. The\nformer works well in terms of the true positive rate only if the marginal\neffects of all signals are strong enough; the latter helps to pick up hidden\nsignals that have weak marginal effects. Simulations demonstrate the good\nfinite sample performance of the proposed procedures. As an empirical\napplication, we use the OCMT procedure on a dataset we extracted from the\nLongitudinal Survey on Rural Urban Migration in China. We find that our\nprocedure works well in terms of the out-of-sample forecast root mean square\nerrors, compared with competing methods.",
        "authors": [
            "Liangjun Su",
            "Thomas Tao Yang",
            "Yonghui Zhang",
            "Qiankun Zhou"
        ],
        "categories": "econ.EM",
        "published": "2022-04-26T01:37:22Z",
        "updated": "2024-05-14T04:22:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.11748v2",
        "title": "Optimal Decision Rules when Payoffs are Partially Identified",
        "abstract": "We derive optimal statistical decision rules for discrete choice problems\nwhen payoffs depend on a partially-identified parameter $\\theta$ and the\ndecision maker can use a point-identified parameter $P$ to deduce restrictions\non $\\theta$. Leading examples include optimal treatment choice under partial\nidentification and optimal pricing with rich unobserved heterogeneity. Our\noptimal decision rules minimize the maximum risk or regret over the identified\nset of payoffs conditional on $P$ and use the data efficiently to learn about\n$P$. We discuss implementation of optimal decision rules via the bootstrap and\nBayesian methods, in both parametric and semiparametric models. We provide\ndetailed applications to treatment choice and optimal pricing. Using a limits\nof experiments framework, we show that our optimal decision rules can dominate\nseemingly natural alternatives. Our asymptotic approach is well suited for\nrealistic empirical settings in which the derivation of finite-sample optimal\nrules is intractable.",
        "authors": [
            "Timothy Christensen",
            "Hyungsik Roger Moon",
            "Frank Schorfheide"
        ],
        "categories": "econ.EM",
        "published": "2022-04-25T16:06:16Z",
        "updated": "2023-05-14T14:06:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.11318v2",
        "title": "Identification and Statistical Decision Theory",
        "abstract": "Econometricians have usefully separated study of estimation into\nidentification and statistical components. Identification analysis, which\nassumes knowledge of the probability distribution generating observable data,\nplaces an upper bound on what may be learned about population parameters of\ninterest with finite sample data. Yet Wald's statistical decision theory\nstudies decision making with sample data without reference to identification,\nindeed without reference to estimation. This paper asks if identification\nanalysis is useful to statistical decision theory. The answer is positive, as\nit can yield an informative and tractable upper bound on the achievable finite\nsample performance of decision criteria. The reasoning is simple when the\ndecision relevant parameter is point identified. It is more delicate when the\ntrue state is partially identified and a decision must be made under ambiguity.\nThen the performance of some criteria, such as minimax regret, is enhanced by\nrandomizing choice of an action. This may be accomplished by making choice a\nfunction of sample data. I find it useful to recast choice of a statistical\ndecision function as selection of choice probabilities for the elements of the\nchoice set. Using sample data to randomize choice conceptually differs from and\nis complementary to its traditional use to estimate population parameters.",
        "authors": [
            "Charles F. Manski"
        ],
        "categories": "econ.EM",
        "published": "2022-04-24T16:35:05Z",
        "updated": "2024-03-21T14:29:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.10963v2",
        "title": "Local Gaussian process extrapolation for BART models with applications to causal inference",
        "abstract": "Bayesian additive regression trees (BART) is a semi-parametric regression\nmodel offering state-of-the-art performance on out-of-sample prediction.\nDespite this success, standard implementations of BART typically provide\ninaccurate prediction and overly narrow prediction intervals at points outside\nthe range of the training data. This paper proposes a novel extrapolation\nstrategy that grafts Gaussian processes to the leaf nodes in BART for\npredicting points outside the range of the observed data. The new method is\ncompared to standard BART implementations and recent frequentist\nresampling-based methods for predictive inference. We apply the new approach to\na challenging problem from causal inference, wherein for some regions of\npredictor space, only treated or untreated units are observed (but not both).\nIn simulation studies, the new approach boasts superior performance compared to\npopular alternatives, such as Jackknife+.",
        "authors": [
            "Meijiang Wang",
            "Jingyu He",
            "P. Richard Hahn"
        ],
        "categories": "stat.ME",
        "published": "2022-04-23T00:37:53Z",
        "updated": "2023-02-24T07:15:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.10495v3",
        "title": "Adversarial Estimators",
        "abstract": "We develop an asymptotic theory of adversarial estimators ('A-estimators').\nThey generalize maximum-likelihood-type estimators ('M-estimators') as their\naverage objective is maximized by some parameters and minimized by others. This\nclass subsumes the continuous-updating Generalized Method of Moments,\nGenerative Adversarial Networks and more recent proposals in machine learning\nand econometrics. In these examples, researchers state which aspects of the\nproblem may in principle be used for estimation, and an adversary learns how to\nemphasize them optimally. We derive the convergence rates of A-estimators under\npointwise and partial identification, and the normality of functionals of their\nparameters. Unknown functions may be approximated via sieves such as deep\nneural networks, for which we provide simplified low-level conditions. As a\ncorollary, we obtain the normality of neural-net M-estimators, overcoming\ntechnical issues previously identified by the literature. Our theory yields\nnovel results about a variety of A-estimators, providing intuition and formal\njustification for their success in recent applications.",
        "authors": [
            "Jonas Metzger"
        ],
        "categories": "econ.EM",
        "published": "2022-04-22T04:39:44Z",
        "updated": "2022-06-17T16:40:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.10445v1",
        "title": "MTE with Misspecification",
        "abstract": "This paper studies the implication of a fraction of the population not\nresponding to the instrument when selecting into treatment. We show that, in\ngeneral, the presence of non-responders biases the Marginal Treatment Effect\n(MTE) curve and many of its functionals. Yet, we show that, when the propensity\nscore is fully supported on the unit interval, it is still possible to restore\nidentification of the MTE curve and its functionals with an appropriate\nre-weighting.",
        "authors": [
            "Juli\u00e1n Mart\u00ednez-Iriarte",
            "Pietro Emilio Spini"
        ],
        "categories": "econ.EM",
        "published": "2022-04-22T00:19:25Z",
        "updated": "2022-04-22T00:19:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.10359v4",
        "title": "Boundary Adaptive Local Polynomial Conditional Density Estimators",
        "abstract": "We begin by introducing a class of conditional density estimators based on\nlocal polynomial techniques. The estimators are boundary adaptive and easy to\nimplement. We then study the (pointwise and) uniform statistical properties of\nthe estimators, offering characterizations of both probability concentration\nand distributional approximation. In particular, we establish uniform\nconvergence rates in probability and valid Gaussian distributional\napproximations for the Studentized t-statistic process. We also discuss\nimplementation issues such as consistent estimation of the covariance function\nfor the Gaussian approximation, optimal integrated mean squared error bandwidth\nselection, and valid robust bias-corrected inference. We illustrate the\napplicability of our results by constructing valid confidence bands and\nhypothesis tests for both parametric specification and shape constraints,\nexplicitly characterizing their approximation errors. A companion R software\npackage implementing our main results is provided.",
        "authors": [
            "Matias D. Cattaneo",
            "Rajita Chandak",
            "Michael Jansson",
            "Xinwei Ma"
        ],
        "categories": "math.ST",
        "published": "2022-04-21T18:35:22Z",
        "updated": "2023-12-18T02:31:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.10275v4",
        "title": "Do t-Statistic Hurdles Need to be Raised?",
        "abstract": "Many scholars have called for raising statistical hurdles to guard against\nfalse discoveries in academic publications. I show these calls may be difficult\nto justify empirically. Published data exhibit bias: results that fail to meet\nexisting hurdles are often unobserved. These unobserved results must be\nextrapolated, which can lead to weak identification of revised hurdles. In\ncontrast, statistics that can target only published findings (e.g. empirical\nBayes shrinkage and the FDR) can be strongly identified, as data on published\nfindings is plentiful. I demonstrate these results theoretically and in an\nempirical analysis of the cross-sectional return predictability literature.",
        "authors": [
            "Andrew Y. Chen"
        ],
        "categories": "q-fin.GN",
        "published": "2022-04-21T17:19:35Z",
        "updated": "2024-04-06T15:12:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.10154v1",
        "title": "From point forecasts to multivariate probabilistic forecasts: The Schaake shuffle for day-ahead electricity price forecasting",
        "abstract": "Modeling price risks is crucial for economic decision making in energy\nmarkets. Besides the risk of a single price, the dependence structure of\nmultiple prices is often relevant. We therefore propose a generic and\neasy-to-implement method for creating multivariate probabilistic forecasts\nbased on univariate point forecasts of day-ahead electricity prices. While each\nunivariate point forecast refers to one of the day's 24 hours, the multivariate\nforecast distribution models dependencies across hours. The proposed method is\nbased on simple copula techniques and an optional time series component. We\nillustrate the method for five benchmark data sets recently provided by Lago et\nal. (2020). Furthermore, we demonstrate an example for constructing realistic\nprediction intervals for the weighted sum of consecutive electricity prices,\nas, e.g., needed for pricing individual load profiles.",
        "authors": [
            "Oliver Grothe",
            "Fabian K\u00e4chele",
            "Fabian Kr\u00fcger"
        ],
        "categories": "econ.EM",
        "published": "2022-04-21T15:04:39Z",
        "updated": "2022-04-21T15:04:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.09231v1",
        "title": "Optimal reconciliation with immutable forecasts",
        "abstract": "The practical importance of coherent forecasts in hierarchical forecasting\nhas inspired many studies on forecast reconciliation. Under this approach,\nso-called base forecasts are produced for every series in the hierarchy and are\nsubsequently adjusted to be coherent in a second reconciliation step.\nReconciliation methods have been shown to improve forecast accuracy, but will,\nin general, adjust the base forecast of every series. However, in an\noperational context, it is sometimes necessary or beneficial to keep forecasts\nof some variables unchanged after forecast reconciliation. In this paper, we\nformulate reconciliation methodology that keeps forecasts of a pre-specified\nsubset of variables unchanged or \"immutable\". In contrast to existing\napproaches, these immutable forecasts need not all come from the same level of\na hierarchy, and our method can also be applied to grouped hierarchies. We\nprove that our approach preserves unbiasedness in base forecasts. Our method\ncan also account for correlations between base forecasting errors and ensure\nnon-negativity of forecasts. We also perform empirical experiments, including\nan application to sales of a large scale online retailer, to assess the impacts\nof our proposed methodology.",
        "authors": [
            "Bohan Zhang",
            "Yanfei Kang",
            "Anastasios Panagiotelis",
            "Feng Li"
        ],
        "categories": "stat.ME",
        "published": "2022-04-20T05:23:31Z",
        "updated": "2022-04-20T05:23:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.08986v1",
        "title": "The 2020 Census Disclosure Avoidance System TopDown Algorithm",
        "abstract": "The Census TopDown Algorithm (TDA) is a disclosure avoidance system using\ndifferential privacy for privacy-loss accounting. The algorithm ingests the\nfinal, edited version of the 2020 Census data and the final tabulation\ngeographic definitions. The algorithm then creates noisy versions of key\nqueries on the data, referred to as measurements, using zero-Concentrated\nDifferential Privacy. Another key aspect of the TDA are invariants, statistics\nthat the Census Bureau has determined, as matter of policy, to exclude from the\nprivacy-loss accounting. The TDA post-processes the measurements together with\nthe invariants to produce a Microdata Detail File (MDF) that contains one\nrecord for each person and one record for each housing unit enumerated in the\n2020 Census. The MDF is passed to the 2020 Census tabulation system to produce\nthe 2020 Census Redistricting Data (P.L. 94-171) Summary File. This paper\ndescribes the mathematics and testing of the TDA for this purpose.",
        "authors": [
            "John M. Abowd",
            "Robert Ashmead",
            "Ryan Cumings-Menon",
            "Simson Garfinkel",
            "Micah Heineck",
            "Christine Heiss",
            "Robert Johns",
            "Daniel Kifer",
            "Philip Leclerc",
            "Ashwin Machanavajjhala",
            "Brett Moran",
            "William Sexton",
            "Matthew Spence",
            "Pavel Zhuravlev"
        ],
        "categories": "cs.CR",
        "published": "2022-04-19T16:35:29Z",
        "updated": "2022-04-19T16:35:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.08356v7",
        "title": "Inference for Cluster Randomized Experiments with Non-ignorable Cluster Sizes",
        "abstract": "This paper considers the problem of inference in cluster randomized\nexperiments when cluster sizes are non-ignorable. Here, by a cluster randomized\nexperiment, we mean one in which treatment is assigned at the cluster level. By\nnon-ignorable cluster sizes, we refer to the possibility that the treatment\neffects may depend non-trivially on the cluster sizes. We frame our analysis in\na super-population framework in which cluster sizes are random. In this way,\nour analysis departs from earlier analyses of cluster randomized experiments in\nwhich cluster sizes are treated as non-random. We distinguish between two\ndifferent parameters of interest: the equally-weighted cluster-level average\ntreatment effect, and the size-weighted cluster-level average treatment effect.\nFor each parameter, we provide methods for inference in an asymptotic framework\nwhere the number of clusters tends to infinity and treatment is assigned using\na covariate-adaptive stratified randomization procedure. We additionally permit\nthe experimenter to sample only a subset of the units within each cluster\nrather than the entire cluster and demonstrate the implications of such\nsampling for some commonly used estimators. A small simulation study and\nempirical demonstration show the practical relevance of our theoretical\nresults.",
        "authors": [
            "Federico Bugni",
            "Ivan Canay",
            "Azeem Shaikh",
            "Max Tabord-Meehan"
        ],
        "categories": "econ.EM",
        "published": "2022-04-18T15:00:45Z",
        "updated": "2024-04-09T19:00:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.08283v2",
        "title": "Feature-based intermittent demand forecast combinations: bias, accuracy and inventory implications",
        "abstract": "Intermittent demand forecasting is a ubiquitous and challenging problem in\nproduction systems and supply chain management. In recent years, there has been\na growing focus on developing forecasting approaches for intermittent demand\nfrom academic and practical perspectives. However, limited attention has been\ngiven to forecast combination methods, which have achieved competitive\nperformance in forecasting fast-moving time series. The current study aims to\nexamine the empirical outcomes of some existing forecast combination methods\nand propose a generalized feature-based framework for intermittent demand\nforecasting. The proposed framework has been shown to improve the accuracy of\npoint and quantile forecasts based on two real data sets. Further, some\nanalysis of features, forecasting pools and computational efficiency is also\nprovided. The findings indicate the intelligibility and flexibility of the\nproposed approach in intermittent demand forecasting and offer insights\nregarding inventory decisions.",
        "authors": [
            "Li Li",
            "Yanfei Kang",
            "Fotios Petropoulos",
            "Feng Li"
        ],
        "categories": "stat.AP",
        "published": "2022-04-18T12:24:34Z",
        "updated": "2022-08-31T14:01:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.08168v2",
        "title": "Nonlinear and Nonseparable Structural Functions in Fuzzy Regression Discontinuity Designs",
        "abstract": "Many empirical examples of regression discontinuity (RD) designs concern a\ncontinuous treatment variable, but the theoretical aspects of such models are\nless studied. This study examines the identification and estimation of the\nstructural function in fuzzy RD designs with a continuous treatment variable.\nThe structural function fully describes the causal impact of the treatment on\nthe outcome. We show that the nonlinear and nonseparable structural function\ncan be nonparametrically identified at the RD cutoff under shape restrictions,\nincluding monotonicity and smoothness conditions. Based on the nonparametric\nidentification equation, we propose a three-step semiparametric estimation\nprocedure and establish the asymptotic normality of the estimator. The\nsemiparametric estimator achieves the same convergence rate as in the case of a\nbinary treatment variable. As an application of the method, we estimate the\ncausal effect of sleep time on health status by using the discontinuity in\nnatural light timing at time zone boundaries.",
        "authors": [
            "Haitian Xie"
        ],
        "categories": "econ.EM",
        "published": "2022-04-18T05:41:57Z",
        "updated": "2022-07-17T04:00:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.07672v4",
        "title": "Abadie's Kappa and Weighting Estimators of the Local Average Treatment Effect",
        "abstract": "Recent research has demonstrated the importance of flexibly controlling for\ncovariates in instrumental variables estimation. In this paper we study the\nfinite sample and asymptotic properties of various weighting estimators of the\nlocal average treatment effect (LATE), motivated by Abadie's (2003) kappa\ntheorem and offering the requisite flexibility relative to standard practice.\nWe argue that two of the estimators under consideration, which are weight\nnormalized, are generally preferable. Several other estimators, which are\nunnormalized, do not satisfy the properties of scale invariance with respect to\nthe natural logarithm and translation invariance, thereby exhibiting\nsensitivity to the units of measurement when estimating the LATE in logs and\nthe centering of the outcome variable more generally. We also demonstrate that,\nwhen noncompliance is one sided, certain weighting estimators have the\nadvantage of being based on a denominator that is strictly greater than zero by\nconstruction. This is the case for only one of the two normalized estimators,\nand we recommend this estimator for wider use. We illustrate our findings with\na simulation study and three empirical applications, which clearly document the\nsensitivity of unnormalized estimators to how the outcome variable is coded. We\nimplement the proposed estimators in the Stata package kappalate.",
        "authors": [
            "Tymon S\u0142oczy\u0144ski",
            "S. Derya Uysal",
            "Jeffrey M. Wooldridge"
        ],
        "categories": "econ.EM",
        "published": "2022-04-15T22:51:50Z",
        "updated": "2024-02-28T20:42:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.06943v3",
        "title": "Option Pricing with Time-Varying Volatility Risk Aversion",
        "abstract": "We introduce a pricing kernel with time-varying volatility risk aversion that\ncan explain the observed time variation in the shape of the pricing kernel.\nDynamic volatility risk aversion, combined with the Heston-Nandi GARCH model,\nleads to a convenient option pricing model, denoted DHNG. The variance risk\nratio emerges as a fundamental variable, and we show that it is closely related\nto economic fundamentals and common measures of sentiment and uncertainty. DHNG\nyields a closed-form pricing formula for the VIX, and we propose a novel\napproximation method that provides analytical expressions for option prices. We\nestimate the model using S&P 500 returns, the VIX, and option prices, and find\nthat dynamic volatility risk aversion leads to a substantial reduction in VIX\nand option pricing errors.",
        "authors": [
            "Peter Reinhard Hansen",
            "Chen Tong"
        ],
        "categories": "q-fin.PR",
        "published": "2022-04-14T13:15:44Z",
        "updated": "2024-08-28T01:26:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.06637v2",
        "title": "Nonparametric Identification of Differentiated Products Demand Using Micro Data",
        "abstract": "We examine identification of differentiated products demand when one has\n\"micro data\" linking individual consumers' characteristics and choices. Our\nmodel nests standard specifications featuring rich observed and unobserved\nconsumer heterogeneity as well as product/market-level unobservables that\nintroduce the problem of econometric endogeneity. Previous work establishes\nidentification of such models using market-level data and instruments for all\nprices and quantities. Micro data provides a panel structure that facilitates\nricher demand specifications and reduces requirements on both the number and\ntypes of instrumental variables. We address identification of demand in the\nstandard case in which non-price product characteristics are assumed exogenous,\nbut also cover identification of demand elasticities and other key features\nwhen product characteristics are endogenous. We discuss implications of these\nresults for applied work.",
        "authors": [
            "Steven T. Berry",
            "Philip A. Haile"
        ],
        "categories": "econ.EM",
        "published": "2022-04-13T21:00:57Z",
        "updated": "2022-04-29T20:25:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.06115v3",
        "title": "Integrating Distributed Energy Resources: Optimal Prosumer Decisions and Impacts of Net Metering Tariffs",
        "abstract": "The rapid growth of the behind-the-meter (BTM) distributed generation has led\nto initiatives to reform the net energy metering (NEM) policies to address\npressing concerns of rising electricity bills, fairness of cost allocation, and\nthe long-term growth of distributed energy resources. This article presents an\nanalytical framework for the optimal prosumer consumption decision using an\ninclusive NEM X tariff model that covers existing and proposed NEM tariff\ndesigns. The structure of the optimal consumption policy lends itself to near\nclosed-form optimal solutions suitable for practical energy management systems\nthat are responsive to stochastic BTM generation and dynamic pricing. The short\nand long-run performance of NEM and feed-in tariffs (FiT) are considered under\na sequential rate-setting decision process. Also presented are numerical\nresults that characterize social welfare distributions, cross-subsidies, and\nlong-run solar adoption performance for selected NEM and FiT policy designs.",
        "authors": [
            "Ahmed S. Alahmed",
            "Lang Tong"
        ],
        "categories": "eess.SY",
        "published": "2022-04-12T23:19:50Z",
        "updated": "2022-05-24T16:40:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.05952v2",
        "title": "Retrieval from Mixed Sampling Frequency: Generic Identifiability in the Unit Root VAR",
        "abstract": "The \"REtrieval from MIxed Sampling\" (REMIS) approach based on blocking\ndeveloped in Anderson et al. (2016a) is concerned with retrieving an underlying\nhigh frequency model from mixed frequency observations. In this paper we\ninvestigate parameter-identifiability in the Johansen (1995) vector error\ncorrection model for mixed frequency data. We prove that from the second\nmoments of the blocked process after taking differences at lag N (N is the slow\nsampling rate), the parameters of the high frequency system are generically\nidentified. We treat the stock and the flow case as well as deterministic\nterms.",
        "authors": [
            "Philipp Gersing",
            "Leopold Soegner",
            "Manfred Deistler"
        ],
        "categories": "econ.EM",
        "published": "2022-04-12T17:09:10Z",
        "updated": "2023-07-06T15:15:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.05793v3",
        "title": "Coarse Personalization",
        "abstract": "Advances in estimating heterogeneous treatment effects enable firms to\npersonalize marketing mix elements and target individuals at an unmatched level\nof granularity, but feasibility constraints limit such personalization. In\npractice, firms choose which unique treatments to offer and which individuals\nto offer these treatments with the goal of maximizing profits: we call this the\ncoarse personalization problem. We propose a two-step solution that makes\nsegmentation and targeting decisions in concert. First, the firm personalizes\nby estimating conditional average treatment effects. Second, the firm\ndiscretizes by utilizing treatment effects to choose which unique treatments to\noffer and who to assign to these treatments. We show that a combination of\navailable machine learning tools for estimating heterogeneous treatment effects\nand a novel application of optimal transport methods provides a viable and\nefficient solution. With data from a large-scale field experiment for\npromotions management, we find that our methodology outperforms extant\napproaches that segment on consumer characteristics or preferences and those\nthat only search over a prespecified grid. Using our procedure, the firm\nrecoups over 99.5% of its expected incremental profits under fully granular\npersonalization while offering only five unique treatments. We conclude by\ndiscussing how coarse personalization arises in other domains.",
        "authors": [
            "Walter W. Zhang",
            "Sanjog Misra"
        ],
        "categories": "econ.EM",
        "published": "2022-04-12T13:29:33Z",
        "updated": "2024-08-25T21:42:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.05611v1",
        "title": "Portfolio Optimization Using a Consistent Vector-Based MSE Estimation Approach",
        "abstract": "This paper is concerned with optimizing the global minimum-variance\nportfolio's (GMVP) weights in high-dimensional settings where both observation\nand population dimensions grow at a bounded ratio. Optimizing the GMVP weights\nis highly influenced by the data covariance matrix estimation. In a\nhigh-dimensional setting, it is well known that the sample covariance matrix is\nnot a proper estimator of the true covariance matrix since it is not invertible\nwhen we have fewer observations than the data dimension. Even with more\nobservations, the sample covariance matrix may not be well-conditioned. This\npaper determines the GMVP weights based on a regularized covariance matrix\nestimator to overcome the aforementioned difficulties. Unlike other methods,\nthe proper selection of the regularization parameter is achieved by minimizing\nthe mean-squared error of an estimate of the noise vector that accounts for the\nuncertainty in the data mean estimation. Using random-matrix-theory tools, we\nderive a consistent estimator of the achievable mean-squared error that allows\nus to find the optimal regularization parameter using a simple line search.\nSimulation results demonstrate the effectiveness of the proposed method when\nthe data dimension is larger than the number of data samples or of the same\norder.",
        "authors": [
            "Maaz Mahadi",
            "Tarig Ballal",
            "Muhammad Moinuddin",
            "Tareq Y. Al-Naffouri",
            "Ubaid Al-Saggaf"
        ],
        "categories": "eess.SP",
        "published": "2022-04-12T08:19:14Z",
        "updated": "2022-04-12T08:19:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.05527v7",
        "title": "Neyman allocation is minimax optimal for best arm identification with two arms",
        "abstract": "This note describes the optimal policy rule, according to the local\nasymptotic minimax regret criterion, for best arm identification when there are\nonly two treatments. It is shown that the optimal sampling rule is the Neyman\nallocation, which allocates a constant fraction of units to each treatment in a\nmanner that is proportional to the standard deviation of the treatment\noutcomes. When the variances are equal, the optimal ratio is one-half. This\npolicy is independent of the data, so there is no adaptation to previous\noutcomes. At the end of the experiment, the policy maker adopts the treatment\nwith higher average outcomes.",
        "authors": [
            "Karun Adusumilli"
        ],
        "categories": "econ.EM",
        "published": "2022-04-12T04:59:13Z",
        "updated": "2022-08-25T22:31:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.05480v3",
        "title": "Tuning Parameter-Free Nonparametric Density Estimation from Tabulated Summary Data",
        "abstract": "Administrative data are often easier to access as tabulated summaries than in\nthe original format due to confidentiality concerns. Motivated by this\npractical feature, we propose a novel nonparametric density estimation method\nfrom tabulated summary data based on maximum entropy and prove its strong\nuniform consistency. Unlike existing kernel-based estimators, our estimator is\nfree from tuning parameters and admits a closed-form density that is convenient\nfor post-estimation analysis. We apply the proposed method to the tabulated\nsummary data of the U.S. tax returns to estimate the income distribution.",
        "authors": [
            "Ji Hyung Lee",
            "Yuya Sasaki",
            "Alexis Akira Toda",
            "Yulong Wang"
        ],
        "categories": "econ.EM",
        "published": "2022-04-12T02:11:41Z",
        "updated": "2023-05-17T23:26:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.05298v3",
        "title": "Two-step estimation in linear regressions with adaptive learning",
        "abstract": "Weak consistency and asymptotic normality of the ordinary least-squares\nestimator in a linear regression with adaptive learning is derived when the\ncrucial, so-called, `gain' parameter is estimated in a first step by nonlinear\nleast squares from an auxiliary model. The singular limiting distribution of\nthe two-step estimator is normal and in general affected by the sampling\nuncertainty from the first step. However, this `generated-regressor' issue\ndisappears for certain parameter combinations.",
        "authors": [
            "Alexander Mayer"
        ],
        "categories": "econ.EM",
        "published": "2022-04-11T17:55:52Z",
        "updated": "2022-11-02T13:53:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.05175v3",
        "title": "Partially Linear Models under Data Combination",
        "abstract": "We study partially linear models when the outcome of interest and some of the\ncovariates are observed in two different datasets that cannot be linked. This\ntype of data combination problem arises very frequently in empirical\nmicroeconomics. Using recent tools from optimal transport theory, we derive a\nconstructive characterization of the sharp identified set. We then build on\nthis result and develop a novel inference method that exploits the specific\ngeometric properties of the identified set. Our method exhibits good\nperformances in finite samples, while remaining very tractable. We apply our\napproach to study intergenerational income mobility over the period 1850-1930\nin the United States. Our method allows us to relax the exclusion restrictions\nused in earlier work, while delivering confidence regions that are informative.",
        "authors": [
            "Xavier D'Haultf\u0153uille",
            "Christophe Gaillac",
            "Arnaud Maurel"
        ],
        "categories": "econ.EM",
        "published": "2022-04-11T15:08:05Z",
        "updated": "2023-08-22T14:14:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.04939v1",
        "title": "Bootstrap Cointegration Tests in ARDL Models",
        "abstract": "The paper proposes a new bootstrap approach to the Pesaran, Shin and Smith's\nbound tests in a conditional equilibrium correction model with the aim to\novercome some typical drawbacks of the latter, such as inconclusive inference\nand distortion in size. The bootstrap tests are worked out under several data\ngenerating processes, including degenerate cases. Monte Carlo simulations\nconfirm the better performance of the bootstrap tests with respect to bound\nones and to the asymptotic F test on the independent variables of the ARDL\nmodel. It is also proved that any inference carried out in misspecified models,\nsuch as unconditional ARDLs, may be misleading. Empirical applications\nhighlight the importance of employing the appropriate specification and provide\ndefinitive answers to the inconclusive inference of the bound tests when\nexploring the long-term equilibrium relationship between economic variables.",
        "authors": [
            "Stefano Bertelli",
            "Gianmarco Vacca",
            "Maria Grazia Zoia"
        ],
        "categories": "econ.EM",
        "published": "2022-04-11T08:31:19Z",
        "updated": "2022-04-11T08:31:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.04860v1",
        "title": "State capital involvement, managerial sentiment and firm innovation performance Evidence from China",
        "abstract": "In recent years, more and more state-owned enterprises (SOEs) have been\nembedded in the restructuring and governance of private enterprises through\nequity participation, providing a more advantageous environment for private\nenterprises in financing and innovation. However, there is a lack of knowledge\nabout the underlying mechanisms of SOE intervention on corporate innovation\nperformance. Hence, in this study, we investigated the association of state\ncapital intervention with innovation performance, meanwhile further\ninvestigated the potential mediating and moderating role of managerial\nsentiment and financing constraints, respectively, using all listed non-ST\nfirms from 2010 to 2020 as the sample. The results revealed two main findings:\n1) state capital intervention would increase innovation performance through\nmanagerial sentiment; 2) financing constraints would moderate the effect of\nstate capital intervention on firms' innovation performance.",
        "authors": [
            "Xiangtai Zuo"
        ],
        "categories": "stat.AP",
        "published": "2022-04-11T04:20:35Z",
        "updated": "2022-04-11T04:20:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.04774v1",
        "title": "Revenue Management Under the Markov Chain Choice Model with Joint Price and Assortment Decisions",
        "abstract": "Finding the optimal product prices and product assortment are two fundamental\nproblems in revenue management. Usually, a seller needs to jointly determine\nthe prices and assortment while managing a network of resources with limited\ncapacity. However, there is not yet a tractable method to efficiently solve\nsuch a problem. Existing papers studying static joint optimization of price and\nassortment cannot incorporate resource constraints. Then we study the revenue\nmanagement problem with resource constraints and price bounds, where the prices\nand the product assortments need to be jointly determined over time. We showed\nthat under the Markov chain (MC) choice model (which subsumes the multinomial\nlogit (MNL) model), we could reformulate the choice-based joint optimization\nproblem as a tractable convex conic optimization problem. We also proved that\nan optimal solution with a constant price vector exists even with constraints\non resources. In addition, a solution with both constant assortment and price\nvector can be optimal when there is no resource constraint.",
        "authors": [
            "Anton J. Kleywegt",
            "Hongzhang Shao"
        ],
        "categories": "math.OC",
        "published": "2022-04-10T21:30:51Z",
        "updated": "2022-04-10T21:30:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.03094v1",
        "title": "Super-linear Scaling Behavior for Electric Vehicle Chargers and Road Map to Addressing the Infrastructure Gap",
        "abstract": "Enabling widespread electric vehicle (EV) adoption requires substantial\nbuild-out of charging infrastructure in the coming decade. We formulate the\ncharging infrastructure needs as a scaling analysis problem and use it to\nestimate the EV infrastructure needs of the US at a county-level resolution.\nSurprisingly, we find that the current EV infrastructure deployment scales\nsuper-linearly with population, deviating from the sub-linear scaling of\ngasoline stations and other infrastructure. We discuss how this demonstrates\nthe infancy of EV station abundance compared to other mature transportation\ninfrastructures. By considering the power delivery of existing gasoline\nstations, and appropriate EV efficiencies, we estimate the EV infrastructure\ngap at the county level, providing a road map for future EV infrastructure\nexpansion. Our reliance on scaling analysis allows us to make a unique forecast\nin this domain.",
        "authors": [
            "Alexius Wadell",
            "Matthew Guttenberg",
            "Christopher P. Kempes",
            "Venkatasubramanian Viswanathan"
        ],
        "categories": "econ.EM",
        "published": "2022-04-06T21:18:42Z",
        "updated": "2022-04-06T21:18:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.02757v2",
        "title": "Risk budget portfolios with convex Non-negative Matrix Factorization",
        "abstract": "We propose a portfolio allocation method based on risk factor budgeting using\nconvex Nonnegative Matrix Factorization (NMF). Unlike classical factor\nanalysis, PCA, or ICA, NMF ensures positive factor loadings to obtain\ninterpretable long-only portfolios. As the NMF factors represent separate\nsources of risk, they have a quasi-diagonal correlation matrix, promoting\ndiversified portfolio allocations. We evaluate our method in the context of\nvolatility targeting on two long-only global portfolios of cryptocurrencies and\ntraditional assets. Our method outperforms classical portfolio allocations\nregarding diversification and presents a better risk profile than hierarchical\nrisk parity (HRP). We assess the robustness of our findings using Monte Carlo\nsimulation.",
        "authors": [
            "Bruno Spilak",
            "Wolfgang Karl H\u00e4rdle"
        ],
        "categories": "q-fin.PM",
        "published": "2022-04-06T12:02:15Z",
        "updated": "2023-06-12T11:18:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.02346v5",
        "title": "Finitely Heterogeneous Treatment Effect in Event-study",
        "abstract": "A key assumption of the differences-in-differences designs is that the\naverage evolution of untreated potential outcomes is the same across different\ntreatment cohorts: a parallel trends assumption. In this paper, we relax the\nparallel trend assumption by assuming a latent type variable and developing a\ntype-specific parallel trend assumption. With a finite support assumption on\nthe latent type variable and long pretreatment time periods, we show that an\nextremum classifier consistently estimates the type assignment. Based on the\nclassification result, we propose a type-specific diff-in-diff estimator for\ntype-specific ATT. By estimating the type-specific ATT, we study heterogeneity\nin treatment effect, in addition to heterogeneity in baseline outcomes.",
        "authors": [
            "Myungkou Shin"
        ],
        "categories": "econ.EM",
        "published": "2022-04-05T16:58:53Z",
        "updated": "2024-10-09T14:55:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.02073v2",
        "title": "Asymptotic Theory for Unit Root Moderate Deviations in Quantile Autoregressions and Predictive Regressions",
        "abstract": "We establish the asymptotic theory in quantile autoregression when the model\nparameter is specified with respect to moderate deviations from the unit\nboundary of the form (1 + c / k) with a convergence sequence that diverges at a\nrate slower than the sample size n. Then, extending the framework proposed by\nPhillips and Magdalinos (2007), we consider the limit theory for the\nnear-stationary and the near-explosive cases when the model is estimated with a\nconditional quantile specification function and model parameters are\nquantile-dependent. Additionally, a Bahadur-type representation and limiting\ndistributions based on the M-estimators of the model parameters are derived.\nSpecifically, we show that the serial correlation coefficient converges in\ndistribution to a ratio of two independent random variables. Monte Carlo\nsimulations illustrate the finite-sample performance of the estimation\nprocedure under investigation.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2022-04-05T09:15:51Z",
        "updated": "2023-08-19T15:50:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.01974v1",
        "title": "Microtransit adoption in the wake of the COVID-19 pandemic: evidence from a choice experiment with transit and car commuters",
        "abstract": "On-demand mobility platforms play an increasingly important role in urban\nmobility systems. Impacts are still debated, as these platforms supply\npersonalized and optimized services, while also contributing to existing\nsustainability challenges. Recently, microtransit services have emerged,\npromising to combine advantages of pooled on-demand rides with more sustainable\nfixed-route public transit services. Understanding traveler behavior becomes a\nprimary focus to analyze adoption likelihood and perceptions of different\nmicrotransit attributes. The COVID-19 pandemic context adds an additional layer\nof complexity to analyzing mobility innovation acceptance. This study\ninvestigates the potential demand for microtransit options against the\nbackground of the pandemic. We use a stated choice experiment to study the\ndecision-making of Israeli public transit and car commuters when offered to use\nnovel microtransit options (sedan vs. passenger van). We investigate the\ntradeoffs related to traditional fare and travel time attributes, along with\nmicrotransit features; namely walking time to pickup location, vehicle sharing,\nwaiting time, minimum advanced reservation time, and shelter at designated\nboarding locations. Additionally, we analyze two latent constructs: attitudes\ntowards sharing, as well as experiences and risk-perceptions related to the\nCOVID-19 pandemic. We develop Integrated Choice and Latent Variable models to\ncompare the two commuter groups in terms of the likelihood to switch to\nmicrotransit, attribute trade-offs, sharing preferences and pandemic impacts.\nThe results reveal high elasticities of several time and COVID effects for car\ncommuters compared to relative insensitivity of transit commuters to the risk\nof COVID contraction. Moreover, for car commuters, those with strong sharing\nidentities were more likely to be comfortable in COVID risk situations, and to\naccept microtransit.",
        "authors": [
            "Jason Soria",
            "Shelly Etzioni",
            "Yoram Shiftan",
            "Amanda Stathopoulos",
            "Eran Ben-Elia"
        ],
        "categories": "physics.soc-ph",
        "published": "2022-04-05T04:00:35Z",
        "updated": "2022-04-05T04:00:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.01884v4",
        "title": "Policy Learning with Competing Agents",
        "abstract": "Decision makers often aim to learn a treatment assignment policy under a\ncapacity constraint on the number of agents that they can treat. When agents\ncan respond strategically to such policies, competition arises, complicating\nestimation of the optimal policy. In this paper, we study capacity-constrained\ntreatment assignment in the presence of such interference. We consider a\ndynamic model where the decision maker allocates treatments at each time step\nand heterogeneous agents myopically best respond to the previous treatment\nassignment policy. When the number of agents is large but finite, we show that\nthe threshold for receiving treatment under a given policy converges to the\npolicy's mean-field equilibrium threshold. Based on this result, we develop a\nconsistent estimator for the policy gradient. In a semi-synthetic experiment\nwith data from the National Education Longitudinal Study of 1988, we\ndemonstrate that this estimator can be used for learning capacity-constrained\npolicies in the presence of strategic behavior.",
        "authors": [
            "Roshni Sahoo",
            "Stefan Wager"
        ],
        "categories": "stat.ML",
        "published": "2022-04-04T23:15:00Z",
        "updated": "2024-04-17T04:06:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.01683v3",
        "title": "Kernel-weighted specification testing under general distributions",
        "abstract": "Kernel-weighted test statistics have been widely used in a variety of\nsettings including non-stationary regression, inference on propensity score and\npanel data models. We develop the limit theory for a kernel-based specification\ntest of a parametric conditional mean when the law of the regressors may not be\nabsolutely continuous to the Lebesgue measure and is contaminated with singular\ncomponents. This result is of independent interest and may be useful in other\napplications that utilize kernel smoothed U-statistics. Simulations illustrate\nthe non-trivial impact of the distribution of the conditioning variables on the\npower properties of the test statistic.",
        "authors": [
            "Sid Kankanala",
            "Victoria Zinde-Walsh"
        ],
        "categories": "econ.EM",
        "published": "2022-04-04T17:51:56Z",
        "updated": "2023-05-25T20:08:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.01373v1",
        "title": "A Bootstrap-Assisted Self-Normalization Approach to Inference in Cointegrating Regressions",
        "abstract": "Traditional inference in cointegrating regressions requires tuning parameter\nchoices to estimate a long-run variance parameter. Even in case these choices\nare \"optimal\", the tests are severely size distorted. We propose a novel\nself-normalization approach, which leads to a nuisance parameter free limiting\ndistribution without estimating the long-run variance parameter directly. This\nmakes our self-normalized test tuning parameter free and considerably less\nprone to size distortions at the cost of only small power losses. In\ncombination with an asymptotically justified vector autoregressive sieve\nbootstrap to construct critical values, the self-normalization approach shows\nfurther improvement in small to medium samples when the level of error serial\ncorrelation or regressor endogeneity is large. We illustrate the usefulness of\nthe bootstrap-assisted self-normalized test in empirical applications by\nanalyzing the validity of the Fisher effect in Germany and the United States.",
        "authors": [
            "Karsten Reichold",
            "Carsten Jentsch"
        ],
        "categories": "econ.EM",
        "published": "2022-04-04T10:38:30Z",
        "updated": "2022-04-04T10:38:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.01215v3",
        "title": "Capturing positive network attributes during the estimation of recursive logit models: A prism-based approach",
        "abstract": "Although the recursive logit (RL) model has been recently popular and has led\nto many applications and extensions, an important numerical issue with respect\nto the computation of value functions remains unsolved. This issue is\nparticularly significant for model estimation, during which the parameters are\nupdated every iteration and may violate the feasibility condition of the value\nfunction. To solve this numerical issue of the value function in the model\nestimation, this study performs an extensive analysis of a prism-constrained RL\n(Prism-RL) model proposed by Oyama and Hato (2019), which has a path set\nconstrained by the prism defined based upon a state-extended network\nrepresentation. The numerical experiments have shown two important properties\nof the Prism-RL model for parameter estimation. First, the prism-based approach\nenables estimation regardless of the initial and true parameter values, even in\ncases where the original RL model cannot be estimated due to the numerical\nproblem. We also successfully captured a positive effect of the presence of\nstreet green on pedestrian route choice in a real application. Second, the\nPrism-RL model achieved better fit and prediction performance than the RL\nmodel, by implicitly restricting paths with large detour or many loops.\nDefining the prism-based path set in a data-oriented manner, we demonstrated\nthe possibility of the Prism-RL model describing more realistic route choice\nbehavior. The capture of positive network attributes while retaining the\ndiversity of path alternatives is important in many applications such as\npedestrian route choice and sequential destination choice behavior, and thus\nthe prism-based approach significantly extends the practical applicability of\nthe RL model.",
        "authors": [
            "Yuki Oyama"
        ],
        "categories": "econ.EM",
        "published": "2022-04-04T02:49:25Z",
        "updated": "2023-01-02T02:21:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.00801v2",
        "title": "Robust Estimation of Conditional Factor Models",
        "abstract": "This paper develops estimation and inference methods for conditional quantile\nfactor models. We first introduce a simple sieve estimation, and establish\nasymptotic properties of the estimators under large $N$. We then provide a\nbootstrap procedure for estimating the distributions of the estimators. We also\nprovide two consistent estimators for the number of factors. The methods allow\nus not only to estimate conditional factor structures of distributions of asset\nreturns utilizing characteristics, but also to conduct robust inference in\nconditional factor models, which enables us to analyze the cross section of\nasset returns with heavy tails. We apply the methods to analyze the cross\nsection of individual US stock returns.",
        "authors": [
            "Qihui Chen"
        ],
        "categories": "econ.EM",
        "published": "2022-04-02T08:13:17Z",
        "updated": "2022-04-06T15:31:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.00551v2",
        "title": "Decomposition of Differences in Distribution under Sample Selection and the Gender Wage Gap",
        "abstract": "I address the decomposition of the differences between the distribution of\noutcomes of two groups when individuals self-select themselves into\nparticipation. I differentiate between the decomposition for participants and\nthe entire population, highlighting how the primitive components of the model\naffect each of the distributions of outcomes. Additionally, I introduce two\nancillary decompositions that help uncover the sources of differences in the\ndistribution of unobservables and participation between the two groups. The\nestimation is done using existing quantile regression methods, for which I show\nhow to perform uniformly valid inference. I illustrate these methods by\nrevisiting the gender wage gap, finding that changes in female participation\nand self-selection have been the main drivers for reducing the gap.",
        "authors": [
            "Santiago Pereda-Fern\u00e1ndez"
        ],
        "categories": "econ.EM",
        "published": "2022-04-01T16:20:57Z",
        "updated": "2023-05-11T20:29:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.00473v2",
        "title": "Finite Sample Inference in Incomplete Models",
        "abstract": "We propose confidence regions for the parameters of incomplete models with\nexact coverage of the true parameter in finite samples. Our confidence region\ninverts a test, which generalizes Monte Carlo tests to incomplete models. The\ntest statistic is a discrete analogue of a new optimal transport\ncharacterization of the sharp identified region. Both test statistic and\ncritical values rely on simulation drawn from the distribution of latent\nvariables and are computed using solutions to discrete optimal transport, hence\nlinear programming problems. We also propose a fast preliminary search in the\nparameter space with an alternative, more conservative yet consistent test,\nbased on a parameter free critical value.",
        "authors": [
            "Lixiong Li",
            "Marc Henry"
        ],
        "categories": "econ.EM",
        "published": "2022-04-01T14:30:34Z",
        "updated": "2024-04-30T17:59:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.00362v1",
        "title": "Estimating Separable Matching Models",
        "abstract": "In this paper we propose two simple methods to estimate models of matching\nwith transferable and separable utility introduced in Galichon and Salani\\'e\n(2022). The first method is a minimum distance estimator that relies on the\ngeneralized entropy of matching. The second relies on a reformulation of the\nmore special but popular Choo and Siow (2006) model; it uses generalized linear\nmodels (GLMs) with two-way fixed effects.",
        "authors": [
            "Alfred Galichon",
            "Bernard Salani\u00e9"
        ],
        "categories": "econ.EM",
        "published": "2022-04-01T11:26:18Z",
        "updated": "2022-04-01T11:26:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2204.00180v4",
        "title": "Measuring Diagnostic Test Performance Using Imperfect Reference Tests: A Partial Identification Approach",
        "abstract": "Diagnostic tests are almost never perfect. Studies quantifying their\nperformance use knowledge of the true health status, measured with a reference\ndiagnostic test. Researchers commonly assume that the reference test is\nperfect, which is often not the case in practice. When the assumption fails,\nconventional studies identify \"apparent\" performance or performance with\nrespect to the reference, but not true performance. This paper provides the\nsmallest possible bounds on the measures of true performance - sensitivity\n(true positive rate) and specificity (true negative rate), or equivalently\nfalse positive and negative rates, in standard settings. Implied bounds on\npolicy-relevant parameters are derived: 1) Prevalence in screened populations;\n2) Predictive values. Methods for inference based on moment inequalities are\nused to construct uniformly consistent confidence sets in level over a relevant\nfamily of data distributions. Emergency Use Authorization (EUA) and independent\nstudy data for the BinaxNOW COVID-19 antigen test demonstrate that the bounds\ncan be very informative. Analysis reveals that the estimated false negative\nrates for symptomatic and asymptomatic patients are up to 3.17 and 4.59 times\nhigher than the frequently cited \"apparent\" false negative rate. Further\napplicability of the results in the context of imperfect proxies such as survey\nresponses and imputed protected classes is indicated.",
        "authors": [
            "Filip Obradovi\u0107"
        ],
        "categories": "stat.AP",
        "published": "2022-04-01T03:15:25Z",
        "updated": "2024-08-19T13:55:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.15890v4",
        "title": "Testing the identification of causal effects in observational data",
        "abstract": "This study demonstrates the existence of a testable condition for the\nidentification of the causal effect of a treatment on an outcome in\nobservational data, which relies on two sets of variables: observed covariates\nto be controlled for and a suspected instrument. Under a causal structure\ncommonly found in empirical applications, the testable conditional independence\nof the suspected instrument and the outcome given the treatment and the\ncovariates has two implications. First, the instrument is valid, i.e. it does\nnot directly affect the outcome (other than through the treatment) and is\nunconfounded conditional on the covariates. Second, the treatment is\nunconfounded conditional on the covariates such that the treatment effect is\nidentified. We suggest tests of this conditional independence based on machine\nlearning methods that account for covariates in a data-driven way and\ninvestigate their asymptotic behavior and finite sample performance in a\nsimulation study. We also apply our testing approach to evaluating the impact\nof fertility on female labor supply when using the sibling sex ratio of the\nfirst two children as supposed instrument, which by and large points to a\nviolation of our testable implication for the moderate set of socio-economic\ncovariates considered.",
        "authors": [
            "Martin Huber",
            "Jannis Kueck"
        ],
        "categories": "econ.EM",
        "published": "2022-03-29T20:45:11Z",
        "updated": "2023-06-11T15:36:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.15646v1",
        "title": "Difference-in-Differences for Policy Evaluation",
        "abstract": "Difference-in-differences is one of the most used identification strategies\nin empirical work in economics. This chapter reviews a number of important,\nrecent developments related to difference-in-differences. First, this chapter\nreviews recent work pointing out limitations of two way fixed effects\nregressions (these are panel data regressions that have been the dominant\napproach to implementing difference-in-differences identification strategies)\nthat arise in empirically relevant settings where there are more than two time\nperiods, variation in treatment timing across units, and treatment effect\nheterogeneity. Second, this chapter reviews recently proposed alternative\napproaches that are able to circumvent these issues without being substantially\nmore complicated to implement. Third, this chapter covers a number of\nextensions to these results, paying particular attention to (i) parallel trends\nassumptions that hold only after conditioning on observed covariates and (ii)\nstrategies to partially identify causal effect parameters in\ndifference-in-differences applications in cases where the parallel trends\nassumption may be violated.",
        "authors": [
            "Brantly Callaway"
        ],
        "categories": "econ.EM",
        "published": "2022-03-29T15:06:20Z",
        "updated": "2022-03-29T15:06:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.15603v2",
        "title": "Estimating Nonlinear Network Data Models with Fixed Effects",
        "abstract": "I introduce a new method for bias correction of dyadic models with\nagent-specific fixed-effects, including the dyadic link formation model with\nhomophily and degree heterogeneity. The proposed approach uses a jackknife\nprocedure to deal with the incidental parameters problem. The method can be\napplied to both directed and undirected networks, allows for non-binary outcome\nvariables, and can be used to bias correct estimates of average effects and\ncounterfactual outcomes. I also show how the jackknife can be used to\nbias-correct fixed effect averages over functions that depend on multiple\nnodes, e.g. triads or tetrads in the network. As an example, I implement\nspecification tests for dependence across dyads, such as reciprocity or\ntransitivity. Finally, I demonstrate the usefulness of the estimator in an\napplication to a gravity model for import/export relationships across\ncountries.",
        "authors": [
            "David W. Hughes"
        ],
        "categories": "econ.EM",
        "published": "2022-03-29T14:12:13Z",
        "updated": "2023-03-17T20:06:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.14488v1",
        "title": "Network structure and fragmentation of the Argentinean interbank markets",
        "abstract": "This paper studies the network structure and fragmentation of the Argentinean\ninterbank market. Both the unsecured (CALL) and the secured (REPO) markets are\nexamined, applying complex network analysis. Results indicate that, although\nthe secured market has less participants, its nodes are more densely connected\nthan in the unsecured market. The interrelationships in the unsecured market\nare less stable, making its structure more volatile and vulnerable to negative\nshocks. The analysis identifies two 'hidden' underlying sub-networks within the\nREPO market: one based on the transactions collateralized by Treasury bonds\n(REPO-T) and other based on the operations collateralized by Central Bank (CB)\nsecurities (REPO-CB). The changes in monetary policy stance and monetary\nconditions seem to have a substantially smaller impact in the former than in\nthe latter 'sub-market'. The connectivity levels within the REPO-T market and\nits structure remain relatively unaffected by the (in some period pronounced)\nswings in the other segment of the market. Hence, the REPO market shows signs\nof fragmentation in its inner structure, according to the type of collateral\nasset involved in the transactions, so the average REPO interest rate reflects\nthe interplay between these two partially fragmented sub-markets. This mixed\nstructure of the REPO market entails one of the main sources of differentiation\nwith respect to the CALL market.",
        "authors": [
            "Federico Forte",
            "Pedro Elosegui",
            "Gabriel Montes-Rojas"
        ],
        "categories": "physics.soc-ph",
        "published": "2022-03-28T04:20:05Z",
        "updated": "2022-03-28T04:20:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.13887v5",
        "title": "Automatic Debiased Machine Learning for Dynamic Treatment Effects and General Nested Functionals",
        "abstract": "We extend the idea of automated debiased machine learning to the dynamic\ntreatment regime and more generally to nested functionals. We show that the\nmultiply robust formula for the dynamic treatment regime with discrete\ntreatments can be re-stated in terms of a recursive Riesz representer\ncharacterization of nested mean regressions. We then apply a recursive Riesz\nrepresenter estimation learning algorithm that estimates de-biasing corrections\nwithout the need to characterize how the correction terms look like, such as\nfor instance, products of inverse probability weighting terms, as is done in\nprior work on doubly robust estimation in the dynamic regime. Our approach\ndefines a sequence of loss minimization problems, whose minimizers are the\nmulitpliers of the de-biasing correction, hence circumventing the need for\nsolving auxiliary propensity models and directly optimizing for the mean\nsquared error of the target de-biasing correction. We provide further\napplications of our approach to estimation of dynamic discrete choice models\nand estimation of long-term effects with surrogates.",
        "authors": [
            "Victor Chernozhukov",
            "Whitney Newey",
            "Rahul Singh",
            "Vasilis Syrgkanis"
        ],
        "categories": "econ.EM",
        "published": "2022-03-25T19:54:17Z",
        "updated": "2023-06-20T22:00:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.13001v1",
        "title": "The application of techniques derived from artificial intelligence to the prediction of the solvency of bank customers: case of the application of the cart type decision tree (dt)",
        "abstract": "In this study we applied the CART-type Decision Tree (DT-CART) method derived\nfrom artificial intelligence technique to the prediction of the solvency of\nbank customers, for this we used historical data of bank customers. However we\nhave adopted the process of Data Mining techniques, for this purpose we started\nwith a data preprocessing in which we clean the data and we deleted all rows\nwith outliers or missing values as well as rows with empty columns, then we\nfixed the variable to be explained (dependent or Target) and we also thought to\neliminate all explanatory (independent) variables that are not significant\nusing univariate analysis as well as the correlation matrix, then we applied\nour CART decision tree method using the SPSS tool. After completing our process\nof building our model (AD-CART), we started the process of evaluating and\ntesting the performance of our model, by which we found that the accuracy and\nprecision of our model is 71%, so we calculated the error ratios, and we found\nthat the error rate equal to 29%, this allowed us to conclude that our model at\na fairly good level in terms of precision, predictability and very precisely in\npredicting the solvency of our banking customers.",
        "authors": [
            "Karim Amzile",
            "Rajaa Amzile"
        ],
        "categories": "q-fin.RM",
        "published": "2022-03-24T11:49:16Z",
        "updated": "2022-03-24T11:49:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.12740v5",
        "title": "Correcting Attrition Bias using Changes-in-Changes",
        "abstract": "Attrition is a common and potentially important threat to internal validity\nin treatment effect studies. We extend the changes-in-changes approach to\nidentify the average treatment effect for respondents and the entire study\npopulation in the presence of attrition. Our method, which exploits baseline\noutcome data, can be applied to randomized experiments as well as\nquasi-experimental difference-in-difference designs. A formal comparison\nhighlights that while widely used corrections typically impose restrictions on\nwhether or how response depends on treatment, our proposed attrition correction\nexploits restrictions on the outcome model. We further show that the conditions\nrequired for our correction can accommodate a broad class of response models\nthat depend on treatment in an arbitrary way. We illustrate the implementation\nof the proposed corrections in an application to a large-scale randomized\nexperiment.",
        "authors": [
            "Dalia Ghanem",
            "Sarojini Hirshleifer",
            "D\u00e9sir\u00e9 K\u00e9dagni",
            "Karen Ortiz-Becerra"
        ],
        "categories": "econ.EM",
        "published": "2022-03-23T21:55:32Z",
        "updated": "2024-03-27T18:52:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.12431v1",
        "title": "Bounds for Bias-Adjusted Treatment Effect in Linear Econometric Models",
        "abstract": "In linear econometric models with proportional selection on unobservables,\nomitted variable bias in estimated treatment effects are real roots of a cubic\nequation involving estimated parameters from a short and intermediate\nregression. The roots of the cubic are functions of $\\delta$, the degree of\nselection on unobservables, and $R_{max}$, the R-squared in a hypothetical long\nregression that includes the unobservable confounder and all observable\ncontrols. In this paper I propose and implement a novel algorithm to compute\nroots of the cubic equation over relevant regions of the $\\delta$-$R_{max}$\nplane and use the roots to construct bounding sets for the true treatment\neffect. The algorithm is based on two well-known mathematical results: (a) the\ndiscriminant of the cubic equation can be used to demarcate regions of unique\nreal roots from regions of three real roots, and (b) a small change in the\ncoefficients of a polynomial equation will lead to small change in its roots\nbecause the latter are continuous functions of the former. I illustrate my\nmethod by applying it to the analysis of maternal behavior on child outcomes.",
        "authors": [
            "Deepankar Basu"
        ],
        "categories": "econ.EM",
        "published": "2022-03-23T14:11:44Z",
        "updated": "2022-03-23T14:11:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.12408v1",
        "title": "Exabel's Factor Model",
        "abstract": "Factor models have become a common and valued tool for understanding the\nrisks associated with an investing strategy. In this report we describe\nExabel's factor model, we quantify the fraction of the variability of the\nreturns explained by the different factors, and we show some examples of annual\nreturns of portfolios with different factor exposure.",
        "authors": [
            "\u00d8yvind Grotmol",
            "Michael Scheuerer",
            "Kjersti Aas",
            "Martin Jullum"
        ],
        "categories": "stat.AP",
        "published": "2022-03-23T13:34:06Z",
        "updated": "2022-03-23T13:34:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.12402v1",
        "title": "Performance evaluation of volatility estimation methods for Exabel",
        "abstract": "Quantifying both historic and future volatility is key in portfolio risk\nmanagement. This note presents and compares estimation strategies for\nvolatility estimation in an estimation universe consisting on 28 629 unique\ncompanies from February 2010 to April 2021, with 858 different portfolios. The\nestimation methods are compared in terms of how they rank the volatility of the\ndifferent subsets of portfolios. The overall best performing approach estimates\nvolatility from direct entity returns using a GARCH model for variance\nestimation.",
        "authors": [
            "\u00d8yvind Grotmol",
            "Martin Jullum",
            "Kjersti Aas",
            "Michael Scheuerer"
        ],
        "categories": "stat.AP",
        "published": "2022-03-23T13:26:36Z",
        "updated": "2022-03-23T13:26:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.12228v3",
        "title": "Bivariate Distribution Regression with Application to Insurance Data",
        "abstract": "Understanding variable dependence, particularly eliciting their statistical\nproperties given a set of covariates, provides the mathematical foundation in\npractical operations management such as risk analysis and decision-making given\nobserved circumstances. This article presents an estimation method for modeling\nthe conditional joint distribution of bivariate outcomes based on the\ndistribution regression and factorization methods. This method is considered\nsemiparametric in that it allows for flexible modeling of both the marginal and\njoint distributions conditional on covariates without imposing global\nparametric assumptions across the entire distribution. In contrast to existing\nparametric approaches, our method can accommodate discrete, continuous, or\nmixed variables, and provides a simple yet effective way to capture\ndistributional dependence structures between bivariate outcomes and covariates.\nVarious simulation results confirm that our method can perform similarly or\nbetter in finite samples compared to the alternative methods. In an application\nto the study of a motor third-party liability insurance portfolio, the proposed\nmethod effectively estimates risk measures such as the conditional\nValue-at-Risk and Expected Shortfall. This result suggests that this\nsemiparametric approach can serve as an alternative in insurance risk\nmanagement.",
        "authors": [
            "Yunyun Wang",
            "Tatsushi Oka",
            "Dan Zhu"
        ],
        "categories": "stat.ME",
        "published": "2022-03-23T06:50:37Z",
        "updated": "2023-09-03T10:22:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.11872v1",
        "title": "Performance of long short-term memory artificial neural networks in nowcasting during the COVID-19 crisis",
        "abstract": "The COVID-19 pandemic has demonstrated the increasing need of policymakers\nfor timely estimates of macroeconomic variables. A prior UNCTAD research paper\nexamined the suitability of long short-term memory artificial neural networks\n(LSTM) for performing economic nowcasting of this nature. Here, the LSTM's\nperformance during the COVID-19 pandemic is compared and contrasted with that\nof the dynamic factor model (DFM), a commonly used methodology in the field.\nThree separate variables, global merchandise export values and volumes and\nglobal services exports, were nowcast with actual data vintages and performance\nevaluated for the second, third, and fourth quarters of 2020 and the first and\nsecond quarters of 2021. In terms of both mean absolute error and root mean\nsquare error, the LSTM obtained better performance in two-thirds of\nvariable/quarter combinations, as well as displayed more gradual forecast\nevolutions with more consistent narratives and smaller revisions. Additionally,\na methodology to introduce interpretability to LSTMs is introduced and made\navailable in the accompanying nowcast_lstm Python library, which is now also\navailable in R, MATLAB, and Julia.",
        "authors": [
            "Daniel Hopp"
        ],
        "categories": "stat.ML",
        "published": "2022-03-22T16:48:41Z",
        "updated": "2022-03-22T16:48:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.11820v1",
        "title": "Dealing with Logs and Zeros in Regression Models",
        "abstract": "Log-linear models are prevalent in empirical research. Yet, how to handle\nzeros in the dependent variable remains an unsettled issue. This article\nclarifies it and addresses the log of zero by developing a new family of\nestimators called iterated Ordinary Least Squares (iOLS). This family nests\nstandard approaches such as log-linear and Poisson regressions, offers several\ncomputational advantages, and corresponds to the correct way to perform the\npopular $\\log(Y+1)$ transformation. We extend it to the endogenous regressor\nsetting (i2SLS) and overcome other common issues with Poisson models, such as\ncontrolling for many fixed-effects. We also develop specification tests to help\nresearchers select between alternative estimators. Finally, our methods are\nillustrated through numerical simulations and replications of landmark\npublications.",
        "authors": [
            "Christophe Bell\u00e9go",
            "David Benatia",
            "Louis Pape"
        ],
        "categories": "econ.EM",
        "published": "2022-03-22T15:40:01Z",
        "updated": "2022-03-22T15:40:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.11576v2",
        "title": "Predictor Selection for Synthetic Controls",
        "abstract": "Synthetic control methods often rely on matching pre-treatment\ncharacteristics (called predictors) of the treated unit. The choice of\npredictors and how they are weighted plays a key role in the performance and\ninterpretability of synthetic control estimators. This paper proposes the use\nof a sparse synthetic control procedure that penalizes the number of predictors\nused in generating the counterfactual to select the most important predictors.\nWe derive, in a linear factor model framework, a new model selection\nconsistency result and show that the penalized procedure has a faster mean\nsquared error convergence rate. Through a simulation study, we then show that\nthe sparse synthetic control achieves lower bias and has better post-treatment\nperformance than the un-penalized synthetic control. Finally, we apply the\nmethod to revisit the study of the passage of Proposition 99 in California in\nan augmented setting with a large number of predictors available.",
        "authors": [
            "Jaume Vives-i-Bastida"
        ],
        "categories": "stat.ME",
        "published": "2022-03-22T09:54:06Z",
        "updated": "2022-12-29T09:23:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.10683v2",
        "title": "Indirect Inference for Nonlinear Panel Models with Fixed Effects",
        "abstract": "Fixed effect estimators of nonlinear panel data models suffer from the\nincidental parameter problem. This leads to two undesirable consequences in\napplied research: (1) point estimates are subject to large biases, and (2)\nconfidence intervals have incorrect coverages. This paper proposes a\nsimulation-based method for bias reduction. The method simulates data using the\nmodel with estimated individual effects, and finds values of parameters by\nequating fixed effect estimates obtained from observed and simulated data. The\nasymptotic framework provides consistency, bias correction, and asymptotic\nnormality results. An application and simulations to female labor force\nparticipation illustrates the finite-sample performance of the method.",
        "authors": [
            "Shuowen Chen"
        ],
        "categories": "econ.EM",
        "published": "2022-03-21T00:27:49Z",
        "updated": "2022-04-15T01:57:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.11691v1",
        "title": "GAM(L)A: An econometric model for interpretable Machine Learning",
        "abstract": "Despite their high predictive performance, random forest and gradient\nboosting are often considered as black boxes or uninterpretable models which\nhas raised concerns from practitioners and regulators. As an alternative, we\npropose in this paper to use partial linear models that are inherently\ninterpretable. Specifically, this article introduces GAM-lasso (GAMLA) and\nGAM-autometrics (GAMA), denoted as GAM(L)A in short. GAM(L)A combines\nparametric and non-parametric functions to accurately capture linearities and\nnon-linearities prevailing between dependent and explanatory variables, and a\nvariable selection procedure to control for overfitting issues. Estimation\nrelies on a two-step procedure building upon the double residual method. We\nillustrate the predictive performance and interpretability of GAM(L)A on a\nregression and a classification problem. The results show that GAM(L)A\noutperforms parametric models augmented by quadratic, cubic and interaction\neffects. Moreover, the results also suggest that the performance of GAM(L)A is\nnot significantly different from that of random forest and gradient boosting.",
        "authors": [
            "Emmanuel Flachaire",
            "Gilles Hacheme",
            "Sullivan Hu\u00e9",
            "S\u00e9bastien Laurent"
        ],
        "categories": "stat.ML",
        "published": "2022-03-17T09:09:44Z",
        "updated": "2022-03-17T09:09:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.09001v11",
        "title": "Selection and parallel trends",
        "abstract": "We study the role of selection into treatment in difference-in-differences\n(DiD) designs. We derive necessary and sufficient conditions for parallel\ntrends assumptions under general classes of selection mechanisms. These\nconditions characterize the empirical content of parallel trends. For settings\nwhere the necessary conditions are questionable, we propose tools for\nselection-based sensitivity analysis. We also provide templates for justifying\nDiD in applications with and without covariates. A reanalysis of the causal\neffect of NSW training programs demonstrates the usefulness of our\nselection-based approach to sensitivity analysis.",
        "authors": [
            "Dalia Ghanem",
            "Pedro H. C. Sant'Anna",
            "Kaspar W\u00fcthrich"
        ],
        "categories": "econ.EM",
        "published": "2022-03-17T00:44:30Z",
        "updated": "2024-03-27T19:11:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.09000v4",
        "title": "Lorenz map, inequality ordering and curves based on multidimensional rearrangements",
        "abstract": "We propose a multivariate extension of the Lorenz curve based on multivariate\nrearrangements of optimal transport theory. We define a vector Lorenz map as\nthe integral of the vector quantile map associated with a multivariate resource\nallocation. Each component of the Lorenz map is the cumulative share of each\nresource, as in the traditional univariate case. The pointwise ordering of such\nLorenz maps defines a new multivariate majorization order, which is equivalent\nto preference by any social planner with inequality averse multivariate rank\ndependent social evaluation functional. We define a family of multi-attribute\nGini index and complete ordering based on the Lorenz map. We propose the level\nsets of an Inverse Lorenz Function as a practical tool to visualize and compare\ninequality in two dimensions, and apply it to income-wealth inequality in the\nUnited States between 1989 and 2022.",
        "authors": [
            "Yanqin Fan",
            "Marc Henry",
            "Brendan Pass",
            "Jorge A. Rivero"
        ],
        "categories": "econ.EM",
        "published": "2022-03-17T00:39:47Z",
        "updated": "2024-04-15T23:49:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.08879v4",
        "title": "A Simple and Computationally Trivial Estimator for Grouped Fixed Effects Models",
        "abstract": "This paper introduces a new fixed effects estimator for linear panel data\nmodels with clustered time patterns of unobserved heterogeneity. The method\navoids non-convex and combinatorial optimization by combining a preliminary\nconsistent estimator of the slope coefficient, an agglomerative\npairwise-differencing clustering of cross-sectional units, and a pooled\nordinary least squares regression. Asymptotic guarantees are established in a\nframework where $T$ can grow at any power of $N$, as both $N$ and $T$ approach\ninfinity. Unlike most existing approaches, the proposed estimator is\ncomputationally straightforward and does not require a known upper bound on the\nnumber of groups. As existing approaches, this method leads to a consistent\nestimation of well-separated groups and an estimator of common parameters\nasymptotically equivalent to the infeasible regression controlling for the true\ngroups. An application revisits the statistical association between income and\ndemocracy.",
        "authors": [
            "Martin Mugnier"
        ],
        "categories": "econ.EM",
        "published": "2022-03-16T18:50:22Z",
        "updated": "2024-09-27T08:40:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.08635v1",
        "title": "Measurability of functionals and of ideal point forecasts",
        "abstract": "The ideal probabilistic forecast for a random variable $Y$ based on an\ninformation set $\\mathcal{F}$ is the conditional distribution of $Y$ given\n$\\mathcal{F}$. In the context of point forecasts aiming to specify a functional\n$T$ such as the mean, a quantile or a risk measure, the ideal point forecast is\nthe respective functional applied to the conditional distribution. This paper\nprovides a theoretical justification why this ideal forecast is actually a\nforecast, that is, an $\\mathcal{F}$-measurable random variable. To that end,\nthe appropriate notion of measurability of $T$ is clarified and this\nmeasurability is established for a large class of practically relevant\nfunctionals, including elicitable ones. More generally, the measurability of\n$T$ implies the measurability of any point forecast which arises by applying\n$T$ to a probabilistic forecast. Similar measurability results are established\nfor proper scoring rules, the main tool to evaluate the predictive accuracy of\nprobabilistic forecasts.",
        "authors": [
            "Tobias Fissler",
            "Hajo Holzmann"
        ],
        "categories": "math.ST",
        "published": "2022-03-16T14:00:53Z",
        "updated": "2022-03-16T14:00:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.08050v4",
        "title": "Pairwise Valid Instruments",
        "abstract": "Finding valid instruments is difficult. We propose Validity Set Instrumental\nVariable (VSIV) estimation, a method for estimating local average treatment\neffects (LATEs) in heterogeneous causal effect models when the instruments are\npartially invalid. We consider settings with pairwise valid instruments, that\nis, instruments that are valid for a subset of instrument value pairs. VSIV\nestimation exploits testable implications of instrument validity to remove\ninvalid pairs and provides estimates of the LATEs for all remaining pairs,\nwhich can be aggregated into a single parameter of interest using\nresearcher-specified weights. We show that the proposed VSIV estimators are\nasymptotically normal under weak conditions and remove or reduce the asymptotic\nbias relative to standard LATE estimators (that is, LATE estimators that do not\nuse testable implications to remove invalid variation). We evaluate the finite\nsample properties of VSIV estimation in application-based simulations and apply\nour method to estimate the returns to college education using parental\neducation as an instrument.",
        "authors": [
            "Zhenting Sun",
            "Kaspar W\u00fcthrich"
        ],
        "categories": "econ.EM",
        "published": "2022-03-15T16:44:20Z",
        "updated": "2024-01-16T16:32:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.08014v3",
        "title": "Non-Existent Moments of Earnings Growth",
        "abstract": "The literature often employs moment-based earnings risk measures like\nvariance, skewness, and kurtosis. However, under heavy-tailed distributions,\nthese moments may not exist in the population. Our empirical analysis reveals\nthat population kurtosis, skewness, and variance often do not exist for the\nconditional distribution of earnings growth. This challenges moment-based\nanalyses. We propose robust conditional Pareto exponents as novel earnings risk\nmeasures, developing estimation and inference methods. Using the UK New\nEarnings Survey Panel Dataset (NESPD) and US Panel Study of Income Dynamics\n(PSID), we find: 1) Moments often fail to exist; 2) Earnings risk increases\nover the life cycle; 3) Job stayers face higher earnings risk; 4) These\npatterns persist during the 2007--2008 recession and the 2015--2016 positive\ngrowth period.",
        "authors": [
            "Silvia Sarpietro",
            "Yuya Sasaki",
            "Yulong Wang"
        ],
        "categories": "econ.EM",
        "published": "2022-03-15T15:50:49Z",
        "updated": "2024-02-17T12:55:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.06685v3",
        "title": "Encompassing Tests for Nonparametric Regressions",
        "abstract": "We set up a formal framework to characterize encompassing of nonparametric\nmodels through the L2 distance. We contrast it to previous literature on the\ncomparison of nonparametric regression models. We then develop testing\nprocedures for the encompassing hypothesis that are fully nonparametric. Our\ntest statistics depend on kernel regression, raising the issue of bandwidth's\nchoice. We investigate two alternative approaches to obtain a \"small bias\nproperty\" for our test statistics. We show the validity of a wild bootstrap\nmethod. We empirically study the use of a data-driven bandwidth and illustrate\nthe attractive features of our tests for small and moderate samples.",
        "authors": [
            "Elia Lapenta",
            "Pascal Lavergne"
        ],
        "categories": "econ.EM",
        "published": "2022-03-13T15:41:40Z",
        "updated": "2023-10-25T14:47:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.06640v1",
        "title": "Measuring anomalies in cigarette sales by using official data from Spanish provinces: Are there only the anomalies detected by the Empty Pack Surveys (EPS) used by Transnational Tobacco Companies (TTCs)?",
        "abstract": "There is literature that questions the veracity of the studies commissioned\nby the transnational tobacco companies (TTC) to measure the illicit tobacco\ntrade. Furthermore, there are studies that indicate that the Empty Pack Surveys\n(EPS) ordered by the TTCs are oversized. The novelty of this study is that, in\naddition to detecting the anomalies analyzed in the EPSs, there are provinces\nin which cigarette sales are higher than reasonable values, something that the\nTTCs ignore. This study analyzed simultaneously, firstly, if the EPSs\nestablished in each of the 47 Spanish provinces were fulfilled. Second,\nanomalies observed in provinces where sales exceed expected values are\nmeasured. To achieve the objective of the paper, provincial data on cigarette\nsales, price and GDP per capita are used. These data are modeled with machine\nlearning techniques widely used to detect anomalies in other areas. The results\nreveal that the provinces in which sales below reasonable values are observed\n(as detected by the EPSs) present a clear geographical pattern. Furthermore,\nthe values provided by the EPSs in Spain, as indicated in the previous\nliterature, are slightly oversized. Finally, there are regions bordering other\ncountries or with a high tourist influence in which the observed sales are\nhigher than the expected values.",
        "authors": [
            "Pedro Cadahia",
            "Antonio A. Golpe",
            "Juan M. Mart\u00edn \u00c1lvarez",
            "E. Asensio"
        ],
        "categories": "stat.AP",
        "published": "2022-03-13T12:05:28Z",
        "updated": "2022-03-13T12:05:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.06279v1",
        "title": "Synthetic Controls in Action",
        "abstract": "In this article we propose a set of simple principles to guide empirical\npractice in synthetic control studies. The proposed principles follow from\nformal properties of synthetic control estimators, and pertain to the nature,\nimplications, and prevention of over-fitting biases within a synthetic control\nframework, to the interpretability of the results, and to the availability of\nvalidation exercises. We discuss and visually demonstrate the relevance of the\nproposed principles under a variety of data configurations.",
        "authors": [
            "Alberto Abadie",
            "Jaume Vives-i-Bastida"
        ],
        "categories": "stat.ME",
        "published": "2022-03-11T23:07:34Z",
        "updated": "2022-03-11T23:07:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.04768v1",
        "title": "Explainable Machine Learning for Predicting Homicide Clearance in the United States",
        "abstract": "Purpose: To explore the potential of Explainable Machine Learning in the\nprediction and detection of drivers of cleared homicides at the national- and\nstate-levels in the United States.\n  Methods: First, nine algorithmic approaches are compared to assess the best\nperformance in predicting cleared homicides country-wise, using data from the\nMurder Accountability Project. The most accurate algorithm among all (XGBoost)\nis then used for predicting clearance outcomes state-wise. Second, SHAP, a\nframework for Explainable Artificial Intelligence, is employed to capture the\nmost important features in explaining clearance patterns both at the national\nand state levels.\n  Results: At the national level, XGBoost demonstrates to achieve the best\nperformance overall. Substantial predictive variability is detected state-wise.\nIn terms of explainability, SHAP highlights the relevance of several features\nin consistently predicting investigation outcomes. These include homicide\ncircumstances, weapons, victims' sex and race, as well as number of involved\noffenders and victims.\n  Conclusions: Explainable Machine Learning demonstrates to be a helpful\nframework for predicting homicide clearance. SHAP outcomes suggest a more\norganic integration of the two theoretical perspectives emerged in the\nliterature. Furthermore, jurisdictional heterogeneity highlights the importance\nof developing ad hoc state-level strategies to improve police performance in\nclearing homicides.",
        "authors": [
            "Gian Maria Campedelli"
        ],
        "categories": "cs.LG",
        "published": "2022-03-09T14:35:12Z",
        "updated": "2022-03-09T14:35:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.04080v3",
        "title": "On Robust Inference in Time Series Regression",
        "abstract": "Least squares regression with heteroskedasticity consistent standard errors\n(\"OLS-HC regression\") has proved very useful in cross section environments.\nHowever, several major difficulties, which are generally overlooked, must be\nconfronted when transferring the HC technology to time series environments via\nheteroskedasticity and autocorrelation consistent standard errors (\"OLS-HAC\nregression\"). First, in plausible time-series environments, OLS parameter\nestimates can be inconsistent, so that OLS-HAC inference fails even\nasymptotically. Second, most economic time series have autocorrelation, which\nrenders OLS parameter estimates inefficient. Third, autocorrelation similarly\nrenders conditional predictions based on OLS parameter estimates inefficient.\nFinally, the structure of popular HAC covariance matrix estimators is\nill-suited for capturing the autoregressive autocorrelation typically present\nin economic time series, which produces large size distortions and reduced\npower in HAC-based hypothesis testing, in all but the largest samples. We show\nthat all four problems are largely avoided by the use of a simple and\neasily-implemented dynamic regression procedure, which we call DURBIN. We\ndemonstrate the advantages of DURBIN with detailed simulations covering a range\nof practical issues.",
        "authors": [
            "Richard T. Baillie",
            "Francis X. Diebold",
            "George Kapetanios",
            "Kun Ho Kim",
            "Aaron Mora"
        ],
        "categories": "econ.EM",
        "published": "2022-03-08T13:49:10Z",
        "updated": "2024-05-27T20:02:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.04065v2",
        "title": "Honest calibration assessment for binary outcome predictions",
        "abstract": "Probability predictions from binary regressions or machine learning methods\nought to be calibrated: If an event is predicted to occur with probability $x$,\nit should materialize with approximately that frequency, which means that the\nso-called calibration curve $p(\\cdot)$ should equal the identity, $p(x) = x$\nfor all $x$ in the unit interval. We propose honest calibration assessment\nbased on novel confidence bands for the calibration curve, which are valid only\nsubject to the natural assumption of isotonicity. Besides testing the classical\ngoodness-of-fit null hypothesis of perfect calibration, our bands facilitate\ninverted goodness-of-fit tests whose rejection allows for the sought-after\nconclusion of a sufficiently well specified model. We show that our bands have\na finite sample coverage guarantee, are narrower than existing approaches, and\nadapt to the local smoothness of the calibration curve $p$ and the local\nvariance of the binary observations. In an application to model predictions of\nan infant having a low birth weight, the bounds give informative insights on\nmodel calibration.",
        "authors": [
            "Timo Dimitriadis",
            "Lutz Duembgen",
            "Alexander Henzi",
            "Marius Puke",
            "Johanna Ziegel"
        ],
        "categories": "math.ST",
        "published": "2022-03-08T13:18:14Z",
        "updated": "2022-11-02T20:33:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.04040v3",
        "title": "When Will Arctic Sea Ice Disappear? Projections of Area, Extent, Thickness, and Volume",
        "abstract": "Rapidly diminishing Arctic summer sea ice is a strong signal of the pace of\nglobal climate change. We provide point, interval, and density forecasts for\nfour measures of Arctic sea ice: area, extent, thickness, and volume.\nImportantly, we enforce the joint constraint that these measures must\nsimultaneously arrive at an ice-free Arctic. We apply this constrained joint\nforecast procedure to models relating sea ice to atmospheric carbon dioxide\nconcentration and models relating sea ice directly to time. The resulting\n\"carbon-trend\" and \"time-trend\" projections are mutually consistent and predict\na nearly ice-free summer Arctic Ocean by the mid-2030s with an 80% probability.\nMoreover, the carbon-trend projections show that global adoption of a lower\ncarbon path would likely delay the arrival of a seasonally ice-free Arctic by\nonly a few years.",
        "authors": [
            "Francis X. Diebold",
            "Glenn D. Rudebusch",
            "Maximilian Goebel",
            "Philippe Goulet Coulombe",
            "Boyuan Zhang"
        ],
        "categories": "econ.EM",
        "published": "2022-03-08T12:20:57Z",
        "updated": "2023-05-23T17:54:05Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.03613v2",
        "title": "Bayesian Bilinear Neural Network for Predicting the Mid-price Dynamics in Limit-Order Book Markets",
        "abstract": "The prediction of financial markets is a challenging yet important task. In\nmodern electronically-driven markets, traditional time-series econometric\nmethods often appear incapable of capturing the true complexity of the\nmulti-level interactions driving the price dynamics. While recent research has\nestablished the effectiveness of traditional machine learning (ML) models in\nfinancial applications, their intrinsic inability to deal with uncertainties,\nwhich is a great concern in econometrics research and real business\napplications, constitutes a major drawback. Bayesian methods naturally appear\nas a suitable remedy conveying the predictive ability of ML methods with the\nprobabilistically-oriented practice of econometric research. By adopting a\nstate-of-the-art second-order optimization algorithm, we train a Bayesian\nbilinear neural network with temporal attention, suitable for the challenging\ntime-series task of predicting mid-price movements in ultra-high-frequency\nlimit-order book markets. We thoroughly compare our Bayesian model with\ntraditional ML alternatives by addressing the use of predictive distributions\nto analyze errors and uncertainties associated with the estimated parameters\nand model forecasts. Our results underline the feasibility of the Bayesian\ndeep-learning approach and its predictive and decisional advantages in complex\neconometric tasks, prompting future research in this direction.",
        "authors": [
            "Martin Magris",
            "Mostafa Shabani",
            "Alexandros Iosifidis"
        ],
        "categories": "econ.EM",
        "published": "2022-03-07T18:59:54Z",
        "updated": "2023-01-31T11:06:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.03497v5",
        "title": "Inference in Linear Dyadic Data Models with Network Spillovers",
        "abstract": "When using dyadic data (i.e., data indexed by pairs of units), researchers\ntypically assume a linear model, estimate it using Ordinary Least Squares and\nconduct inference using ``dyadic-robust\" variance estimators. The latter\nassumes that dyads are uncorrelated if they do not share a common unit (e.g.,\nif the same individual is not present in both pairs of data). We show that this\nassumption does not hold in many empirical applications because indirect links\nmay exist due to network connections, generating correlated outcomes. Hence,\n``dyadic-robust'' estimators can be biased in such situations. We develop a\nconsistent variance estimator for such contexts by leveraging results in\nnetwork statistics. Our estimator has good finite sample properties in\nsimulations, while allowing for decay in spillover effects. We illustrate our\nmessage with an application to politicians' voting behavior when they are\nseating neighbors in the European Parliament.",
        "authors": [
            "Nathan Canen",
            "Ko Sugiura"
        ],
        "categories": "econ.EM",
        "published": "2022-03-07T16:18:15Z",
        "updated": "2023-06-28T14:43:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.03342v2",
        "title": "High-Resolution Peak Demand Estimation Using Generalized Additive Models and Deep Neural Networks",
        "abstract": "This paper covers predicting high-resolution electricity peak demand features\ngiven lower-resolution data. This is a relevant setup as it answers whether\nlimited higher-resolution monitoring helps to estimate future high-resolution\npeak loads when the high-resolution data is no longer available. That question\nis particularly interesting for network operators considering replacing\nhigh-resolution monitoring predictive models due to economic considerations. We\npropose models to predict half-hourly minima and maxima of high-resolution\n(every minute) electricity load data while model inputs are of a lower\nresolution (30 minutes). We combine predictions of generalized additive models\n(GAM) and deep artificial neural networks (DNN), which are popular in load\nforecasting. We extensively analyze the prediction models, including the input\nparameters' importance, focusing on load, weather, and seasonal effects. The\nproposed method won a data competition organized by Western Power Distribution,\na British distribution network operator. In addition, we provide a rigorous\nevaluation study that goes beyond the competition frame to analyze the models'\nrobustness. The results show that the proposed methods are superior to the\ncompetition benchmark concerning the out-of-sample root mean squared error\n(RMSE). This holds regarding the competition month and the supplementary\nevaluation study, which covers an additional eleven months. Overall, our\nproposed model combination reduces the out-of-sample RMSE by 57.4\\% compared to\nthe benchmark.",
        "authors": [
            "Jonathan Berrisch",
            "Micha\u0142 Narajewski",
            "Florian Ziel"
        ],
        "categories": "cs.LG",
        "published": "2022-03-07T12:39:59Z",
        "updated": "2022-11-02T08:43:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.03051v1",
        "title": "Estimation of a Factor-Augmented Linear Model with Applications Using Student Achievement Data",
        "abstract": "In many longitudinal settings, economic theory does not guide practitioners\non the type of restrictions that must be imposed to solve the rotational\nindeterminacy of factor-augmented linear models. We study this problem and\noffer several novel results on identification using internally generated\ninstruments. We propose a new class of estimators and establish large sample\nresults using recent developments on clustered samples and high-dimensional\nmodels. We carry out simulation studies which show that the proposed approaches\nimprove the performance of existing methods on the estimation of unknown\nfactors. Lastly, we consider three empirical applications using administrative\ndata of students clustered in different subjects in elementary school, high\nschool and college.",
        "authors": [
            "Matthew Harding",
            "Carlos Lamarche",
            "Chris Muris"
        ],
        "categories": "econ.EM",
        "published": "2022-03-06T21:01:21Z",
        "updated": "2022-03-06T21:01:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.03040v3",
        "title": "Modelplasticity and Abductive Decision Making",
        "abstract": "`All models are wrong but some are useful' (George Box 1979). But, how to\nfind those useful ones starting from an imperfect model? How to make informed\ndata-driven decisions equipped with an imperfect model? These fundamental\nquestions appear to be pervasive in virtually all empirical fields -- including\neconomics, finance, marketing, healthcare, climate change, defense planning,\nand operations research. This article presents a modern approach (builds on two\ncore ideas: abductive thinking and density-sharpening principle) and practical\nguidelines to tackle these issues in a systematic manner.",
        "authors": [
            "Subhadeep",
            "Mukhopadhyay"
        ],
        "categories": "econ.EM",
        "published": "2022-03-06T20:05:07Z",
        "updated": "2023-03-07T23:38:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.03032v1",
        "title": "Weighted-average quantile regression",
        "abstract": "In this paper, we introduce the weighted-average quantile regression\nframework, $\\int_0^1 q_{Y|X}(u)\\psi(u)du = X'\\beta$, where $Y$ is a dependent\nvariable, $X$ is a vector of covariates, $q_{Y|X}$ is the quantile function of\nthe conditional distribution of $Y$ given $X$, $\\psi$ is a weighting function,\nand $\\beta$ is a vector of parameters. We argue that this framework is of\ninterest in many applied settings and develop an estimator of the vector of\nparameters $\\beta$. We show that our estimator is $\\sqrt T$-consistent and\nasymptotically normal with mean zero and easily estimable covariance matrix,\nwhere $T$ is the size of available sample. We demonstrate the usefulness of our\nestimator by applying it in two empirical settings. In the first setting, we\nfocus on financial data and study the factor structures of the expected\nshortfalls of the industry portfolios. In the second setting, we focus on wage\ndata and study inequality and social welfare dependence on commonly used\nindividual characteristics.",
        "authors": [
            "Denis Chetverikov",
            "Yukun Liu",
            "Aleh Tsyvinski"
        ],
        "categories": "econ.EM",
        "published": "2022-03-06T19:06:53Z",
        "updated": "2022-03-06T19:06:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.02235v1",
        "title": "Latent Unbalancedness in Three-Way Gravity Models",
        "abstract": "Many panel data sets used for pseudo-poisson estimation of three-way gravity\nmodels are implicitly unbalanced because uninformative observations are\nredundant for the estimation. We show with real data as well as simulations\nthat this phenomenon, which we call latent unbalancedness, amplifies the\ninference problem recently studied by Weidner and Zylkin (2021).",
        "authors": [
            "Daniel Czarnowske",
            "Amrei Stammann"
        ],
        "categories": "econ.EM",
        "published": "2022-03-04T10:47:53Z",
        "updated": "2022-03-04T10:47:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.02220v1",
        "title": "A Classifier-Lasso Approach for Estimating Production Functions with Latent Group Structures",
        "abstract": "I present a new estimation procedure for production functions with latent\ngroup structures. I consider production functions that are heterogeneous across\ngroups but time-homogeneous within groups, and where the group membership of\nthe firms is unknown. My estimation procedure is fully data-driven and embeds\nrecent identification strategies from the production function literature into\nthe classifier-Lasso. Simulation experiments demonstrate that firms are\nassigned to their correct latent group with probability close to one. I apply\nmy estimation procedure to a panel of Chilean firms and find sizable\ndifferences in the estimates compared to the standard approach of\nclassification by industry.",
        "authors": [
            "Daniel Czarnowske"
        ],
        "categories": "econ.EM",
        "published": "2022-03-04T10:01:58Z",
        "updated": "2022-03-04T10:01:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.01425v5",
        "title": "A Modern Gauss-Markov Theorem? Really?",
        "abstract": "We show that the theorems in Hansen (2021a) (the version accepted by\nEconometrica), except for one, are not new as they coincide with classical\ntheorems like the good old Gauss-Markov or Aitken Theorem, respectively; the\nexceptional theorem is incorrect. Hansen (2021b) corrects this theorem. As a\nresult, all theorems in the latter version coincide with the above mentioned\nclassical theorems. Furthermore, we also show that the theorems in Hansen\n(2022) (the version published in Econometrica) either coincide with the\nclassical theorems just mentioned, or contain extra assumptions that are alien\nto the Gauss-Markov or Aitken Theorem.",
        "authors": [
            "Benedikt M. P\u00f6tscher",
            "David Preinerstorfer"
        ],
        "categories": "math.ST",
        "published": "2022-03-02T21:34:12Z",
        "updated": "2023-10-23T13:42:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.00349v1",
        "title": "Minimax Risk in Estimating Kink Threshold and Testing Continuity",
        "abstract": "We derive a risk lower bound in estimating the threshold parameter without\nknowing whether the threshold regression model is continuous or not. The bound\ngoes to zero as the sample size $ n $ grows only at the cube root rate.\nMotivated by this finding, we develop a continuity test for the threshold\nregression model and a bootstrap to compute its \\textit{p}-values. The validity\nof the bootstrap is established, and its finite sample property is explored\nthrough Monte Carlo simulations.",
        "authors": [
            "Javier Hidalgo",
            "Heejun Lee",
            "Jungyoon Lee",
            "Myung Hwan Seo"
        ],
        "categories": "econ.EM",
        "published": "2022-03-01T10:51:25Z",
        "updated": "2022-03-01T10:51:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2203.00097v1",
        "title": "Estimating causal effects with optimization-based methods: A review and empirical comparison",
        "abstract": "In the absence of randomized controlled and natural experiments, it is\nnecessary to balance the distributions of (observable) covariates of the\ntreated and control groups in order to obtain an unbiased estimate of a causal\neffect of interest; otherwise, a different effect size may be estimated, and\nincorrect recommendations may be given. To achieve this balance, there exist a\nwide variety of methods. In particular, several methods based on optimization\nmodels have been recently proposed in the causal inference literature. While\nthese optimization-based methods empirically showed an improvement over a\nlimited number of other causal inference methods in their relative ability to\nbalance the distributions of covariates and to estimate causal effects, they\nhave not been thoroughly compared to each other and to other noteworthy causal\ninference methods. In addition, we believe that there exist several unaddressed\nopportunities that operational researchers could contribute with their advanced\nknowledge of optimization, for the benefits of the applied researchers that use\ncausal inference tools. In this review paper, we present an overview of the\ncausal inference literature and describe in more detail the optimization-based\ncausal inference methods, provide a comparative analysis of the prevailing\noptimization-based methods, and discuss opportunities for new methods.",
        "authors": [
            "Martin Cousineau",
            "Vedat Verter",
            "Susan A. Murphy",
            "Joelle Pineau"
        ],
        "categories": "stat.ME",
        "published": "2022-02-28T21:15:40Z",
        "updated": "2022-02-28T21:15:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.13856v1",
        "title": "Dynamic Spatiotemporal ARCH Models",
        "abstract": "Geo-referenced data are characterized by an inherent spatial dependence due\nto the geographical proximity. In this paper, we introduce a dynamic\nspatiotemporal autoregressive conditional heteroscedasticity (ARCH) process to\ndescribe the effects of (i) the log-squared time-lagged outcome variable, i.e.,\nthe temporal effect, (ii) the spatial lag of the log-squared outcome variable,\ni.e., the spatial effect, and (iii) the spatial lag of the log-squared\ntime-lagged outcome variable, i.e., the spatiotemporal effect, on the\nvolatility of an outcome variable. Furthermore, our suggested process allows\nfor the fixed effects over time and space to account for the unobserved\nheterogeneity. For this dynamic spatiotemporal ARCH model, we derive a\ngeneralized method of moments (GMM) estimator based on the linear and quadratic\nmoment conditions of a specific transformation. We show the consistency and\nasymptotic normality of the GMM estimator, and determine the best set of moment\nfunctions. We investigate the finite-sample properties of the proposed GMM\nestimator in a series of Monte-Carlo simulations with different model\nspecifications and error distributions. Our simulation results show that our\nsuggested GMM estimator has good finite sample properties. In an empirical\napplication, we use monthly log-returns of the average condominium prices of\neach postcode of Berlin from 1995 to 2015 (190 spatial units, 240 time points)\nto demonstrate the use of our suggested model. Our estimation results show that\nthe temporal, spatial and spatiotemporal lags of the log-squared returns have\nstatistically significant effects on the volatility of the log-returns.",
        "authors": [
            "Philipp Otto",
            "Osman Do\u011fan",
            "S\u00fcleyman Ta\u015fp\u0131nar"
        ],
        "categories": "stat.ME",
        "published": "2022-02-28T15:12:55Z",
        "updated": "2022-02-28T15:12:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.13793v1",
        "title": "Forecasting US Inflation Using Bayesian Nonparametric Models",
        "abstract": "The relationship between inflation and predictors such as unemployment is\npotentially nonlinear with a strength that varies over time, and prediction\nerrors error may be subject to large, asymmetric shocks. Inspired by these\nconcerns, we develop a model for inflation forecasting that is nonparametric\nboth in the conditional mean and in the error using Gaussian and Dirichlet\nprocesses, respectively. We discuss how both these features may be important in\nproducing accurate forecasts of inflation. In a forecasting exercise involving\nCPI inflation, we find that our approach has substantial benefits, both overall\nand in the left tail, with nonparametric modeling of the conditional mean being\nof particular importance.",
        "authors": [
            "Todd E. Clark",
            "Florian Huber",
            "Gary Koop",
            "Massimiliano Marcellino"
        ],
        "categories": "econ.EM",
        "published": "2022-02-28T13:39:04Z",
        "updated": "2022-02-28T13:39:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.13545v2",
        "title": "Personalized Subsidy Rules",
        "abstract": "Subsidies are commonly used to encourage behaviors that can lead to short- or\nlong-term benefits. Typical examples include subsidized job training programs\nand provisions of preventive health products, in which both behavioral\nresponses and associated gains can exhibit heterogeneity. This study uses the\nmarginal treatment effect (MTE) framework to study personalized assignments of\nsubsidies based on individual characteristics. First, we derive the optimality\ncondition for a welfare-maximizing subsidy rule by showing that the welfare can\nbe represented as a function of the MTE. Next, we show that subsidies generally\nresult in better welfare than directly mandating the encouraged behavior\nbecause subsidy rules implicitly target individuals through unobserved\nheterogeneity in the behavioral response. When there is positive selection,\nthat is, when individuals with higher returns are more likely to select the\nencouraged behavior, the optimal subsidy rule achieves the first-best welfare,\nwhich is the optimal welfare if a policy-maker can observe individuals' private\ninformation. We then provide methods to (partially) identify the optimal\nsubsidy rule when the MTE is identified and unidentified. Particularly,\npositive selection allows for the point identification of the optimal subsidy\nrule even when the MTE curve is not. As an empirical application, we study the\noptimal wage subsidy using the experimental data from the Jordan New\nOpportunities for Women pilot study.",
        "authors": [
            "Yu-Chang Chen",
            "Haitian Xie"
        ],
        "categories": "econ.EM",
        "published": "2022-02-28T05:01:36Z",
        "updated": "2022-03-17T00:07:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.12644v3",
        "title": "Variational inference for large Bayesian vector autoregressions",
        "abstract": "We propose a novel variational Bayes approach to estimate high-dimensional\nvector autoregression (VAR) models with hierarchical shrinkage priors. Our\napproach does not rely on a conventional structural VAR representation of the\nparameter space for posterior inference. Instead, we elicit hierarchical\nshrinkage priors directly on the matrix of regression coefficients so that (1)\nthe prior structure directly maps into posterior inference on the reduced-form\ntransition matrix, and (2) posterior estimates are more robust to variables\npermutation. An extensive simulation study provides evidence that our approach\ncompares favourably against existing linear and non-linear Markov Chain Monte\nCarlo and variational Bayes methods. We investigate both the statistical and\neconomic value of the forecasts from our variational inference approach within\nthe context of a mean-variance investor allocating her wealth in a large set of\ndifferent industry portfolios. The results show that more accurate estimates\ntranslate into substantial statistical and economic out-of-sample gains. The\nresults hold across different hierarchical shrinkage priors and model\ndimensions.",
        "authors": [
            "Mauro Bernardi",
            "Daniele Bianchi",
            "Nicolas Bianco"
        ],
        "categories": "econ.EM",
        "published": "2022-02-25T12:09:43Z",
        "updated": "2023-06-30T07:52:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.12511v2",
        "title": "A general characterization of optimal tie-breaker designs",
        "abstract": "Tie-breaker designs trade off a statistical design objective with short-term\ngain from preferentially assigning a binary treatment to those with high values\nof a running variable $x$. The design objective is any continuous function of\nthe expected information matrix in a two-line regression model, and short-term\ngain is expressed as the covariance between the running variable and the\ntreatment indicator. We investigate how to specify design functions indicating\ntreatment probabilities as a function of $x$ to optimize these competing\nobjectives, under external constraints on the number of subjects receiving\ntreatment. Our results include sharp existence and uniqueness guarantees, while\naccommodating the ethically appealing requirement that treatment probabilities\nare non-decreasing in $x$. Under such a constraint, there always exists an\noptimal design function that is constant below and above a single\ndiscontinuity. When the running variable distribution is not symmetric or the\nfraction of subjects receiving the treatment is not $1/2$, our optimal designs\nimprove upon a $D$-optimality objective without sacrificing short-term gain,\ncompared to the three level tie-breaker designs of Owen and Varian (2020) that\nfix treatment probabilities at $0$, $1/2$, and $1$. We illustrate our optimal\ndesigns with data from Head Start, an early childhood government intervention\nprogram.",
        "authors": [
            "Harrison H. Li",
            "Art B. Owen"
        ],
        "categories": "stat.ME",
        "published": "2022-02-25T06:13:09Z",
        "updated": "2022-10-19T05:09:46Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.12495v2",
        "title": "Fast variational Bayes methods for multinomial probit models",
        "abstract": "The multinomial probit model is often used to analyze choice behaviour.\nHowever, estimation with existing Markov chain Monte Carlo (MCMC) methods is\ncomputationally costly, which limits its applicability to large choice data\nsets. This paper proposes a variational Bayes method that is accurate and fast,\neven when a large number of choice alternatives and observations are\nconsidered. Variational methods usually require an analytical expression for\nthe unnormalized posterior density and an adequate choice of variational\nfamily. Both are challenging to specify in a multinomial probit, which has a\nposterior that requires identifying restrictions and is augmented with a large\nset of latent utilities. We employ a spherical transformation on the covariance\nmatrix of the latent utilities to construct an unnormalized augmented posterior\nthat identifies the parameters, and use the conditional posterior of the latent\nutilities as part of the variational family. The proposed method is faster than\nMCMC, and can be made scalable to both a large number of choice alternatives\nand a large number of observations. The accuracy and scalability of our method\nis illustrated in numerical experiments and real purchase data with one million\nobservations.",
        "authors": [
            "Rub\u00e9n Loaiza-Maya",
            "Didier Nibbering"
        ],
        "categories": "econ.EM",
        "published": "2022-02-25T04:45:42Z",
        "updated": "2022-10-15T23:14:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.12078v1",
        "title": "Confidence Intervals of Treatment Effects in Panel Data Models with Interactive Fixed Effects",
        "abstract": "We consider the construction of confidence intervals for treatment effects\nestimated using panel models with interactive fixed effects. We first use the\nfactor-based matrix completion technique proposed by Bai and Ng (2021) to\nestimate the treatment effects, and then use bootstrap method to construct\nconfidence intervals of the treatment effects for treated units at each\npost-treatment period. Our construction of confidence intervals requires\nneither specific distributional assumptions on the error terms nor large number\nof post-treatment periods. We also establish the validity of the proposed\nbootstrap procedure that these confidence intervals have asymptotically correct\ncoverage probabilities. Simulation studies show that these confidence intervals\nhave satisfactory finite sample performances, and empirical applications using\nclassical datasets yield treatment effect estimates of similar magnitudes and\nreliable confidence intervals.",
        "authors": [
            "Xingyu Li",
            "Yan Shen",
            "Qiankun Zhou"
        ],
        "categories": "econ.EM",
        "published": "2022-02-24T13:02:59Z",
        "updated": "2022-02-24T13:02:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.12062v4",
        "title": "Semiparametric Estimation of Dynamic Binary Choice Panel Data Models",
        "abstract": "We propose a new approach to the semiparametric analysis of panel data binary\nchoice models with fixed effects and dynamics (lagged dependent variables). The\nmodel we consider has the same random utility framework as in Honore and\nKyriazidou (2000). We demonstrate that, with additional serial dependence\nconditions on the process of deterministic utility and tail restrictions on the\nerror distribution, the (point) identification of the model can proceed in two\nsteps, and only requires matching the value of an index function of explanatory\nvariables over time, as opposed to that of each explanatory variable. Our\nidentification approach motivates an easily implementable, two-step maximum\nscore (2SMS) procedure -- producing estimators whose rates of convergence, in\ncontrast to Honore and Kyriazidou's (2000) methods, are independent of the\nmodel dimension. We then derive the asymptotic properties of the 2SMS procedure\nand propose bootstrap-based distributional approximations for inference. Monte\nCarlo evidence indicates that our procedure performs adequately in finite\nsamples.",
        "authors": [
            "Fu Ouyang",
            "Thomas Tao Yang"
        ],
        "categories": "econ.EM",
        "published": "2022-02-24T12:39:15Z",
        "updated": "2024-02-07T13:12:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.11671v2",
        "title": "Distributional Counterfactual Analysis in High-Dimensional Setup",
        "abstract": "In the context of treatment effect estimation, this paper proposes a new\nmethodology to recover the counterfactual distribution when there is a single\n(or a few) treated unit and possibly a high-dimensional number of potential\ncontrols observed in a panel structure. The methodology accommodates, albeit\ndoes not require, the number of units to be larger than the number of time\nperiods (high-dimensional setup). As opposed to modeling only the conditional\nmean, we propose to model the entire conditional quantile function (CQF)\nwithout intervention and estimate it using the pre-intervention period by a\nl1-penalized regression. We derive non-asymptotic bounds for the estimated CQF\nvalid uniformly over the quantiles. The bounds are explicit in terms of the\nnumber of time periods, the number of control units, the weak dependence\ncoefficient (beta-mixing), and the tail decay of the random variables. The\nresults allow practitioners to re-construct the entire counterfactual\ndistribution. Moreover, we bound the probability coverage of this estimated\nCQF, which can be used to construct valid confidence intervals for the\n(possibly random) treatment effect for every post-intervention period. We also\npropose a new hypothesis test for the sharp null of no-effect based on the Lp\nnorm of deviation of the estimated CQF to the population one. Interestingly,\nthe null distribution is quasi-pivotal in the sense that it only depends on the\nestimated CQF, Lp norm, and the number of post-intervention periods, but not on\nthe size of the post-intervention period. For that reason, critical values can\nthen be easily simulated. We illustrate the methodology by revisiting the\nempirical study in Acemoglu, Johnson, Kermani, Kwak and Mitton (2016).",
        "authors": [
            "Ricardo Masini"
        ],
        "categories": "econ.EM",
        "published": "2022-02-23T18:22:56Z",
        "updated": "2023-09-06T22:37:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.11043v1",
        "title": "Differentially Private Estimation of Heterogeneous Causal Effects",
        "abstract": "Estimating heterogeneous treatment effects in domains such as healthcare or\nsocial science often involves sensitive data where protecting privacy is\nimportant. We introduce a general meta-algorithm for estimating conditional\naverage treatment effects (CATE) with differential privacy (DP) guarantees. Our\nmeta-algorithm can work with simple, single-stage CATE estimators such as\nS-learner and more complex multi-stage estimators such as DR and R-learner. We\nperform a tight privacy analysis by taking advantage of sample splitting in our\nmeta-algorithm and the parallel composition property of differential privacy.\nIn this paper, we implement our approach using DP-EBMs as the base learner.\nDP-EBMs are interpretable, high-accuracy models with privacy guarantees, which\nallow us to directly observe the impact of DP noise on the learned causal\nmodel. Our experiments show that multi-stage CATE estimators incur larger\naccuracy loss than single-stage CATE or ATE estimators and that most of the\naccuracy loss from differential privacy is due to an increase in variance, not\nbiased estimates of treatment effects.",
        "authors": [
            "Fengshi Niu",
            "Harsha Nori",
            "Brian Quistorff",
            "Rich Caruana",
            "Donald Ngwe",
            "Aadharsh Kannan"
        ],
        "categories": "stat.ML",
        "published": "2022-02-22T17:21:18Z",
        "updated": "2022-02-22T17:21:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.10030v5",
        "title": "Multivariate Tie-breaker Designs",
        "abstract": "In a tie-breaker design (TBD), subjects with high values of a running\nvariable are given some (usually desirable) treatment, subjects with low values\nare not, and subjects in the middle are randomized. TBDs are intermediate\nbetween regression discontinuity designs (RDDs) and randomized controlled\ntrials (RCTs). TBDs allow a tradeoff between the resource allocation efficiency\nof an RDD and the statistical efficiency of an RCT. We study a model where the\nexpected response is one multivariate regression for treated subjects and\nanother for control subjects. We propose a prospective D-optimality, analogous\nto Bayesian optimal design, to understand design tradeoffs without reference to\na specific data set. For given covariates, we show how to use convex\noptimization to choose treatment probabilities that optimize this criterion. We\ncan incorporate a variety of constraints motivated by economic and ethical\nconsiderations. In our model, D-optimality for the treatment effect coincides\nwith D-optimality for the whole regression, and, without constraints, an RCT is\nglobally optimal. We show that a monotonicity constraint favoring more\ndeserving subjects induces sparsity in the number of distinct treatment\nprobabilities. We apply the convex optimization solution to a semi-synthetic\nexample involving triage data from the MIMIC-IV-ED database.",
        "authors": [
            "Tim P. Morrison",
            "Art B. Owen"
        ],
        "categories": "stat.ME",
        "published": "2022-02-21T07:53:09Z",
        "updated": "2024-10-12T20:20:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.09854v2",
        "title": "Score Driven Generalized Fitness Model for Sparse and Weighted Temporal Networks",
        "abstract": "While the vast majority of the literature on models for temporal networks\nfocuses on binary graphs, often one can associate a weight to each link. In\nsuch cases the data are better described by a weighted, or valued, network. An\nimportant well known fact is that real world weighted networks are typically\nsparse. We propose a novel time varying parameter model for sparse and weighted\ntemporal networks as a combination of the fitness model, appropriately\nextended, and the score driven framework. We consider a zero augmented\ngeneralized linear model to handle the weights and an observation driven\napproach to describe time varying parameters. The result is a flexible approach\nwhere the probability of a link to exist is independent from its expected\nweight. This represents a crucial difference with alternative specifications\nproposed in the recent literature, with relevant implications for the\nflexibility of the model.\n  Our approach also accommodates for the dependence of the network dynamics on\nexternal variables. We present a link forecasting analysis to data describing\nthe overnight exposures in the Euro interbank market and investigate whether\nthe influence of EONIA rates on the interbank network dynamics has changed over\ntime.",
        "authors": [
            "Domenico Di Gangi",
            "Giacomo Bormetti",
            "Fabrizio Lillo"
        ],
        "categories": "stat.AP",
        "published": "2022-02-20T16:17:52Z",
        "updated": "2022-03-01T09:03:34Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.11031v2",
        "title": "A Unified Nonparametric Test of Transformations on Distribution Functions with Nuisance Parameters",
        "abstract": "This paper proposes a simple unified approach to testing transformations on\ncumulative distribution functions (CDFs) in the presence of nuisance\nparameters. The proposed test is constructed based on a new characterization\nthat avoids the estimation of nuisance parameters. The critical values are\nobtained through a numerical bootstrap method which can easily be implemented\nin practice. Under suitable conditions, the proposed test is shown to be\nasymptotically size controlled and consistent. The local power property of the\ntest is established. Finally, Monte Carlo simulations and an empirical study\nshow that the test performs well on finite samples.",
        "authors": [
            "Xingyu Li",
            "Xiaojun Song",
            "Zhenting Sun"
        ],
        "categories": "stat.ME",
        "published": "2022-02-20T08:08:51Z",
        "updated": "2022-08-16T14:35:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.09473v1",
        "title": "Long Run Risk in Stationary Structural Vector Autoregressive Models",
        "abstract": "This paper introduces a local-to-unity/small sigma process for a stationary\ntime series with strong persistence and non-negligible long run risk. This\nprocess represents the stationary long run component in an unobserved short-\nand long-run components model involving different time scales. More\nspecifically, the short run component evolves in the calendar time and the long\nrun component evolves in an ultra long time scale. We develop the methods of\nestimation and long run prediction for the univariate and multivariate\nStructural VAR (SVAR) models with unobserved components and reveal the\nimpossibility to consistently estimate some of the long run parameters. The\napproach is illustrated by a Monte-Carlo study and an application to\nmacroeconomic data.",
        "authors": [
            "Christian Gourieroux",
            "Joann Jasiak"
        ],
        "categories": "econ.EM",
        "published": "2022-02-18T23:35:28Z",
        "updated": "2022-02-18T23:35:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.09225v1",
        "title": "A multivariate extension of the Misspecification-Resistant Information Criterion",
        "abstract": "The Misspecification-Resistant Information Criterion (MRIC) proposed in\n[H.-L. Hsu, C.-K. Ing, H. Tong: On model selection from a finite family of\npossibly misspecified time series models. The Annals of Statistics. 47 (2),\n1061--1087 (2019)] is a model selection criterion for univariate parametric\ntime series that enjoys both the property of consistency and asymptotic\nefficiency. In this article we extend the MRIC to the case where the response\nis a multivariate time series and the predictor is univariate. The extension\nrequires novel derivations based upon random matrix theory. We obtain an\nasymptotic expression for the mean squared prediction error matrix, the\nvectorial MRIC and prove the consistency of its method-of-moments estimator.\nMoreover, we prove its asymptotic efficiency. Finally, we show with an example\nthat, in presence of misspecification, the vectorial MRIC identifies the best\npredictive model whereas traditional information criteria like AIC or BIC fail\nto achieve the task.",
        "authors": [
            "Gery Andr\u00e9s D\u00edaz Rubio",
            "Simone Giannerini",
            "Greta Goracci"
        ],
        "categories": "math.ST",
        "published": "2022-02-18T14:45:58Z",
        "updated": "2022-02-18T14:45:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.09391v1",
        "title": "Counterfactual Analysis of the Impact of the IMF Program on Child Poverty in the Global-South Region using Causal-Graphical Normalizing Flows",
        "abstract": "This work demonstrates the application of a particular branch of causal\ninference and deep learning models: \\emph{causal-Graphical Normalizing Flows\n(c-GNFs)}. In a recent contribution, scholars showed that normalizing flows\ncarry certain properties, making them particularly suitable for causal and\ncounterfactual analysis. However, c-GNFs have only been tested in a simulated\ndata setting and no contribution to date have evaluated the application of\nc-GNFs on large-scale real-world data. Focusing on the \\emph{AI for social\ngood}, our study provides a counterfactual analysis of the impact of the\nInternational Monetary Fund (IMF) program on child poverty using c-GNFs. The\nanalysis relies on a large-scale real-world observational data: 1,941,734\nchildren under the age of 18, cared for by 567,344 families residing in the 67\ncountries from the Global-South. While the primary objective of the IMF is to\nsupport governments in achieving economic stability, our results find that an\nIMF program reduces child poverty as a positive side-effect by about\n1.2$\\pm$0.24 degree (`0' equals no poverty and `7' is maximum poverty). Thus,\nour article shows how c-GNFs further the use of deep learning and causal\ninference in AI for social good. It shows how learning algorithms can be used\nfor addressing the untapped potential for a significant social impact through\ncounterfactual inference at population level (ACE), sub-population level\n(CACE), and individual level (ICE). In contrast to most works that model ACE or\nCACE but not ICE, c-GNFs enable personalization using \\emph{`The First Law of\nCausal Inference'}.",
        "authors": [
            "Sourabh Balgi",
            "Jose M. Pe\u00f1a",
            "Adel Daoud"
        ],
        "categories": "cs.AI",
        "published": "2022-02-17T12:18:14Z",
        "updated": "2022-02-17T12:18:14Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.08426v2",
        "title": "Synthetic Control As Online Linear Regression",
        "abstract": "This paper notes a simple connection between synthetic control and online\nlearning. Specifically, we recognize synthetic control as an instance of\nFollow-The-Leader (FTL). Standard results in online convex optimization then\nimply that, even when outcomes are chosen by an adversary, synthetic control\npredictions of counterfactual outcomes for the treated unit perform almost as\nwell as an oracle weighted average of control units' outcomes. Synthetic\ncontrol on differenced data performs almost as well as oracle weighted\ndifference-in-differences, potentially making it an attractive choice in\npractice. We argue that this observation further supports the use of synthetic\ncontrol estimators in comparative case studies.",
        "authors": [
            "Jiafeng Chen"
        ],
        "categories": "econ.EM",
        "published": "2022-02-17T03:10:12Z",
        "updated": "2022-11-13T20:44:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.08370v4",
        "title": "CAREER: A Foundation Model for Labor Sequence Data",
        "abstract": "Labor economists regularly analyze employment data by fitting predictive\nmodels to small, carefully constructed longitudinal survey datasets. Although\nmachine learning methods offer promise for such problems, these survey datasets\nare too small to take advantage of them. In recent years large datasets of\nonline resumes have also become available, providing data about the career\ntrajectories of millions of individuals. However, standard econometric models\ncannot take advantage of their scale or incorporate them into the analysis of\nsurvey data. To this end we develop CAREER, a foundation model for job\nsequences. CAREER is first fit to large, passively-collected resume data and\nthen fine-tuned to smaller, better-curated datasets for economic inferences. We\nfit CAREER to a dataset of 24 million job sequences from resumes, and adjust it\non small longitudinal survey datasets. We find that CAREER forms accurate\npredictions of job sequences, outperforming econometric baselines on three\nwidely-used economics datasets. We further find that CAREER can be used to form\ngood predictions of other downstream variables. For example, incorporating\nCAREER into a wage model provides better predictions than the econometric\nmodels currently in use.",
        "authors": [
            "Keyon Vafa",
            "Emil Palikot",
            "Tianyu Du",
            "Ayush Kanodia",
            "Susan Athey",
            "David M. Blei"
        ],
        "categories": "cs.LG",
        "published": "2022-02-16T23:23:50Z",
        "updated": "2024-02-29T16:58:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.08977v1",
        "title": "Fairness constraint in Structural Econometrics and Application to fair estimation using Instrumental Variables",
        "abstract": "A supervised machine learning algorithm determines a model from a learning\nsample that will be used to predict new observations. To this end, it\naggregates individual characteristics of the observations of the learning\nsample. But this information aggregation does not consider any potential\nselection on unobservables and any status-quo biases which may be contained in\nthe training sample. The latter bias has raised concerns around the so-called\n\\textit{fairness} of machine learning algorithms, especially towards\ndisadvantaged groups. In this chapter, we review the issue of fairness in\nmachine learning through the lenses of structural econometrics models in which\nthe unknown index is the solution of a functional equation and issues of\nendogeneity are explicitly accounted for. We model fairness as a linear\noperator whose null space contains the set of strictly {\\it fair} indexes. A\n{\\it fair} solution is obtained by projecting the unconstrained index into the\nnull space of this operator or by directly finding the closest solution of the\nfunctional equation into this null space. We also acknowledge that policymakers\nmay incur a cost when moving away from the status quo. Achieving\n\\textit{approximate fairness} is obtained by introducing a fairness penalty in\nthe learning procedure and balancing more or less heavily the influence between\nthe status quo and a full fair solution.",
        "authors": [
            "Samuele Centorrino",
            "Jean-Pierre Florens",
            "Jean-Michel Loubes"
        ],
        "categories": "econ.EM",
        "published": "2022-02-16T15:34:07Z",
        "updated": "2022-02-16T15:34:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.07517v2",
        "title": "An Equilibrium Model of the First-Price Auction with Strategic Uncertainty: Theory and Empirics",
        "abstract": "In many first-price auctions, bidders face considerable strategic\nuncertainty: They cannot perfectly anticipate the other bidders' bidding\nbehavior. We propose a model in which bidders do not know the entire\ndistribution of opponent bids but only the expected (winning) bid and lower and\nupper bounds on the opponent bids. We characterize the optimal bidding\nstrategies and prove the existence of equilibrium beliefs. Finally, we apply\nthe model to estimate the cost distribution in highway procurement auctions and\nfind good performance out-of-sample.",
        "authors": [
            "Bernhard Kasberger"
        ],
        "categories": "econ.TH",
        "published": "2022-02-15T15:45:37Z",
        "updated": "2022-03-28T19:04:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.07234v5",
        "title": "Long-term Causal Inference Under Persistent Confounding via Data Combination",
        "abstract": "We study the identification and estimation of long-term treatment effects\nwhen both experimental and observational data are available. Since the\nlong-term outcome is observed only after a long delay, it is not measured in\nthe experimental data, but only recorded in the observational data. However,\nboth types of data include observations of some short-term outcomes. In this\npaper, we uniquely tackle the challenge of persistent unmeasured confounders,\ni.e., some unmeasured confounders that can simultaneously affect the treatment,\nshort-term outcomes and the long-term outcome, noting that they invalidate\nidentification strategies in previous literature. To address this challenge, we\nexploit the sequential structure of multiple short-term outcomes, and develop\nthree novel identification strategies for the average long-term treatment\neffect. We further propose three corresponding estimators and prove their\nasymptotic consistency and asymptotic normality. We finally apply our methods\nto estimate the effect of a job training program on long-term employment using\nsemi-synthetic data. We numerically show that our proposals outperform existing\nmethods that fail to handle persistent confounders.",
        "authors": [
            "Guido Imbens",
            "Nathan Kallus",
            "Xiaojie Mao",
            "Yuhao Wang"
        ],
        "categories": "stat.ME",
        "published": "2022-02-15T07:44:20Z",
        "updated": "2024-08-31T02:01:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.07150v4",
        "title": "Asymptotics of Cointegration Tests for High-Dimensional VAR($k$)",
        "abstract": "The paper studies nonstationary high-dimensional vector autoregressions of\norder $k$, VAR($k$). Additional deterministic terms such as trend or\nseasonality are allowed. The number of time periods, $T$, and the number of\ncoordinates, $N$, are assumed to be large and of the same order. Under this\nregime the first-order asymptotics of the Johansen likelihood ratio (LR),\nPillai-Bartlett, and Hotelling-Lawley tests for cointegration are derived: the\ntest statistics converge to nonrandom integrals. For more refined analysis, the\npaper proposes and analyzes a modification of the Johansen test. The new test\nfor the absence of cointegration converges to the partial sum of the Airy$_1$\npoint process. Supporting Monte Carlo simulations indicate that the same\nbehavior persists universally in many situations beyond those considered in our\ntheorems.\n  The paper presents empirical implementations of the approach for the analysis\nof S$\\&$P$100$ stocks and of cryptocurrencies. The latter example has a strong\npresence of multiple cointegrating relationships, while the results for the\nformer are consistent with the null of no cointegration.",
        "authors": [
            "Anna Bykhovskaya",
            "Vadim Gorin"
        ],
        "categories": "econ.EM",
        "published": "2022-02-15T02:53:51Z",
        "updated": "2023-11-27T19:19:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.07070v1",
        "title": "Sequential Monte Carlo With Model Tempering",
        "abstract": "Modern macroeconometrics often relies on time series models for which it is\ntime-consuming to evaluate the likelihood function. We demonstrate how Bayesian\ncomputations for such models can be drastically accelerated by reweighting and\nmutating posterior draws from an approximating model that allows for fast\nlikelihood evaluations, into posterior draws from the model of interest, using\na sequential Monte Carlo (SMC) algorithm. We apply the technique to the\nestimation of a vector autoregression with stochastic volatility and a\nnonlinear dynamic stochastic general equilibrium model. The runtime reductions\nwe obtain range from 27% to 88%.",
        "authors": [
            "Marko Mlikota",
            "Frank Schorfheide"
        ],
        "categories": "econ.EM",
        "published": "2022-02-14T22:28:51Z",
        "updated": "2022-02-14T22:28:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.05984v3",
        "title": "scpi: Uncertainty Quantification for Synthetic Control Methods",
        "abstract": "The synthetic control method offers a way to quantify the effect of an\nintervention using weighted averages of untreated units to approximate the\ncounterfactual outcome that the treated unit(s) would have experienced in the\nabsence of the intervention. This method is useful for program evaluation and\ncausal inference in observational studies. We introduce the software package\nscpi for prediction and inference using synthetic controls, implemented in\nPython, R, and Stata. For point estimation or prediction of treatment effects,\nthe package offers an array of (possibly penalized) approaches leveraging the\nlatest optimization methods. For uncertainty quantification, the package offers\nthe prediction interval methods introduced by Cattaneo, Feng and Titiunik\n(2021) and Cattaneo, Feng, Palomba and Titiunik (2022). The paper includes\nnumerical illustrations and a comparison with other synthetic control software.",
        "authors": [
            "Matias D. Cattaneo",
            "Yingjie Feng",
            "Filippo Palomba",
            "Rocio Titiunik"
        ],
        "categories": "stat.ME",
        "published": "2022-02-12T04:54:39Z",
        "updated": "2022-10-11T12:14:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.05245v2",
        "title": "Benign-Overfitting in Conditional Average Treatment Effect Prediction with Linear Regression",
        "abstract": "We study the benign overfitting theory in the prediction of the conditional\naverage treatment effect (CATE), with linear regression models. As the\ndevelopment of machine learning for causal inference, a wide range of\nlarge-scale models for causality are gaining attention. One problem is that\nsuspicions have been raised that the large-scale models are prone to\noverfitting to observations with sample selection, hence the large models may\nnot be suitable for causal prediction. In this study, to resolve the\nsuspicious, we investigate on the validity of causal inference methods for\noverparameterized models, by applying the recent theory of benign overfitting\n(Bartlett et al., 2020). Specifically, we consider samples whose distribution\nswitches depending on an assignment rule, and study the prediction of CATE with\nlinear models whose dimension diverges to infinity. We focus on two methods:\nthe T-learner, which based on a difference between separately constructed\nestimators with each treatment group, and the inverse probability weight\n(IPW)-learner, which solves another regression problem approximated by a\npropensity score. In both methods, the estimator consists of interpolators that\nfit the samples perfectly. As a result, we show that the T-learner fails to\nachieve the consistency except the random assignment, while the IPW-learner\nconverges the risk to zero if the propensity score is known. This difference\nstems from that the T-learner is unable to preserve eigenspaces of the\ncovariances, which is necessary for benign overfitting in the overparameterized\nsetting. Our result provides new insights into the usage of causal inference\nmethods in the overparameterizated setting, in particular, doubly robust\nestimators.",
        "authors": [
            "Masahiro Kato",
            "Masaaki Imaizumi"
        ],
        "categories": "econ.EM",
        "published": "2022-02-10T18:51:52Z",
        "updated": "2022-02-11T23:37:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.05192v2",
        "title": "von Mises-Fisher distributions and their statistical divergence",
        "abstract": "The von Mises-Fisher family is a parametric family of distributions on the\nsurface of the unit ball, summarised by a concentration parameter and a mean\ndirection. As a quasi-Bayesian prior, the von Mises-Fisher distribution is a\nconvenient and parsimonious choice when parameter spaces are isomorphic to the\nhypersphere (e.g., maximum score estimation in semi-parametric discrete choice,\nestimation of single-index treatment assignment rules via empirical welfare\nmaximisation, under-identifying linear simultaneous equation models). Despite a\nlong history of application, measures of statistical divergence have not been\nanalytically characterised for von Mises-Fisher distributions. This paper\nprovides analytical expressions for the $f$-divergence of a von Mises-Fisher\ndistribution from another, distinct, von Mises-Fisher distribution in\n$\\mathbb{R}^p$ and the uniform distribution over the hypersphere. This paper\nalso collect several other results pertaining to the von Mises-Fisher family of\ndistributions, and characterises the limiting behaviour of the measures of\ndivergence that we consider.",
        "authors": [
            "Toru Kitagawa",
            "Jeff Rowley"
        ],
        "categories": "econ.EM",
        "published": "2022-02-10T17:50:21Z",
        "updated": "2022-11-19T13:35:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.04796v4",
        "title": "The Transfer Performance of Economic Models",
        "abstract": "Economists often estimate models using data from a particular domain, e.g.\nestimating risk preferences in a particular subject pool or for a specific\nclass of lotteries. Whether a model's predictions extrapolate well across\ndomains depends on whether the estimated model has captured generalizable\nstructure. We provide a tractable formulation for this \"out-of-domain\"\nprediction problem and define the transfer error of a model based on how well\nit performs on data from a new domain. We derive finite-sample forecast\nintervals that are guaranteed to cover realized transfer errors with a\nuser-selected probability when domains are iid, and use these intervals to\ncompare the transferability of economic models and black box algorithms for\npredicting certainty equivalents. We find that in this application, the black\nbox algorithms we consider outperform standard economic models when estimated\nand tested on data from the same domain, but the economic models generalize\nacross domains better than the black-box algorithms do.",
        "authors": [
            "Isaiah Andrews",
            "Drew Fudenberg",
            "Lihua Lei",
            "Annie Liang",
            "Chaofeng Wu"
        ],
        "categories": "econ.TH",
        "published": "2022-02-10T02:13:50Z",
        "updated": "2024-07-27T14:34:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.04339v3",
        "title": "Semiparametric Bayesian Estimation of Dynamic Discrete Choice Models",
        "abstract": "We propose a tractable semiparametric estimation method for structural\ndynamic discrete choice models. The distribution of additive utility shocks in\nthe proposed framework is modeled by location-scale mixtures of extreme value\ndistributions with varying numbers of mixture components. Our approach exploits\nthe analytical tractability of extreme value distributions in the multinomial\nchoice settings and the flexibility of the location-scale mixtures. We\nimplement the Bayesian approach to inference using Hamiltonian Monte Carlo and\nan approximately optimal reversible jump algorithm. In our simulation\nexperiments, we show that the standard dynamic logit model can deliver\nmisleading results, especially about counterfactuals, when the shocks are not\nextreme value distributed. Our semiparametric approach delivers reliable\ninference in these settings. We develop theoretical results on approximations\nby location-scale mixtures in an appropriate distance and posterior\nconcentration of the set identified utility parameters and the distribution of\nshocks in the model.",
        "authors": [
            "Andriy Norets",
            "Kenichi Shimizu"
        ],
        "categories": "econ.EM",
        "published": "2022-02-09T08:51:37Z",
        "updated": "2023-08-03T04:45:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.04245v2",
        "title": "Regulatory Instruments for Fair Personalized Pricing",
        "abstract": "Personalized pricing is a business strategy to charge different prices to\nindividual consumers based on their characteristics and behaviors. It has\nbecome common practice in many industries nowadays due to the availability of a\ngrowing amount of high granular consumer data. The discriminatory nature of\npersonalized pricing has triggered heated debates among policymakers and\nacademics on how to design regulation policies to balance market efficiency and\nequity. In this paper, we propose two sound policy instruments, i.e., capping\nthe range of the personalized prices or their ratios. We investigate the\noptimal pricing strategy of a profit-maximizing monopoly under both regulatory\nconstraints and the impact of imposing them on consumer surplus, producer\nsurplus, and social welfare. We theoretically prove that both proposed\nconstraints can help balance consumer surplus and producer surplus at the\nexpense of total surplus for common demand distributions, such as uniform,\nlogistic, and exponential distributions. Experiments on both simulation and\nreal-world datasets demonstrate the correctness of these theoretical results.\nOur findings and insights shed light on regulatory policy design for the\nincreasingly monopolized business in the digital era.",
        "authors": [
            "Renzhe Xu",
            "Xingxuan Zhang",
            "Peng Cui",
            "Bo Li",
            "Zheyan Shen",
            "Jiazheng Xu"
        ],
        "categories": "cs.CY",
        "published": "2022-02-09T03:07:08Z",
        "updated": "2022-02-19T13:07:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.04218v1",
        "title": "Managers versus Machines: Do Algorithms Replicate Human Intuition in Credit Ratings?",
        "abstract": "We use machine learning techniques to investigate whether it is possible to\nreplicate the behavior of bank managers who assess the risk of commercial loans\nmade by a large commercial US bank. Even though a typical bank already relies\non an algorithmic scorecard process to evaluate risk, bank managers are given\nsignificant latitude in adjusting the risk score in order to account for other\nholistic factors based on their intuition and experience. We show that it is\npossible to find machine learning algorithms that can replicate the behavior of\nthe bank managers. The input to the algorithms consists of a combination of\nstandard financials and soft information available to bank managers as part of\nthe typical loan review process. We also document the presence of significant\nheterogeneity in the adjustment process that can be traced to differences\nacross managers and industries. Our results highlight the effectiveness of\nmachine learning based analytic approaches to banking and the potential\nchallenges to high-skill jobs in the financial sector.",
        "authors": [
            "Matthew Harding",
            "Gabriel F. R. Vasconcelos"
        ],
        "categories": "econ.EM",
        "published": "2022-02-09T01:20:44Z",
        "updated": "2022-02-09T01:20:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.04208v5",
        "title": "Validating Causal Inference Methods",
        "abstract": "The fundamental challenge of drawing causal inference is that counterfactual\noutcomes are not fully observed for any unit. Furthermore, in observational\nstudies, treatment assignment is likely to be confounded. Many statistical\nmethods have emerged for causal inference under unconfoundedness conditions\ngiven pre-treatment covariates, including propensity score-based methods,\nprognostic score-based methods, and doubly robust methods. Unfortunately for\napplied researchers, there is no `one-size-fits-all' causal method that can\nperform optimally universally. In practice, causal methods are primarily\nevaluated quantitatively on handcrafted simulated data. Such data-generative\nprocedures can be of limited value because they are typically stylized models\nof reality. They are simplified for tractability and lack the complexities of\nreal-world data. For applied researchers, it is critical to understand how well\na method performs for the data at hand. Our work introduces a deep generative\nmodel-based framework, Credence, to validate causal inference methods. The\nframework's novelty stems from its ability to generate synthetic data anchored\nat the empirical distribution for the observed sample, and therefore virtually\nindistinguishable from the latter. The approach allows the user to specify\nground truth for the form and magnitude of causal effects and confounding bias\nas functions of covariates. Thus simulated data sets are used to evaluate the\npotential performance of various causal estimation methods when applied to data\nsimilar to the observed sample. We demonstrate Credence's ability to accurately\nassess the relative performance of causal estimation techniques in an extensive\nsimulation study and two real-world data applications from Lalonde and Project\nSTAR studies.",
        "authors": [
            "Harsh Parikh",
            "Carlos Varjao",
            "Louise Xu",
            "Eric Tchetgen Tchetgen"
        ],
        "categories": "stat.ME",
        "published": "2022-02-09T00:21:22Z",
        "updated": "2022-07-29T17:26:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.04154v3",
        "title": "Dynamic Heterogeneous Distribution Regression Panel Models, with an Application to Labor Income Processes",
        "abstract": "We consider the estimation of a dynamic distribution regression panel data\nmodel with heterogeneous coefficients across units. The objects of primary\ninterest are specific functionals of these coefficients. These include\npredicted actual and stationary distributions of the outcome variable and\nquantile treatment effects. Coefficients and their functionals are estimated\nvia fixed effect methods. We investigate how these functionals vary in response\nto changes in initial conditions or covariate values. We also identify a\nuniformity issue related to the robustness of inference to the unknown degree\nof heterogeneity, and propose a cross-sectional bootstrap method for uniformly\nvalid inference on function-valued objects. Employing PSID annual labor income\ndata we illustrate some important empirical issues we can address. We first\nquantify the impact of a negative labor income shock on the distribution of\nfuture labor income. We also examine the impact on the distribution of labor\nincome from increasing the education level of a chosen group of workers.\nFinally, we demonstrate the existence of heterogeneity in income mobility, and\nhow this leads to substantial variation in individuals' incidences to be\ntrapped in poverty. We also provide simulation evidence confirming that our\nprocedures work well.",
        "authors": [
            "Ivan Fernandez-Val",
            "Wayne Yuan Gao",
            "Yuan Liao",
            "Francis Vella"
        ],
        "categories": "econ.EM",
        "published": "2022-02-08T21:30:54Z",
        "updated": "2023-01-15T02:04:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.04146v2",
        "title": "A Neural Phillips Curve and a Deep Output Gap",
        "abstract": "Many problems plague empirical Phillips curves (PCs). Among them is the\nhurdle that the two key components, inflation expectations and the output gap,\nare both unobserved. Traditional remedies include proxying for the absentees or\nextracting them via assumptions-heavy filtering procedures. I propose an\nalternative route: a Hemisphere Neural Network (HNN) whose architecture yields\na final layer where components can be interpreted as latent states within a\nNeural PC. There are benefits. First, HNN conducts the supervised estimation of\nnonlinearities that arise when translating a high-dimensional set of observed\nregressors into latent states. Second, forecasts are economically\ninterpretable. Among other findings, the contribution of real activity to\ninflation appears understated in traditional PCs. In contrast, HNN captures the\n2021 upswing in inflation and attributes it to a large positive output gap\nstarting from late 2020. The unique path of HNN's gap comes from dispensing\nwith unemployment and GDP in favor of an amalgam of nonlinearly processed\nalternative tightness indicators.",
        "authors": [
            "Philippe Goulet Coulombe"
        ],
        "categories": "econ.EM",
        "published": "2022-02-08T21:10:09Z",
        "updated": "2024-10-24T17:20:20Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.03960v3",
        "title": "Continuous permanent unobserved heterogeneity in dynamic discrete choice models",
        "abstract": "In dynamic discrete choice (DDC) analysis, it is common to use mixture models\nto control for unobserved heterogeneity. However, consistent estimation\ntypically requires both restrictions on the support of unobserved heterogeneity\nand a high-level injectivity condition that is difficult to verify. This paper\nprovides primitive conditions for point identification of a broad class of DDC\nmodels with multivariate continuous permanent unobserved heterogeneity. The\nresults apply to both finite- and infinite-horizon DDC models, do not require a\nfull support assumption, nor a long panel, and place no parametric restriction\non the distribution of unobserved heterogeneity. In addition, I propose a\nseminonparametric estimator that is computationally attractive and can be\nimplemented using familiar parametric methods.",
        "authors": [
            "Jackson Bunting"
        ],
        "categories": "econ.EM",
        "published": "2022-02-08T16:12:19Z",
        "updated": "2024-02-27T23:13:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.03351v2",
        "title": "Threshold Asymmetric Conditional Autoregressive Range (TACARR) Model",
        "abstract": "This paper introduces a Threshold Asymmetric Conditional Autoregressive Range\n(TACARR) formulation for modeling the daily price ranges of financial assets.\nIt is assumed that the process generating the conditional expected ranges at\neach time point switches between two regimes, labeled as upward market and\ndownward market states. The disturbance term of the error process is also\nallowed to switch between two distributions depending on the regime. It is\nassumed that a self-adjusting threshold component that is driven by the past\nvalues of the time series determines the current market regime. The proposed\nmodel is able to capture aspects such as asymmetric and heteroscedastic\nbehavior of volatility in financial markets. The proposed model is an attempt\nat addressing several potential deficits found in existing price range models\nsuch as the Conditional Autoregressive Range (CARR), Asymmetric CARR (ACARR),\nFeedback ACARR (FACARR) and Threshold Autoregressive Range (TARR) models.\nParameters of the model are estimated using the Maximum Likelihood (ML) method.\nA simulation study shows that the ML method performs well in estimating the\nTACARR model parameters. The empirical performance of the TACARR model was\ninvestigated using IBM index data and results show that the proposed model is a\ngood alternative for in-sample prediction and out-of-sample forecasting of\nvolatility.\n  Key Words: Volatility Modeling, Asymmetric Volatility, CARR Models, Regime\nSwitching.",
        "authors": [
            "Isuru Ratnayake",
            "V. A. Samaranayake"
        ],
        "categories": "econ.EM",
        "published": "2022-02-07T16:50:58Z",
        "updated": "2022-03-17T08:58:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.03332v1",
        "title": "Forecasting Environmental Data: An example to ground-level ozone concentration surfaces",
        "abstract": "Environmental problems are receiving increasing attention in socio-economic\nand health studies. This in turn fosters advances in recording and data\ncollection of many related real-life processes. Available tools for data\nprocessing are often found too restrictive as they do not account for the rich\nnature of such data sets. In this paper, we propose a new statistical\nperspective on forecasting spatial environmental data collected sequentially\nover time. We treat this data set as a surface (functional) time series with a\npossibly complicated geographical domain. By employing novel techniques from\nfunctional data analysis we develop a new forecasting methodology. Our approach\nconsists of two steps. In the first step, time series of surfaces are\nreconstructed from measurements sampled over some spatial domain using a finite\nelement spline smoother. In the second step, we adapt the dynamic functional\nfactor model to forecast a surface time series. The advantage of this approach\nis that we can account for and explore simultaneously spatial as well as\ntemporal dependencies in the data. A forecasting study of ground-level ozone\nconcentration over the geographical domain of Germany demonstrates the\npractical value of this new perspective, where we compare our approach with\nstandard functional benchmark models.",
        "authors": [
            "Alexander Gleim",
            "Nazarii Salish"
        ],
        "categories": "stat.ME",
        "published": "2022-02-07T16:22:33Z",
        "updated": "2022-02-07T16:22:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.03110v1",
        "title": "Predicting Default Probabilities for Stress Tests: A Comparison of Models",
        "abstract": "Since the Great Financial Crisis (GFC), the use of stress tests as a tool for\nassessing the resilience of financial institutions to adverse financial and\neconomic developments has increased significantly. One key part in such\nexercises is the translation of macroeconomic variables into default\nprobabilities for credit risk by using macrofinancial linkage models. A key\nrequirement for such models is that they should be able to properly detect\nsignals from a wide array of macroeconomic variables in combination with a\nmostly short data sample. The aim of this paper is to compare a great number of\ndifferent regression models to find the best performing credit risk model. We\nset up an estimation framework that allows us to systematically estimate and\nevaluate a large set of models within the same environment. Our results\nindicate that there are indeed better performing models than the current\nstate-of-the-art model. Moreover, our comparison sheds light on other potential\ncredit risk models, specifically highlighting the advantages of machine\nlearning models and forecast combinations.",
        "authors": [
            "Martin Guth"
        ],
        "categories": "econ.EM",
        "published": "2022-02-07T12:45:03Z",
        "updated": "2022-02-07T12:45:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.02988v1",
        "title": "Detecting Structural Breaks in Foreign Exchange Markets by using the group LASSO technique",
        "abstract": "This article proposes an estimation method to detect breakpoints for linear\ntime series models with their parameters that jump scarcely. Its basic idea\nowes the group LASSO (group least absolute shrinkage and selection operator).\nThe method practically provides estimates of such time-varying parameters of\nthe models. An example shows that our method can detect each structural\nbreakpoint's date and magnitude.",
        "authors": [
            "Mikio Ito"
        ],
        "categories": "econ.EM",
        "published": "2022-02-07T08:04:01Z",
        "updated": "2022-02-07T08:04:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.02903v3",
        "title": "Difference in Differences with Time-Varying Covariates",
        "abstract": "This paper considers identification and estimation of causal effect\nparameters from participating in a binary treatment in a difference in\ndifferences (DID) setup when the parallel trends assumption holds after\nconditioning on observed covariates. Relative to existing work in the\neconometrics literature, we consider the case where the value of covariates can\nchange over time and, potentially, where participating in the treatment can\naffect the covariates themselves. We propose new empirical strategies in both\ncases. We also consider two-way fixed effects (TWFE) regressions that include\ntime-varying regressors, which is the most common way that DID identification\nstrategies are implemented under conditional parallel trends. We show that,\neven in the case with only two time periods, these TWFE regressions are not\ngenerally robust to (i) time-varying covariates being affected by the\ntreatment, (ii) treatment effects and/or paths of untreated potential outcomes\ndepending on the level of time-varying covariates in addition to only the\nchange in the covariates over time, (iii) treatment effects and/or paths of\nuntreated potential outcomes depending on time-invariant covariates, (iv)\ntreatment effect heterogeneity with respect to observed covariates, and (v)\nviolations of strong functional form assumptions, both for outcomes over time\nand the propensity score, that are unlikely to be plausible in most DID\napplications. Thus, TWFE regressions can deliver misleading estimates of causal\neffect parameters in a number of empirically relevant cases. We propose both\ndoubly robust estimands and regression adjustment/imputation strategies that\nare robust to these issues while not being substantially more challenging to\nimplement.",
        "authors": [
            "Carolina Caetano",
            "Brantly Callaway",
            "Stroud Payne",
            "Hugo Sant'Anna Rodrigues"
        ],
        "categories": "econ.EM",
        "published": "2022-02-07T01:50:18Z",
        "updated": "2024-06-24T02:21:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.02532v1",
        "title": "Adaptive information-based methods for determining the co-integration rank in heteroskedastic VAR models",
        "abstract": "Standard methods, such as sequential procedures based on Johansen's\n(pseudo-)likelihood ratio (PLR) test, for determining the co-integration rank\nof a vector autoregressive (VAR) system of variables integrated of order one\ncan be significantly affected, even asymptotically, by unconditional\nheteroskedasticity (non-stationary volatility) in the data. Known solutions to\nthis problem include wild bootstrap implementations of the PLR test or the use\nof an information criterion, such as the BIC, to select the co-integration\nrank. Although asymptotically valid in the presence of heteroskedasticity,\nthese methods can display very low finite sample power under some patterns of\nnon-stationary volatility. In particular, they do not exploit potential\nefficiency gains that could be realised in the presence of non-stationary\nvolatility by using adaptive inference methods. Under the assumption of a known\nautoregressive lag length, Boswijk and Zu (2022) develop adaptive PLR test\nbased methods using a non-parameteric estimate of the covariance matrix\nprocess. It is well-known, however, that selecting an incorrect lag length can\nsignificantly impact on the efficacy of both information criteria and bootstrap\nPLR tests to determine co-integration rank in finite samples. We show that\nadaptive information criteria-based approaches can be used to estimate the\nautoregressive lag order to use in connection with bootstrap adaptive PLR\ntests, or to jointly determine the co-integration rank and the VAR lag length\nand that in both cases they are weakly consistent for these parameters in the\npresence of non-stationary volatility provided standard conditions hold on the\npenalty term. Monte Carlo simulations are used to demonstrate the potential\ngains from using adaptive methods and an empirical application to the U.S. term\nstructure is provided.",
        "authors": [
            "H. Peter Boswijk",
            "Giuseppe Cavaliere",
            "Luca De Angelis",
            "A. M. Robert Taylor"
        ],
        "categories": "econ.EM",
        "published": "2022-02-05T11:00:47Z",
        "updated": "2022-02-05T11:00:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.02029v1",
        "title": "First-order integer-valued autoregressive processes with Generalized Katz innovations",
        "abstract": "A new integer-valued autoregressive process (INAR) with Generalised\nLagrangian Katz (GLK) innovations is defined. We show that our GLK-INAR process\nis stationary, discrete semi-self-decomposable, infinite divisible, and\nprovides a flexible modelling framework for count data allowing for under- and\nover-dispersion, asymmetry, and excess of kurtosis. A Bayesian inference\nframework and an efficient posterior approximation procedure based on Markov\nChain Monte Carlo are provided. The proposed model family is applied to a\nGoogle Trend dataset which proxies the public concern about climate change\naround the world. The empirical results provide new evidence of heterogeneity\nacross countries and keywords in the persistence, uncertainty, and long-run\npublic awareness level.",
        "authors": [
            "Federico Bassetti",
            "Giulia Carallo",
            "Roberto Casarin"
        ],
        "categories": "stat.ME",
        "published": "2022-02-04T09:10:39Z",
        "updated": "2022-02-04T09:10:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.00877v1",
        "title": "Efficient Volatility Estimation for L\u00e9vy Processes with Jumps of Unbounded Variation",
        "abstract": "Statistical inference for stochastic processes based on high-frequency\nobservations has been an active research area for more than a decade. One of\nthe most well-known and widely studied problems is that of estimation of the\nquadratic variation of the continuous component of an It\\^o semimartingale with\njumps. Several rate- and variance-efficient estimators have been proposed in\nthe literature when the jump component is of bounded variation. However, to\ndate, very few methods can deal with jumps of unbounded variation. By\ndeveloping new high-order expansions of the truncated moments of a L\\'evy\nprocess, we construct a new rate- and variance-efficient estimator for a class\nof L\\'evy processes of unbounded variation, whose small jumps behave like those\nof a stable L\\'evy process with Blumenthal-Getoor index less than $8/5$. The\nproposed method is based on a two-step debiasing procedure for the truncated\nrealized quadratic variation of the process. Our Monte Carlo experiments\nindicate that the method outperforms other efficient alternatives in the\nliterature in the setting covered by our theoretical framework.",
        "authors": [
            "B. Cooper Boniece",
            "Jos\u00e9 E. Figueroa-L\u00f3pez",
            "Yuchen Han"
        ],
        "categories": "econ.EM",
        "published": "2022-02-02T05:00:09Z",
        "updated": "2022-02-02T05:00:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.00793v1",
        "title": "Long-Horizon Return Predictability from Realized Volatility in Pure-Jump Point Processes",
        "abstract": "We develop and justify methodology to consistently test for long-horizon\nreturn predictability based on realized variance. To accomplish this, we\npropose a parametric transaction-level model for the continuous-time log price\nprocess based on a pure jump point process. The model determines the returns\nand realized variance at any level of aggregation with properties shown to be\nconsistent with the stylized facts in the empirical finance literature. Under\nour model, the long-memory parameter propagates unchanged from the\ntransaction-level drift to the calendar-time returns and the realized variance,\nleading endogenously to a balanced predictive regression equation. We propose\nan asymptotic framework using power-law aggregation in the predictive\nregression. Within this framework, we propose a hypothesis test for long\nhorizon return predictability which is asymptotically correctly sized and\nconsistent.",
        "authors": [
            "Meng-Chen Hsieh",
            "Clifford Hurvich",
            "Philippe Soulier"
        ],
        "categories": "econ.EM",
        "published": "2022-02-01T22:29:27Z",
        "updated": "2022-02-01T22:29:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.00625v1",
        "title": "Black-box Bayesian inference for economic agent-based models",
        "abstract": "Simulation models, in particular agent-based models, are gaining popularity\nin economics. The considerable flexibility they offer, as well as their\ncapacity to reproduce a variety of empirically observed behaviours of complex\nsystems, give them broad appeal, and the increasing availability of cheap\ncomputing power has made their use feasible. Yet a widespread adoption in\nreal-world modelling and decision-making scenarios has been hindered by the\ndifficulty of performing parameter estimation for such models. In general,\nsimulation models lack a tractable likelihood function, which precludes a\nstraightforward application of standard statistical inference techniques.\nSeveral recent works have sought to address this problem through the\napplication of likelihood-free inference techniques, in which parameter\nestimates are determined by performing some form of comparison between the\nobserved data and simulation output. However, these approaches are (a) founded\non restrictive assumptions, and/or (b) typically require many hundreds of\nthousands of simulations. These qualities make them unsuitable for large-scale\nsimulations in economics and can cast doubt on the validity of these inference\nmethods in such scenarios. In this paper, we investigate the efficacy of two\nclasses of black-box approximate Bayesian inference methods that have recently\ndrawn significant attention within the probabilistic machine learning\ncommunity: neural posterior estimation and neural density ratio estimation. We\npresent benchmarking experiments in which we demonstrate that neural network\nbased black-box methods provide state of the art parameter inference for\neconomic simulation models, and crucially are compatible with generic\nmultivariate time-series data. In addition, we suggest appropriate assessment\ncriteria for future benchmarking of approximate Bayesian inference procedures\nfor economic simulation models.",
        "authors": [
            "Joel Dyer",
            "Patrick Cannon",
            "J. Doyne Farmer",
            "Sebastian Schmon"
        ],
        "categories": "econ.EM",
        "published": "2022-02-01T18:16:12Z",
        "updated": "2022-02-01T18:16:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.00310v2",
        "title": "Estimation of Impulse-Response Functions with Dynamic Factor Models: A New Parametrization",
        "abstract": "We propose a new parametrization for the estimation and identification of the\nimpulse-response functions (IRFs) of dynamic factor models (DFMs). The\ntheoretical contribution of this paper concerns the problem of observational\nequivalence between different IRFs, which implies non-identification of the IRF\nparameters without further restrictions. We show how the previously proposed\nminimal identification conditions are nested in the new framework and can be\nfurther augmented with overidentifying restrictions leading to efficiency\ngains. The current standard practice for the IRF estimation of DFMs is based on\nprincipal components, compared to which the new parametrization is less\nrestrictive and allows for modelling richer dynamics. As the empirical\ncontribution of the paper, we develop an estimation method based on the EM\nalgorithm, which incorporates the proposed identification restrictions. In the\nempirical application, we use a standard high-dimensional macroeconomic dataset\nto estimate the effects of a monetary policy shock. We estimate a strong\nreaction of the macroeconomic variables, while the benchmark models appear to\ngive qualitatively counterintuitive results. The estimation methods are\nimplemented in the accompanying R package.",
        "authors": [
            "Juho Koistinen",
            "Bernd Funovits"
        ],
        "categories": "econ.EM",
        "published": "2022-02-01T10:16:59Z",
        "updated": "2022-02-22T18:45:22Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.00229v1",
        "title": "Protection or Peril of Following the Crowd in a Pandemic-Concurrent Flood Evacuation",
        "abstract": "The decisions of whether and how to evacuate during a climate disaster are\ninfluenced by a wide range of factors, including sociodemographics, emergency\nmessaging, and social influence. Further complexity is introduced when multiple\nhazards occur simultaneously, such as a flood evacuation taking place amid a\nviral pandemic that requires physical distancing. Such multi-hazard events can\nnecessitate a nuanced navigation of competing decision-making strategies\nwherein a desire to follow peers is weighed against contagion risks. To better\nunderstand these nuances, we distributed an online survey during a pandemic\nsurge in July 2020 to 600 individuals in three midwestern and three southern\nstates in the United States with high risk of flooding. In this paper, we\nestimate a random parameter logit model in both preference space and\nwillingness-to-pay space. Our results show that the directionality and\nmagnitude of the influence of peers' choices of whether and how to evacuate\nvary widely across respondents. Overall, the decision of whether to evacuate is\npositively impacted by peer behavior, while the decision of how to evacuate is\nnegatively impacted by peers. Furthermore, an increase in flood threat level\nlessens the magnitude of these impacts. These findings have important\nimplications for the design of tailored emergency messaging strategies.\nSpecifically, emphasizing or deemphasizing the severity of each threat in a\nmulti-hazard scenario may assist in: (1) encouraging a reprioritization of\ncompeting risk perceptions and (2) magnifying or neutralizing the impacts of\nsocial influence, thereby (3) nudging evacuation decision-making toward a\ndesired outcome.",
        "authors": [
            "Elisa Borowski",
            "Amanda Stathopoulos"
        ],
        "categories": "econ.EM",
        "published": "2022-02-01T05:36:13Z",
        "updated": "2022-02-01T05:36:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2202.00141v2",
        "title": "Partial Sum Processes of Residual-Based and Wald-type Break-Point Statistics in Time Series Regression Models",
        "abstract": "We revisit classical asymptotics when testing for a structural break in\nlinear regression models by obtaining the limit theory of residual-based and\nWald-type processes. First, we establish the Brownian bridge limiting\ndistribution of these test statistics. Second, we study the asymptotic\nbehaviour of the partial-sum processes in nonstationary (linear) time series\nregression models. Although, the particular comparisons of these two different\nmodelling environments is done from the perspective of the partial-sum\nprocesses, it emphasizes that the presence of nuisance parameters can change\nthe asymptotic behaviour of the functionals under consideration. Simulation\nexperiments verify size distortions when testing for a break in nonstationary\ntime series regressions which indicates that the Brownian bridge limit cannot\nprovide a suitable asymptotic approximation in this case. Further research is\nrequired to establish the cause of size distortions under the null hypothesis\nof parameter stability.",
        "authors": [
            "Christis Katsouris"
        ],
        "categories": "econ.EM",
        "published": "2022-01-31T23:10:30Z",
        "updated": "2022-02-15T17:45:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.13380v1",
        "title": "Deep Learning Macroeconomics",
        "abstract": "Limited datasets and complex nonlinear relationships are among the challenges\nthat may emerge when applying econometrics to macroeconomic problems. This\nresearch proposes deep learning as an approach to transfer learning in the\nformer case and to map relationships between variables in the latter case.\nAlthough macroeconomists already apply transfer learning when assuming a given\na priori distribution in a Bayesian context, estimating a structural VAR with\nsignal restriction and calibrating parameters based on results observed in\nother models, to name a few examples, advance in a more systematic transfer\nlearning strategy in applied macroeconomics is the innovation we are\nintroducing. We explore the proposed strategy empirically, showing that data\nfrom different but related domains, a type of transfer learning, helps identify\nthe business cycle phases when there is no business cycle dating committee and\nto quick estimate a economic-based output gap. Next, since deep learning\nmethods are a way of learning representations, those that are formed by the\ncomposition of multiple non-linear transformations, to yield more abstract\nrepresentations, we apply deep learning for mapping low-frequency from\nhigh-frequency variables. The results obtained show the suitability of deep\nlearning models applied to macroeconomic problems. First, models learned to\nclassify United States business cycles correctly. Then, applying transfer\nlearning, they were able to identify the business cycles of out-of-sample\nBrazilian and European data. Along the same lines, the models learned to\nestimate the output gap based on the U.S. data and obtained good performance\nwhen faced with Brazilian data. Additionally, deep learning proved adequate for\nmapping low-frequency variables from high-frequency data to interpolate,\ndistribute, and extrapolate time series by related series.",
        "authors": [
            "Rafael R. S. Guimaraes"
        ],
        "categories": "econ.EM",
        "published": "2022-01-31T17:43:43Z",
        "updated": "2022-01-31T17:43:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.13004v5",
        "title": "Improving Estimation Efficiency via Regression-Adjustment in Covariate-Adaptive Randomizations with Imperfect Compliance",
        "abstract": "We investigate how to improve efficiency using regression adjustments with\ncovariates in covariate-adaptive randomizations (CARs) with imperfect subject\ncompliance. Our regression-adjusted estimators, which are based on the doubly\nrobust moment for local average treatment effects, are consistent and\nasymptotically normal even with heterogeneous probability of assignment and\nmisspecified regression adjustments. We propose an optimal but potentially\nmisspecified linear adjustment and its further improvement via a nonlinear\nadjustment, both of which lead to more efficient estimators than the one\nwithout adjustments. We also provide conditions for nonparametric and\nregularized adjustments to achieve the semiparametric efficiency bound under\nCARs.",
        "authors": [
            "Liang Jiang",
            "Oliver B. Linton",
            "Haihan Tang",
            "Yichong Zhang"
        ],
        "categories": "econ.EM",
        "published": "2022-01-31T05:37:28Z",
        "updated": "2023-06-16T13:21:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.13000v1",
        "title": "A General Description of Growth Trends",
        "abstract": "Time series that display periodicity can be described with a Fourier\nexpansion. In a similar vein, a recently developed formalism enables\ndescription of growth patterns with the optimal number of parameters (Elitzur\net al, 2020). The method has been applied to the growth of national GDP,\npopulation and the COVID-19 pandemic; in all cases the deviations of long-term\ngrowth patterns from pure exponential required no more than two additional\nparameters, mostly only one. Here I utilize the new framework to develop a\nunified formulation for all functions that describe growth deceleration,\nwherein the growth rate decreases with time. The result offers the prospects\nfor a new general tool for trend removal in time-series analysis.",
        "authors": [
            "Moshe Elitzur"
        ],
        "categories": "econ.EM",
        "published": "2022-01-31T05:05:08Z",
        "updated": "2022-01-31T05:05:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.12936v6",
        "title": "Pigeonhole Design: Balancing Sequential Experiments from an Online Matching Perspective",
        "abstract": "Practitioners and academics have long appreciated the benefits of covariate\nbalancing when they conduct randomized experiments. For web-facing firms\nrunning online A/B tests, however, it still remains challenging in balancing\ncovariate information when experimental subjects arrive sequentially. In this\npaper, we study an online experimental design problem, which we refer to as the\n\"Online Blocking Problem.\" In this problem, experimental subjects with\nheterogeneous covariate information arrive sequentially and must be immediately\nassigned into either the control or the treated group. The objective is to\nminimize the total discrepancy, which is defined as the minimum weight perfect\nmatching between the two groups. To solve this problem, we propose a randomized\ndesign of experiment, which we refer to as the \"Pigeonhole Design.\" The\npigeonhole design first partitions the covariate space into smaller spaces,\nwhich we refer to as pigeonholes, and then, when the experimental subjects\narrive at each pigeonhole, balances the number of control and treated subjects\nfor each pigeonhole. We analyze the theoretical performance of the pigeonhole\ndesign and show its effectiveness by comparing against two well-known benchmark\ndesigns: the match-pair design and the completely randomized design. We\nidentify scenarios when the pigeonhole design demonstrates more benefits over\nthe benchmark design. To conclude, we conduct extensive simulations using\nYahoo! data to show a 10.2% reduction in variance if we use the pigeonhole\ndesign to estimate the average treatment effect.",
        "authors": [
            "Jinglong Zhao",
            "Zijie Zhou"
        ],
        "categories": "stat.ME",
        "published": "2022-01-30T23:14:24Z",
        "updated": "2024-05-23T20:10:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.12752v1",
        "title": "On the Use of Instrumental Variables in Mediation Analysis",
        "abstract": "Empirical researchers are often interested in not only whether a treatment\naffects an outcome of interest, but also how the treatment effect arises.\nCausal mediation analysis provides a formal framework to identify causal\nmechanisms through which a treatment affects an outcome. The most popular\nidentification strategy relies on so-called sequential ignorability (SI)\nassumption which requires that there is no unobserved confounder that lies in\nthe causal paths between the treatment and the outcome. Despite its popularity,\nsuch assumption is deemed to be too strong in many settings as it excludes the\nexistence of unobserved confounders. This limitation has inspired recent\nliterature to consider an alternative identification strategy based on an\ninstrumental variable (IV). This paper discusses the identification of causal\nmediation effects in a setting with a binary treatment and a binary\ninstrumental variable that is both assumed to be random. We show that while IV\nmethods allow for the possible existence of unobserved confounders, additional\nmonotonicity assumptions are required unless the strong constant effect is\nassumed. Furthermore, even when such monotonicity assumptions are satisfied, IV\nestimands are not necessarily equivalent to target parameters.",
        "authors": [
            "Bora Kim"
        ],
        "categories": "econ.EM",
        "published": "2022-01-30T08:45:42Z",
        "updated": "2022-01-30T08:45:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.12696v1",
        "title": "Sharing Behavior in Ride-hailing Trips: A Machine Learning Inference Approach",
        "abstract": "Ride-hailing is rapidly changing urban and personal transportation. Ride\nsharing or pooling is important to mitigate negative externalities of\nride-hailing such as increased congestion and environmental impacts. However,\nthere lacks empirical evidence on what affect trip-level sharing behavior in\nride-hailing. Using a novel dataset from all ride-hailing trips in Chicago in\n2019, we show that the willingness of riders to request a shared ride has\nmonotonically decreased from 27.0% to 12.8% throughout the year, while the trip\nvolume and mileage have remained statistically unchanged. We find that the\ndecline in sharing preference is due to an increased per-mile costs of shared\ntrips and shifting shorter trips to solo. Using ensemble machine learning\nmodels, we find that the travel impedance variables (trip cost, distance, and\nduration) collectively contribute to 95% and 91% of the predictive power in\ndetermining whether a trip is requested to share and whether it is successfully\nshared, respectively. Spatial and temporal attributes, sociodemographic, built\nenvironment, and transit supply variables do not entail predictive power at the\ntrip level in presence of these travel impedance variables. This implies that\npricing signals are most effective to encourage riders to share their rides.\nOur findings shed light on sharing behavior in ride-hailing trips and can help\ndevise strategies that increase shared ride-hailing, especially as the demand\nrecovers from pandemic.",
        "authors": [
            "Morteza Taiebat",
            "Elham Amini",
            "Ming Xu"
        ],
        "categories": "cs.LG",
        "published": "2022-01-30T01:17:36Z",
        "updated": "2022-01-30T01:17:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.12692v1",
        "title": "Meta-Learners for Estimation of Causal Effects: Finite Sample Cross-Fit Performance",
        "abstract": "Estimation of causal effects using machine learning methods has become an\nactive research field in econometrics. In this paper, we study the finite\nsample performance of meta-learners for estimation of heterogeneous treatment\neffects under the usage of sample-splitting and cross-fitting to reduce the\noverfitting bias. In both synthetic and semi-synthetic simulations we find that\nthe performance of the meta-learners in finite samples greatly depends on the\nestimation procedure. The results imply that sample-splitting and cross-fitting\nare beneficial in large samples for bias reduction and efficiency of the\nmeta-learners, respectively, whereas full-sample estimation is preferable in\nsmall samples. Furthermore, we derive practical recommendations for application\nof specific meta-learners in empirical studies depending on particular data\ncharacteristics such as treatment shares and sample size.",
        "authors": [
            "Gabriel Okasa"
        ],
        "categories": "econ.EM",
        "published": "2022-01-30T00:52:33Z",
        "updated": "2022-01-30T00:52:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.11482v2",
        "title": "A semiparametric approach for interactive fixed effects panel data models",
        "abstract": "This paper presents a new approach for the estimation and inference of the\nregression parameters in a panel data model with interactive fixed effects. It\nrelies on the assumption that the factor loadings can be expressed as an\nunknown smooth function of the time average of covariates plus an idiosyncratic\nerror term. Compared to existing approaches, our estimator has a simple partial\nleast squares form and does neither require iterative procedures nor the\nprevious estimation of factors.\n  We derive its asymptotic properties by finding out that the limiting\ndistribution has a discontinuity, depending on the explanatory power of our\nbasis functions which is expressed by the variance of the error of the factor\nloadings. As a result, the usual ``plug-in\" methods based on estimates of the\nasymptotic covariance are only valid pointwise and may produce either over- or\nunder-coverage probabilities. We show that uniformly valid inference can be\nachieved by using the cross-sectional bootstrap. A Monte Carlo study indicates\ngood performance in terms of mean squared error. We apply our methodology to\nanalyze the determinants of growth rates in OECD countries.",
        "authors": [
            "Georg Keilbar",
            "Juan M. Rodriguez-Poo",
            "Alexandra Soberon",
            "Weining Wang"
        ],
        "categories": "econ.EM",
        "published": "2022-01-27T12:37:23Z",
        "updated": "2023-03-12T11:06:33Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.11341v2",
        "title": "Towards Agnostic Feature-based Dynamic Pricing: Linear Policies vs Linear Valuation with Unknown Noise",
        "abstract": "In feature-based dynamic pricing, a seller sets appropriate prices for a\nsequence of products (described by feature vectors) on the fly by learning from\nthe binary outcomes of previous sales sessions (\"Sold\" if valuation $\\geq$\nprice, and \"Not Sold\" otherwise). Existing works either assume noiseless linear\nvaluation or precisely-known noise distribution, which limits the applicability\nof those algorithms in practice when these assumptions are hard to verify. In\nthis work, we study two more agnostic models: (a) a \"linear policy\" problem\nwhere we aim at competing with the best linear pricing policy while making no\nassumptions on the data, and (b) a \"linear noisy valuation\" problem where the\nrandom valuation is linear plus an unknown and assumption-free noise. For the\nformer model, we show a $\\tilde{\\Theta}(d^{\\frac13}T^{\\frac23})$ minimax regret\nup to logarithmic factors. For the latter model, we present an algorithm that\nachieves an $\\tilde{O}(T^{\\frac34})$ regret, and improve the best-known lower\nbound from $\\Omega(T^{\\frac35})$ to $\\tilde{\\Omega}(T^{\\frac23})$. These\nresults demonstrate that no-regret learning is possible for feature-based\ndynamic pricing under weak assumptions, but also reveal a disappointing fact\nthat the seemingly richer pricing feedback is not significantly more useful\nthan the bandit-feedback in regret reduction.",
        "authors": [
            "Jianyu Xu",
            "Yu-Xiang Wang"
        ],
        "categories": "cs.LG",
        "published": "2022-01-27T06:40:03Z",
        "updated": "2022-04-01T06:00:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.11304v4",
        "title": "Standard errors for two-way clustering with serially correlated time effects",
        "abstract": "We propose improved standard errors and an asymptotic distribution theory for\ntwo-way clustered panels. Our proposed estimator and theory allow for arbitrary\nserial dependence in the common time effects, which is excluded by existing\ntwo-way methods, including the popular two-way cluster standard errors of\nCameron, Gelbach, and Miller (2011) and the cluster bootstrap of Menzel (2021).\nOur asymptotic distribution theory is the first which allows for this level of\ninter-dependence among the observations. Under weak regularity conditions, we\ndemonstrate that the least squares estimator is asymptotically normal, our\nproposed variance estimator is consistent, and t-ratios are asymptotically\nstandard normal, permitting conventional inference. We present simulation\nevidence that confidence intervals constructed with our proposed standard\nerrors obtain superior coverage performance relative to existing methods. We\nillustrate the relevance of the proposed method in an empirical application to\na standard Fama-French three-factor regression.",
        "authors": [
            "Harold D Chiang",
            "Bruce E Hansen",
            "Yuya Sasaki"
        ],
        "categories": "econ.EM",
        "published": "2022-01-27T03:49:07Z",
        "updated": "2023-12-13T22:17:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.13267v1",
        "title": "Micro-level Reserving for General Insurance Claims using a Long Short-Term Memory Network",
        "abstract": "Detailed information about individual claims are completely ignored when\ninsurance claims data are aggregated and structured in development triangles\nfor loss reserving. In the hope of extracting predictive power from the\nindividual claims characteristics, researchers have recently proposed to move\naway from these macro-level methods in favor of micro-level loss reserving\napproaches. We introduce a discrete-time individual reserving framework\nincorporating granular information in a deep learning approach named Long\nShort-Term Memory (LSTM) neural network. At each time period, the network has\ntwo tasks: first, classifying whether there is a payment or a recovery, and\nsecond, predicting the corresponding non-zero amount, if any. We illustrate the\nestimation procedure on a simulated and a real general insurance dataset. We\ncompare our approach with the chain-ladder aggregate method using the\npredictive outstanding loss estimates and their actual values. Based on a\ngeneralized Pareto model for excess payments over a threshold, we adjust the\nLSTM reserve prediction to account for extreme payments.",
        "authors": [
            "Ihsan Chaoubi",
            "Camille Besse",
            "H\u00e9l\u00e8ne Cossette",
            "Marie-Pier C\u00f4t\u00e9"
        ],
        "categories": "cs.LG",
        "published": "2022-01-27T02:49:42Z",
        "updated": "2022-01-27T02:49:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.11156v1",
        "title": "Bootstrap inference for fixed-effect models",
        "abstract": "The maximum-likelihood estimator of nonlinear panel data models with fixed\neffects is consistent but asymptotically-biased under rectangular-array\nasymptotics. The literature has thus far concentrated its effort on devising\nmethods to correct the maximum-likelihood estimator for its bias as a means to\nsalvage standard inferential procedures. Instead, we show that the parametric\nbootstrap replicates the distribution of the (uncorrected) maximum-likelihood\nestimator in large samples. This justifies the use of confidence sets\nconstructed via standard bootstrap percentile methods. No adjustment for the\npresence of bias needs to be made.",
        "authors": [
            "Ayden Higgins",
            "Koen Jochmans"
        ],
        "categories": "econ.EM",
        "published": "2022-01-26T19:40:16Z",
        "updated": "2022-01-26T19:40:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.10826v6",
        "title": "Instrumental variable estimation of dynamic treatment effects on a duration outcome",
        "abstract": "This paper considers identification and estimation of the causal effect of\nthe time Z until a subject is treated on a survival outcome T. The treatment is\nnot randomly assigned, T is randomly right censored by a random variable C and\nthe time to treatment Z is right censored by min(T,C). The endogeneity issue is\ntreated using an instrumental variable explaining Z and independent of the\nerror term of the model. We study identification in a fully nonparametric\nframework. We show that our specification generates an integral equation, of\nwhich the regression function of interest is a solution. We provide\nidentification conditions that rely on this identification equation. For\nestimation purposes, we assume that the regression function follows a\nparametric model. We propose an estimation procedure and give conditions under\nwhich the estimator is asymptotically normal. The estimators exhibit good\nfinite sample properties in simulations. Our methodology is applied to find\nevidence supporting the efficacy of a therapy for burn-out.",
        "authors": [
            "Jad Beyhum",
            "Samuele Centorrino",
            "Jean-Pierre Florens",
            "Ingrid Van Keilegom"
        ],
        "categories": "math.ST",
        "published": "2022-01-26T09:10:03Z",
        "updated": "2022-12-17T08:41:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.10743v3",
        "title": "Combining Experimental and Observational Data for Identification and Estimation of Long-Term Causal Effects",
        "abstract": "We consider the task of identifying and estimating the causal effect of a\ntreatment variable on a long-term outcome variable using data from an\nobservational domain and an experimental domain. The observational domain is\nsubject to unobserved confounding. Furthermore, subjects in the experiment are\nonly followed for a short period of time; hence, long-term effects of treatment\nare unobserved but short-term effects will be observed. Therefore, data from\nneither domain alone suffices for causal inference about the effect of the\ntreatment on the long-term outcome, and must be pooled in a principled way,\ninstead. Athey et al. (2020) proposed a method for systematically combining\nsuch data for identifying the downstream causal effect in view. Their approach\nis based on the assumptions of internal and external validity of the\nexperimental data, and an extra novel assumption called latent\nunconfoundedness. In this paper, we first review their proposed approach, and\nthen we propose three alternative approaches for data fusion for the purpose of\nidentifying and estimating average treatment effect as well as the effect of\ntreatment on the treated. Our first approach is based on assuming\nequi-confounding bias for the short-term and long-term outcomes. Our second\napproach is based on a relaxed version of the equi-confounding bias assumption,\nwhere we assume the existence of an observed confounder such that the\nshort-term and long-term potential outcome variables have the same partial\nadditive association with that confounder. Our third approach is based on the\nproximal causal inference framework, in which we assume the existence of an\nextra variable in the system which is a proxy of the latent confounder of the\ntreatment-outcome relation. We propose influence function-based estimation\nstrategies for each of our data fusion frameworks and study the robustness\nproperties of the proposed estimators.",
        "authors": [
            "AmirEmad Ghassami",
            "Alan Yang",
            "David Richardson",
            "Ilya Shpitser",
            "Eric Tchetgen Tchetgen"
        ],
        "categories": "stat.ME",
        "published": "2022-01-26T04:21:14Z",
        "updated": "2022-04-29T04:37:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.10173v1",
        "title": "Modeling bid and ask price dynamics with an extended Hawkes process and its empirical applications for high-frequency stock market data",
        "abstract": "This study proposes a versatile model for the dynamics of the best bid and\nask prices using an extended Hawkes process. The model incorporates the zero\nintensities of the spread-narrowing processes at the minimum bid-ask spread,\nspread-dependent intensities, possible negative excitement, and nonnegative\nintensities. We apply the model to high-frequency best bid and ask price data\nfrom US stock markets. The empirical findings demonstrate a spread-narrowing\ntendency, excitations of the intensities caused by previous events, the impact\nof flash crashes, characteristic trends in fast trading over time, and the\ndifferent features of market participants in the various exchanges.",
        "authors": [
            "Kyungsub Lee",
            "Byoung Ki Seo"
        ],
        "categories": "q-fin.TR",
        "published": "2022-01-25T08:35:47Z",
        "updated": "2022-01-25T08:35:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.08837v1",
        "title": "Marginal Effects for Non-Linear Prediction Functions",
        "abstract": "Beta coefficients for linear regression models represent the ideal form of an\ninterpretable feature effect. However, for non-linear models and especially\ngeneralized linear models, the estimated coefficients cannot be interpreted as\na direct feature effect on the predicted outcome. Hence, marginal effects are\ntypically used as approximations for feature effects, either in the shape of\nderivatives of the prediction function or forward differences in prediction due\nto a change in a feature value. While marginal effects are commonly used in\nmany scientific fields, they have not yet been adopted as a model-agnostic\ninterpretation method for machine learning models. This may stem from their\ninflexibility as a univariate feature effect and their inability to deal with\nthe non-linearities found in black box models. We introduce a new class of\nmarginal effects termed forward marginal effects. We argue to abandon\nderivatives in favor of better-interpretable forward differences. Furthermore,\nwe generalize marginal effects based on forward differences to multivariate\nchanges in feature values. To account for the non-linearity of prediction\nfunctions, we introduce a non-linearity measure for marginal effects. We argue\nagainst summarizing feature effects of a non-linear prediction function in a\nsingle metric such as the average marginal effect. Instead, we propose to\npartition the feature space to compute conditional average marginal effects on\nfeature subspaces, which serve as conditional feature effect estimates.",
        "authors": [
            "Christian A. Scholbeck",
            "Giuseppe Casalicchio",
            "Christoph Molnar",
            "Bernd Bischl",
            "Christian Heumann"
        ],
        "categories": "cs.LG",
        "published": "2022-01-21T18:47:38Z",
        "updated": "2022-01-21T18:47:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.08826v1",
        "title": "Minimax-Regret Climate Policy with Deep Uncertainty in Climate Modeling and Intergenerational Discounting",
        "abstract": "Integrated assessment models have become the primary tools for comparing\nclimate policies that seek to reduce greenhouse gas emissions. Policy\ncomparisons have often been performed by considering a planner who seeks to\nmake optimal trade-offs between the costs of carbon abatement and the economic\ndamages from climate change. The planning problem has been formalized as one of\noptimal control, the objective being to minimize the total costs of abatement\nand damages over a time horizon. Studying climate policy as a control problem\npresumes that a planner knows enough to make optimization feasible, but\nphysical and economic uncertainties abound. Earlier, Manski, Sanstad, and\nDeCanio proposed and studied use of the minimax-regret (MMR) decision criterion\nto account for deep uncertainty in climate modeling. Here we study choice of\nclimate policy that minimizes maximum regret with deep uncertainty regarding\nboth the correct climate model and the appropriate time discount rate to use in\nintergenerational assessment of policy consequences. The analysis specifies a\nrange of discount rates to express both empirical and normative uncertainty\nabout the appropriate rate. The findings regarding climate policy are novel and\ninformative. The MMR analysis points to use of a relatively low discount rate\nof 0.02 for climate policy. The MMR decision rule keeps the maximum future\ntemperature increase below 2C above the 1900-10 level for most of the parameter\nvalues used to weight costs and damages.",
        "authors": [
            "Stephen J. DeCanio",
            "Charles F. Manski",
            "Alan H. Sanstad"
        ],
        "categories": "econ.EM",
        "published": "2022-01-21T18:21:21Z",
        "updated": "2022-01-21T18:21:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.08584v2",
        "title": "High-Dimensional Sparse Multivariate Stochastic Volatility Models",
        "abstract": "Although multivariate stochastic volatility models usually produce more\naccurate forecasts compared to the MGARCH models, their estimation techniques\nsuch as Bayesian MCMC typically suffer from the curse of dimensionality. We\npropose a fast and efficient estimation approach for MSV based on a penalized\nOLS framework. Specifying the MSV model as a multivariate state space model, we\ncarry out a two-step penalized procedure. We provide the asymptotic properties\nof the two-step estimator and the oracle property of the first-step estimator\nwhen the number of parameters diverges. The performances of our method are\nillustrated through simulations and financial data.",
        "authors": [
            "Benjamin Poignard",
            "Manabu Asai"
        ],
        "categories": "econ.EM",
        "published": "2022-01-21T08:06:10Z",
        "updated": "2022-05-17T08:28:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.08366v1",
        "title": "Estimation of Conditional Random Coefficient Models using Machine Learning Techniques",
        "abstract": "Nonparametric random coefficient (RC)-density estimation has mostly been\nconsidered in the marginal density case under strict independence of RCs and\ncovariates. This paper deals with the estimation of RC-densities conditional on\na (large-dimensional) set of control variables using machine learning\ntechniques. The conditional RC-density allows to disentangle observable from\nunobservable heterogeneity in partial effects of continuous treatments adding\nto a growing literature on heterogeneous effect estimation using machine\nlearning. %It is also informative of the conditional potential outcome\ndistribution. This paper proposes a two-stage sieve estimation procedure. First\na closed-form sieve approximation of the conditional RC density is derived\nwhere each sieve coefficient can be expressed as conditional expectation\nfunction varying with controls. Second, sieve coefficients are estimated with\ngeneric machine learning procedures and under appropriate sample splitting\nrules. The $L_2$-convergence rate of the conditional RC-density estimator is\nderived. The rate is slower by a factor then typical rates of mean regression\nmachine learning estimators which is due to the ill-posedness of the RC density\nestimation problem. The performance and applicability of the estimator is\nillustrated using random forest algorithms over a range of Monte Carlo\nsimulations and with real data from the SOEP-IS. Here behavioral heterogeneity\nin an economic experiment on portfolio choice is studied. The method reveals\ntwo types of behavior in the population, one type complying with economic\ntheory and one not. The assignment to types appears largely based on\nunobservables not available in the data.",
        "authors": [
            "Stephan Martin"
        ],
        "categories": "econ.EM",
        "published": "2022-01-20T18:50:52Z",
        "updated": "2022-01-20T18:50:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.08326v1",
        "title": "Learning with latent group sparsity via heat flow dynamics on networks",
        "abstract": "Group or cluster structure on explanatory variables in machine learning\nproblems is a very general phenomenon, which has attracted broad interest from\npractitioners and theoreticians alike. In this work we contribute an approach\nto learning under such group structure, that does not require prior information\non the group identities. Our paradigm is motivated by the Laplacian geometry of\nan underlying network with a related community structure, and proceeds by\ndirectly incorporating this into a penalty that is effectively computed via a\nheat flow-based local network dynamics. In fact, we demonstrate a procedure to\nconstruct such a network based on the available data. Notably, we dispense with\ncomputationally intensive pre-processing involving clustering of variables,\nspectral or otherwise. Our technique is underpinned by rigorous theorems that\nguarantee its effective performance and provide bounds on its sample\ncomplexity. In particular, in a wide range of settings, it provably suffices to\nrun the heat flow dynamics for time that is only logarithmic in the problem\ndimensions. We explore in detail the interfaces of our approach with key\nstatistical physics models in network science, such as the Gaussian Free Field\nand the Stochastic Block Model. We validate our approach by successful\napplications to real-world data from a wide array of application domains,\nincluding computer science, genetics, climatology and economics. Our work\nraises the possibility of applying similar diffusion-based techniques to\nclassical learning tasks, exploiting the interplay between geometric, dynamical\nand stochastic structures underlying the data.",
        "authors": [
            "Subhroshekhar Ghosh",
            "Soumendu Sundar Mukherjee"
        ],
        "categories": "stat.ME",
        "published": "2022-01-20T17:45:57Z",
        "updated": "2022-01-20T17:45:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07903v1",
        "title": "Identification of Direct Socio-Geographical Price Discrimination: An Empirical Study on iPhones",
        "abstract": "Price discrimination is a practice where firms utilize varying sensitivities\nto prices among consumers to increase profits. The welfare effects of price\ndiscrimination are not agreed on among economists, but identification of such\nactions may contribute to our standing of firms' pricing behaviors. In this\nletter, I use econometric tools to analyze whether Apple Inc, one of the\nlargest companies in the globe, is practicing price discrimination on the basis\nof socio-economical and geographical factors. My results indicate that iPhones\nare significantly (p $<$ 0.01) more expensive in markets where competitions are\nweak or where Apple has a strong market presence. Furthermore, iPhone prices\nare likely to increase (p $<$ 0.01) in developing countries/regions or markets\nwith high income inequality.",
        "authors": [
            "Davidson Cheng"
        ],
        "categories": "econ.EM",
        "published": "2022-01-19T23:01:17Z",
        "updated": "2022-01-19T23:01:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07319v1",
        "title": "Asymptotic properties of Bayesian inference in linear regression with a structural break",
        "abstract": "This paper studies large sample properties of a Bayesian approach to\ninference about slope parameters $\\gamma$ in linear regression models with a\nstructural break. In contrast to the conventional approach to inference about\n$\\gamma$ that does not take into account the uncertainty of the unknown break\nlocation $\\tau$, the Bayesian approach that we consider incorporates such\nuncertainty. Our main theoretical contribution is a Bernstein-von Mises type\ntheorem (Bayesian asymptotic normality) for $\\gamma$ under a wide class of\npriors, which essentially indicates an asymptotic equivalence between the\nconventional frequentist and Bayesian inference. Consequently, a frequentist\nresearcher could look at credible intervals of $\\gamma$ to check robustness\nwith respect to the uncertainty of $\\tau$. Simulation studies show that the\nconventional confidence intervals of $\\gamma$ tend to undercover in finite\nsamples whereas the credible intervals offer more reasonable coverages in\ngeneral. As the sample size increases, the two methods coincide, as predicted\nfrom our theoretical conclusion. Using data from Paye and Timmermann (2006) on\nstock return prediction, we illustrate that the traditional confidence\nintervals on $\\gamma$ might underrepresent the true sampling uncertainty.",
        "authors": [
            "Kenichi Shimizu"
        ],
        "categories": "econ.EM",
        "published": "2022-01-18T21:24:52Z",
        "updated": "2022-01-18T21:24:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07303v2",
        "title": "Large Hybrid Time-Varying Parameter VARs",
        "abstract": "Time-varying parameter VARs with stochastic volatility are routinely used for\nstructural analysis and forecasting in settings involving a few endogenous\nvariables. Applying these models to high-dimensional datasets has proved to be\nchallenging due to intensive computations and over-parameterization concerns.\nWe develop an efficient Bayesian sparsification method for a class of models we\ncall hybrid TVP-VARs--VARs with time-varying parameters in some equations but\nconstant coefficients in others. Specifically, for each equation, the new\nmethod automatically decides whether the VAR coefficients and contemporaneous\nrelations among variables are constant or time-varying. Using US datasets of\nvarious dimensions, we find evidence that the parameters in some, but not all,\nequations are time varying. The large hybrid TVP-VAR also forecasts better than\nmany standard benchmarks.",
        "authors": [
            "Joshua C. C. Chan"
        ],
        "categories": "econ.EM",
        "published": "2022-01-18T20:39:58Z",
        "updated": "2022-06-16T20:13:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07168v1",
        "title": "Bayesian inference of spatial and temporal relations in AI patents for EU countries",
        "abstract": "In the paper, we propose two models of Artificial Intelligence (AI) patents\nin European Union (EU) countries addressing spatial and temporal behaviour. In\nparticular, the models can quantitatively describe the interaction between\ncountries or explain the rapidly growing trends in AI patents. For spatial\nanalysis Poisson regression is used to explain collaboration between a pair of\ncountries measured by the number of common patents. Through Bayesian inference,\nwe estimated the strengths of interactions between countries in the EU and the\nrest of the world. In particular, a significant lack of cooperation has been\nidentified for some pairs of countries.\n  Alternatively, an inhomogeneous Poisson process combined with the logistic\ncurve growth accurately models the temporal behaviour by an accurate trend\nline. Bayesian analysis in the time domain revealed an upcoming slowdown in\npatenting intensity.",
        "authors": [
            "Krzysztof Rusek",
            "Agnieszka Kleszcz",
            "Albert Cabellos-Aparicio"
        ],
        "categories": "econ.EM",
        "published": "2022-01-18T18:13:03Z",
        "updated": "2022-01-18T18:13:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07072v4",
        "title": "Who Increases Emergency Department Use? New Insights from the Oregon Health Insurance Experiment",
        "abstract": "We provide new insights regarding the headline result that Medicaid increased\nemergency department (ED) use from the Oregon experiment. We find meaningful\nheterogeneous impacts of Medicaid on ED use using causal machine learning\nmethods. The individualized treatment effect distribution includes a wide range\nof negative and positive values, suggesting the average effect masks\nsubstantial heterogeneity. A small group-about 14% of participants-in the right\ntail of the distribution drives the overall effect. We identify priority groups\nwith economically significant increases in ED usage based on demographics and\nprevious utilization. Intensive margin effects are an important driver of\nincreases in ED utilization.",
        "authors": [
            "Augustine Denteh",
            "Helge Liebert"
        ],
        "categories": "econ.EM",
        "published": "2022-01-18T15:53:28Z",
        "updated": "2023-04-08T11:02:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07069v1",
        "title": "The Time-Varying Multivariate Autoregressive Index Model",
        "abstract": "Many economic variables feature changes in their conditional mean and\nvolatility, and Time Varying Vector Autoregressive Models are often used to\nhandle such complexity in the data. Unfortunately, when the number of series\ngrows, they present increasing estimation and interpretation problems. This\npaper tries to address this issue proposing a new Multivariate Autoregressive\nIndex model that features time varying means and volatility. Technically, we\ndevelop a new estimation methodology that mix switching algorithms with the\nforgetting factors strategy of Koop and Korobilis (2012). This substantially\nreduces the computational burden and allows to select or weight, in real time,\nthe number of common components and other features of the data using Dynamic\nModel Selection or Dynamic Model Averaging without further computational cost.\nUsing USA macroeconomic data, we provide a structural analysis and a\nforecasting exercise that demonstrates the feasibility and usefulness of this\nnew model.\n  Keywords: Large datasets, Multivariate Autoregressive Index models,\nStochastic volatility, Bayesian VARs.",
        "authors": [
            "G. Cubadda",
            "S. Grassi",
            "B. Guardabascio"
        ],
        "categories": "econ.EM",
        "published": "2022-01-18T15:49:47Z",
        "updated": "2022-01-18T15:49:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07055v2",
        "title": "Close Enough? A Large-Scale Exploration of Non-Experimental Approaches to Advertising Measurement",
        "abstract": "Despite their popularity, randomized controlled trials (RCTs) are not always\navailable for the purposes of advertising measurement. Non-experimental data is\nthus required. However, Facebook and other ad platforms use complex and\nevolving processes to select ads for users. Therefore, successful\nnon-experimental approaches need to \"undo\" this selection. We analyze 663\nlarge-scale experiments at Facebook to investigate whether this is possible\nwith the data typically logged at large ad platforms. With access to over 5,000\nuser-level features, these data are richer than what most advertisers or their\nmeasurement partners can access. We investigate how accurately two\nnon-experimental methods -- double/debiased machine learning (DML) and\nstratified propensity score matching (SPSM) -- can recover the experimental\neffects. Although DML performs better than SPSM, neither method performs well,\neven using flexible deep learning models to implement the propensity and\noutcome models. The median RCT lifts are 29%, 18%, and 5% for the upper,\nmiddle, and lower funnel outcomes, respectively. Using DML (SPSM), the median\nlift by funnel is 83% (173%), 58% (176%), and 24% (64%), respectively,\nindicating significant relative measurement errors. We further characterize the\ncircumstances under which each method performs comparatively better. Overall,\ndespite having access to large-scale experiments and rich user-level data, we\nare unable to reliably estimate an ad campaign's causal effect.",
        "authors": [
            "Brett R. Gordon",
            "Robert Moakler",
            "Florian Zettelmeyer"
        ],
        "categories": "econ.EM",
        "published": "2022-01-18T15:31:18Z",
        "updated": "2022-10-04T16:44:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07026v1",
        "title": "Socioeconomic disparities and COVID-19: the causal connections",
        "abstract": "The analysis of causation is a challenging task that can be approached in\nvarious ways. With the increasing use of machine learning based models in\ncomputational socioeconomics, explaining these models while taking causal\nconnections into account is a necessity. In this work, we advocate the use of\nan explanatory framework from cooperative game theory augmented with $do$\ncalculus, namely causal Shapley values. Using causal Shapley values, we analyze\nsocioeconomic disparities that have a causal link to the spread of COVID-19 in\nthe USA. We study several phases of the disease spread to show how the causal\nconnections change over time. We perform a causal analysis using random effects\nmodels and discuss the correspondence between the two methods to verify our\nresults. We show the distinct advantages a non-linear machine learning models\nhave over linear models when performing a multivariate analysis, especially\nsince the machine learning models can map out non-linear correlations in the\ndata. In addition, the causal Shapley values allow for including the causal\nstructure in the variable importance computed for the machine learning model.",
        "authors": [
            "Tannista Banerjee",
            "Ayan Paul",
            "Vishak Srikanth",
            "Inga Str\u00fcmke"
        ],
        "categories": "cs.LG",
        "published": "2022-01-18T14:46:32Z",
        "updated": "2022-01-18T14:46:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.06898v4",
        "title": "Difference-in-Differences Estimators for Treatments Continuously Distributed at Every Period",
        "abstract": "We propose difference-in-differences estimators in designs where the\ntreatment is continuously distributed at every period, as is often the case\nwhen one studies the effects of taxes, tariffs, or prices. We assume that\nbetween consecutive periods, the treatment of some units, the switchers,\nchanges, while the treatment of other units remains constant. We show that\nunder a placebo-testable parallel-trends assumption, averages of the slopes of\nswitchers' potential outcomes can be nonparametrically estimated. We generalize\nour estimators to the instrumental-variable case. We use our estimators to\nestimate the price-elasticity of gasoline consumption.",
        "authors": [
            "Cl\u00e9ment de Chaisemartin",
            "Xavier D'Haultfoeuille",
            "F\u00e9lix Pasquier",
            "Doulo Sow",
            "Gonzalo Vazquez-Bare"
        ],
        "categories": "econ.EM",
        "published": "2022-01-18T12:03:10Z",
        "updated": "2024-07-26T07:56:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.06694v4",
        "title": "Homophily in preferences or meetings? Identifying and estimating an iterative network formation model",
        "abstract": "Is homophily in social and economic networks driven by a taste for\nhomogeneity (preferences) or by a higher probability of meeting individuals\nwith similar attributes (opportunity)? This paper studies identification and\nestimation of an iterative network game that distinguishes between these two\nmechanisms. Our approach enables us to assess the counterfactual effects of\nchanging the meeting protocol between agents. As an application, we study the\nrole of preferences and meetings in shaping classroom friendship networks in\nBrazil. In a network structure in which homophily due to preferences is\nstronger than homophily due to meeting opportunities, tracking students may\nimprove welfare. Still, the relative benefit of this policy diminishes over the\nschool year.",
        "authors": [
            "Luis Alvarez",
            "Cristine Pinto",
            "Vladimir Ponczek"
        ],
        "categories": "econ.EM",
        "published": "2022-01-18T01:40:53Z",
        "updated": "2024-03-20T20:46:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.06647v1",
        "title": "An Entropy-Based Approach for Nonparametrically Testing Simple Probability Distribution Hypotheses",
        "abstract": "In this paper, we introduce a flexible and widely applicable nonparametric\nentropy-based testing procedure that can be used to assess the validity of\nsimple hypotheses about a specific parametric population distribution. The\ntesting methodology relies on the characteristic function of the population\nprobability distribution being tested and is attractive in that, regardless of\nthe null hypothesis being tested, it provides a unified framework for\nconducting such tests. The testing procedure is also computationally tractable\nand relatively straightforward to implement. In contrast to some alternative\ntest statistics, the proposed entropy test is free from user-specified kernel\nand bandwidth choices, idiosyncratic and complex regularity conditions, and/or\nchoices of evaluation grids. Several simulation exercises were performed to\ndocument the empirical performance of our proposed test, including a regression\nexample that is illustrative of how, in some contexts, the approach can be\napplied to composite hypothesis-testing situations via data transformations.\nOverall, the testing procedure exhibits notable promise, exhibiting appreciable\nincreasing power as sample size increases for a number of alternative\ndistributions when contrasted with hypothesized null distributions. Possible\ngeneral extensions of the approach to composite hypothesis-testing contexts,\nand directions for future work are also discussed.",
        "authors": [
            "Ron Mittelhammer",
            "George Judge",
            "Miguel Henry"
        ],
        "categories": "econ.EM",
        "published": "2022-01-17T22:29:04Z",
        "updated": "2022-01-17T22:29:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.06605v2",
        "title": "Inferential Theory for Granular Instrumental Variables in High Dimensions",
        "abstract": "The Granular Instrumental Variables (GIV) methodology exploits panels with\nfactor error structures to construct instruments to estimate structural time\nseries models with endogeneity even after controlling for latent factors. We\nextend the GIV methodology in several dimensions. First, we extend the\nidentification procedure to a large $N$ and large $T$ framework, which depends\non the asymptotic Herfindahl index of the size distribution of $N$\ncross-sectional units. Second, we treat both the factors and loadings as\nunknown and show that the sampling error in the estimated instrument and\nfactors is negligible when considering the limiting distribution of the\nstructural parameters. Third, we show that the sampling error in the\nhigh-dimensional precision matrix is negligible in our estimation algorithm.\nFourth, we overidentify the structural parameters with additional constructed\ninstruments, which leads to efficiency gains. Monte Carlo evidence is presented\nto support our asymptotic theory and application to the global crude oil market\nleads to new results.",
        "authors": [
            "Saman Banafti",
            "Tae-Hwy Lee"
        ],
        "categories": "econ.EM",
        "published": "2022-01-17T19:41:24Z",
        "updated": "2023-09-25T16:56:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.06169v3",
        "title": "On Well-posedness and Minimax Optimal Rates of Nonparametric Q-function Estimation in Off-policy Evaluation",
        "abstract": "We study the off-policy evaluation (OPE) problem in an infinite-horizon\nMarkov decision process with continuous states and actions. We recast the\n$Q$-function estimation into a special form of the nonparametric instrumental\nvariables (NPIV) estimation problem. We first show that under one mild\ncondition the NPIV formulation of $Q$-function estimation is well-posed in the\nsense of $L^2$-measure of ill-posedness with respect to the data generating\ndistribution, bypassing a strong assumption on the discount factor $\\gamma$\nimposed in the recent literature for obtaining the $L^2$ convergence rates of\nvarious $Q$-function estimators. Thanks to this new well-posed property, we\nderive the first minimax lower bounds for the convergence rates of\nnonparametric estimation of $Q$-function and its derivatives in both sup-norm\nand $L^2$-norm, which are shown to be the same as those for the classical\nnonparametric regression (Stone, 1982). We then propose a sieve two-stage least\nsquares estimator and establish its rate-optimality in both norms under some\nmild conditions. Our general results on the well-posedness and the minimax\nlower bounds are of independent interest to study not only other nonparametric\nestimators for $Q$-function but also efficient estimation on the value of any\ntarget policy in off-policy settings.",
        "authors": [
            "Xiaohong Chen",
            "Zhengling Qi"
        ],
        "categories": "math.ST",
        "published": "2022-01-17T01:09:38Z",
        "updated": "2022-06-27T02:34:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.06140v1",
        "title": "Nonparametric Identification of Random Coefficients in Endogenous and Heterogeneous Aggregate Demand Models",
        "abstract": "This paper studies nonparametric identification in market level demand models\nfor differentiated products with heterogeneous consumers. We consider a general\nclass of models that allows for the individual specific coefficients to vary\ncontinuously across the population and give conditions under which the density\nof these coefficients, and hence also functionals such as welfare measures, is\nidentified. A key finding is that two leading models, the BLP-model (Berry,\nLevinsohn, and Pakes, 1995) and the pure characteristics model (Berry and\nPakes, 2007), require considerably different conditions on the support of the\nproduct characteristics.",
        "authors": [
            "Fabian Dunker",
            "Stefan Hoderlein",
            "Hiroaki Kaido"
        ],
        "categories": "econ.EM",
        "published": "2022-01-16T21:40:44Z",
        "updated": "2022-01-16T21:40:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.05893v2",
        "title": "Treatment Effect Risk: Bounds and Inference",
        "abstract": "Since the average treatment effect (ATE) measures the change in social\nwelfare, even if positive, there is a risk of negative effect on, say, some 10%\nof the population. Assessing such risk is difficult, however, because any one\nindividual treatment effect (ITE) is never observed, so the 10% worst-affected\ncannot be identified, while distributional treatment effects only compare the\nfirst deciles within each treatment group, which does not correspond to any\n10%-subpopulation. In this paper we consider how to nonetheless assess this\nimportant risk measure, formalized as the conditional value at risk (CVaR) of\nthe ITE-distribution. We leverage the availability of pre-treatment covariates\nand characterize the tightest-possible upper and lower bounds on ITE-CVaR given\nby the covariate-conditional average treatment effect (CATE) function. We then\nproceed to study how to estimate these bounds efficiently from data and\nconstruct confidence intervals. This is challenging even in randomized\nexperiments as it requires understanding the distribution of the unknown CATE\nfunction, which can be very complex if we use rich covariates so as to best\ncontrol for heterogeneity. We develop a debiasing method that overcomes this\nand prove it enjoys favorable statistical properties even when CATE and other\nnuisances are estimated by black-box machine learning or even inconsistently.\nStudying a hypothetical change to French job-search counseling services, our\nbounds and inference demonstrate a small social benefit entails a negative\nimpact on a substantial subpopulation.",
        "authors": [
            "Nathan Kallus"
        ],
        "categories": "stat.ME",
        "published": "2022-01-15T17:21:26Z",
        "updated": "2022-07-19T16:57:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.05672v1",
        "title": "Measuring Changes in Disparity Gaps: An Application to Health Insurance",
        "abstract": "We propose a method for reporting how program evaluations reduce gaps between\ngroups, such as the gender or Black-white gap. We first show that the reduction\nin disparities between groups can be written as the difference in conditional\naverage treatment effects (CATE) for each group. Then, using a\nKitagawa-Oaxaca-Blinder-style decomposition, we highlight how these CATE can be\ndecomposed into unexplained differences in CATE in other observables versus\ndifferences in composition across other observables (e.g. the \"endowment\").\nFinally, we apply this approach to study the impact of Medicare on American's\naccess to health insurance.",
        "authors": [
            "Paul Goldsmith-Pinkham",
            "Karen Jiang",
            "Zirui Song",
            "Jacob Wallace"
        ],
        "categories": "econ.EM",
        "published": "2022-01-14T21:09:25Z",
        "updated": "2022-01-14T21:09:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.05556v2",
        "title": "Monitoring the Economy in Real Time: Trends and Gaps in Real Activity and Prices",
        "abstract": "We propose two specifications of a real-time mixed-frequency semi-structural\ntime series model for evaluating the output potential, output gap, Phillips\ncurve, and Okun's law for the US. The baseline model uses minimal theory-based\nmultivariate identification restrictions to inform trend-cycle decomposition,\nwhile the alternative model adds the CBO's output gap measure as an observed\nvariable. The latter model results in a smoother output potential and lower\ncyclical correlation between inflation and real variables but performs worse in\nforecasting beyond the short term. This methodology allows for the assessment\nand real-time monitoring of official trend and gap estimates.",
        "authors": [
            "Thomas Hasenzagl",
            "Filippo Pellegrino",
            "Lucrezia Reichlin",
            "Giovanni Ricco"
        ],
        "categories": "econ.EM",
        "published": "2022-01-14T17:01:57Z",
        "updated": "2023-03-31T16:43:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.05430v4",
        "title": "Detecting Multiple Structural Breaks in Systems of Linear Regression Equations with Integrated and Stationary Regressors",
        "abstract": "In this paper, we propose a two-step procedure based on the group LASSO\nestimator in combination with a backward elimination algorithm to detect\nmultiple structural breaks in linear regressions with multivariate responses.\nApplying the two-step estimator, we jointly detect the number and location of\nstructural breaks, and provide consistent estimates of the coefficients. Our\nframework is flexible enough to allow for a mix of integrated and stationary\nregressors, as well as deterministic terms. Using simulation experiments, we\nshow that the proposed two-step estimator performs competitively against the\nlikelihood-based approach (Qu and Perron, 2007; Li and Perron, 2017; Oka and\nPerron, 2018) in finite samples. However, the two-step estimator is\ncomputationally much more efficient. An economic application to the\nidentification of structural breaks in the term structure of interest rates\nillustrates this methodology.",
        "authors": [
            "Karsten Schweikert"
        ],
        "categories": "econ.EM",
        "published": "2022-01-14T13:01:58Z",
        "updated": "2024-09-23T11:21:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.05139v1",
        "title": "Generalized Kernel Ridge Regression for Long Term Causal Inference: Treatment Effects, Dose Responses, and Counterfactual Distributions",
        "abstract": "I propose kernel ridge regression estimators for long term causal inference,\nwhere a short term experimental data set containing randomized treatment and\nshort term surrogates is fused with a long term observational data set\ncontaining short term surrogates and long term outcomes. I propose estimators\nof treatment effects, dose responses, and counterfactual distributions with\nclosed form solutions in terms of kernel matrix operations. I allow covariates,\ntreatment, and surrogates to be discrete or continuous, and low, high, or\ninfinite dimensional. For long term treatment effects, I prove $\\sqrt{n}$\nconsistency, Gaussian approximation, and semiparametric efficiency. For long\nterm dose responses, I prove uniform consistency with finite sample rates. For\nlong term counterfactual distributions, I prove convergence in distribution.",
        "authors": [
            "Rahul Singh"
        ],
        "categories": "econ.EM",
        "published": "2022-01-13T18:51:56Z",
        "updated": "2022-01-13T18:51:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.04811v4",
        "title": "Binary response model with many weak instruments",
        "abstract": "This paper considers an endogenous binary response model with many weak\ninstruments. We employ a control function approach and a regularization scheme\nto obtain better estimation results for the endogenous binary response model in\nthe presence of many weak instruments. Two consistent and asymptotically\nnormally distributed estimators are provided, each of which is called a\nregularized conditional maximum likelihood estimator (RCMLE) and a regularized\nnonlinear least squares estimator (RNLSE). Monte Carlo simulations show that\nthe proposed estimators outperform the existing ones when there are many weak\ninstruments. We use the proposed estimation method to examine the effect of\nfamily income on college completion.",
        "authors": [
            "Dakyung Seong"
        ],
        "categories": "econ.EM",
        "published": "2022-01-13T07:00:58Z",
        "updated": "2024-07-01T01:03:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.04469v8",
        "title": "Optimal Best Arm Identification in Two-Armed Bandits with a Fixed Budget under a Small Gap",
        "abstract": "We consider fixed-budget best-arm identification in two-armed Gaussian bandit\nproblems. One of the longstanding open questions is the existence of an optimal\nstrategy under which the probability of misidentification matches a lower\nbound. We show that a strategy following the Neyman allocation rule (Neyman,\n1934) is asymptotically optimal when the gap between the expected rewards is\nsmall. First, we review a lower bound derived by Kaufmann et al. (2016). Then,\nwe propose the \"Neyman Allocation (NA)-Augmented Inverse Probability weighting\n(AIPW)\" strategy, which consists of the sampling rule using the Neyman\nallocation with an estimated standard deviation and the recommendation rule\nusing an AIPW estimator. Our proposed strategy is optimal because the upper\nbound matches the lower bound when the budget goes to infinity and the gap goes\nto zero.",
        "authors": [
            "Masahiro Kato",
            "Kaito Ariu",
            "Masaaki Imaizumi",
            "Masahiro Nomura",
            "Chao Qin"
        ],
        "categories": "stat.ML",
        "published": "2022-01-12T13:38:33Z",
        "updated": "2022-12-28T21:31:01Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.03286v1",
        "title": "A machine learning search for optimal GARCH parameters",
        "abstract": "Here, we use Machine Learning (ML) algorithms to update and improve the\nefficiencies of fitting GARCH model parameters to empirical data. We employ an\nArtificial Neural Network (ANN) to predict the parameters of these models. We\npresent a fitting algorithm for GARCH-normal(1,1) models to predict one of the\nmodel's parameters, $\\alpha_1$ and then use the analytical expressions for the\nfourth order standardised moment, $\\Gamma_4$ and the unconditional second order\nmoment, $\\sigma^2$ to fit the other two parameters; $\\beta_1$ and $\\alpha_0$,\nrespectively. The speed of fitting of the parameters and quick implementation\nof this approach allows for real time tracking of GARCH parameters. We further\nshow that different inputs to the ANN namely, higher order standardised moments\nand the autocovariance of time series can be used for fitting model parameters\nusing the ANN, but not always with the same level of accuracy.",
        "authors": [
            "Luke De Clerk",
            "Sergey Savl'ev"
        ],
        "categories": "econ.EM",
        "published": "2022-01-10T11:07:27Z",
        "updated": "2022-01-10T11:07:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.02532v3",
        "title": "Approximate Factor Models for Functional Time Series",
        "abstract": "We propose a novel approximate factor model tailored for analyzing\ntime-dependent curve data. Our model decomposes such data into two distinct\ncomponents: a low-dimensional predictable factor component and an unpredictable\nerror term. These components are identified through the autocovariance\nstructure of the underlying functional time series. The model parameters are\nconsistently estimated using the eigencomponents of a cumulative autocovariance\noperator and an information criterion is proposed to determine the appropriate\nnumber of factors. The methodology is applied to yield curve modeling and\nforecasting. Our results indicate that more than three factors are required to\ncharacterize the dynamics of the term structure of bond yields.",
        "authors": [
            "Sven Otto",
            "Nazarii Salish"
        ],
        "categories": "econ.EM",
        "published": "2022-01-07T16:44:13Z",
        "updated": "2024-05-31T16:46:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07666v2",
        "title": "Microeconomic Foundations of Decentralised Organisations",
        "abstract": "In this article, we analyse how decentralised digital infrastructures can\nprovide a fundamental change in the structure and dynamics of organisations.\nThe works of R.H.Coase and M. Olson, on the nature of the firm and the logic of\ncollective action, respectively, are revisited under the light of these\nemerging new digital foundations. We also analyse how these technologies can\naffect the fundamental assumptions on the role of organisations (either private\nor public) as mechanisms for the coordination of labour. We propose that these\ntechnologies can fundamentally affect: (i) the distribution of rewards within\nan organisation and (ii) the structure of its transaction costs. These changes\nbring the potential for addressing some of the trade-offs between the private\nand public sectors.",
        "authors": [
            "Mauricio Jacobo Romero",
            "Andr\u00e9 Freitas"
        ],
        "categories": "cs.CE",
        "published": "2022-01-07T16:07:05Z",
        "updated": "2022-10-09T12:26:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.02292v3",
        "title": "Unconditional Effects of General Policy Interventions",
        "abstract": "This paper studies the unconditional effects of a general policy\nintervention, which includes location-scale shifts and simultaneous shifts as\nspecial cases. The location-scale shift is intended to study a counterfactual\npolicy aimed at changing not only the mean or location of a covariate but also\nits dispersion or scale. The simultaneous shift refers to the situation where\nshifts in two or more covariates take place simultaneously. For example, a\nshift in one covariate is compensated at a certain rate by a shift in another\ncovariate. Not accounting for these possible scale or simultaneous shifts will\nresult in an incorrect assessment of the potential policy effects on an outcome\nvariable of interest. The unconditional policy parameters are estimated with\nsimple semiparametric estimators, for which asymptotic properties are studied.\nMonte Carlo simulations are implemented to study their finite sample\nperformances. The proposed approach is applied to a Mincer equation to study\nthe effects of changing years of education on wages and to study the effect of\nsmoking during pregnancy on birth weight.",
        "authors": [
            "Julian Martinez-Iriarte",
            "Gabriel Montes-Rojas",
            "Yixiao Sun"
        ],
        "categories": "econ.EM",
        "published": "2022-01-07T01:56:49Z",
        "updated": "2023-07-18T19:14:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.01770v1",
        "title": "NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting",
        "abstract": "Financial forecasting has been an important and active area of machine\nlearning research because of the challenges it presents and the potential\nrewards that even minor improvements in prediction accuracy or forecasting may\nentail. Traditionally, financial forecasting has heavily relied on quantitative\nindicators and metrics derived from structured financial statements. Earnings\nconference call data, including text and audio, is an important source of\nunstructured data that has been used for various prediction tasks using deep\nearning and related approaches. However, current deep learning-based methods\nare limited in the way that they deal with numeric data; numbers are typically\ntreated as plain-text tokens without taking advantage of their underlying\nnumeric structure. This paper describes a numeric-oriented hierarchical\ntransformer model to predict stock returns, and financial risk using\nmulti-modal aligned earnings calls data by taking advantage of the different\ncategories of numbers (monetary, temporal, percentages etc.) and their\nmagnitude. We present the results of a comprehensive evaluation of NumHTML\nagainst several state-of-the-art baselines using a real-world publicly\navailable dataset. The results indicate that NumHTML significantly outperforms\nthe current state-of-the-art across a variety of evaluation metrics and that it\nhas the potential to offer significant financial gains in a practical trading\ncontext.",
        "authors": [
            "Linyi Yang",
            "Jiazheng Li",
            "Ruihai Dong",
            "Yue Zhang",
            "Barry Smyth"
        ],
        "categories": "cs.LG",
        "published": "2022-01-05T10:17:02Z",
        "updated": "2022-01-05T10:17:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.01194v3",
        "title": "What's Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature",
        "abstract": "This paper synthesizes recent advances in the econometrics of\ndifference-in-differences (DiD) and provides concrete recommendations for\npractitioners. We begin by articulating a simple set of ``canonical''\nassumptions under which the econometrics of DiD are well-understood. We then\nargue that recent advances in DiD methods can be broadly classified as relaxing\nsome components of the canonical DiD setup, with a focus on $(i)$ multiple\nperiods and variation in treatment timing, $(ii)$ potential violations of\nparallel trends, or $(iii)$ alternative frameworks for inference. Our\ndiscussion highlights the different ways that the DiD literature has advanced\nbeyond the canonical model, and helps to clarify when each of the papers will\nbe relevant for empirical work. We conclude by discussing some promising areas\nfor future research.",
        "authors": [
            "Jonathan Roth",
            "Pedro H. C. Sant'Anna",
            "Alyssa Bilinski",
            "John Poe"
        ],
        "categories": "econ.EM",
        "published": "2022-01-04T15:21:33Z",
        "updated": "2023-01-09T21:11:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.01132v1",
        "title": "A Multivariate Dependence Analysis for Electricity Prices, Demand and Renewable Energy Sources",
        "abstract": "This paper examines the dependence between electricity prices, demand, and\nrenewable energy sources by means of a multivariate copula model {while\nstudying Germany, the widest studied market in Europe}. The inter-dependencies\nare investigated in-depth and monitored over time, with particular emphasis on\nthe tail behavior. To this end, suitable tail dependence measures are\nintroduced to take into account a multivariate extreme scenario appropriately\nidentified {through the} Kendall's distribution function. The empirical\nevidence demonstrates a strong association between electricity prices,\nrenewable energy sources, and demand within a day and over the studied years.\nHence, this analysis provides guidance for further and different incentives for\npromoting green energy generation while considering the time-varying\ndependencies of the involved variables",
        "authors": [
            "Fabrizio Durante",
            "Angelica Gianfreda",
            "Francesco Ravazzolo",
            "Luca Rossini"
        ],
        "categories": "stat.AP",
        "published": "2022-01-04T13:33:37Z",
        "updated": "2022-01-04T13:33:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.01094v1",
        "title": "Efficient Likelihood-based Estimation via Annealing for Dynamic Structural Macrofinance Models",
        "abstract": "Most solved dynamic structural macrofinance models are non-linear and/or\nnon-Gaussian state-space models with high-dimensional and complex structures.\nWe propose an annealed controlled sequential Monte Carlo method that delivers\nnumerically stable and low variance estimators of the likelihood function. The\nmethod relies on an annealing procedure to gradually introduce information from\nobservations and constructs globally optimal proposal distributions by solving\nassociated optimal control problems that yield zero variance likelihood\nestimators. To perform parameter inference, we develop a new adaptive SMC$^2$\nalgorithm that employs likelihood estimators from annealed controlled\nsequential Monte Carlo. We provide a theoretical stability analysis that\nelucidates the advantages of our methodology and asymptotic results concerning\nthe consistency and convergence rates of our SMC$^2$ estimators. We illustrate\nthe strengths of our proposed methodology by estimating two popular\nmacrofinance models: a non-linear new Keynesian dynamic stochastic general\nequilibrium model and a non-linear non-Gaussian consumption-based long-run risk\nmodel.",
        "authors": [
            "Andras Fulop",
            "Jeremy Heng",
            "Junye Li"
        ],
        "categories": "stat.CO",
        "published": "2022-01-04T11:31:47Z",
        "updated": "2022-01-04T11:31:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.01010v2",
        "title": "A Double Robust Approach for Non-Monotone Missingness in Multi-Stage Data",
        "abstract": "Multivariate missingness with a non-monotone missing pattern is complicated\nto deal with in empirical studies. The traditional Missing at Random (MAR)\nassumption is difficult to justify in such cases. Previous studies have\nstrengthened the MAR assumption, suggesting that the missing mechanism of any\nvariable is random when conditioned on a uniform set of fully observed\nvariables. However, empirical evidence indicates that this assumption may be\nviolated for variables collected at different stages. This paper proposes a new\nMAR-type assumption that fits non-monotone missing scenarios involving\nmulti-stage variables. Based on this assumption, we construct an Augmented\nInverse Probability Weighted GMM (AIPW-GMM) estimator. This estimator features\nan asymmetric format for the augmentation term, guarantees double robustness,\nand achieves the closed-form semiparametric efficiency bound. We apply this\nmethod to cases of missingness in both endogenous regressor and outcome, using\nthe Oregon Health Insurance Experiment as an example. We check the correlation\nbetween missing probabilities and partially observed variables to justify the\nassumption. Moreover, we find that excluding incomplete data results in a loss\nof efficiency and insignificant estimators. The proposed estimator reduces the\nstandard error by more than 50% for the estimated effects of the Oregon Health\nPlan on the elderly.",
        "authors": [
            "Shenshen Yang"
        ],
        "categories": "econ.EM",
        "published": "2022-01-04T07:07:53Z",
        "updated": "2024-08-03T03:37:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.00426v2",
        "title": "Deep Learning and Linear Programming for Automated Ensemble Forecasting and Interpretation",
        "abstract": "This paper presents an ensemble forecasting method that shows strong results\non the M4 Competition dataset by decreasing feature and model selection\nassumptions, termed DONUT (DO Not UTilize human beliefs). Our assumption\nreductions, primarily consisting of auto-generated features and a more diverse\nmodel pool for the ensemble, significantly outperform the statistical,\nfeature-based ensemble method FFORMA by Montero-Manso et al. (2020). We also\ninvestigate feature extraction with a Long Short-term Memory Network (LSTM)\nAutoencoder and find that such features contain crucial information not\ncaptured by standard statistical feature approaches. The ensemble weighting\nmodel uses LSTM and statistical features to combine the models accurately. The\nanalysis of feature importance and interaction shows a slight superiority for\nLSTM features over the statistical ones alone. Clustering analysis shows that\nessential LSTM features differ from most statistical features and each other.\nWe also find that increasing the solution space of the weighting model by\naugmenting the ensemble with new models is something the weighting model learns\nto use, thus explaining part of the accuracy gains. Moreover, we present a\nformal ex-post-facto analysis of an optimal combination and selection for\nensembles, quantifying differences through linear optimization on the M4\ndataset. Our findings indicate that classical statistical time series features,\nsuch as trend and seasonality, alone do not capture all relevant information\nfor forecasting a time series. On the contrary, our novel LSTM features contain\nsignificantly more predictive power than the statistical ones alone, but\ncombining the two feature sets proved the best in practice.",
        "authors": [
            "Lars Lien Ankile",
            "Kjartan Krange"
        ],
        "categories": "cs.LG",
        "published": "2022-01-02T22:19:26Z",
        "updated": "2022-11-25T20:26:39Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.01182v1",
        "title": "Modelling Cournot Games as Multi-agent Multi-armed Bandits",
        "abstract": "We investigate the use of a multi-agent multi-armed bandit (MA-MAB) setting\nfor modeling repeated Cournot oligopoly games, where the firms acting as agents\nchoose from the set of arms representing production quantity (a discrete\nvalue). Agents interact with separate and independent bandit problems. In this\nformulation, each agent makes sequential choices among arms to maximize its own\nreward. Agents do not have any information about the environment; they can only\nsee their own rewards after taking an action. However, the market demand is a\nstationary function of total industry output, and random entry or exit from the\nmarket is not allowed. Given these assumptions, we found that an\n$\\epsilon$-greedy approach offers a more viable learning mechanism than other\ntraditional MAB approaches, as it does not require any additional knowledge of\nthe system to operate. We also propose two novel approaches that take advantage\nof the ordered action space: $\\epsilon$-greedy+HL and $\\epsilon$-greedy+EL.\nThese new approaches help firms to focus on more profitable actions by\neliminating less profitable choices and hence are designed to optimize the\nexploration. We use computer simulations to study the emergence of various\nequilibria in the outcomes and do the empirical analysis of joint cumulative\nregrets.",
        "authors": [
            "Kshitija Taywade",
            "Brent Harrison",
            "Adib Bagh"
        ],
        "categories": "cs.GT",
        "published": "2022-01-01T22:02:47Z",
        "updated": "2022-01-01T22:02:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.15155v2",
        "title": "Auction Throttling and Causal Inference of Online Advertising Effects",
        "abstract": "Causally identifying the effect of digital advertising is challenging,\nbecause experimentation is expensive, and observational data lacks random\nvariation. This paper identifies a pervasive source of naturally occurring,\nquasi-experimental variation in user-level ad-exposure in digital advertising\ncampaigns. It shows how this variation can be utilized by ad-publishers to\nidentify the causal effect of advertising campaigns. The variation pertains to\nauction throttling, a probabilistic method of budget pacing that is widely used\nto spread an ad-campaign`s budget over its deployed duration, so that the\ncampaign`s budget is not exceeded or overly concentrated in any one period. The\nthrottling mechanism is implemented by computing a participation probability\nbased on the campaign`s budget spending rate and then including the campaign in\na random subset of available ad-auctions each period according to this\nprobability. We show that access to logged-participation probabilities enables\nidentifying the local average treatment effect (LATE) in the ad-campaign. We\npresent a new estimator that leverages this identification strategy and outline\na bootstrap procedure for quantifying its variability. We apply our method to\nreal-world ad-campaign data from an e-commerce advertising platform, which uses\nsuch throttling for budget pacing. We show our estimate is statistically\ndifferent from estimates derived using other standard observational methods\nsuch as OLS and two-stage least squares estimators. Our estimated conversion\nlift is 110%, a more plausible number than 600%, the conversion lifts estimated\nusing naive observational methods.",
        "authors": [
            "George Gui",
            "Harikesh Nair",
            "Fengshi Niu"
        ],
        "categories": "econ.EM",
        "published": "2021-12-30T18:21:04Z",
        "updated": "2022-02-16T19:11:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.15114v3",
        "title": "Estimating a Continuous Treatment Model with Spillovers: A Control Function Approach",
        "abstract": "We study a continuous treatment effect model in the presence of treatment\nspillovers through social networks. We assume that one's outcome is affected\nnot only by his/her own treatment but also by a (weighted) average of his/her\nneighbors' treatments, both of which are treated as endogenous variables. Using\na control function approach with appropriate instrumental variables, we show\nthat the conditional mean potential outcome can be nonparametrically\nidentified. We also consider a more empirically tractable semiparametric model\nand develop a three-step estimation procedure for this model. As an empirical\nillustration, we investigate the causal effect of the regional unemployment\nrate on the crime rate.",
        "authors": [
            "Tadao Hoshino"
        ],
        "categories": "econ.EM",
        "published": "2021-12-30T16:16:30Z",
        "updated": "2023-01-11T05:05:06Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.15108v1",
        "title": "Modeling and Forecasting Intraday Market Returns: a Machine Learning Approach",
        "abstract": "In this paper we examine the relation between market returns and volatility\nmeasures through machine learning methods in a high-frequency environment. We\nimplement a minute-by-minute rolling window intraday estimation method using\ntwo nonlinear models: Long-Short-Term Memory (LSTM) neural networks and Random\nForests (RF). Our estimations show that the CBOE Volatility Index (VIX) is the\nstrongest candidate predictor for intraday market returns in our analysis,\nspecially when implemented through the LSTM model. This model also improves\nsignificantly the performance of the lagged market return as predictive\nvariable. Finally, intraday RF estimation outputs indicate that there is no\nperformance improvement with this method, and it may even worsen the results in\nsome cases.",
        "authors": [
            "Iuri H. Ferreira",
            "Marcelo C. Medeiros"
        ],
        "categories": "econ.EM",
        "published": "2021-12-30T16:05:17Z",
        "updated": "2021-12-30T16:05:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.14846v1",
        "title": "An Analysis of an Alternative Pythagorean Expected Win Percentage Model: Applications Using Major League Baseball Team Quality Simulations",
        "abstract": "We ask if there are alternative contest models that minimize error or\ninformation loss from misspecification and outperform the Pythagorean model.\nThis article aims to use simulated data to select the optimal expected win\npercentage model among the choice of relevant alternatives. The choices include\nthe traditional Pythagorean model and the difference-form contest success\nfunction (CSF). Method. We simulate 1,000 iterations of the 2014 MLB season for\nthe purpose of estimating and analyzing alternative models of expected win\npercentage (team quality). We use the open-source, Strategic Baseball Simulator\nand develop an AutoHotKey script that programmatically executes the SBS\napplication, chooses the correct settings for the 2014 season, enters a unique\nID for the simulation data file, and iterates these steps 1,000 times. We\nestimate expected win percentage using the traditional Pythagorean model, as\nwell as the difference-form CSF model that is used in game theory and public\nchoice economics. Each model is estimated while accounting for fixed (team)\neffects. We find that the difference-form CSF model outperforms the traditional\nPythagorean model in terms of explanatory power and in terms of\nmisspecification-based information loss as estimated by the Akaike Information\nCriterion. Through parametric estimation, we further confirm that the simulator\nyields realistic statistical outcomes. The simulation methodology offers the\nadvantage of greatly improved sample size. As the season is held constant, our\nsimulation-based statistical inference also allows for estimation and model\ncomparison without the (time series) issue of non-stationarity. The results\nsuggest that improved win (productivity) estimation can be achieved through\nalternative CSF specifications.",
        "authors": [
            "Justin Ehrlich",
            "Christopher Boudreaux",
            "James Boudreau",
            "Shane Sanders"
        ],
        "categories": "econ.EM",
        "published": "2021-12-29T22:08:24Z",
        "updated": "2021-12-29T22:08:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.14529v3",
        "title": "Volatility of volatility estimation: central limit theorems for the Fourier transform estimator and empirical study of the daily time series stylized facts",
        "abstract": "We study the asymptotic normality of two feasible estimators of the\nintegrated volatility of volatility based on the Fourier methodology, which\ndoes not require the pre-estimation of the spot volatility. We show that the\nbias-corrected estimator reaches the optimal rate $n^{1/4}$, while the\nestimator without bias-correction has a slower convergence rate and a smaller\nasymptotic variance. Additionally, we provide simulation results that support\nthe theoretical asymptotic distribution of the rate-efficient estimator and\nshow the accuracy of the latter in comparison with a rate-optimal estimator\nbased on the pre-estimation of the spot volatility. Finally, using the\nrate-optimal Fourier estimator, we reconstruct the time series of the daily\nvolatility of volatility of the S\\&P500 and EUROSTOXX50 indices over long\nsamples and provide novel insight into the existence of stylized facts about\nthe volatility of volatility dynamics.",
        "authors": [
            "Giacomo Toscano",
            "Giulia Livieri",
            "Maria Elvira Mancino",
            "Stefano Marmi"
        ],
        "categories": "math.ST",
        "published": "2021-12-29T12:53:02Z",
        "updated": "2022-09-05T16:04:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.14249v3",
        "title": "Nested Nonparametric Instrumental Variable Regression: Long Term, Mediated, and Time Varying Treatment Effects",
        "abstract": "Several causal parameters in short panel data models are scalar summaries of\na function called a nested nonparametric instrumental variable regression\n(nested NPIV). Examples include long term, mediated, and time varying treatment\neffects identified using proxy variables. However, it appears that no prior\nestimators or guarantees for nested NPIV exist, preventing flexible estimation\nand inference for these causal parameters. A major challenge is compounding ill\nposedness due to the nested inverse problems. We analyze adversarial estimators\nof nested NPIV, and provide sufficient conditions for efficient inference on\nthe causal parameter. Our nonasymptotic analysis has three salient features:\n(i) introducing techniques that limit how ill posedness compounds; (ii)\naccommodating neural networks, random forests, and reproducing kernel Hilbert\nspaces; and (iii) extending to causal functions, e.g. long term heterogeneous\ntreatment effects. We measure long term heterogeneous treatment effects of\nProject STAR and mediated proximal treatment effects of the Job Corps.",
        "authors": [
            "Isaac Meza",
            "Rahul Singh"
        ],
        "categories": "stat.ML",
        "published": "2021-12-28T18:29:56Z",
        "updated": "2024-03-11T02:12:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.13649v1",
        "title": "Random Rank-Dependent Expected Utility",
        "abstract": "We present a novel characterization of random rank-dependent expected utility\nfor finite datasets and finite prizes. The test lends itself to statistical\ntesting using the tools in Kitamura and Stoye (2018).",
        "authors": [
            "Nail Kashaev",
            "Victor Aguiar"
        ],
        "categories": "econ.TH",
        "published": "2021-12-27T13:18:42Z",
        "updated": "2021-12-27T13:18:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.13506v1",
        "title": "Estimation based on nearest neighbor matching: from density ratio to average treatment effect",
        "abstract": "Nearest neighbor (NN) matching as a tool to align data sampled from different\ngroups is both conceptually natural and practically well-used. In a landmark\npaper, Abadie and Imbens (2006) provided the first large-sample analysis of NN\nmatching under, however, a crucial assumption that the number of NNs, $M$, is\nfixed. This manuscript reveals something new out of their study and shows that,\nonce allowing $M$ to diverge with the sample size, an intrinsic statistic in\ntheir analysis actually constitutes a consistent estimator of the density\nratio. Furthermore, through selecting a suitable $M$, this statistic can attain\nthe minimax lower bound of estimation over a Lipschitz density function class.\nConsequently, with a diverging $M$, the NN matching provably yields a doubly\nrobust estimator of the average treatment effect and is semiparametrically\nefficient if the density functions are sufficiently smooth and the outcome\nmodel is appropriately specified. It can thus be viewed as a precursor of\ndouble machine learning estimators.",
        "authors": [
            "Zhexiao Lin",
            "Peng Ding",
            "Fang Han"
        ],
        "categories": "math.ST",
        "published": "2021-12-27T04:45:29Z",
        "updated": "2021-12-27T04:45:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.13495v1",
        "title": "Multiple Randomization Designs",
        "abstract": "In this study we introduce a new class of experimental designs. In a\nclassical randomized controlled trial (RCT), or A/B test, a randomly selected\nsubset of a population of units (e.g., individuals, plots of land, or\nexperiences) is assigned to a treatment (treatment A), and the remainder of the\npopulation is assigned to the control treatment (treatment B). The difference\nin average outcome by treatment group is an estimate of the average effect of\nthe treatment. However, motivating our study, the setting for modern\nexperiments is often different, with the outcomes and treatment assignments\nindexed by multiple populations. For example, outcomes may be indexed by buyers\nand sellers, by content creators and subscribers, by drivers and riders, or by\ntravelers and airlines and travel agents, with treatments potentially varying\nacross these indices. Spillovers or interference can arise from interactions\nbetween units across populations. For example, sellers' behavior may depend on\nbuyers' treatment assignment, or vice versa. This can invalidate the simple\ncomparison of means as an estimator for the average effect of the treatment in\nclassical RCTs. We propose new experiment designs for settings in which\nmultiple populations interact. We show how these designs allow us to study\nquestions about interference that cannot be answered by classical randomized\nexperiments. Finally, we develop new statistical methods for analyzing these\nMultiple Randomization Designs.",
        "authors": [
            "Patrick Bajari",
            "Brian Burdick",
            "Guido W. Imbens",
            "Lorenzo Masoero",
            "James McQueen",
            "Thomas Richardson",
            "Ido M. Rosen"
        ],
        "categories": "stat.ME",
        "published": "2021-12-27T03:31:10Z",
        "updated": "2021-12-27T03:31:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.13398v5",
        "title": "Long Story Short: Omitted Variable Bias in Causal Machine Learning",
        "abstract": "We develop a general theory of omitted variable bias for a wide range of\ncommon causal parameters, including (but not limited to) averages of potential\noutcomes, average treatment effects, average causal derivatives, and policy\neffects from covariate shifts. Our theory applies to nonparametric models,\nwhile naturally allowing for (semi-)parametric restrictions (such as partial\nlinearity) when such assumptions are made. We show how simple plausibility\njudgments on the maximum explanatory power of omitted variables are sufficient\nto bound the magnitude of the bias, thus facilitating sensitivity analysis in\notherwise complex, nonlinear models. Finally, we provide flexible and efficient\nstatistical inference methods for the bounds, which can leverage modern machine\nlearning algorithms for estimation. These results allow empirical researchers\nto perform sensitivity analyses in a flexible class of machine-learned causal\nmodels using very simple, and interpretable, tools. We demonstrate the utility\nof our approach with two empirical examples.",
        "authors": [
            "Victor Chernozhukov",
            "Carlos Cinelli",
            "Whitney Newey",
            "Amit Sharma",
            "Vasilis Syrgkanis"
        ],
        "categories": "econ.EM",
        "published": "2021-12-26T15:38:23Z",
        "updated": "2024-05-26T18:43:02Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.13228v2",
        "title": "Robust Estimation of Average Treatment Effects from Panel Data",
        "abstract": "In order to evaluate the impact of a policy intervention on a group of units\nover time, it is important to correctly estimate the average treatment effect\n(ATE) measure. Due to lack of robustness of the existing procedures of\nestimating ATE from panel data, in this paper, we introduce a robust estimator\nof the ATE and the subsequent inference procedures using the popular approach\nof minimum density power divergence inference. Asymptotic properties of the\nproposed ATE estimator are derived and used to construct robust test statistics\nfor testing parametric hypotheses related to the ATE. Besides asymptotic\nanalyses of efficiency and powers, extensive simulation studies are conducted\nto study the finite-sample performances of our proposed estimation and testing\nprocedures under both pure and contaminated data. The robustness of the ATE\nestimator is further investigated theoretically through the influence functions\nanalyses. Finally our proposal is applied to study the long-term economic\neffects of the 2004 Indian Ocean earthquake and tsunami on the (per-capita)\ngross domestic products (GDP) of five mostly affected countries, namely\nIndonesia, Sri Lanka, Thailand, India and Maldives.",
        "authors": [
            "Sayoni Roychowdhury",
            "Indrila Ganguly",
            "Abhik Ghosh"
        ],
        "categories": "stat.ME",
        "published": "2021-12-25T12:20:35Z",
        "updated": "2022-12-18T07:41:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.11751v1",
        "title": "Bayesian Approaches to Shrinkage and Sparse Estimation",
        "abstract": "In all areas of human knowledge, datasets are increasing in both size and\ncomplexity, creating the need for richer statistical models. This trend is also\ntrue for economic data, where high-dimensional and nonlinear/nonparametric\ninference is the norm in several fields of applied econometric work. The\npurpose of this paper is to introduce the reader to the world of Bayesian model\ndetermination, by surveying modern shrinkage and variable selection algorithms\nand methodologies. Bayesian inference is a natural probabilistic framework for\nquantifying uncertainty and learning about model parameters, and this feature\nis particularly important for inference in modern models of high dimensions and\nincreased complexity.\n  We begin with a linear regression setting in order to introduce various\nclasses of priors that lead to shrinkage/sparse estimators of comparable value\nto popular penalized likelihood estimators (e.g.\\ ridge, lasso). We explore\nvarious methods of exact and approximate inference, and discuss their pros and\ncons. Finally, we explore how priors developed for the simple regression\nsetting can be extended in a straightforward way to various classes of\ninteresting econometric models. In particular, the following case-studies are\nconsidered, that demonstrate application of Bayesian shrinkage and variable\nselection strategies to popular econometric contexts: i) vector autoregressive\nmodels; ii) factor models; iii) time-varying parameter regressions; iv)\nconfounder selection in treatment effects models; and v) quantile regression\nmodels. A MATLAB package and an accompanying technical manual allow the reader\nto replicate many of the algorithms described in this review.",
        "authors": [
            "Dimitris Korobilis",
            "Kenichi Shimizu"
        ],
        "categories": "econ.EM",
        "published": "2021-12-22T09:35:27Z",
        "updated": "2021-12-22T09:35:27Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.11449v2",
        "title": "Doubly-Valid/Doubly-Sharp Sensitivity Analysis for Causal Inference with Unmeasured Confounding",
        "abstract": "We consider the problem of constructing bounds on the average treatment\neffect (ATE) when unmeasured confounders exist but have bounded influence.\nSpecifically, we assume that omitted confounders could not change the odds of\ntreatment for any unit by more than a fixed factor. We derive the sharp partial\nidentification bounds implied by this assumption by leveraging distributionally\nrobust optimization, and we propose estimators of these bounds with several\nnovel robustness properties. The first is double sharpness: our estimators\nconsistently estimate the sharp ATE bounds when one of two nuisance parameters\nis misspecified and achieve semiparametric efficiency when all nuisance\nparameters are suitably consistent. The second is double validity: even when\nmost nuisance parameters are misspecified, our estimators still provide valid\nbut possibly conservative bounds for the ATE and our Wald confidence intervals\nremain valid even when our estimators are not asymptotically normal. As a\nresult, our estimators provide a highly credible method for sensitivity\nanalysis of causal inferences.",
        "authors": [
            "Jacob Dorn",
            "Kevin Guo",
            "Nathan Kallus"
        ],
        "categories": "stat.ME",
        "published": "2021-12-21T18:55:12Z",
        "updated": "2022-07-22T14:19:50Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.11315v1",
        "title": "Efficient Estimation of State-Space Mixed-Frequency VARs: A Precision-Based Approach",
        "abstract": "State-space mixed-frequency vector autoregressions are now widely used for\nnowcasting. Despite their popularity, estimating such models can be\ncomputationally intensive, especially for large systems with stochastic\nvolatility. To tackle the computational challenges, we propose two novel\nprecision-based samplers to draw the missing observations of the low-frequency\nvariables in these models, building on recent advances in the band and sparse\nmatrix algorithms for state-space models. We show via a simulation study that\nthe proposed methods are more numerically accurate and computationally\nefficient compared to standard Kalman-filter based methods. We demonstrate how\nthe proposed method can be applied in two empirical macroeconomic applications:\nestimating the monthly output gap and studying the response of GDP to a\nmonetary policy shock at the monthly frequency. Results from these two\nempirical applications highlight the importance of incorporating high-frequency\nindicators in macroeconomic models.",
        "authors": [
            "Joshua C. C. Chan",
            "Aubrey Poon",
            "Dan Zhu"
        ],
        "categories": "econ.EM",
        "published": "2021-12-21T16:06:04Z",
        "updated": "2021-12-21T16:06:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.11064v1",
        "title": "Ranking and Selection from Pairwise Comparisons: Empirical Bayes Methods for Citation Analysis",
        "abstract": "We study the Stigler model of citation flows among journals adapting the\npairwise comparison model of Bradley and Terry to do ranking and selection of\njournal influence based on nonparametric empirical Bayes procedures.\nComparisons with several other rankings are made.",
        "authors": [
            "Jiaying Gu",
            "Roger Koenker"
        ],
        "categories": "econ.EM",
        "published": "2021-12-21T09:46:29Z",
        "updated": "2021-12-21T09:46:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.10542v2",
        "title": "Heckman-Selection or Two-Part models for alcohol studies? Depends",
        "abstract": "Aims: To re-introduce the Heckman model as a valid empirical technique in\nalcohol studies. Design: To estimate the determinants of problem drinking using\na Heckman and a two-part estimation model. Psychological and neuro-scientific\nstudies justify my underlying estimation assumptions and covariate exclusion\nrestrictions. Higher order tests checking for multicollinearity validate the\nuse of Heckman over the use of two-part estimation models. I discuss the\ngeneralizability of the two models in applied research. Settings and\nParticipants: Two pooled national population surveys from 2016 and 2017 were\nused: the Behavioral Risk Factor Surveillance Survey (BRFS), and the National\nSurvey of Drug Use and Health (NSDUH). Measurements: Participation in problem\ndrinking and meeting the criteria for problem drinking. Findings: Both U.S.\nnational surveys perform well with the Heckman model and pass all higher order\ntests. The Heckman model corrects for selection bias and reveals the direction\nof bias, where the two-part model does not. For example, the coefficients on\nage are upward biased and unemployment is downward biased in the two-part where\nthe Heckman model does not have a selection bias. Covariate exclusion\nrestrictions are sensitive to survey conditions and are contextually\ngeneralizable. Conclusions: The Heckman model can be used for alcohol (smoking\nstudies as well) if the underlying estimation specification passes higher order\ntests for multicollinearity and the exclusion restrictions are justified with\nintegrity for the data used. Its use is merit-worthy because it corrects for\nand reveals the direction and the magnitude of selection bias where the\ntwo-part does not.",
        "authors": [
            "Reka Sundaram-Stukel"
        ],
        "categories": "econ.EM",
        "published": "2021-12-20T14:08:35Z",
        "updated": "2023-06-29T18:53:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.09259v2",
        "title": "Robustness, Heterogeneous Treatment Effects and Covariate Shifts",
        "abstract": "This paper studies the robustness of estimated policy effects to changes in\nthe distribution of covariates. Robustness to covariate shifts is important,\nfor example, when evaluating the external validity of quasi-experimental\nresults, which are often used as a benchmark for evidence-based policy-making.\nI propose a novel scalar robustness metric. This metric measures the magnitude\nof the smallest covariate shift needed to invalidate a claim on the policy\neffect (for example, $ATE \\geq 0$) supported by the quasi-experimental\nevidence. My metric links the heterogeneity of policy effects and robustness in\na flexible, nonparametric way and does not require functional form assumptions.\nI cast the estimation of the robustness metric as a de-biased GMM problem. This\napproach guarantees a parametric convergence rate for the robustness metric\nwhile allowing for machine learning-based estimators of policy effect\nheterogeneity (for example, lasso, random forest, boosting, neural nets). I\napply my procedure to the Oregon Health Insurance experiment. I study the\nrobustness of policy effects estimates of health-care utilization and financial\nstrain outcomes, relative to a shift in the distribution of context-specific\ncovariates. Such covariates are likely to differ across US states, making\nquantification of robustness an important exercise for adoption of the\ninsurance policy in states other than Oregon. I find that the effect on\noutpatient visits is the most robust among the metrics of health-care\nutilization considered.",
        "authors": [
            "Pietro Emilio Spini"
        ],
        "categories": "econ.EM",
        "published": "2021-12-16T23:53:42Z",
        "updated": "2024-08-18T16:04:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.09170v5",
        "title": "Reinforcing RCTs with Multiple Priors while Learning about External Validity",
        "abstract": "This paper introduces a framework for incorporating prior information into\nthe design of sequential experiments. These sources may include past\nexperiments, expert opinions, or the experimenter's intuition. We model the\nproblem using a multi-prior Bayesian approach, mapping each source to a\nBayesian model and aggregating them based on posterior probabilities. Policies\nare evaluated on three criteria: learning the parameters of payoff\ndistributions, the probability of choosing the wrong treatment, and average\nrewards. Our framework demonstrates several desirable properties, including\nrobustness to sources lacking external validity, while maintaining strong\nfinite sample performance.",
        "authors": [
            "Frederico Finan",
            "Demian Pouzo"
        ],
        "categories": "econ.EM",
        "published": "2021-12-16T19:38:09Z",
        "updated": "2024-09-29T15:56:21Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.08934v4",
        "title": "Lassoed Boosting and Linear Prediction in the Equities Market",
        "abstract": "We consider a two-stage estimation method for linear regression. First, it\nuses the lasso in Tibshirani (1996) to screen variables and, second,\nre-estimates the coefficients using the least-squares boosting method in\nFriedman (2001) on every set of selected variables. Based on the large-scale\nsimulation experiment in Hastie et al. (2020), lassoed boosting performs as\nwell as the relaxed lasso in Meinshausen (2007) and, under certain scenarios,\ncan yield a sparser model. Applied to predicting equity returns, lassoed\nboosting gives the smallest mean-squared prediction error compared to several\nother methods.",
        "authors": [
            "Xiao Huang"
        ],
        "categories": "econ.EM",
        "published": "2021-12-16T15:00:37Z",
        "updated": "2024-05-20T03:15:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.08546v2",
        "title": "Uniform Convergence Results for the Local Linear Regression Estimation of the Conditional Distribution",
        "abstract": "This paper examines the local linear regression (LLR) estimate of the\nconditional distribution function $F(y|x)$. We derive three uniform convergence\nresults: the uniform bias expansion, the uniform convergence rate, and the\nuniform asymptotic linear representation. The uniformity in the above results\nis with respect to both $x$ and $y$ and therefore has not previously been\naddressed in the literature on local polynomial regression. Such uniform\nconvergence results are especially useful when the conditional distribution\nestimator is the first stage of a semiparametric estimator. We demonstrate the\nusefulness of these uniform results with two examples: the stochastic\nequicontinuity condition in $y$, and the estimation of the integrated\nconditional distribution function.",
        "authors": [
            "Haitian Xie"
        ],
        "categories": "econ.EM",
        "published": "2021-12-16T01:04:23Z",
        "updated": "2023-06-06T02:02:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.08092v2",
        "title": "Testing Instrument Validity with Covariates",
        "abstract": "We develop a novel test of the instrumental variable identifying assumptions\nfor heterogeneous treatment effect models with conditioning covariates. We\nassume semiparametric dependence between potential outcomes and conditioning\ncovariates. This allows us to obtain testable equality and inequality\nrestrictions among the subdensities of estimable partial residuals. We propose\njointly testing these restrictions. To improve power, we introduce\ndistillation, where a trimmed sample is used to test the inequality\nrestrictions. In Monte Carlo exercises we find gains in finite sample power\nfrom testing restrictions jointly and distillation. We apply our test procedure\nto three instruments and reject the null for one.",
        "authors": [
            "Thomas Carr",
            "Toru Kitagawa"
        ],
        "categories": "econ.EM",
        "published": "2021-12-15T13:06:22Z",
        "updated": "2023-09-16T19:45:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.07985v1",
        "title": "Solving the Data Sparsity Problem in Predicting the Success of the Startups with Machine Learning Methods",
        "abstract": "Predicting the success of startup companies is of great importance for both\nstartup companies and investors. It is difficult due to the lack of available\ndata and appropriate general methods. With data platforms like Crunchbase\naggregating the information of startup companies, it is possible to predict\nwith machine learning algorithms. Existing research suffers from the data\nsparsity problem as most early-stage startup companies do not have much data\navailable to the public. We try to leverage the recent algorithms to solve this\nproblem. We investigate several machine learning algorithms with a large\ndataset from Crunchbase. The results suggest that LightGBM and XGBoost perform\nbest and achieve 53.03% and 52.96% F1 scores. We interpret the predictions from\nthe perspective of feature contribution. We construct portfolios based on the\nmodels and achieve high success rates. These findings have substantial\nimplications on how machine learning methods can help startup companies and\ninvestors.",
        "authors": [
            "Dafei Yin",
            "Jing Li",
            "Gaosheng Wu"
        ],
        "categories": "cs.LG",
        "published": "2021-12-15T09:21:32Z",
        "updated": "2021-12-15T09:21:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.07155v2",
        "title": "Behavioral Foundations of Nested Stochastic Choice and Nested Logit",
        "abstract": "We provide the first behavioral characterization of nested logit, a\nfoundational and widely applied discrete choice model, through the introduction\nof a non-parametric version of nested logit that we call Nested Stochastic\nChoice (NSC). NSC is characterized by a single axiom that weakens Independence\nof Irrelevant Alternatives based on revealed similarity to allow for the\nsimilarity effect. Nested logit is characterized by an additional\nmenu-independence axiom. Our axiomatic characterization leads to a practical,\ndata-driven algorithm that identifies the true nest structure from choice data.\nWe also discuss limitations of generalizing nested logit by studying the\ntestable implications of cross-nested logit.",
        "authors": [
            "Matthew Kovach",
            "Gerelt Tserenjigmid"
        ],
        "categories": "econ.TH",
        "published": "2021-12-14T04:30:14Z",
        "updated": "2022-02-04T18:09:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.07149v2",
        "title": "Factor Models with Sparse VAR Idiosyncratic Components",
        "abstract": "We reconcile the two worlds of dense and sparse modeling by exploiting the\npositive aspects of both. We employ a factor model and assume {the dynamic of\nthe factors is non-pervasive while} the idiosyncratic term follows a sparse\nvector autoregressive model (VAR) {which allows} for cross-sectional and time\ndependence. The estimation is articulated in two steps: first, the factors and\ntheir loadings are estimated via principal component analysis and second, the\nsparse VAR is estimated by regularized regression on the estimated\nidiosyncratic components. We prove the consistency of the proposed estimation\napproach as the time and cross-sectional dimension diverge. In the second step,\nthe estimation error of the first step needs to be accounted for. Here, we do\nnot follow the naive approach of simply plugging in the standard rates derived\nfor the factor estimation. Instead, we derive a more refined expression of the\nerror. This enables us to derive tighter rates. We discuss the implications of\nour model for forecasting, factor augmented regression, bootstrap of factor\nmodels, and time series dependence networks via semi-parametric estimation of\nthe inverse of the spectral density matrix.",
        "authors": [
            "Jonas Krampe",
            "Luca Margaritella"
        ],
        "categories": "stat.ME",
        "published": "2021-12-14T04:10:59Z",
        "updated": "2022-05-24T15:30:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.07121v4",
        "title": "Semiparametric Conditional Factor Models: Estimation and Inference",
        "abstract": "This paper introduces a simple and tractable sieve estimation of\nsemiparametric conditional factor models with latent factors. We establish\nlarge-$N$-asymptotic properties of the estimators without requiring large $T$.\nWe also develop a simple bootstrap procedure for conducting inference about the\nconditional pricing errors as well as the shapes of the factor loading\nfunctions. These results enable us to estimate conditional factor structure of\na large set of individual assets by utilizing arbitrary nonlinear functions of\na number of characteristics without the need to pre-specify the factors, while\nallowing us to disentangle the characteristics' role in capturing factor betas\nfrom alphas (i.e., undiversifiable risk from mispricing). We apply these\nmethods to the cross-section of individual U.S. stock returns and find strong\nevidence of large nonzero pricing errors that combine to produce arbitrage\nportfolios with Sharpe ratios above 3. We also document a significant decline\nin apparent mispricing over time.",
        "authors": [
            "Qihui Chen",
            "Nikolai Roussanov",
            "Xiaoliang Wang"
        ],
        "categories": "econ.EM",
        "published": "2021-12-14T02:46:21Z",
        "updated": "2023-09-29T07:39:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.07014v1",
        "title": "Identifying Marginal Treatment Effects in the Presence of Sample Selection",
        "abstract": "This article presents identification results for the marginal treatment\neffect (MTE) when there is sample selection. We show that the MTE is partially\nidentified for individuals who are always observed regardless of treatment, and\nderive uniformly sharp bounds on this parameter under three increasingly\nrestrictive sets of assumptions. The first result imposes standard MTE\nassumptions with an unrestricted sample selection mechanism. The second set of\nconditions imposes monotonicity of the sample selection variable with respect\nto treatment, considerably shrinking the identified set. Finally, we\nincorporate a stochastic dominance assumption which tightens the lower bound\nfor the MTE. Our analysis extends to discrete instruments. The results rely on\na mixture reformulation of the problem where the mixture weights are\nidentified, extending Lee's (2009) trimming procedure to the MTE context. We\npropose estimators for the bounds derived and use data made available by Deb,\nMunking and Trivedi (2006) to empirically illustrate the usefulness of our\napproach.",
        "authors": [
            "Ot\u00e1vio Bartalotti",
            "D\u00e9sir\u00e9 K\u00e9dagni",
            "Vitor Possebom"
        ],
        "categories": "econ.EM",
        "published": "2021-12-13T21:08:49Z",
        "updated": "2021-12-13T21:08:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.06822v1",
        "title": "Quantile Regression under Limited Dependent Variable",
        "abstract": "A new Stata command, ldvqreg, is developed to estimate quantile regression\nmodels for the cases of censored (with lower and/or upper censoring) and binary\ndependent variables. The estimators are implemented using a smoothed version of\nthe quantile regression objective function. Simulation exercises show that it\ncorrectly estimates the parameters and it should be implemented instead of the\navailable quantile regression methods when censoring is present. An empirical\napplication to women's labor supply in Uruguay is considered.",
        "authors": [
            "Javier Alejo",
            "Gabriel Montes-Rojas"
        ],
        "categories": "econ.EM",
        "published": "2021-12-13T17:33:54Z",
        "updated": "2021-12-13T17:33:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.06363v15",
        "title": "Risk and optimal policies in bandit experiments",
        "abstract": "We provide a decision theoretic analysis of bandit experiments under local\nasymptotics. Working within the framework of diffusion processes, we define\nsuitable notions of asymptotic Bayes and minimax risk for these experiments.\nFor normally distributed rewards, the minimal Bayes risk can be characterized\nas the solution to a second-order partial differential equation (PDE). Using a\nlimit of experiments approach, we show that this PDE characterization also\nholds asymptotically under both parametric and non-parametric distributions of\nthe rewards. The approach further describes the state variables it is\nasymptotically sufficient to restrict attention to, and thereby suggests a\npractical strategy for dimension reduction. The PDEs characterizing minimal\nBayes risk can be solved efficiently using sparse matrix routines or\nMonte-Carlo methods. We derive the optimal Bayes and minimax policies from\ntheir numerical solutions. These optimal policies substantially dominate\nexisting methods such as Thompson sampling; the risk of the latter is often\ntwice as high.",
        "authors": [
            "Karun Adusumilli"
        ],
        "categories": "econ.EM",
        "published": "2021-12-13T00:41:19Z",
        "updated": "2024-01-31T06:02:47Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.06192v1",
        "title": "Housing Price Prediction Model Selection Based on Lorenz and Concentration Curves: Empirical Evidence from Tehran Housing Market",
        "abstract": "This study contributes a house price prediction model selection in Tehran\nCity based on the area between Lorenz curve (LC) and concentration curve (CC)\nof the predicted price by using 206,556 observed transaction data over the\nperiod from March 21, 2018, to February 19, 2021. Several different methods\nsuch as generalized linear models (GLM) and recursive partitioning and\nregression trees (RPART), random forests (RF) regression models, and neural\nnetwork (NN) models were examined house price prediction. We used 90% of all\ndata samples which were chosen randomly to estimate the parameters of pricing\nmodels and 10% of remaining datasets to test the accuracy of prediction.\nResults showed that the area between the LC and CC curves (which are known as\nABC criterion) of real and predicted prices in the test data sample of the\nrandom forest regression model was less than by other models under study. The\ncomparison of the calculated ABC criteria leads us to conclude that the\nnonlinear regression models such as RF regression models give an accurate\nprediction of house prices in Tehran City.",
        "authors": [
            "Mohammad Mirbagherijam"
        ],
        "categories": "econ.EM",
        "published": "2021-12-12T09:44:28Z",
        "updated": "2021-12-12T09:44:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.05876v1",
        "title": "The Past as a Stochastic Process",
        "abstract": "Historical processes manifest remarkable diversity. Nevertheless, scholars\nhave long attempted to identify patterns and categorize historical actors and\ninfluences with some success. A stochastic process framework provides a\nstructured approach for the analysis of large historical datasets that allows\nfor detection of sometimes surprising patterns, identification of relevant\ncausal actors both endogenous and exogenous to the process, and comparison\nbetween different historical cases. The combination of data, analytical tools\nand the organizing theoretical framework of stochastic processes complements\ntraditional narrative approaches in history and archaeology.",
        "authors": [
            "David H. Wolpert",
            "Michael H. Price",
            "Stefani A. Crabtree",
            "Timothy A. Kohler",
            "Jurgen Jost",
            "James Evans",
            "Peter F. Stadler",
            "Hajime Shimao",
            "Manfred D. Laubichler"
        ],
        "categories": "stat.AP",
        "published": "2021-12-11T00:15:59Z",
        "updated": "2021-12-11T00:15:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.05671v2",
        "title": "On the Assumptions of Synthetic Control Methods",
        "abstract": "Synthetic control (SC) methods have been widely applied to estimate the\ncausal effect of large-scale interventions, e.g., the state-wide effect of a\nchange in policy. The idea of synthetic controls is to approximate one unit's\ncounterfactual outcomes using a weighted combination of some other units'\nobserved outcomes. The motivating question of this paper is: how does the SC\nstrategy lead to valid causal inferences? We address this question by\nre-formulating the causal inference problem targeted by SC with a more\nfine-grained model, where we change the unit of the analysis from \"large units\"\n(e.g., states) to \"small units\" (e.g., individuals in states). Under this\nre-formulation, we derive sufficient conditions for the non-parametric causal\nidentification of the causal effect. We highlight two implications of the\nreformulation: (1) it clarifies where \"linearity\" comes from, and how it falls\nnaturally out of the more fine-grained and flexible model, and (2) it suggests\nnew ways of using available data with SC methods for valid causal inference, in\nparticular, new ways of selecting observations from which to estimate the\ncounterfactual.",
        "authors": [
            "Claudia Shi",
            "Dhanya Sridhar",
            "Vishal Misra",
            "David M. Blei"
        ],
        "categories": "stat.ME",
        "published": "2021-12-10T17:07:14Z",
        "updated": "2021-12-14T15:19:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.05308v2",
        "title": "Option Pricing with State-dependent Pricing Kernel",
        "abstract": "We introduce a new volatility model for option pricing that combines Markov\nswitching with the Realized GARCH framework. This leads to a novel pricing\nkernel with a state-dependent variance risk premium and a pricing formula for\nEuropean options, which is derived with an analytical approximation method. We\napply the Markov switching Realized GARCH model to S&P 500 index options from\n1990 to 2019 and find that investors' aversion to volatility-specific risk is\ntime-varying. The proposed framework outperforms competing models and reduces\n(in-sample and out-of-sample) option pricing errors by 15% or more.",
        "authors": [
            "Chen Tong",
            "Peter Reinhard Hansen",
            "Zhuo Huang"
        ],
        "categories": "q-fin.PR",
        "published": "2021-12-10T02:58:36Z",
        "updated": "2022-04-14T14:09:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.05302v1",
        "title": "Realized GARCH, CBOE VIX, and the Volatility Risk Premium",
        "abstract": "We show that the Realized GARCH model yields close-form expression for both\nthe Volatility Index (VIX) and the volatility risk premium (VRP). The Realized\nGARCH model is driven by two shocks, a return shock and a volatility shock, and\nthese are natural state variables in the stochastic discount factor (SDF). The\nvolatility shock endows the exponentially affine SDF with a compensation for\nvolatility risk. This leads to dissimilar dynamic properties under the physical\nand risk-neutral measures that can explain time-variation in the VRP. In an\nempirical application with the S&P 500 returns, the VIX, and the VRP, we find\nthat the Realized GARCH model significantly outperforms conventional GARCH\nmodels.",
        "authors": [
            "Peter Reinhard Hansen",
            "Zhuo Huang",
            "Chen Tong",
            "Tianyi Wang"
        ],
        "categories": "econ.EM",
        "published": "2021-12-10T02:38:42Z",
        "updated": "2021-12-10T02:38:42Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.04723v1",
        "title": "Covariate Balancing Sensitivity Analysis for Extrapolating Randomized Trials across Locations",
        "abstract": "The ability to generalize experimental results from randomized control trials\n(RCTs) across locations is crucial for informing policy decisions in targeted\nregions. Such generalization is often hindered by the lack of identifiability\ndue to unmeasured effect modifiers that compromise direct transport of\ntreatment effect estimates from one location to another. We build upon\nsensitivity analysis in observational studies and propose an optimization\nprocedure that allows us to get bounds on the treatment effects in targeted\nregions. Furthermore, we construct more informative bounds by balancing on the\nmoments of covariates. In simulation experiments, we show that the covariate\nbalancing approach is promising in getting sharper identification intervals.",
        "authors": [
            "Xinkun Nie",
            "Guido Imbens",
            "Stefan Wager"
        ],
        "categories": "econ.EM",
        "published": "2021-12-09T06:50:32Z",
        "updated": "2021-12-09T06:50:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2201.07880v2",
        "title": "Deep self-consistent learning of local volatility",
        "abstract": "We present an algorithm for the calibration of local volatility from market\noption prices through deep self-consistent learning, by approximating both\nmarket option prices and local volatility using deep neural networks,\nrespectively. Our method uses the initial-boundary value problem of the\nunderlying Dupire's partial differential equation solved by the parameterized\noption prices to bring corrections to the parameterization in a self-consistent\nway. By exploiting the differentiability of the neural networks, we can\nevaluate Dupire's equation locally at each strike-maturity pair; while by\nexploiting their continuity, we sample strike-maturity pairs uniformly from a\ngiven domain, going beyond the discrete points where the options are quoted.\nMoreover, the absence of arbitrage opportunities are imposed by penalizing an\nassociated loss function as a soft constraint. For comparison with existing\napproaches, the proposed method is tested on both synthetic and market option\nprices, which shows an improved performance in terms of reduced interpolation\nand reprice errors, as well as the smoothness of the calibrated local\nvolatility. An ablation study has been performed, asserting the robustness and\nsignificance of the proposed method.",
        "authors": [
            "Zhe Wang",
            "Nicolas Privault",
            "Claude Guet"
        ],
        "categories": "q-fin.CP",
        "published": "2021-12-09T01:48:10Z",
        "updated": "2023-11-06T11:36:36Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.04637v1",
        "title": "Efficient counterfactual estimation in semiparametric discrete choice models: a note on Chiong, Hsieh, and Shum (2017)",
        "abstract": "I suggest an enhancement of the procedure of Chiong, Hsieh, and Shum (2017)\nfor calculating bounds on counterfactual demand in semiparametric discrete\nchoice models. Their algorithm relies on a system of inequalities indexed by\ncycles of a large number $M$ of observed markets and hence seems to require\ncomputationally infeasible enumeration of all such cycles. I show that such\nenumeration is unnecessary because solving the \"fully efficient\" inequality\nsystem exploiting cycles of all possible lengths $K=1,\\dots,M$ can be reduced\nto finding the length of the shortest path between every pair of vertices in a\ncomplete bidirected weighted graph on $M$ vertices. The latter problem can be\nsolved using the Floyd--Warshall algorithm with computational complexity\n$O\\left(M^3\\right)$, which takes only seconds to run even for thousands of\nmarkets. Monte Carlo simulations illustrate the efficiency gain from using\ncycles of all lengths, which turns out to be positive, but small.",
        "authors": [
            "Grigory Franguridi"
        ],
        "categories": "econ.EM",
        "published": "2021-12-09T00:49:56Z",
        "updated": "2021-12-09T00:49:56Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.04565v6",
        "title": "Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous Treatment Effects: A Survey",
        "abstract": "Linear regressions with period and group fixed effects are widely used to\nestimate policies' effects: 26 of the 100 most cited papers published by the\nAmerican Economic Review from 2015 to 2019 estimate such regressions. It has\nrecently been shown that those regressions may produce misleading estimates, if\nthe policy's effect is heterogeneous between groups or over time, as is often\nthe case. This survey reviews a fast-growing literature that documents this\nissue, and that proposes alternative estimators robust to heterogeneous\neffects. We use those alternative estimators to revisit Wolfers (2006).",
        "authors": [
            "Cl\u00e9ment de Chaisemartin",
            "Xavier D'Haultf\u0153uille"
        ],
        "categories": "econ.EM",
        "published": "2021-12-08T20:14:26Z",
        "updated": "2022-06-20T13:27:04Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.04398v2",
        "title": "Matching for causal effects via multimarginal unbalanced optimal transport",
        "abstract": "Matching on covariates is a well-established framework for estimating causal\neffects in observational studies. The principal challenge stems from the often\nhigh-dimensional structure of the problem. Many methods have been introduced to\naddress this, with different advantages and drawbacks in computational and\nstatistical performance as well as interpretability. This article introduces a\nnatural optimal matching method based on multimarginal unbalanced optimal\ntransport that possesses many useful properties in this regard. It provides\ninterpretable weights based on the distance of matched individuals, can be\nefficiently implemented via the iterative proportional fitting procedure, and\ncan match several treatment arms simultaneously. Importantly, the proposed\nmethod only selects good matches from either group, hence is competitive with\nthe classical k-nearest neighbors approach in terms of bias and variance in\nfinite samples. Moreover, we prove a central limit theorem for the empirical\nprocess of the potential functions of the optimal coupling in the unbalanced\noptimal transport problem with a fixed penalty term. This implies a parametric\nrate of convergence of the empirically obtained weights to the optimal weights\nin the population for a fixed penalty term.",
        "authors": [
            "Florian Gunsilius",
            "Yuliang Xu"
        ],
        "categories": "stat.ME",
        "published": "2021-12-08T16:45:31Z",
        "updated": "2022-07-09T07:16:41Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.03872v3",
        "title": "Nonparametric Treatment Effect Identification in School Choice",
        "abstract": "This paper studies nonparametric identification and estimation of causal\neffects in centralized school assignment. In many centralized assignment\nsettings, students are subjected to both lottery-driven variation and\nregression discontinuity (RD) driven variation. We characterize the full set of\nidentified atomic treatment effects (aTEs), defined as the conditional average\ntreatment effect between a pair of schools, given student characteristics.\nAtomic treatment effects are the building blocks of more aggregated notions of\ntreatment contrasts, and common approaches estimating aggregations of aTEs can\nmask important heterogeneity. In particular, many aggregations of aTEs put zero\nweight on aTEs driven by RD variation, and estimators of such aggregations put\nasymptotically vanishing weight on the RD-driven aTEs. We develop a diagnostic\ntool for empirically assessing the weight put on aTEs driven by RD variation.\nLastly, we provide estimators and accompanying asymptotic results for inference\non aggregations of RD-driven aTEs.",
        "authors": [
            "Jiafeng Chen"
        ],
        "categories": "econ.EM",
        "published": "2021-12-07T18:09:28Z",
        "updated": "2023-10-23T15:34:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.03836v1",
        "title": "A decomposition method to evaluate the `paradox of progress' with evidence for Argentina",
        "abstract": "The `paradox of progress' is an empirical regularity that associates more\neducation with larger income inequality. Two driving and competing factors\nbehind this phenomenon are the convexity of the `Mincer equation' (that links\nwages and education) and the heterogeneity in its returns, as captured by\nquantile regressions. We propose a joint least-squares and quantile regression\nstatistical framework to derive a decomposition in order to evaluate the\nrelative contribution of each explanation. The estimators are based on the\n`functional derivative' approach. We apply the proposed decomposition strategy\nto the case of Argentina 1992 to 2015.",
        "authors": [
            "Javier Alejo",
            "Leonardo Gasparini",
            "Gabriel Montes-Rojas",
            "Walter Sosa-Escudero"
        ],
        "categories": "econ.EM",
        "published": "2021-12-07T17:20:26Z",
        "updated": "2021-12-07T17:20:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.03718v1",
        "title": "A Bayesian take on option pricing with Gaussian processes",
        "abstract": "Local volatility is a versatile option pricing model due to its state\ndependent diffusion coefficient. Calibration is, however, non-trivial as it\ninvolves both proposing a hypothesis model of the latent function and a method\nfor fitting it to data. In this paper we present novel Bayesian inference with\nGaussian process priors. We obtain a rich representation of the local\nvolatility function with a probabilistic notion of uncertainty attached to the\ncalibrate. We propose an inference algorithm and apply our approach to S&P 500\nmarket data.",
        "authors": [
            "Martin Tegner",
            "Stephen Roberts"
        ],
        "categories": "q-fin.MF",
        "published": "2021-12-07T14:14:30Z",
        "updated": "2021-12-07T14:14:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.03626v7",
        "title": "Phase transitions in nonparametric regressions",
        "abstract": "When the unknown regression function of a single variable is known to have\nderivatives up to the $(\\gamma+1)$th order bounded in absolute values by a\ncommon constant everywhere or a.e. (i.e., $(\\gamma+1)$th degree of smoothness),\nthe minimax optimal rate of the mean integrated squared error (MISE) is stated\nas $\\left(\\frac{1}{n}\\right)^{\\frac{2\\gamma+2}{2\\gamma+3}}$ in the literature.\nThis paper shows that: (i) if $n\\leq\\left(\\gamma+1\\right)^{2\\gamma+3}$, the\nminimax optimal MISE rate is $\\frac{\\log n}{n\\log(\\log n)}$ and the optimal\ndegree of smoothness to exploit is roughly $\\max\\left\\{ \\left\\lfloor \\frac{\\log\nn}{2\\log\\left(\\log n\\right)}\\right\\rfloor ,\\,1\\right\\} $; (ii) if\n$n>\\left(\\gamma+1\\right)^{2\\gamma+3}$, the minimax optimal MISE rate is\n$\\left(\\frac{1}{n}\\right)^{\\frac{2\\gamma+2}{2\\gamma+3}}$ and the optimal degree\nof smoothness to exploit is $\\gamma+1$. The fundamental contribution of this\npaper is a set of metric entropy bounds we develop for smooth function classes.\nSome of our bounds are original, and some of them improve and/or generalize the\nones in the literature (e.g., Kolmogorov and Tikhomirov, 1959). Our metric\nentropy bounds allow us to show phase transitions in the minimax optimal MISE\nrates associated with some commonly seen smoothness classes as well as\nnon-standard smoothness classes, and can also be of independent interest\noutside the nonparametric regression problems.",
        "authors": [
            "Ying Zhu"
        ],
        "categories": "math.ST",
        "published": "2021-12-07T10:55:31Z",
        "updated": "2023-11-02T20:53:18Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.03096v2",
        "title": "Visual Inference and Graphical Representation in Regression Discontinuity Designs",
        "abstract": "Despite the widespread use of graphs in empirical research, little is known\nabout readers' ability to process the statistical information they are meant to\nconvey (\"visual inference\"). We study visual inference within the context of\nregression discontinuity (RD) designs by measuring how accurately readers\nidentify discontinuities in graphs produced from data generating processes\ncalibrated on 11 published papers from leading economics journals. First, we\nassess the effects of different graphical representation methods on visual\ninference using randomized experiments. We find that bin widths and fit lines\nhave the largest impacts on whether participants correctly perceive the\npresence or absence of a discontinuity. Our experimental results allow us to\nmake evidence-based recommendations to practitioners, and we suggest using\nsmall bins with no fit lines as a starting point to construct RD graphs.\nSecond, we compare visual inference on graphs constructed using our preferred\nmethod with widely used econometric inference procedures. We find that visual\ninference achieves similar or lower type I error (false positive) rates and\ncomplements econometric inference.",
        "authors": [
            "Christina Korting",
            "Carl Lieberman",
            "Jordan Matsudaira",
            "Zhuan Pei",
            "Yi Shen"
        ],
        "categories": "econ.EM",
        "published": "2021-12-06T15:02:14Z",
        "updated": "2023-01-28T01:40:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.03075v1",
        "title": "Deep Quantile and Deep Composite Model Regression",
        "abstract": "A main difficulty in actuarial claim size modeling is that there is no simple\noff-the-shelf distribution that simultaneously provides a good distributional\nmodel for the main body and the tail of the data. In particular, covariates may\nhave different effects for small and for large claim sizes. To cope with this\nproblem, we introduce a deep composite regression model whose splicing point is\ngiven in terms of a quantile of the conditional claim size distribution rather\nthan a constant. To facilitate M-estimation for such models, we introduce and\ncharacterize the class of strictly consistent scoring functions for the triplet\nconsisting a quantile, as well as the lower and upper expected shortfall beyond\nthat quantile. In a second step, this elicitability result is applied to fit\ndeep neural network regression models. We demonstrate the applicability of our\napproach and its superiority over classical approaches on a real accident\ninsurance data set.",
        "authors": [
            "Tobias Fissler",
            "Michael Merz",
            "Mario V. W\u00fcthrich"
        ],
        "categories": "stat.ME",
        "published": "2021-12-06T14:31:25Z",
        "updated": "2021-12-06T14:31:25Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.01995v3",
        "title": "Gaussian Process Vector Autoregressions and Macroeconomic Uncertainty",
        "abstract": "We develop a non-parametric multivariate time series model that remains\nagnostic on the precise relationship between a (possibly) large set of\nmacroeconomic time series and their lagged values. The main building block of\nour model is a Gaussian process prior on the functional relationship that\ndetermines the conditional mean of the model, hence the name of Gaussian\nprocess vector autoregression (GP-VAR). A flexible stochastic volatility\nspecification is used to provide additional flexibility and control for\nheteroskedasticity. Markov chain Monte Carlo (MCMC) estimation is carried out\nthrough an efficient and scalable algorithm which can handle large models. The\nGP-VAR is illustrated by means of simulated data and in a forecasting exercise\nwith US data. Moreover, we use the GP-VAR to analyze the effects of\nmacroeconomic uncertainty, with a particular emphasis on time variation and\nasymmetries in the transmission mechanisms.",
        "authors": [
            "Niko Hauzenberger",
            "Florian Huber",
            "Massimiliano Marcellino",
            "Nico Petz"
        ],
        "categories": "econ.EM",
        "published": "2021-12-03T16:16:10Z",
        "updated": "2022-11-04T13:02:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.01772v1",
        "title": "Inference for ROC Curves Based on Estimated Predictive Indices",
        "abstract": "We provide a comprehensive theory of conducting in-sample statistical\ninference about receiver operating characteristic (ROC) curves that are based\non predicted values from a first stage model with estimated parameters (such as\na logit regression). The term \"in-sample\" refers to the practice of using the\nsame data for model estimation (training) and subsequent evaluation, i.e., the\nconstruction of the ROC curve. We show that in this case the first stage\nestimation error has a generally non-negligible impact on the asymptotic\ndistribution of the ROC curve and develop the appropriate pointwise and\nfunctional limit theory. We propose methods for simulating the distribution of\nthe limit process and show how to use the results in practice in comparing ROC\ncurves.",
        "authors": [
            "Yu-Chin Hsu",
            "Robert P. Lieli"
        ],
        "categories": "econ.EM",
        "published": "2021-12-03T08:10:19Z",
        "updated": "2021-12-03T08:10:19Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.01639v2",
        "title": "Patient-Centered Appraisal of Race-Free Clinical Risk Assessment",
        "abstract": "Until recently, there has been a consensus that clinicians should condition\npatient risk assessments on all observed patient covariates with predictive\npower. The broad idea is that knowing more about patients enables more accurate\npredictions of their health risks and, hence, better clinical decisions. This\nconsensus has recently unraveled with respect to a specific covariate, namely\nrace. There have been increasing calls for race-free risk assessment, arguing\nthat using race to predict patient outcomes contributes to racial disparities\nand inequities in health care. Writers calling for race-free risk assessment\nhave not studied how it would affect the quality of clinical decisions.\nConsidering the matter from the patient-centered perspective of medical\neconomics yields a disturbing conclusion: Race-free risk assessment would harm\npatients of all races.",
        "authors": [
            "Charles F. Manski"
        ],
        "categories": "econ.EM",
        "published": "2021-12-02T23:37:07Z",
        "updated": "2022-02-26T15:12:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.01486v1",
        "title": "Simple Alternatives to the Common Correlated Effects Model",
        "abstract": "We study estimation of factor models in a fixed-T panel data setting and\nsignificantly relax the common correlated effects (CCE) assumptions pioneered\nby Pesaran (2006) and used in dozens of papers since. In the simplest case, we\nmodel the unobserved factors as functions of the cross-sectional averages of\nthe explanatory variables and show that this is implied by Pesaran's\nassumptions when the number of factors does not exceed the number of\nexplanatory variables. Our approach allows discrete explanatory variables and\nflexible functional forms in the covariates. Plus, it extends to a framework\nthat easily incorporates general functions of cross-sectional moments, in\naddition to heterogeneous intercepts and time trends. Our proposed estimators\ninclude Pesaran's pooled correlated common effects (CCEP) estimator as a\nspecial case. We also show that in the presence of heterogeneous slopes our\nestimator is consistent under assumptions much weaker than those previously\nused. We derive the fixed-T asymptotic normality of a general estimator and\nshow how to adjust for estimation of the population moments in the factor\nloading equation.",
        "authors": [
            "Nicholas L. Brown",
            "Peter Schmidt",
            "Jeffrey M. Wooldridge"
        ],
        "categories": "econ.EM",
        "published": "2021-12-02T18:37:52Z",
        "updated": "2021-12-02T18:37:52Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.01435v1",
        "title": "RIF Regression via Sensitivity Curves",
        "abstract": "This paper proposes an empirical method to implement the recentered influence\nfunction (RIF) regression of Firpo, Fortin and Lemieux (2009), a relevant\nmethod to study the effect of covariates on many statistics beyond the mean. In\nempirically relevant situations where the influence function is not available\nor difficult to compute, we suggest to use the \\emph{sensitivity curve} (Tukey,\n1977) as a feasible alternative. This may be computationally cumbersome when\nthe sample size is large. The relevance of the proposed strategy derives from\nthe fact that, under general conditions, the sensitivity curve converges in\nprobability to the influence function. In order to save computational time we\npropose to use a cubic splines non-parametric method for a random subsample and\nthen to interpolate to the rest of the cases where it was not computed. Monte\nCarlo simulations show good finite sample properties. We illustrate the\nproposed estimator with an application to the polarization index of Duclos,\nEsteban and Ray (2004).",
        "authors": [
            "Javier Alejo",
            "Gabriel Montes-Rojas",
            "Walter Sosa-Escudero"
        ],
        "categories": "econ.EM",
        "published": "2021-12-02T17:24:43Z",
        "updated": "2021-12-02T17:24:43Z"
    },
    {
        "id": "http://arxiv.org/abs/2112.01377v2",
        "title": "Structural Sieves",
        "abstract": "This paper explores the use of deep neural networks for semiparametric\nestimation of economic models of maximizing behavior in production or discrete\nchoice. We argue that certain deep networks are particularly well suited as a\nnonparametric sieve to approximate regression functions that result from\nnonlinear latent variable models of continuous or discrete optimization.\nMulti-stage models of this type will typically generate rich interaction\neffects between regressors (\"inputs\") in the regression function so that there\nmay be no plausible separability restrictions on the \"reduced-form\" mapping\nform inputs to outputs to alleviate the curse of dimensionality. Rather,\neconomic shape, sparsity, or separability restrictions either at a global level\nor intermediate stages are usually stated in terms of the latent variable\nmodel. We show that restrictions of this kind are imposed in a more\nstraightforward manner if a sufficiently flexible version of the latent\nvariable model is in fact used to approximate the unknown regression function.",
        "authors": [
            "Konrad Menzel"
        ],
        "categories": "econ.EM",
        "published": "2021-12-01T16:37:02Z",
        "updated": "2022-04-05T13:30:48Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.15320v2",
        "title": "Modelling hetegeneous treatment effects by quantitle local polynomial decision tree and forest",
        "abstract": "To further develop the statistical inference problem for heterogeneous\ntreatment effects, this paper builds on Breiman's (2001) random forest tree\n(RFT)and Wager et al.'s (2018) causal tree to parameterize the nonparametric\nproblem using the excellent statistical properties of classical OLS and the\ndivision of local linear intervals based on covariate quantile points, while\npreserving the random forest trees with the advantages of constructible\nconfidence intervals and asymptotic normality properties [Athey and Imbens\n(2016),Efron (2014),Wager et al.(2014)\\citep{wager2014asymptotic}], we propose\na decision tree using quantile classification according to fixed rules combined\nwith polynomial estimation of local samples, which we call the quantile local\nlinear causal tree (QLPRT) and forest (QLPRF).",
        "authors": [
            "Lai Xinglin"
        ],
        "categories": "econ.EM",
        "published": "2021-11-30T12:02:16Z",
        "updated": "2022-03-13T15:33:12Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.14938v2",
        "title": "Distribution Shift in Airline Customer Behavior during COVID-19",
        "abstract": "Traditional AI approaches in customized (personalized) contextual pricing\napplications assume that the data distribution at the time of online pricing is\nsimilar to that observed during training. However, this assumption may be\nviolated in practice because of the dynamic nature of customer buying patterns,\nparticularly due to unanticipated system shocks such as COVID-19. We study the\nchanges in customer behavior for a major airline during the COVID-19 pandemic\nby framing it as a covariate shift and concept drift detection problem. We\nidentify which customers changed their travel and purchase behavior and the\nattributes affecting that change using (i) Fast Generalized Subset Scanning and\n(ii) Causal Forests. In our experiments with simulated and real-world data, we\npresent how these two techniques can be used through qualitative analysis.",
        "authors": [
            "Abhinav Garg",
            "Naman Shukla",
            "Lavanya Marla",
            "Sriram Somanchi"
        ],
        "categories": "cs.LG",
        "published": "2021-11-29T20:32:48Z",
        "updated": "2021-12-23T11:35:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.14590v2",
        "title": "The Fixed-b Limiting Distribution and the ERP of HAR Tests Under Nonstationarity",
        "abstract": "We show that the nonstandard limiting distribution of HAR test statistics\nunder fixed-b asymptotics is not pivotal (even after studentization) when the\ndata are nonstationarity. It takes the form of a complicated function of\nGaussian processes and depends on the integrated local long-run variance and on\non the second moments of the relevant series (e.g., of the regressors and\nerrors for the case of the linear regression model). Hence, existing fixed-b\ninference methods based on stationarity are not theoretically valid in general.\nThe nuisance parameters entering the fixed-b limiting distribution can be\nconsistently estimated under small-b asymptotics but only with nonparametric\nrate of convergence. Hence, We show that the error in rejection probability\n(ERP) is an order of magnitude larger than that under stationarity and is also\nlarger than that of HAR tests based on HAC estimators under conventional\nasymptotics. These theoretical results reconcile with recent finite-sample\nevidence in Casini (2021) and Casini, Deng and Perron (2021) who showing that\nfixed-b HAR tests can perform poorly when the data are nonstationary. They can\nbe conservative under the null hypothesis and have non-monotonic power under\nthe alternative hypothesis irrespective of how large the sample size is.",
        "authors": [
            "Alessandro Casini"
        ],
        "categories": "econ.EM",
        "published": "2021-11-29T15:23:59Z",
        "updated": "2024-08-07T10:59:16Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.14000v6",
        "title": "Factor-augmented tree ensembles",
        "abstract": "This manuscript proposes to extend the information set of time-series\nregression trees with latent stationary factors extracted via state-space\nmethods. In doing so, this approach generalises time-series regression trees on\ntwo dimensions. First, it allows to handle predictors that exhibit measurement\nerror, non-stationary trends, seasonality and/or irregularities such as missing\nobservations. Second, it gives a transparent way for using domain-specific\ntheory to inform time-series regression trees. Empirically, ensembles of these\nfactor-augmented trees provide a reliable approach for macro-finance problems.\nThis article highlights it focussing on the lead-lag effect between equity\nvolatility and the business cycle in the United States.",
        "authors": [
            "Filippo Pellegrino"
        ],
        "categories": "stat.ML",
        "published": "2021-11-27T22:44:54Z",
        "updated": "2023-06-12T19:37:37Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.13774v4",
        "title": "Robust Permutation Tests in Linear Instrumental Variables Regression",
        "abstract": "This paper develops permutation versions of identification-robust tests in\nlinear instrumental variables (IV) regression. Unlike the existing\nrandomization and rank-based tests in which independence between the\ninstruments and the error terms is assumed, the permutation Anderson- Rubin\n(AR), Lagrange Multiplier (LM) and Conditional Likelihood Ratio (CLR) tests are\nasymptotically similar and robust to conditional heteroskedasticity under\nstandard exclusion restriction i.e. the orthogonality between the instruments\nand the error terms. Moreover, when the instruments are independent of the\nstructural error term, the permutation AR tests are exact, hence robust to\nheavy tails. As such, these tests share the strengths of the rank-based tests\nand the wild bootstrap AR tests. Numerical illustrations corroborate the\ntheoretical results.",
        "authors": [
            "Purevdorj Tuvaandorj"
        ],
        "categories": "econ.EM",
        "published": "2021-11-26T23:46:22Z",
        "updated": "2024-07-23T01:00:15Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.13744v1",
        "title": "Yogurts Choose Consumers? Estimation of Random-Utility Models via Two-Sided Matching",
        "abstract": "The problem of demand inversion - a crucial step in the estimation of random\nutility discrete-choice models - is equivalent to the determination of stable\noutcomes in two-sided matching models. This equivalence applies to random\nutility models that are not necessarily additive, smooth, nor even invertible.\nBased on this equivalence, algorithms for the determination of stable matchings\nprovide effective computational methods for estimating these models. For\nnon-invertible models, the identified set of utility vectors is a lattice, and\nthe matching algorithms recover sharp upper and lower bounds on the utilities.\nOur matching approach facilitates estimation of models that were previously\ndifficult to estimate, such as the pure characteristics model. An empirical\napplication to voting data from the 1999 European Parliament elections\nillustrates the good performance of our matching-based demand inversion\nalgorithms in practice.",
        "authors": [
            "Odran Bonnet",
            "Alfred Galichon",
            "Yu-Wei Hsieh",
            "Keith O'Hara",
            "Matt Shum"
        ],
        "categories": "econ.EM",
        "published": "2021-11-26T20:44:55Z",
        "updated": "2021-11-26T20:44:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.15365v4",
        "title": "Expert Aggregation for Financial Forecasting",
        "abstract": "Machine learning algorithms dedicated to financial time series forecasting\nhave gained a lot of interest. But choosing between several algorithms can be\nchallenging, as their estimation accuracy may be unstable over time. Online\naggregation of experts combine the forecasts of a finite set of models in a\nsingle approach without making any assumption about the models. In this paper,\na Bernstein Online Aggregation (BOA) procedure is applied to the construction\nof long-short strategies built from individual stock return forecasts coming\nfrom different machine learning models. The online mixture of experts leads to\nattractive portfolio performances even in environments characterised by\nnon-stationarity. The aggregation outperforms individual algorithms, offering a\nhigher portfolio Sharpe Ratio, lower shortfall, with a similar turnover.\nExtensions to expert and aggregation specialisations are also proposed to\nimprove the overall mixture on a family of portfolio evaluation metrics.",
        "authors": [
            "Carl Remlinger",
            "Bri\u00e8re Marie",
            "Alasseur Cl\u00e9mence",
            "Joseph Mikael"
        ],
        "categories": "q-fin.ST",
        "published": "2021-11-25T10:43:58Z",
        "updated": "2023-07-06T09:38:40Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.12948v2",
        "title": "Difference in Differences and Ratio in Ratios for Limited Dependent Variables",
        "abstract": "Difference in differences (DD) is widely used to find policy/treatment\neffects with observational data, but applying DD to limited dependent variables\n(LDV's) Y has been problematic. This paper addresses how to apply DD and\nrelated approaches (such as \"ratio in ratios\" or \"ratio in odds ratios\") to\nbinary, count, fractional, multinomial or zero-censored Y under the unifying\nframework of `generalized linear models with link functions'. We evaluate DD\nand the related approaches with simulation and empirical studies, and recommend\n'Poisson Quasi-MLE' for non-negative (such as count or zero-censored) Y and\n(multinomial) logit MLE for binary, fractional or multinomial Y.",
        "authors": [
            "Myoung-jae Lee",
            "Sanghyeok Lee"
        ],
        "categories": "econ.EM",
        "published": "2021-11-25T07:12:22Z",
        "updated": "2023-08-14T11:22:35Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.12921v1",
        "title": "Network regression and supervised centrality estimation",
        "abstract": "The centrality in a network is a popular metric for agents' network positions\nand is often used in regression models to model the network effect on an\noutcome variable of interest. In empirical studies, researchers often adopt a\ntwo-stage procedure to first estimate the centrality and then infer the network\neffect using the estimated centrality. Despite its prevalent adoption, this\ntwo-stage procedure lacks theoretical backing and can fail in both estimation\nand inference. We, therefore, propose a unified framework, under which we prove\nthe shortcomings of the two-stage in centrality estimation and the undesirable\nconsequences in the regression. We then propose a novel supervised network\ncentrality estimation (SuperCENT) methodology that simultaneously yields\nsuperior estimations of the centrality and the network effect and provides\nvalid and narrower confidence intervals than those from the two-stage. We\nshowcase the superiority of SuperCENT in predicting the currency risk premium\nbased on the global trade network.",
        "authors": [
            "Junhui Cai",
            "Dan Yang",
            "Wu Zhu",
            "Haipeng Shen",
            "Linda Zhao"
        ],
        "categories": "econ.EM",
        "published": "2021-11-25T05:48:55Z",
        "updated": "2021-11-25T05:48:55Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.12397v1",
        "title": "Maximum Likelihood Estimation of Differentiated Products Demand Systems",
        "abstract": "We discuss estimation of the differentiated products demand system of Berry\net al (1995) (BLP) by maximum likelihood estimation (MLE). We derive the\nmaximum likelihood estimator in the case where prices are endogenously\ngenerated by firms that set prices in Bertrand-Nash equilibrium. In Monte Carlo\nsimulations the MLE estimator outperforms the best-practice GMM estimator on\nboth bias and mean squared error when the model is correctly specified. This\nremains true under some forms of misspecification. In our simulations, the\ncoverage of the ML estimator is close to its nominal level, whereas the GMM\nestimator tends to under-cover. We conclude the paper by estimating BLP on the\ncar data used in the original Berry et al (1995) paper, obtaining similar\nestimates with considerably tighter standard errors.",
        "authors": [
            "Greg Lewis",
            "Bora Ozaltun",
            "Georgios Zervas"
        ],
        "categories": "econ.EM",
        "published": "2021-11-24T10:31:09Z",
        "updated": "2021-11-24T10:31:09Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.12258v4",
        "title": "On Recoding Ordered Treatments as Binary Indicators",
        "abstract": "Researchers using instrumental variables to investigate ordered treatments\noften recode treatment into an indicator for any exposure. We investigate this\nestimand under the assumption that the instruments shift compliers from no\ntreatment to some but not from some treatment to more. We show that when there\nare extensive margin compliers only (EMCO) this estimand captures a weighted\naverage of treatment effects that can be partially unbundled into each complier\ngroup's potential outcome means. We also establish an equivalence between EMCO\nand a two-factor selection model and apply our results to study treatment\nheterogeneity in the Oregon Health Insurance Experiment.",
        "authors": [
            "Evan K. Rose",
            "Yotam Shem-Tov"
        ],
        "categories": "econ.EM",
        "published": "2021-11-24T04:19:27Z",
        "updated": "2024-03-01T18:50:30Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.11506v1",
        "title": "Interactive Effects Panel Data Models with General Factors and Regressors",
        "abstract": "This paper considers a model with general regressors and unobservable\nfactors. An estimator based on iterated principal components is proposed, which\nis shown to be not only asymptotically normal and oracle efficient, but under\ncertain conditions also free of the otherwise so common asymptotic incidental\nparameters bias. Interestingly, the conditions required to achieve unbiasedness\nbecome weaker the stronger the trends in the factors, and if the trending is\nstrong enough unbiasedness comes at no cost at all. In particular, the approach\ndoes not require any knowledge of how many factors there are, or whether they\nare deterministic or stochastic. The order of integration of the factors is\nalso treated as unknown, as is the order of integration of the regressors,\nwhich means that there is no need to pre-test for unit roots, or to decide on\nwhich deterministic terms to include in the model.",
        "authors": [
            "Bin Peng",
            "Liangjun Su",
            "Joakim Westerlund",
            "Yanrong Yang"
        ],
        "categories": "econ.EM",
        "published": "2021-11-22T20:06:26Z",
        "updated": "2021-11-22T20:06:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.10904v3",
        "title": "Orthogonal Policy Learning Under Ambiguity",
        "abstract": "This paper studies the problem of estimating individualized treatment rules\nwhen treatment effects are partially identified, as it is often the case with\nobservational data. By drawing connections between the treatment assignment\nproblem and classical decision theory, we characterize several notions of\noptimal treatment policies in the presence of partial identification. Our\nunified framework allows to incorporate user-defined constraints on the set of\nallowable policies, such as restrictions for transparency or interpretability,\nwhile also ensuring computational feasibility. We show how partial\nidentification leads to a new policy learning problem where the objective\nfunction is directionally -- but not fully -- differentiable with respect to\nthe nuisance first-stage. We then propose an estimation procedure that ensures\nNeyman-orthogonality with respect to the nuisance components and we provide\nstatistical guarantees that depend on the amount of concentration around the\npoints of non-differentiability in the data-generating-process. The proposed\nmethods are illustrated using data from the Job Partnership Training Act study.",
        "authors": [
            "Riccardo D'Adamo"
        ],
        "categories": "econ.EM",
        "published": "2021-11-21T21:50:42Z",
        "updated": "2022-12-30T15:46:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.10784v1",
        "title": "Why Synthetic Control estimators are biased and what to do about it: Introducing Relaxed and Penalized Synthetic Controls",
        "abstract": "This paper extends the literature on the theoretical properties of synthetic\ncontrols to the case of non-linear generative models, showing that the\nsynthetic control estimator is generally biased in such settings. I derive a\nlower bound for the bias, showing that the only component of it that is\naffected by the choice of synthetic control is the weighted sum of pairwise\ndifferences between the treated unit and the untreated units in the synthetic\ncontrol. To address this bias, I propose a novel synthetic control estimator\nthat allows for a constant difference of the synthetic control to the treated\nunit in the pre-treatment period, and that penalizes the pairwise\ndiscrepancies. Allowing for a constant offset makes the model more flexible,\nthus creating a larger set of potential synthetic controls, and the\npenalization term allows for the selection of the potential solution that will\nminimize bias. I study the properties of this estimator and propose a\ndata-driven process for parameterizing the penalization term.",
        "authors": [
            "Oscar Engelbrektson"
        ],
        "categories": "econ.EM",
        "published": "2021-11-21T10:27:29Z",
        "updated": "2021-11-21T10:27:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.10721v4",
        "title": "Identifying Dynamic Discrete Choice Models with Hyperbolic Discounting",
        "abstract": "We study identification of dynamic discrete choice models with hyperbolic\ndiscounting. We show that the standard discount factor, present bias factor,\nand instantaneous utility functions for the sophisticated agent are\npoint-identified from observed conditional choice probabilities and transition\nprobabilities in a finite horizon model. The main idea to achieve\nidentification is to exploit variation in the observed conditional choice\nprobabilities over time. We present the estimation method and demonstrate a\ngood performance of the estimator by simulation.",
        "authors": [
            "Taiga Tsubota"
        ],
        "categories": "econ.EM",
        "published": "2021-11-21T03:05:46Z",
        "updated": "2024-10-31T03:24:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.10713v1",
        "title": "Optimized Inference in Regression Kink Designs",
        "abstract": "We propose a method to remedy finite sample coverage problems and improve\nupon the efficiency of commonly employed procedures for the construction of\nnonparametric confidence intervals in regression kink designs. The proposed\ninterval is centered at the half-length optimal, numerically obtained linear\nminimax estimator over distributions with Lipschitz constrained conditional\nmean function. Its construction ensures excellent finite sample coverage and\nlength properties which are demonstrated in a simulation study and an empirical\nillustration. Given the Lipschitz constant that governs how much curvature one\nplausibly allows for, the procedure is fully data driven, computationally\ninexpensive, incorporates shape constraints and is valid irrespective of the\ndistribution of the assignment variable.",
        "authors": [
            "Majed Dodin"
        ],
        "categories": "econ.EM",
        "published": "2021-11-21T02:13:08Z",
        "updated": "2021-11-21T02:13:08Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.08664v2",
        "title": "An Empirical Evaluation of the Impact of New York's Bail Reform on Crime Using Synthetic Controls",
        "abstract": "We conduct an empirical evaluation of the impact of New York's bail reform on\ncrime. New York State's Bail Elimination Act went into effect on January 1,\n2020, eliminating money bail and pretrial detention for nearly all misdemeanor\nand nonviolent felony defendants. Our analysis of effects on aggregate crime\nrates after the reform informs the understanding of bail reform and general\ndeterrence. We conduct a synthetic control analysis for a comparative case\nstudy of impact of bail reform. We focus on synthetic control analysis of\npost-intervention changes in crime for assault, theft, burglary, robbery, and\ndrug crimes, constructing a dataset from publicly reported crime data of 27\nlarge municipalities. Our findings, including placebo checks and other\nrobustness checks, show that for assault, theft, and drug crimes, there is no\nsignificant impact of bail reform on crime; for burglary and robbery, we\nsimilarly have null findings but the synthetic control is also more variable so\nthese are deemed less conclusive.",
        "authors": [
            "Angela Zhou",
            "Andrew Koo",
            "Nathan Kallus",
            "Rene Ropac",
            "Richard Peterson",
            "Stephen Koppel",
            "Tiffany Bergin"
        ],
        "categories": "stat.AP",
        "published": "2021-11-16T17:59:02Z",
        "updated": "2023-06-25T13:57:31Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.08157v2",
        "title": "Optimal Stratification of Survey Experiments",
        "abstract": "This paper studies a two-stage model of experimentation, where the researcher\nfirst samples representative units from an eligible pool, then assigns each\nsampled unit to treatment or control. To implement balanced sampling and\nassignment, we introduce a new family of finely stratified designs that\ngeneralize matched pairs randomization to propensities p(x) not equal to 1/2.\nWe show that two-stage stratification nonparametrically dampens the variance of\ntreatment effect estimation. We formulate and solve the optimal stratification\nproblem with heterogeneous costs and fixed budget, providing simple heuristics\nfor the optimal design. In settings with pilot data, we show that implementing\na consistent estimate of this design is also efficient, minimizing asymptotic\nvariance subject to the budget constraint. We also provide new asymptotically\nexact inference methods, allowing experimenters to fully exploit the efficiency\ngains from both stratified sampling and assignment. An application to nine\npapers recently published in top economics journals demonstrates the value of\nour methods.",
        "authors": [
            "Max Cytrynbaum"
        ],
        "categories": "econ.EM",
        "published": "2021-11-16T00:42:28Z",
        "updated": "2023-08-20T03:57:24Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.08054v3",
        "title": "Abductive Inference and C. S. Peirce: 150 Years Later",
        "abstract": "This paper is about two things: (i) Charles Sanders Peirce (1837-1914) -- an\niconoclastic philosopher and polymath who is among the greatest of American\nminds. (ii) Abductive inference -- a term coined by C. S. Peirce, which he\ndefined as \"the process of forming explanatory hypotheses. It is the only\nlogical operation which introduces any new idea.\"\n  Abductive inference and quantitative economics: Abductive inference plays a\nfundamental role in empirical scientific research as a tool for discovery and\ndata analysis. Heckman and Singer (2017) strongly advocated \"Economists should\nabduct.\" Arnold Zellner (2007) stressed that \"much greater emphasis on\nreductive [abductive] inference in teaching econometrics, statistics, and\neconomics would be desirable.\" But currently, there are no established theory\nor practical tools that can allow an empirical analyst to abduct. This paper\nattempts to fill this gap by introducing new principles and concrete procedures\nto the Economics and Statistics community. I termed the proposed approach as\nAbductive Inference Machine (AIM).\n  The historical Peirce's experiment: In 1872, Peirce conducted a series of\nexperiments to determine the distribution of response times to an auditory\nstimulus, which is widely regarded as one of the most significant statistical\ninvestigations in the history of nineteenth-century American mathematical\nresearch (Stigler, 1978). On the 150th anniversary of this historical\nexperiment, we look back at the Peircean-style abductive inference through a\nmodern statistical lens. Using Peirce's data, it is shown how empirical\nanalysts can abduct in a systematic and automated manner using AIM.",
        "authors": [
            "Deep Mukhopadhyay"
        ],
        "categories": "econ.EM",
        "published": "2021-11-15T19:17:54Z",
        "updated": "2023-02-02T06:48:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.07889v1",
        "title": "An Outcome Test of Discrimination for Ranked Lists",
        "abstract": "This paper extends Becker (1957)'s outcome test of discrimination to settings\nwhere a (human or algorithmic) decision-maker produces a ranked list of\ncandidates. Ranked lists are particularly relevant in the context of online\nplatforms that produce search results or feeds, and also arise when human\ndecisionmakers express ordinal preferences over a list of candidates. We show\nthat non-discrimination implies a system of moment inequalities, which\nintuitively impose that one cannot permute the position of a lower-ranked\ncandidate from one group with a higher-ranked candidate from a second group and\nsystematically improve the objective. Moreover, we show that that these moment\ninequalities are the only testable implications of non-discrimination when the\nauditor observes only outcomes and group membership by rank. We show how to\nstatistically test the implied inequalities, and validate our approach in an\napplication using data from LinkedIn.",
        "authors": [
            "Jonathan Roth",
            "Guillaume Saint-Jacques",
            "YinYin Yu"
        ],
        "categories": "econ.EM",
        "published": "2021-11-15T16:42:57Z",
        "updated": "2021-11-15T16:42:57Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.07633v1",
        "title": "Dynamic Network Quantile Regression Model",
        "abstract": "We propose a dynamic network quantile regression model to investigate the\nquantile connectedness using a predetermined network information. We extend the\nexisting network quantile autoregression model of Zhu et al. (2019b) by\nexplicitly allowing the contemporaneous network effects and controlling for the\ncommon factors across quantiles. To cope with the endogeneity issue due to\nsimultaneous network spillovers, we adopt the instrumental variable quantile\nregression (IVQR) estimation and derive the consistency and asymptotic\nnormality of the IVQR estimator using the near epoch dependence property of the\nnetwork process. Via Monte Carlo simulations, we confirm the satisfactory\nperformance of the IVQR estimator across different quantiles under the\ndifferent network structures. Finally, we demonstrate the usefulness of our\nproposed approach with an application to the dataset on the stocks traded in\nNYSE and NASDAQ in 2016.",
        "authors": [
            "Xiu Xu",
            "Weining Wang",
            "Yongcheol Shin",
            "Chaowen Zheng"
        ],
        "categories": "econ.EM",
        "published": "2021-11-15T09:40:00Z",
        "updated": "2021-11-15T09:40:00Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.07465v2",
        "title": "Decoding Causality by Fictitious VAR Modeling",
        "abstract": "In modeling multivariate time series for either forecast or policy analysis,\nit would be beneficial to have figured out the cause-effect relations within\nthe data. Regression analysis, however, is generally for correlation relation,\nand very few researches have focused on variance analysis for causality\ndiscovery. We first set up an equilibrium for the cause-effect relations using\na fictitious vector autoregressive model. In the equilibrium, long-run\nrelations are identified from noise, and spurious ones are negligibly close to\nzero. The solution, called causality distribution, measures the relative\nstrength causing the movement of all series or specific affected ones. If a\ngroup of exogenous data affects the others but not vice versa, then, in theory,\nthe causality distribution for other variables is necessarily zero. The\nhypothesis test of zero causality is the rule to decide a variable is\nendogenous or not. Our new approach has high accuracy in identifying the true\ncause-effect relations among the data in the simulation studies. We also apply\nthe approach to estimating the causal factors' contribution to climate change.",
        "authors": [
            "Xingwei Hu"
        ],
        "categories": "stat.ML",
        "published": "2021-11-14T22:43:02Z",
        "updated": "2021-11-22T01:59:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.07388v4",
        "title": "When Can We Ignore Measurement Error in the Running Variable?",
        "abstract": "In many applications of regression discontinuity designs, the running\nvariable used by the administrator to assign treatment is only observed with\nerror. We show that, provided the observed running variable (i) correctly\nclassifies the treatment assignment, and (ii) affects the conditional means of\nthe potential outcomes smoothly, ignoring the measurement error nonetheless\nyields an estimate with a causal interpretation: the average treatment effect\nfor units whose observed running variable equals to the cutoff. We show that,\npossibly after doughnut trimming, these assumptions accommodate a variety of\nsettings where support of the measurement error is not too wide. We propose to\nconduct inference using bias-aware methods, which remain valid even when\ndiscreteness or irregular support in the observed running variable may lead to\npartial identification. We illustrate the results for both sharp and fuzzy\ndesigns in an empirical application.",
        "authors": [
            "Yingying Dong",
            "Michal Koles\u00e1r"
        ],
        "categories": "econ.EM",
        "published": "2021-11-14T16:58:17Z",
        "updated": "2023-02-02T02:40:45Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.07295v1",
        "title": "Rational AI: A comparison of human and AI responses to triggers of economic irrationality in poker",
        "abstract": "Humans exhibit irrational decision-making patterns in response to\nenvironmental triggers, such as experiencing an economic loss or gain. In this\npaper we investigate whether algorithms exhibit the same behavior by examining\nthe observed decisions and latent risk and rationality parameters estimated by\na random utility model with constant relative risk-aversion utility function.\nWe use a dataset consisting of 10,000 hands of poker played by Pluribus, the\nfirst algorithm in the world to beat professional human players and find (1)\nPluribus does shift its playing style in response to economic losses and gains,\nceteris paribus; (2) Pluribus becomes more risk-averse and rational following a\ntrigger but the humans become more risk-seeking and irrational; (3) the\ndifference in playing styles between Pluribus and the humans on the dimensions\nof risk-aversion and rationality are particularly differentiable when both have\nexperienced a trigger. This provides support that decision-making patterns\ncould be used as \"behavioral signatures\" to identify human versus algorithmic\ndecision-makers in unlabeled contexts.",
        "authors": [
            "C. Grace Haaf",
            "Devansh Singh",
            "Cinny Lin",
            "Scofield Zou"
        ],
        "categories": "econ.TH",
        "published": "2021-11-14T09:48:53Z",
        "updated": "2021-11-14T09:48:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.07225v1",
        "title": "Large Order-Invariant Bayesian VARs with Stochastic Volatility",
        "abstract": "Many popular specifications for Vector Autoregressions (VARs) with\nmultivariate stochastic volatility are not invariant to the way the variables\nare ordered due to the use of a Cholesky decomposition for the error covariance\nmatrix. We show that the order invariance problem in existing approaches is\nlikely to become more serious in large VARs. We propose the use of a\nspecification which avoids the use of this Cholesky decomposition. We show that\nthe presence of multivariate stochastic volatility allows for identification of\nthe proposed model and prove that it is invariant to ordering. We develop a\nMarkov Chain Monte Carlo algorithm which allows for Bayesian estimation and\nprediction. In exercises involving artificial and real macroeconomic data, we\ndemonstrate that the choice of variable ordering can have non-negligible\neffects on empirical results. In a macroeconomic forecasting exercise involving\nVARs with 20 variables we find that our order-invariant approach leads to the\nbest forecasts and that some choices of variable ordering can lead to poor\nforecasts using a conventional, non-order invariant, approach.",
        "authors": [
            "Joshua C. C. Chan",
            "Gary Koop",
            "Xuewen Yu"
        ],
        "categories": "econ.EM",
        "published": "2021-11-14T02:52:28Z",
        "updated": "2021-11-14T02:52:28Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.07170v1",
        "title": "Asymmetric Conjugate Priors for Large Bayesian VARs",
        "abstract": "Large Bayesian VARs are now widely used in empirical macroeconomics. One\npopular shrinkage prior in this setting is the natural conjugate prior as it\nfacilitates posterior simulation and leads to a range of useful analytical\nresults. This is, however, at the expense of modeling flexibility, as it rules\nout cross-variable shrinkage -- i.e., shrinking coefficients on lags of other\nvariables more aggressively than those on own lags. We develop a prior that has\nthe best of both worlds: it can accommodate cross-variable shrinkage, while\nmaintaining many useful analytical results, such as a closed-form expression of\nthe marginal likelihood. This new prior also leads to fast posterior simulation\n-- for a BVAR with 100 variables and 4 lags, obtaining 10,000 posterior draws\ntakes less than half a minute on a standard desktop. We demonstrate the\nusefulness of the new prior via a structural analysis using a 15-variable VAR\nwith sign restrictions to identify 5 structural shocks.",
        "authors": [
            "Joshua C. C. Chan"
        ],
        "categories": "econ.EM",
        "published": "2021-11-13T18:48:11Z",
        "updated": "2021-11-13T18:48:11Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.06941v2",
        "title": "Absolute and Relative Bias in Eight Common Observational Study Designs: Evidence from a Meta-analysis",
        "abstract": "Observational studies are needed when experiments are not possible. Within\nstudy comparisons (WSC) compare observational and experimental estimates that\ntest the same hypothesis using the same treatment group, outcome, and estimand.\nMeta-analyzing 39 of them, we compare mean bias and its variance for the eight\nobservational designs that result from combining whether there is a pretest\nmeasure of the outcome or not, whether the comparison group is local to the\ntreatment group or not, and whether there is a relatively rich set of other\ncovariates or not. Of these eight designs, one combines all three design\nelements, another has none, and the remainder include any one or two. We found\nthat both the mean and variance of bias decline as design elements are added,\nwith the lowest mean and smallest variance in a design with all three elements.\nThe probability of bias falling within 0.10 standard deviations of the\nexperimental estimate varied from 59 to 83 percent in Bayesian analyses and\nfrom 86 to 100 percent in non-Bayesian ones -- the ranges depending on the\nlevel of data aggregation. But confounding remains possible due to each of the\neight observational study design cells including a different set of WSC\nstudies.",
        "authors": [
            "Jelena Zurovac",
            "Thomas D. Cook",
            "John Deke",
            "Mariel M. Finucane",
            "Duncan Chaplin",
            "Jared S. Coopersmith",
            "Michael Barna",
            "Lauren Vollmer Forrow"
        ],
        "categories": "stat.ME",
        "published": "2021-11-12T21:00:40Z",
        "updated": "2021-11-16T01:54:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.06818v2",
        "title": "Dynamic treatment effects: high-dimensional inference under model misspecification",
        "abstract": "Estimating dynamic treatment effects is essential across various disciplines,\noffering nuanced insights into the time-dependent causal impact of\ninterventions. However, this estimation presents challenges due to the \"curse\nof dimensionality\" and time-varying confounding, which can lead to biased\nestimates. Additionally, correctly specifying the growing number of treatment\nassignments and outcome models with multiple exposures seems overly complex.\nGiven these challenges, the concept of double robustness, where model\nmisspecification is permitted, is extremely valuable, yet unachieved in\npractical applications. This paper introduces a new approach by proposing\nnovel, robust estimators for both treatment assignments and outcome models. We\npresent a \"sequential model double robust\" solution, demonstrating that double\nrobustness over multiple time points can be achieved when each time exposure is\ndoubly robust. This approach improves the robustness and reliability of dynamic\ntreatment effects estimation, addressing a significant gap in this field.",
        "authors": [
            "Yuqian Zhang",
            "Weijie Ji",
            "Jelena Bradic"
        ],
        "categories": "stat.ME",
        "published": "2021-11-12T17:05:47Z",
        "updated": "2023-06-16T01:13:51Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.06573v2",
        "title": "Bounds for Treatment Effects in the Presence of Anticipatory Behavior",
        "abstract": "In program evaluations, units can often anticipate the implementation of a\nnew policy before it occurs. Such anticipatory behavior can lead to units'\noutcomes becoming dependent on their future treatment assignments. In this\npaper, I employ a potential-outcomes framework to analyze the treatment effect\nwith anticipation. I start with a classical difference-in-differences model\nwith two time periods and provide identified sets with easy-to-implement\nestimation and inference strategies for causal parameters. Empirical\napplications and generalizations are provided. I illustrate my results by\nanalyzing the effect of an early retirement incentive program for teachers,\nwhich the target units were likely to anticipate, on student achievement. The\nempirical results show the result can be overestimated by up to 30\\% in the\nworst case and demonstrate the potential pitfalls of failing to consider\nanticipation in policy evaluation.",
        "authors": [
            "Aibo Gong"
        ],
        "categories": "econ.EM",
        "published": "2021-11-12T06:09:59Z",
        "updated": "2022-12-01T12:44:53Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.05277v1",
        "title": "Generalized Kernel Ridge Regression for Causal Inference with Missing-at-Random Sample Selection",
        "abstract": "I propose kernel ridge regression estimators for nonparametric dose response\ncurves and semiparametric treatment effects in the setting where an analyst has\naccess to a selected sample rather than a random sample; only for select\nobservations, the outcome is observed. I assume selection is as good as random\nconditional on treatment and a sufficiently rich set of observed covariates,\nwhere the covariates are allowed to cause treatment or be caused by treatment\n-- an extension of missingness-at-random (MAR). I propose estimators of means,\nincrements, and distributions of counterfactual outcomes with closed form\nsolutions in terms of kernel matrix operations, allowing treatment and\ncovariates to be discrete or continuous, and low, high, or infinite\ndimensional. For the continuous treatment case, I prove uniform consistency\nwith finite sample rates. For the discrete treatment case, I prove root-n\nconsistency, Gaussian approximation, and semiparametric efficiency.",
        "authors": [
            "Rahul Singh"
        ],
        "categories": "econ.EM",
        "published": "2021-11-09T17:10:49Z",
        "updated": "2021-11-09T17:10:49Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.05243v4",
        "title": "Bounding Treatment Effects by Pooling Limited Information across Observations",
        "abstract": "We provide novel bounds on average treatment effects (on the treated) that\nare valid under an unconfoundedness assumption. Our bounds are designed to be\nrobust in challenging situations, for example, when the conditioning variables\ntake on a large number of different values in the observed sample, or when the\noverlap condition is violated. This robustness is achieved by only using\nlimited \"pooling\" of information across observations. Namely, the bounds are\nconstructed as sample averages over functions of the observed outcomes such\nthat the contribution of each outcome only depends on the treatment status of a\nlimited number of observations. No information pooling across observations\nleads to so-called \"Manski bounds\", while unlimited information pooling leads\nto standard inverse propensity score weighting. We explore the intermediate\nrange between these two extremes and provide corresponding inference methods.\nWe show in Monte Carlo experiments and through an empirical application that\nour bounds are indeed robust and informative in practice.",
        "authors": [
            "Sokbae Lee",
            "Martin Weidner"
        ],
        "categories": "econ.EM",
        "published": "2021-11-09T16:27:25Z",
        "updated": "2023-12-12T11:18:32Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.04926v3",
        "title": "Optimal Decision Rules Under Partial Identification",
        "abstract": "I consider a class of statistical decision problems in which the policy maker\nmust decide between two alternative policies to maximize social welfare based\non a finite sample. The central assumption is that the underlying, possibly\ninfinite-dimensional parameter, lies in a known convex set, potentially leading\nto partial identification of the welfare effect. An example of such\nrestrictions is the smoothness of counterfactual outcome functions. As the main\ntheoretical result, I derive a finite-sample, exact minimax regret decision\nrule within the class of all decision rules under normal errors with known\nvariance. When the error distribution is unknown, I obtain a feasible decision\nrule that is asymptotically minimax regret. I apply my results to the problem\nof whether to change a policy eligibility cutoff in a regression discontinuity\nsetup, and illustrate them in an empirical application to a school construction\nprogram in Burkina Faso.",
        "authors": [
            "Kohei Yata"
        ],
        "categories": "econ.EM",
        "published": "2021-11-09T03:20:59Z",
        "updated": "2023-08-30T16:37:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.04919v1",
        "title": "Pair copula constructions of point-optimal sign-based tests for predictive linear and nonlinear regressions",
        "abstract": "We propose pair copula constructed point-optimal sign tests in the context of\nlinear and nonlinear predictive regressions with endogenous, persistent\nregressors, and disturbances exhibiting serial (nonlinear) dependence. The\nproposed approach entails considering the entire dependence structure of the\nsigns to capture the serial dependence, and building feasible test statistics\nbased on pair copula constructions of the sign process. The tests are exact and\nvalid in the presence of heavy tailed and nonstandard errors, as well as\nheterogeneous and persistent volatility. Furthermore, they may be inverted to\nbuild confidence regions for the parameters of the regression function.\nFinally, we adopt an adaptive approach based on the split-sample technique to\nmaximize the power of the test by finding an appropriate alternative\nhypothesis. In a Monte Carlo study, we compare the performance of the proposed\n\"quasi\"-point-optimal sign tests based on pair copula constructions by\ncomparing its size and power to those of certain existing tests that are\nintended to be robust against heteroskedasticity. The simulation results\nmaintain the superiority of our procedures to existing popular tests.",
        "authors": [
            "Kaveh Salehzadeh Nobari"
        ],
        "categories": "econ.EM",
        "published": "2021-11-09T02:54:44Z",
        "updated": "2021-11-09T02:54:44Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.04267v1",
        "title": "Exponential GARCH-Ito Volatility Models",
        "abstract": "This paper introduces a novel Ito diffusion process to model high-frequency\nfinancial data, which can accommodate low-frequency volatility dynamics by\nembedding the discrete-time non-linear exponential GARCH structure with\nlog-integrated volatility in a continuous instantaneous volatility process. The\nkey feature of the proposed model is that, unlike existing GARCH-Ito models,\nthe instantaneous volatility process has a non-linear structure, which ensures\nthat the log-integrated volatilities have the realized GARCH structure. We call\nthis the exponential realized GARCH-Ito (ERGI) model. Given the auto-regressive\nstructure of the log-integrated volatility, we propose a quasi-likelihood\nestimation procedure for parameter estimation and establish its asymptotic\nproperties. We conduct a simulation study to check the finite sample\nperformance of the proposed model and an empirical study with 50 assets among\nthe S\\&P 500 compositions. The numerical studies show the advantages of the new\nproposed model.",
        "authors": [
            "Donggyu Kim"
        ],
        "categories": "econ.EM",
        "published": "2021-11-08T04:20:26Z",
        "updated": "2021-11-08T04:20:26Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.04219v4",
        "title": "Rate-Optimal Cluster-Randomized Designs for Spatial Interference",
        "abstract": "We consider a potential outcomes model in which interference may be present\nbetween any two units but the extent of interference diminishes with spatial\ndistance. The causal estimand is the global average treatment effect, which\ncompares outcomes under the counterfactuals that all or no units are treated.\nWe study a class of designs in which space is partitioned into clusters that\nare randomized into treatment and control. For each design, we estimate the\ntreatment effect using a Horvitz-Thompson estimator that compares the average\noutcomes of units with all or no neighbors treated, where the neighborhood\nradius is of the same order as the cluster size dictated by the design. We\nderive the estimator's rate of convergence as a function of the design and\ndegree of interference and use this to obtain estimator-design pairs that\nachieve near-optimal rates of convergence under relatively minimal assumptions\non interference. We prove that the estimators are asymptotically normal and\nprovide a variance estimator. For practical implementation of the designs, we\nsuggest partitioning space using clustering algorithms.",
        "authors": [
            "Michael P. Leung"
        ],
        "categories": "stat.ME",
        "published": "2021-11-08T01:11:19Z",
        "updated": "2022-09-14T18:01:13Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.03950v4",
        "title": "Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves",
        "abstract": "We propose simple nonparametric estimators for mediated and time-varying dose\nresponse curves based on kernel ridge regression. By embedding Pearl's\nmediation formula and Robins' g-formula with kernels, we allow treatments,\nmediators, and covariates to be continuous in general spaces, and also allow\nfor nonlinear treatment-confounder feedback. Our key innovation is a\nreproducing kernel Hilbert space technique called sequential kernel embedding,\nwhich we use to construct simple estimators for complex causal estimands. Our\nestimators preserve the generality of classic identification while also\nachieving nonasymptotic uniform rates. In nonlinear simulations with many\ncovariates, we demonstrate strong performance. We estimate mediated and\ntime-varying dose response curves of the US Job Corps, and clean data that may\nserve as a benchmark in future work. We extend our results to mediated and\ntime-varying treatment effects and counterfactual distributions, verifying\nsemiparametric efficiency and weak convergence.",
        "authors": [
            "Rahul Singh",
            "Liyuan Xu",
            "Arthur Gretton"
        ],
        "categories": "stat.ME",
        "published": "2021-11-06T19:51:39Z",
        "updated": "2023-07-19T20:46:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.03626v1",
        "title": "Bootstrap inference for panel data quantile regression",
        "abstract": "This paper develops bootstrap methods for practical statistical inference in\npanel data quantile regression models with fixed effects. We consider\nrandom-weighted bootstrap resampling and formally establish its validity for\nasymptotic inference. The bootstrap algorithm is simple to implement in\npractice by using a weighted quantile regression estimation for fixed effects\npanel data. We provide results under conditions that allow for temporal\ndependence of observations within individuals, thus encompassing a large class\nof possible empirical applications. Monte Carlo simulations provide numerical\nevidence the proposed bootstrap methods have correct finite sample properties.\nFinally, we provide an empirical illustration using the environmental Kuznets\ncurve.",
        "authors": [
            "Antonio F. Galvao",
            "Thomas Parker",
            "Zhijie Xiao"
        ],
        "categories": "econ.EM",
        "published": "2021-11-05T17:23:10Z",
        "updated": "2021-11-05T17:23:10Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.03035v1",
        "title": "Structural Breaks in Interactive Effects Panels and the Stock Market Reaction to COVID-19",
        "abstract": "Dealing with structural breaks is an important step in most, if not all,\nempirical economic research. This is particularly true in panel data comprised\nof many cross-sectional units, such as individuals, firms or countries, which\nare all affected by major events. The COVID-19 pandemic has affected most\nsectors of the global economy, and there is by now plenty of evidence to\nsupport this. The impact on stock markets is, however, still unclear. The fact\nthat most markets seem to have partly recovered while the pandemic is still\nongoing suggests that the relationship between stock returns and COVID-19 has\nbeen subject to structural change. It is therefore important to know if a\nstructural break has occurred and, if it has, to infer the date of the break.\nIn the present paper we take this last observation as a source of motivation to\ndevelop a new break detection toolbox that is applicable to different sized\npanels, easy to implement and robust to general forms of unobserved\nheterogeneity. The toolbox, which is the first of its kind, includes a test for\nstructural change, a break date estimator, and a break date confidence\ninterval. Application to a panel covering 61 countries from January 3 to\nSeptember 25, 2020, leads to the detection of a structural break that is dated\nto the first week of April. The effect of COVID-19 is negative before the break\nand zero thereafter, implying that while markets did react, the reaction was\nshort-lived. A possible explanation for this is the quantitative easing\nprograms announced by central banks all over the world in the second half of\nMarch.",
        "authors": [
            "Yiannis Karavias",
            "Paresh Narayan",
            "Joakim Westerlund"
        ],
        "categories": "econ.EM",
        "published": "2021-11-04T17:37:29Z",
        "updated": "2021-11-04T17:37:29Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.09442v2",
        "title": "Monitoring COVID-19-induced gender differences in teleworking rates using Mobile Network Data",
        "abstract": "The COVID-19 pandemic has created a sudden need for a wider uptake of\nhome-based telework as means of sustaining the production. Generally,\nteleworking arrangements impacts directly worker's efficiency and motivation.\nThe direction of this impact, however, depends on the balance between positive\neffects of teleworking (e.g. increased flexibility and autonomy) and its\ndownsides (e.g. blurring boundaries between private and work life). Moreover,\nthese effects of teleworking can be amplified in case of vulnerable groups of\nworkers, such as women. The first step in understanding the implications of\nteleworking on women is to have timely information on the extent of teleworking\nby age and gender. In the absence of timely official statistics, in this paper\nwe propose a method for nowcasting the teleworking trends by age and gender for\n20 Italian regions using mobile network operators (MNO) data. The method is\ndeveloped and validated using MNO data together with the Italian quarterly\nLabour Force Survey. Our results confirm that the MNO data have the potential\nto be used as a tool for monitoring gender and age differences in teleworking\npatterns. This tool becomes even more important today as it could support the\nadequate gender mainstreaming in the ``Next Generation EU'' recovery plan and\nhelp to manage related social impacts of COVID-19 through policymaking.",
        "authors": [
            "Sara Grubanov-Boskovic",
            "Spyridon Spyratos",
            "Stefano Maria Iacus",
            "Umberto Minora",
            "Francesco Sermi"
        ],
        "categories": "cs.SI",
        "published": "2021-11-04T15:11:03Z",
        "updated": "2021-11-19T08:12:59Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.02528v2",
        "title": "occ2vec: A principal approach to representing occupations using natural language processing",
        "abstract": "We propose \\textbf{occ2vec}, a principal approach to representing\noccupations, which can be used in matching, predictive and causal modeling, and\nother economic areas. In particular, we use it to score occupations on any\ndefinable characteristic of interest, say the degree of \\textquote{greenness}.\nUsing more than 17,000 occupation-specific text descriptors, we transform each\noccupation into a high-dimensional vector using natural language processing.\nSimilar, we assign a vector to the target characteristic and estimate the\noccupational degree of this characteristic as the cosine similarity between the\nvectors. The main advantages of this approach are its universal applicability\nand verifiability contrary to existing ad-hoc approaches. We extensively\nvalidate our approach on several exercises and then use it to estimate the\noccupational degree of charisma and emotional intelligence (EQ). We find that\noccupations that score high on these tend to have higher educational\nrequirements. Turning to wages, highly charismatic occupations are either found\nin the lower or upper tail in the wage distribution. This is not found for EQ,\nwhere higher levels of EQ are generally correlated with higher wages.",
        "authors": [
            "Nicolaj S\u00f8ndergaard M\u00fchlbach"
        ],
        "categories": "econ.EM",
        "published": "2021-11-03T21:27:23Z",
        "updated": "2022-07-13T20:54:17Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.02376v1",
        "title": "Multiplicative Component GARCH Model of Intraday Volatility",
        "abstract": "This paper proposes a multiplicative component intraday volatility model. The\nintraday conditional volatility is expressed as the product of intraday\nperiodic component, intraday stochastic volatility component and daily\nconditional volatility component. I extend the multiplicative component\nintraday volatility model of Engle (2012) and Andersen and Bollerslev (1998) by\nincorporating the durations between consecutive transactions. The model can be\napplied to both regularly and irregularly spaced returns. I also provide a\nnonparametric estimation technique of the intraday volatility periodicity. The\nempirical results suggest the model can successfully capture the\ninterdependency of intraday returns.",
        "authors": [
            "Xiufeng Yan"
        ],
        "categories": "econ.EM",
        "published": "2021-11-03T17:44:23Z",
        "updated": "2021-11-03T17:44:23Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.02306v2",
        "title": "Leveraging Causal Graphs for Blocking in Randomized Experiments",
        "abstract": "Randomized experiments are often performed to study the causal effects of\ninterest. Blocking is a technique to precisely estimate the causal effects when\nthe experimental material is not homogeneous. It involves stratifying the\navailable experimental material based on the covariates causing non-homogeneity\nand then randomizing the treatment within those strata (known as blocks). This\neliminates the unwanted effect of the covariates on the causal effects of\ninterest. We investigate the problem of finding a stable set of covariates to\nbe used to form blocks, that minimizes the variance of the causal effect\nestimates. Using the underlying causal graph, we provide an efficient algorithm\nto obtain such a set for a general semi-Markovian causal model.",
        "authors": [
            "Abhishek Kumar Umrawal"
        ],
        "categories": "stat.ME",
        "published": "2021-11-03T15:46:25Z",
        "updated": "2023-02-19T01:44:58Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.02300v1",
        "title": "Autoregressive conditional duration modelling of high frequency data",
        "abstract": "This paper explores the duration dynamics modelling under the Autoregressive\nConditional Durations (ACD) framework (Engle and Russell 1998). I test\ndifferent distributions assumptions for the durations. The empirical results\nsuggest unconditional durations approach the Gamma distributions. Moreover,\ncompared with exponential distributions and Weibull distributions, the ACD\nmodel with Gamma distributed innovations provide the best fit of SPY durations.",
        "authors": [
            "Xiufeng Yan"
        ],
        "categories": "econ.EM",
        "published": "2021-11-03T15:32:38Z",
        "updated": "2021-11-03T15:32:38Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.02092v1",
        "title": "What drives the accuracy of PV output forecasts?",
        "abstract": "Due to the stochastic nature of photovoltaic (PV) power generation, there is\nhigh demand for forecasting PV output to better integrate PV generation into\npower grids. Systematic knowledge regarding the factors influencing forecast\naccuracy is crucially important, but still mostly unknown. In this paper, we\nreview 180 papers on PV forecasts and extract a database of forecast errors for\nstatistical analysis. We show that among the forecast models, hybrid models\nconsistently outperform the others and will most likely be the future of PV\noutput forecasting. The use of data processing techniques is positively\ncorrelated with the forecast quality, while the lengths of the forecast horizon\nand out-of-sample test set have negative effects on the forecast accuracy. We\nalso found that the inclusion of numerical weather prediction variables, data\nnormalization, and data resampling are the most effective data processing\ntechniques. Furthermore, we found some evidence for cherry picking in reporting\nerrors and recommend that the test sets be at least one year to better assess\nmodel performance. The paper also takes the first step towards establishing a\nbenchmark for assessing PV output forecasts.",
        "authors": [
            "Thi Ngoc Nguyen",
            "Felix M\u00fcsgens"
        ],
        "categories": "stat.AP",
        "published": "2021-11-03T09:21:07Z",
        "updated": "2021-11-03T09:21:07Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.02023v1",
        "title": "Multiple-index Nonstationary Time Series Models: Robust Estimation Theory and Practice",
        "abstract": "This paper proposes a class of parametric multiple-index time series models\nthat involve linear combinations of time trends, stationary variables and unit\nroot processes as regressors. The inclusion of the three different types of\ntime series, along with the use of a multiple-index structure for these\nvariables to circumvent the curse of dimensionality, is due to both theoretical\nand practical considerations. The M-type estimators (including OLS, LAD,\nHuber's estimator, quantile and expectile estimators, etc.) for the index\nvectors are proposed, and their asymptotic properties are established, with the\naid of the generalized function approach to accommodate a wide class of loss\nfunctions that may not be necessarily differentiable at every point. The\nproposed multiple-index model is then applied to study the stock return\npredictability, which reveals strong nonlinear predictability under various\nloss measures. Monte Carlo simulations are also included to evaluate the\nfinite-sample performance of the proposed estimators.",
        "authors": [
            "Chaohua Dong",
            "Jiti Gao",
            "Bin Peng",
            "Yundong Tu"
        ],
        "categories": "econ.EM",
        "published": "2021-11-03T05:12:54Z",
        "updated": "2021-11-03T05:12:54Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.01301v4",
        "title": "Asymptotic in a class of network models with an increasing sub-Gamma degree sequence",
        "abstract": "For the differential privacy under the sub-Gamma noise, we derive the\nasymptotic properties of a class of network models with binary values with a\ngeneral link function. In this paper, we release the degree sequences of the\nbinary networks under a general noisy mechanism with the discrete Laplace\nmechanism as a special case. We establish the asymptotic result including both\nconsistency and asymptotically normality of the parameter estimator when the\nnumber of parameters goes to infinity in a class of network models. Simulations\nand a real data example are provided to illustrate asymptotic results.",
        "authors": [
            "Jing Luo",
            "Haoyu Wei",
            "Xiaoyu Lei",
            "Jiaxin Guo"
        ],
        "categories": "math.ST",
        "published": "2021-11-02T00:04:41Z",
        "updated": "2023-11-11T00:56:03Z"
    },
    {
        "id": "http://arxiv.org/abs/2111.01137v1",
        "title": "Stock Price Prediction Using Time Series, Econometric, Machine Learning, and Deep Learning Models",
        "abstract": "For a long-time, researchers have been developing a reliable and accurate\npredictive model for stock price prediction. According to the literature, if\npredictive models are correctly designed and refined, they can painstakingly\nand faithfully estimate future stock values. This paper demonstrates a set of\ntime series, econometric, and various learning-based models for stock price\nprediction. The data of Infosys, ICICI, and SUN PHARMA from the period of\nJanuary 2004 to December 2019 was used here for training and testing the models\nto know which model performs best in which sector. One time series model\n(Holt-Winters Exponential Smoothing), one econometric model (ARIMA), two\nmachine Learning models (Random Forest and MARS), and two deep learning-based\nmodels (simple RNN and LSTM) have been included in this paper. MARS has been\nproved to be the best performing machine learning model, while LSTM has proved\nto be the best performing deep learning model. But overall, for all three\nsectors - IT (on Infosys data), Banking (on ICICI data), and Health (on SUN\nPHARMA data), MARS has proved to be the best performing model in sales\nforecasting.",
        "authors": [
            "Ananda Chatterjee",
            "Hrisav Bhowmick",
            "Jaydip Sen"
        ],
        "categories": "q-fin.ST",
        "published": "2021-11-01T17:17:52Z",
        "updated": "2021-11-01T17:17:52Z"
    }
]